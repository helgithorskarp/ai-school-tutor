{"0": {"text": " Algorithms\nFOURTH EDITION\n This page intentionally left blank \n Algorithms\nRobert Sedgewick\nand\nKevin Wayne\nPrinceton University\nFOURTH EDITION\nUpper Saddle River, NJ \u0081 Boston \u0081 Indianapolis \u0081 San Francisco\nNew York \u0081 Toronto \u0081 Montreal \u0081 London \u0081 Munich \u0081 Paris \u0081 Madrid\nCapetown \u0081 Sydney \u0081 T okyo \u0081 Singapore \u0081 Mexico City\n Many of the designations used by manufacturers and sellers to distinguish their products are claimed as \ntrademarks. Where those designations appear in this book, and the publisher was aware of a trademark \nclaim, the designations have been printed with initial capital letters or in all capitals. \nThe authors and publisher have taken care in the preparation of this book, but make no expressed or im-\nplied warranty of any kind and assume no responsibility for errors or omissions. No liability is assumed \nfor incidental or consequential damages in connection with or arising out of the use of the information or \nprograms contained herein. \nThe publisher offers excellent discounts on this book when ordered in quantity for bulk purchases or \nspecial sales, which may include electronic versions and/or custom covers and content particular to your \nbusiness, training goals, marketing focus, and branding interests. For more information, please contact:\nU.S. Corporate and Government Sales\n(800) 382-3419\ncorpsales@pearsontechgroup.com\nFor sales outside the United States, please contact:\nInternational Sales\ninternational@pearson.com\nVisit us on the Web: informit.com/aw \nCataloging-in-Publication Data is on \ufb01le with the Library of Congress.\nCopyright \u00a9 2011 Pearson Education, Inc. \nAll rights reserved. Printed in the United States of America. This publication is protected by copyright, \nand permission must be obtained from the publisher prior to any prohibited reproduction, storage in \na retrieval system, or ", "start": 0, "end": 4}, "1": {"text": "Education, Inc. \nAll rights reserved. Printed in the United States of America. This publication is protected by copyright, \nand permission must be obtained from the publisher prior to any prohibited reproduction, storage in \na retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, \nrecording, or likewise. For information regarding permissions, write to: \nPearson Education, Inc.\nRights and Contracts Department\n501 Boylston Street, Suite 900\nBoston, MA 02116\nFax: (617) 671-3447 \nISBN-13: 978-0-321-57351-3\nISBN-10:        0-321-57351-X\nTe x t  p r i n te d  i n  t h e  Un i te d  S t a te s  o n  re c yc l e d  p a p e r  a t  C o u r i e r  i n  We s t f o rd , Ma s s a c h u s e t t s .\nFirst printing, March 2011\n ______________________________\nTo Adam, Andrew, Brett, Robbie\nand especially Linda\n______________________________\n___________________\nTo Jackie and Alex\n___________________\n vi\nPreface   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .viii\n1 Fundamentals .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 3\n1.1 Basic Programming Model 8\n1.2 Data Abstraction 64\n1.3 Bags, Queues, and ", "start": 4, "end": 6}, "2": {"text": ".   .   .   .   .   .   .   . 3\n1.1 Basic Programming Model 8\n1.2 Data Abstraction 64\n1.3 Bags, Queues, and Stacks 120\n1.4 Analysis of Algorithms  172\n1.5 Case Study: Union-Find 216\n2 Sorting  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   243\n2.1 Elementary Sorts 244\n2.2 Mergesort 270\n2.3 Quicksort 288\n2.4 Priority Queues  308\n2.5 Applications 336\n3 Searching  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   361\n3.1 Symbol Tables 362\n3.2 Binary Search Trees 396\n3.3 Balanced Search Trees 424\n3.4 Hash Tables 458\n3.5 Applications 486\nCONTENTS\n vii\n4 Graphs  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   515\n4.1 Undirected Graphs 518\n4.2 Directed Graphs 566\n4.3 Minimum Spanning Trees 604\n4.4 Shortest Paths 638\n5 Strings   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . ", "start": 6, "end": 7}, "3": {"text": "Paths 638\n5 Strings   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   695\n5.1 String Sorts 702\n5.2 Tries 730\n5.3 Substring Search 758\n5.4 Regular Expressions 788\n5.5 Data Compression 810\n6 Context .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   853\nIndex  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   933\nAlgorithms    .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   954\nClients   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   955\n viii\nT\nhis book is intended to survey the most important computer algorithms in use today, \nand to teach fundamental techniques to the growing number of people in need of \nknowing them.  It is intended for use as a textbook for a second course in computer \nscience, after students have acquired basic programming skills and familiarity with computer \nsystems. The book also may be useful for self-study or as a reference for people engaged in \nthe ", "start": 7, "end": 8}, "4": {"text": "use as a textbook for a second course in computer \nscience, after students have acquired basic programming skills and familiarity with computer \nsystems. The book also may be useful for self-study or as a reference for people engaged in \nthe development of computer systems or applications programs, since it contains implemen-\ntations of useful algorithms and detailed information on performance characteristics and \nclients. The broad perspective taken makes the book an appropriate introduction to the \ufb01eld.\nthe study of algorithms and data structures  is fundamental to any computer-\nscience curriculum, but it is not just for programmers and computer-science students. Every-\none who uses a computer wants it to run faster or to solve larger problems. The algorithms \nin this book represent a body of knowledge developed over the last 50 years that has become \nindispensable.  From N-body simulation problems in physics to genetic-sequencing problems \nin molecular biology, the basic methods described here have become essential in scienti\ufb01c \nresearch; from architectural modeling systems to aircraft simulation, they have become es -\nsential tools in engineering; and from database systems to internet search engines, they have \nbecome essential parts of modern software systems. And these are but a few examples\u2014as the \nscope of computer applications continues to grow, so grows the impact of the basic methods \ncovered here.\nBefore developing our fundamental approach to studying algorithms, we develop data \ntypes for stacks, queues, and other low-level abstractions that we use throughout the book. \nThen we survey fundamental algorithms for sorting, searching, graphs, and strings. The last \nchapter is an overview placing the rest of the material in the book in a larger context.\nPREFACE\n ix\nDistinctive features The orientation of the book is to study algorithms likely to be of \npractical use. The book teaches a broad variety of algorithms and data structures and pro-\nvides suf\ufb01cient information about them that readers can con\ufb01dently ", "start": 8, "end": 9}, "5": {"text": "of the book is to study algorithms likely to be of \npractical use. The book teaches a broad variety of algorithms and data structures and pro-\nvides suf\ufb01cient information about them that readers can con\ufb01dently implement, debug, and \nput them to work in any computational environment. The approach involves:\nAlgorithms. Our descriptions of algorithms are based on complete implementations and on \na discussion of the operations of these programs on a consistent set of examples. Instead of \npresenting pseudo-code, we work with real code, so that the programs can quickly be put to \npractical use. Our programs are written in Java, but in a style such that most of our code can \nbe reused to develop implementations in other modern programming languages.\nData types. We use a modern prog ramming st yle based on data abstraction, so that algo -\nrithms and their data structures are encapsulated together.\nApplications. Each chapter has a detailed description of applications where the algorithms \ndescribed play a critical role. These range from applications in physics and molecular biology, \nto engineering computers and systems, to familiar tasks such as data compression and search-\ning on the web.\nA scienti\ufb01c approach. We emphasize developing mathematical models for descr ibing the \nperformance of algorithms, using the models to develop hypotheses about performance, and \nthen testing the hypotheses by running the algorithms in realistic contexts.\nBreadth of coverage. We cover basic abstract data t y pes, sor ting algor ithms, searching al-\ngorithms, graph processing, and string processing. We keep the material in algorithmic con-\ntext, describing data structures, algorithm design paradigms, reduction, and problem-solving \nmodels. We cover classic methods that have been taught since the 1960s and new methods \nthat have been invented in recent years.\nOur primary goal is to introduce the most important algorithms in use today to as wide an \naudience as possible. These ", "start": 9, "end": 9}, "6": {"text": "that have been taught since the 1960s and new methods \nthat have been invented in recent years.\nOur primary goal is to introduce the most important algorithms in use today to as wide an \naudience as possible. These algorithms are generally ingenious creations that, remarkably, can \neach be expressed in just a dozen or two lines of code. As a group, they represent problem-\nsolving power of amazing scope. They have enabled the construction of computational ar -\ntifacts, the solution of scienti\ufb01c problems, and the development of commercial applications \nthat would not have been feasible without them.\n x\nBooksite An important feature of the book is its relationship to the booksite \nalgs4.cs.princeton.edu. This site is freely available and contains an extensive amount of \nmaterial about algorithms and data structures, for teachers, students, and practitioners, in-\ncluding:\nAn online synopsis. The text is summarized in the booksite to give it the same overall struc-\nture as the book, but linked so as to provide easy navigation through the material.\nFull implementations. All code in the book is available on the booksite, in a form suitable for \nprogram development. Many other implementations are also available, including advanced \nimplementations and improvements described in the book, answers to selected exercises, and \nclient code for various applications. The emphasis is on testing algorithms in the context of \nmeaningful applications. \nExercises and answers. The booksite expands on the exercises in the book by adding drill \nexercises (with answers available with a click), a wide variety of examples illustrating the \nreach of the material, programming exercises with code solutions, and challenging problems.\nDynamic visualizations. Dynamic simulations are impossible in a printed book, but the \nwebsite is replete with implementations that use a graphics class to present compelling visual \ndemonstrations of algorithm applications.\nCourse materials. A complete set of lecture slides is tied directly to the material in the ", "start": 9, "end": 10}, "7": {"text": "book, but the \nwebsite is replete with implementations that use a graphics class to present compelling visual \ndemonstrations of algorithm applications.\nCourse materials. A complete set of lecture slides is tied directly to the material in the book \nand on the booksite. A full selection of programming assignments, with check lists, test data, \nand preparatory material, is also included.\nLinks to related material. Hundreds of links lead students to background information about \napplications and to resources for studying algorithms.\nOur goal in creating this material was to provide a complementary approach to the ideas. \nGenerally, you should read the book when learning speci\ufb01c algorithms for the \ufb01rst time or \nwhen trying to get a global picture, and you should use the booksite as a reference when pro-\ngramming or as a starting point when searching for more detail while online.\n xi\n \n \n \n \nUse in the curriculum The book is intended as a textbook in a second course in com-\nputer science.  It provides full coverage of core material and is an excellent vehicle for stu -\ndents to gain experience and maturity in programming, quantitative reasoning, and problem-\nsolving. Typically, one course in computer science will suf\ufb01ce as a prerequisite\u2014the book is \nintended for anyone conversant with a modern programming language and with the basic \nfeatures of modern computer systems.\nThe algorithms and data structures are expressed in Java, but in a style accessible to \npeople \ufb02uent in other modern languages. We embrace modern Java abstractions (including \ngenerics) but resist dependence upon esoteric features of the language.\nMost of the mathematical material supporting the analytic results is self-contained (or \nis labeled as beyond the scope of this book), so little speci\ufb01c preparation in mathematics is \nrequired for the bulk of the book, although mathematical maturity is de\ufb01nitely helpful. Ap -\nplications are drawn from introductory material in the sciences, again self-contained.\nThe ", "start": 10, "end": 11}, "8": {"text": "so little speci\ufb01c preparation in mathematics is \nrequired for the bulk of the book, although mathematical maturity is de\ufb01nitely helpful. Ap -\nplications are drawn from introductory material in the sciences, again self-contained.\nThe material covered is a fundamental background for any student intending to major \nin computer science, electrical engineering, or operations research, and is valuable for any \nstudent with interests in science, mathematics, or engineering.\nContext The book is intended to follow our introductory text, An Introduction to Pro -\ngramming in Java: An Interdisciplinary Approach , which is a broad introduction to the \ufb01eld. \nTo g e t h e r, t h e s e  t wo  b o o k s  c a n  s u p p o r t  a  t wo -  o r  t h re e - s e m e s te r  i n t ro d u c t i o n  to  co m p u te r  s c i-\nence that will give any student the requisite background to successfully address computation \nin any chosen \ufb01eld of study in science, engineering, or the social sciences.\nThe starting point for much of the material in the book was the Sedgewick series of Al-\ngorithms books. In spirit, this book is closest to the \ufb01rst and second editions of that book, but \nthis text bene\ufb01ts from decades of experience teaching and learning that material. Sedgewick\u2019s \ncurrent Algorithms in C/C++/Java, Third Edition is more appropriate as a reference or a text \nfor an advanced course; this book is speci\ufb01cally designed to be a textbook for a one-semester \ncourse for \ufb01rst- or second-year college students and as a modern introduction to the basics \nand a reference for use by working programmers.\n xii\nAcknowledgments This book has been nearly 40 years in the making, ", "start": 11, "end": 12}, "9": {"text": "\ncourse for \ufb01rst- or second-year college students and as a modern introduction to the basics \nand a reference for use by working programmers.\n xii\nAcknowledgments This book has been nearly 40 years in the making, so full recogni-\ntion of all the people who have made it possible is simply not feasible. Earlier editions of this \nbook list dozens of names, including (in alphabetical order) Andrew Appel, Trina Avery, Marc \nBrown, Lyn Dupr\u00e9, Philippe Flajolet, T om Freeman, Dave Hanson, Janet Incerpi, Mike Schid-\nlowsky, Steve Summit, and Chris Van Wyk. All of these people deserve acknowledgement, \neven though some of their contributions may have happened decades ago. For this fourth \nedition, we are grateful to the hundreds of students at Princeton and several other institutions \nwho have suffered through preliminary versions of the work, and to readers around the world \nfor sending in comments and corrections through the booksite.\nWe are g rateful for the suppor t of  Pr inceton Universit y in its unwaver ing commitment \nto excellence in teaching and learning, which has provided the basis for the development of \nthis work.\nPeter Gordon has provided wise counsel throughout the evolution of this work almost \nfrom the beginning, including a gentle introduction of the \u201cback to the basics\u201d idea that is \nthe foundation of this edition. For this fourth edition, we are grateful to Barbara Wood for \nher careful and professional copyediting, to Julie Nahil for managing the production, and \nto many others at Pearson for their roles in producing and marketing the book. All were ex-\ntremely responsive to the demands of a rather tight schedule without the slightest sacri\ufb01ce to \nthe quality of the result.\nRobert Sedgewick\nKevin Wayne\nPrinceton, NJ\nJanuary, 2011\n This page intentionally left blank \n 1.1 Basic Programming Model.  . ", "start": 12, "end": 14}, "10": {"text": "sacri\ufb01ce to \nthe quality of the result.\nRobert Sedgewick\nKevin Wayne\nPrinceton, NJ\nJanuary, 2011\n This page intentionally left blank \n 1.1 Basic Programming Model.  .  .  .  .  .  .  .  . 8\n1.2 Data Abstraction .  .  .  .  .  .  .  .  .  .  .  .  .  . 64\n1.3 Bags, Queues, and Stacks   .  .  .  .  .  .  .  120\n1.4 Analysis of Algorithms  .  .  .  .  .  .  .  .  .  172\n1.5 Case Study: Union-Find.  .  .  .  .  .  .  .  .  216\nONE\nFundamentals T\nhe objective of this book is to study a broad variety of important and useful \nalgorithms\u2014methods for solving problems that are suited for computer imple-\nmentation. Algorithms go hand in hand with data structures\u2014schemes for or-\nganizing data that leave them amenable to ef\ufb01cient processing by an algorithm. This \nchapter introduces the basic tools that we need to study algorithms and data structures. \nFirst, we introduce our basic programming model . All of our programs are imple -\nmented using a small subset of the Java programming language plus a few of our own \nlibraries for input/output and for statistical calculations. Section 1.1 is a summary of \nlanguage constructs, features, and libraries that we use in this book. \nNext, we emphasize data abstraction, where we de\ufb01ne abstract data types (ADTs) in \nthe service of modular programming. In Section 1.2 we introduce the process of im-\nplementing an ADT in Java, by specifying an applications ", "start": 14, "end": 15}, "11": {"text": "where we de\ufb01ne abstract data types (ADTs) in \nthe service of modular programming. In Section 1.2 we introduce the process of im-\nplementing an ADT in Java, by specifying an applications programming interface (API) \nand then using the Java class mechanism to develop an implementation for use in client \ncode. \nAs important and useful examples, we next consider three fundamental ADTs: the \nbag, the queue, and the stack. Section 1.3 describes APIs and implementations of bags, \nqueues, and stacks using arrays, resizing arrays, and linked lists that serve as models and \nstarting points for algorithm implementations throughout the book. \nPerformance is a central consideration in the study of algorithms. Section 1.4 de-\nscribes our approach to analyzing algorithm performance. The basis of our approach is \nthe scienti\ufb01c method : we develop hypotheses about performance, create mathematical \nmodels, and run experiments to test them, repeating the process as necessary.\nWe conclude w ith a case study where we consider solutions to a connectivity problem \nthat uses algorithms and data structures that implement the classic union-\ufb01nd ADT.\n3\n   A l g o r i t h m s  When we write a computer program, we are generally implementing a \nmethod that has been devised previously to solve some problem. This method is often \nindependent of the particular programming language being used\u2014it is likely to be \nequally appropriate for many computers and many programming languages. It is the \nmethod, rather than the computer program itself, that speci\ufb01es the steps that we can \ntake to solve the problem. The term algorithm is used in computer science to describe \na \ufb01nite,  deterministic, and effective problem-solving method suitable for implementa-\ntion as a computer program. Algorithms are the stuff of computer science: they are \ncentral objects of study in the \ufb01eld.\nWe can de\ufb01ne ", "start": 15, "end": 16}, "12": {"text": "\ufb01nite,  deterministic, and effective problem-solving method suitable for implementa-\ntion as a computer program. Algorithms are the stuff of computer science: they are \ncentral objects of study in the \ufb01eld.\nWe can de\ufb01ne an algor ithm by descr ibing a procedure for solv ing a problem in a \nnatural language, or by writing a computer program that implements the procedure, \nas shown at right for  Euclid\u2019s algorithm for \ufb01nding the  greatest common divisor of \ntwo numbers, a variant of which was devised \nover 2,300 years ago. If you are not familiar \nwith  Euclid\u2019s algorithm, you are encour -\naged to work Exercise 1.1.24 and Exercise \n1.1.25, perhaps after reading Section 1.1. In \nthis book, we use computer programs to de-\nscribe algorithms. One important reason for \ndoing so is that it makes easier the task of \nchecking whether they are \ufb01nite, determin -\nistic, and effective, as required. But it is also \nimportant to recognize that a program in a \nparticular language is just one way to express \nan algorithm. The fact that many of the al-\ngorithms in this book have been expressed \nin multiple programming languages over the \npast several decades reinforces the idea that each algorithm is a method suitable for \nimplementation on any computer in any programming language.\nMost algorithms of interest involve organizing the data involved in the computa -\ntion. Such organization leads to  data structures, which also are central objects of study \nin computer science. Algorithms and data structures go hand in hand. In this book we \ntake the view that data structures exist as the byproducts or end products of algorithms \nand that we must therefore study them in order to understand the algorithms. Simple \nalgorithms can give rise to complicated data structures and, conversely, complicated \nalgorithms can use simple data structures. ", "start": 16, "end": 16}, "13": {"text": "byproducts or end products of algorithms \nand that we must therefore study them in order to understand the algorithms. Simple \nalgorithms can give rise to complicated data structures and, conversely, complicated \nalgorithms can use simple data structures. We shall study the properties of many data \nstructures in this book; indeed, we might well have titled the book Algorithms and Data \nStructures.\nCompute the greatest common divisor of \ntwo nonnegative integers p and q as follows: \nIf q is 0, the answer is p. If not, divide p by q\nand take the remainder r. The answer is the\ngreatest common divisor of q and r.\npublic static int gcd(int p, int q)\n{\n   if (q == 0) return p;\n   int r = p % q;\n   return gcd(q, r);\n}\nEuclid\u2019s algorithm\nJava-language description\nEnglish-language description\n4 CHAPTER 1 \u25a0 Fundamentals\n When we use a computer to help us solve a problem, we typically are faced with a \nnumber of possible approaches. For small problems, it hardly matters which approach \nwe use, as long as we have one that correctly solves the problem. For huge problems (or \napplications where we need to solve huge numbers of small problems), however, we \nquickly become motivated to devise methods that use time and space ef\ufb01ciently.\nThe primary reason to learn about algorithms is that this discipline gives us the \npotential to reap huge savings, even to the point of enabling us to do tasks that would \notherwise be impossible. In an application where we are processing millions of objects, \nit is not unusual to be able to make a program millions of times faster by using a well-\ndesigned algorithm. We shall see such examples on numerous occasions throughout \nthe book. By contrast, investing additional money or time to buy and install a new \ncomputer holds the potential for speeding up a program by perhaps a factor of ", "start": 16, "end": 17}, "14": {"text": "well-\ndesigned algorithm. We shall see such examples on numerous occasions throughout \nthe book. By contrast, investing additional money or time to buy and install a new \ncomputer holds the potential for speeding up a program by perhaps a factor of only 10 \nor 100. Careful algorithm design is an extremely effective part of the process of solving \na huge problem, whatever the applications area.\nWhen developing a huge or complex computer program, a great deal of effort must \ngo into understanding and de\ufb01ning the problem to be solved, managing its complex -\nity, and decomposing it into smaller subtasks that can be implemented easily. Often, \nmany of the algorithms required after the decomposition are trivial to implement. In \nmost cases, however, there are a few algorithms whose choice is critical because most \nof the system resources will be spent running those algorithms. These are the types of \nalgorithms on which we concentrate in this book. We study fundamental algorithms \nthat are useful for solving challenging problems in a broad variety of applications areas.\nThe sharing of programs in computer systems is becoming more widespread, so \nalthough we might expect to be using a large fraction of the algorithms in this book, we \nalso might expect to have to implement only a small fraction of them. For example, the \nJava libraries contain implementations of a host of fundamental algorithms. However, \nimplementing simple versions of basic algorithms helps us to understand them bet-\nter and thus to more effectively use and tune advanced versions from a library. More \nimportant, the opportunity to reimplement basic algorithms arises frequently. The pri-\nmary reason to do so is that we are faced, all too often, with completely new computing \nenvironments (hardware and software) with new features that old implementations \nmay not use to best advantage. In this book, we concentrate on the simplest reasonable \nimplementations of the best algorithms. We do pay careful attention to coding the criti-\ncal parts of the algorithms, ", "start": 17, "end": 17}, "15": {"text": "that old implementations \nmay not use to best advantage. In this book, we concentrate on the simplest reasonable \nimplementations of the best algorithms. We do pay careful attention to coding the criti-\ncal parts of the algorithms, and take pains to note where low-level optimization effort \ncould be most bene\ufb01cial.\nThe choice of the best algorithm for a particular task can be a complicated process, \nperhaps involving sophisticated mathematical analysis. The branch of computer sci-\nence that comprises the study of such questions is called  analysis of algorithms . Many \n5CHAPTER 1 \u25a0 Fundamentals\n   \n \nof the algorithms that we study have been shown through analysis to have excellent \ntheoretical performance; others are simply known to work well through experience. \nOur primary goal is to learn reasonable algorithms for important tasks, yet we shall also \npay careful attention to comparative performance of the methods. We should not use \nan algorithm without having an idea of what resources it might consume, so we strive \nto be aware of how our algorithms might be expected to perform.\nSummary of topics As an overview, we describe the major parts of the book, giv-\ning speci\ufb01c topics covered and an indication of our general orientation toward the \nmaterial. This set of topics is intended to touch on as many fundamental algorithms as \npossible. Some of the areas covered are core computer-science areas that we study in \ndepth to learn basic algorithms of wide applicability. Other algorithms that we discuss \nare from advanced \ufb01elds of study within computer science and related \ufb01elds. The algo-\nrithms that we consider are the products of decades of research and development and \ncontinue to play an essential role in the ever-expanding applications of computation. \nFundamentals (Chapter 1) in the context of this book are the basic principles and \nmethodology that we use to implement, analyze, and compare algorithms. We consider \nour Java programming model, data abstraction, ", "start": 17, "end": 18}, "16": {"text": "computation. \nFundamentals (Chapter 1) in the context of this book are the basic principles and \nmethodology that we use to implement, analyze, and compare algorithms. We consider \nour Java programming model, data abstraction, basic data structures, abstract data \ntypes for collections, methods of analyzing algorithm performance, and a case study.\nSorting algorithms (Chapter 2) for rearranging arrays in order are of fundamental \nimportance. We consider a variety of algorithms in considerable depth, including in-\nsertion sort, selection sort, shellsort, quicksort, mergesort, and heapsort. We also en -\ncounter algorithms for several related problems, including priority queues, selection, \nand merging. Many of these algorithms will \ufb01nd application as the basis for other algo-\nrithms later in the book.\nSearching algorithms (Chapter 3) for \ufb01nding speci\ufb01c items among large collections \nof items are also of fundamental importance. We discuss basic and advanced methods \nfor searching, including binary search trees, balanced search trees, and hashing. We \nnote relationships among these methods and compare performance.\nGraphs (Chapter 4) are sets of objects and connections, possibly with weights and \norientation. Graphs are useful models for a vast number of dif\ufb01cult and important \nproblems, and the design of algorithms for processing graphs is a major \ufb01eld of study. \nWe consider depth-\ufb01rst search, breadth-\ufb01rst search, connectiv it y problems, and sev -\neral algorithms and applications, including Kruskal\u2019s and Prim\u2019s algorithms for \ufb01nding \nminimum spanning tree and Dijkstra\u2019s and the Bellman-Ford algorithms for solving \nshortest-paths problems.\n6 CHAPTER 1 \u25a0 Fundamentals\n  \n \nStrings (Chapter 5) are an essential data type in modern computing applications. \nWe consider a range of  methods for processing sequences of  characters. We ", "start": 18, "end": 19}, "17": {"text": "\nshortest-paths problems.\n6 CHAPTER 1 \u25a0 Fundamentals\n  \n \nStrings (Chapter 5) are an essential data type in modern computing applications. \nWe consider a range of  methods for processing sequences of  characters. We beg in w ith \nfaster algorithms for sorting and searching when keys are strings. Then we consider \nsubstring search, regular expression pattern matching, and data-compression algo -\nrithms. Again, an introduction to advanced topics is given through treatment of some \nelementary problems that are important in their own right.\nContext (Chapter 6) helps us relate the material in the book to several other advanced \n\ufb01elds of study, including scienti\ufb01c computing, operations research, and the theory of \ncomputing. We survey event-based simulation, B-trees, suf\ufb01x arrays, maximum \ufb02ow, \nand other advanced topics from an introductory viewpoint to develop appreciation for \nthe interesting advanced \ufb01elds of study where algorithms play a critical role. Finally, we \ndescribe search problems, reduction, and NP-completeness to introduce the theoretical \nunderpinnings of the study of algorithms and relationships to material in this book.\nThe study of algorithms is interesting and exciting  because it is a new \ufb01eld \n(almost all the algorithms that we study are less than 50 years old, and some were just \nrecently discovered) with a rich tradition (a few algorithms have been known for hun-\ndreds of years). New discoveries are constantly being made, but few algorithms are \ncompletely understood. In this book we shall consider intricate, complicated, and dif\ufb01-\ncult algorithms as well as elegant, simple, and easy ones. Our challenge is to understand \nthe former and to appreciate the latter in the context of scienti\ufb01c and commercial ap -\nplications. In doing so, we shall explore a variety of useful tools and develop a style of \nalgorithmic ", "start": 19, "end": 19}, "18": {"text": "challenge is to understand \nthe former and to appreciate the latter in the context of scienti\ufb01c and commercial ap -\nplications. In doing so, we shall explore a variety of useful tools and develop a style of \nalgorithmic thinking that will serve us well in computational challenges to come.\n7CHAPTER 1 \u25a0 Fundamentals\n 1.1 BASIC PROGRAMMING MODEL\nOur study of algorithms is based upon implementing them as programs written in \nthe Java programming language. We do so for several reasons:\n\u25a0 Our programs are concise, elegant, and complete descriptions of algorithms.\n\u25a0 Yo u  c a n  r u n  t h e  p ro g r a m s  t o  s t u d y  p ro p e r t i e s  o f  t h e  a l g o r i t h m s .\n\u25a0 Yo u  c a n  p u t  t h e  a l g o r i t h m s  i m m e d i a t e l y  t o  g o o d  u s e  i n  a p p l i c a t i o n s .\nThese are important and signi\ufb01cant advantages over the alternatives of working with \nEnglish-language descriptions of algorithms.\nA potential downside to this approach is that we have to work with a speci\ufb01c pro -\ngramming language, possibly making it dif\ufb01cult to separate the idea of the algorithm \nfrom the details of its implementation. Our implementations are designed to mitigate \nthis dif\ufb01culty, by using programming constructs that are both found in many modern \nlanguages and needed to adequately describe the algorithms. \nWe use only a small subset of  Java. While we stop shor t of  formally de\ufb01ning the \nsubset that we use, you will see that we make use of relatively few Java constructs, and \nthat we emphasize ", "start": 19, "end": 20}, "19": {"text": "only a small subset of  Java. While we stop shor t of  formally de\ufb01ning the \nsubset that we use, you will see that we make use of relatively few Java constructs, and \nthat we emphasize those that are found in many modern programming languages. The \ncode that we present is complete, and our expectation is that you will download it and \nexecute it, on our test data or test data of your own choosing.\nWe refer to the prog ramming constructs, software librar ies, and operating system \nfeatures that we use to implement and describe algorithms as our programming model. \nIn this section and Section 1.2, we fully describe this programming model. The treat-\nment is self-contained and primarily intended for documentation and for your refer -\nence in understanding any code in the book. The model we describe is the same model \nintroduced in our book  An Introduction to Programming in Java: An Interdisciplinary \nApproach, which provides a slower-paced introduction to the material.\nFor reference, the \ufb01gure on the facing page depicts a complete Java program that \nillustrates many of the basic features of our programming model. We use this code for \nexamples when discussing language features, but defer considering it in detail to page \n46 (it implements a classic algorithm known as  binary search and tests it for an applica-\ntion known as    whitelist \ufb01ltering ). We assume that you have experience programming \nin some modern language, so that you are likely to recognize many of these features in \nthis code. Page references are included in the annotations to help you \ufb01nd answers to \nany questions that you might have. Since our code is somewhat stylized and we strive \nto make consistent use of various Java idioms and constructs, it is worthwhile even for \nexperienced Java programmers to read the information in this section.\n8\n import java.util.Arrays;\npublic class BinarySearch\n{\n   public static int ", "start": 20, "end": 21}, "20": {"text": "make consistent use of various Java idioms and constructs, it is worthwhile even for \nexperienced Java programmers to read the information in this section.\n8\n import java.util.Arrays;\npublic class BinarySearch\n{\n   public static int rank(int key, int[] a)\n   {\n      int lo = 0;\n      int hi = a.length - 1;\n      while (lo <= hi)\n      {\n         int mid = lo + (hi - lo) / 2;\n         if      (key < a[mid]) hi = mid - 1;\n         else if (key > a[mid]) lo = mid + 1;\n         else                   return mid;\n      }\n      return -1;\n   }\n   public static void main(String[] args)\n   {\n      \n      int[] whitelist = In.readInts(args[0]);\n      Arrays.sort(whitelist);\n      while (!StdIn.isEmpty())\n      {\n         int key = StdIn.readInt();\n         if (rank(key, whitelist) == -1)\n            StdOut.println(key);\n      }\n   }\n}\nexpression (see page 11)\ncall a method in our standard library;\nneed to download code (see page 27)\ncall a method in a Java library (see page 27)\ncall a local method\n(see page 27)\nimport a Java library (see page 27)\ncode must be in file BinarySearch.java (see page 26)\ninitializing\ndeclaration statement\n(see page 16)\ncommand line\n(see page 36)\nstatic method (see page 22)\nunit test client (see page 26)\nloop statement\n(see page 15)\nconditional statement\n(see page 15)\nsystem calls main()\nsystem passes argument value\n\"whitelist.txt\" to main()\nAnatomy of a Java program and its invocation from the command line\nparameter\nvariables\nreturn type parameter type\nreturn statement\nno return value; just side effects (see page 24)\n% ", "start": 21, "end": 21}, "21": {"text": "value\n\"whitelist.txt\" to main()\nAnatomy of a Java program and its invocation from the command line\nparameter\nvariables\nreturn type parameter type\nreturn statement\nno return value; just side effects (see page 24)\n% java BinarySearch largeW.txt < largeT.txt\n499569\n984875\n...\nfile name (args[0])\nfile redirectd from StdIn\n(see page 40)\nStdOut\n(see page 37)\n91.1 \u25a0 Basic Programming Model\n   \nBasic structure of a Java program  A Java program ( class) is either a library of \nstatic methods (functions) or a data type de\ufb01nition. To create libraries of static methods \nand data-type de\ufb01nitions, we use the following \ufb01ve components, the basis of program-\nming in Java and many other modern languages:\n\u25a0  \n \nPrimitive data types precisely de\ufb01ne the meaning of terms like integer, real num-\nber, and boolean value within a computer program. Their de\ufb01nition includes the \nset of possible values and operations on those values, which can be combined \ninto expressions like mathematical expressions that de\ufb01ne values.\n\u25a0  \n \nStatements allow us to de\ufb01ne a computation by creating and assigning values to \nvariables, controlling execution \ufb02ow, or causing side effects. We use six types of \nstatements: declarations, assignments, conditionals, loops, calls, and returns.\n\u25a0 Arrays allow us to work with multiple values of the same type.\n\u25a0 Static methods allow us to encapsulate and reuse code and to develop programs \nas a set of independent modules.\n\u25a0 Strings are sequences of characters. Some operations on them are built in to Java.\n\u25a0 Input/output sets up communication between programs and the outside world.\n\u25a0 \n \n \nData abstraction extends encapsulation and reuse to allow us to de\ufb01ne non-\nprimitive data types, thus supporting object-oriented programming.\nIn ", "start": 21, "end": 22}, "22": {"text": "Java.\n\u25a0 Input/output sets up communication between programs and the outside world.\n\u25a0 \n \n \nData abstraction extends encapsulation and reuse to allow us to de\ufb01ne non-\nprimitive data types, thus supporting object-oriented programming.\nIn this section, we will consider the \ufb01rst \ufb01ve of these in turn. Data abstraction is the \ntopic of the next section. \nRunning a Java program involves interacting with an operating system or a program \ndevelopment environment. For clarity and economy, we describe such actions in terms \nof a    virtual terminal , where we interact with programs by typing commands to the \nsystem. See the booksite for details on using a virtual terminal on your system, or for \ninformation on using one of the many more advanced program development environ-\nments that are available on modern systems. \nFor example, BinarySearch is two static methods, rank() and main(). The \ufb01rst \nstatic method, rank(), is four statements: two declarations, a loop (which is itself an as-\nsignment and two conditionals), and a return. The second, main(), is three statements: \na declaration, a call, and a loop (which is itself an assignment and a conditional).\nTo  i nvo ke  a  Jav a  p ro g r a m , w e  \ufb01 r s t     compile it using the javac command, then  run it us-\ning the java command. For example, to run BinarySearch, we \ufb01rst type the command \njavac BinarySearch.java (which creates a \ufb01le BinarySearch.class that contains \na lower-level version of the program in Java  bytecode in the \ufb01le BinarySearch.class). \nThen we type java BinarySearch (followed by a whitelist \ufb01le name) to transfer con-\ntrol to the bytecode version of the program. T o develop a basis for understanding the \neffect of these actions, we next consider in detail primitive ", "start": 22, "end": 22}, "23": {"text": "BinarySearch (followed by a whitelist \ufb01le name) to transfer con-\ntrol to the bytecode version of the program. T o develop a basis for understanding the \neffect of these actions, we next consider in detail primitive data types and expressions, \nthe various kinds of Java statements, arrays, static methods, strings, and input/output.\n10 CHAPTER 1 \u25a0 Fundamentals\n   P r i m i t i v e  d a t a  t y p e s  a n d  e x p r e s s i o n s  A data type is a set of values and a set of \noperations on those values. We begin by considering the following four primitive data \ntypes that are the basis of the Java language:\n\u25a0 Integers, with arithmetic operations (int)\n\u25a0 Real numbers, again with arithmetic operations (double)\n\u25a0 Booleans, the set of values { true, false } with logical operations (boolean)\n\u25a0 \n \nCharacters, the alphanumeric characters and symbols that you type (char)\nNext we consider mechanisms for specifying values and operations for these types. \nA Java program manipulates  variables that are named with  identi\ufb01ers. Each variable \nis associated with a data type and stores one of the permissible data-type values. In Java \ncode, we use  expressions like familiar mathematical expressions to apply the operations \nassociated with each type. For primitive types, we use identi\ufb01ers to refer to variables, \noperator symbols such as + - * /  to specify operations,    literals such as 1 or 3.14 to \nspecify values, and expressions such as (x + 2.236)/2 to specify operations on values. \nThe purpose of an expression is to de\ufb01ne one of the data-type values. \nterm examples definition\nprimitive\ndata type\nint double boolean char\na set of values and a set of \noperations on those values\n(built ", "start": 22, "end": 23}, "24": {"text": "of an expression is to de\ufb01ne one of the data-type values. \nterm examples definition\nprimitive\ndata type\nint double boolean char\na set of values and a set of \noperations on those values\n(built in to the Java language)\nidentifier a  abc  Ab$  a_b  ab123  lo  hi  \na sequence of letters, digits,\n_, and $, the \ufb01rst of which is \nnot a digit\nvariable [any identifier] names a data-type value\noperator + - * / names a data-type operation\nliteral source-code representation\nof a value\nint 1  0  -42\ndouble 2.0  1.0e-15  3.14 \nboolean true  false\nchar 'a'  '+'  '9'  '\\n'\nexpression\na literal, a variable, or a \nsequence of operations on \nliterals and/or variables that \nproduces a value\nint     lo + (hi - lo)/2 \ndouble  1.0e-15 * t \nboolean  lo <= hi\nBasic building blocks for Java programs\n111.1 \u25a0 Basic Programming Model\n  \nTo  d e \ufb01 n e  a  d a t a  t y p e , w e  n e e d  o n l y  s p e c i f y  t h e  v a l u e s  a n d  t h e  s e t  o f  o p e r a t i o n s  o n  \nthose values. This information is summarized in the table below for Java\u2019s int, double, \nboolean, and char data types. These data types are similar to the basic data types found \nin many programming languages. For int and double, the operations are familiar \narithmetic operations; for boolean, they are familiar logical operations. It is important \nto note ", "start": 23, "end": 24}, "25": {"text": "These data types are similar to the basic data types found \nin many programming languages. For int and double, the operations are familiar \narithmetic operations; for boolean, they are familiar logical operations. It is important \nto note that +, -, *, and / are  overloaded\u2014the same symbol speci\ufb01es operations in mul-\ntiple different types, depending on context. The key property of these primitive opera-\ntions is that an operation involving values of a given type has a value of that type. This rule \nhighlights the idea that we are often working with approximate values, since it is often \nthe case that the exact value that would seem to be de\ufb01ned by the expression is not a \nvalue of the type. For example, 5/3 has the value 1 and 5.0/3.0 has a value very close \nto 1.66666666666667 but neither of these is exactly equal to 5/3. This table is far from \ncomplete; we discuss some additional operators and various exceptional situations that \nwe occasionally need to consider in the Q&A at the end of this section.\ntype set of values operators typical expressions\nexpression value\nint\nintegers between\n231 and/H11001231/H11002 1\n(32-bit two\u2019s \ncomplement)\n+ (add)\n- (subtract)\n* (multiply)\n/ (divide)\n% (remainder)\n5 + 3 \n5 - 3 \n5 * 3 \n5 / 3 \n5 % 3\n8 \n2\n15\n1\n2\ndouble\ndouble-precision \nreal numbers\n(64-bit IEEE 754 \nstandard)\n+ (add)\n- (subtract)\n* (multiply)\n/ (divide)\n3.141 - .03\n2.0 - 2.0e-7 \n100 * .015\n6.02e23 / 2.0\n3.111\n1.9999998\n1.5\n3.01e23\nboolean ", "start": 24, "end": 24}, "26": {"text": "(subtract)\n* (multiply)\n/ (divide)\n3.141 - .03\n2.0 - 2.0e-7 \n100 * .015\n6.02e23 / 2.0\n3.111\n1.9999998\n1.5\n3.01e23\nboolean true or false\n&& (and)\n|| (or)\n! (not)\n^ (xor)\ntrue && false \nfalse || true\n!false \ntrue ^ true\nfalse \ntrue \ntrue \nfalse\nchar characters\n(16-bit) [arithmetic operations, rarely used]\n P r i m i t i v e  d a t a  t y p e s  i n  J a v a\n12 CHAPTER 1 \u25a0 Fundamentals\n  \n \n \n  \n E x p r e s s i o n s .  As illustrated in the table at the bottom of the previous page, typical ex-\npressions are    in\ufb01x: a literal (or an expression), followed by an operator, followed by \nanother literal (or another expression).  When an expression contains more than one \noperator, the order in which they are applied is often signi\ufb01cant, so the following    pre-\ncedence conventions are part of the Java language speci\ufb01cation: The operators * and / ( \nand %) have higher precedence than (are applied before) the + and - operators; among \nlogical operators, ! is the highest precedence, followed by && and then ||. Generally, \noperators of the same precedence are applied left to right. As in standard arithmetic ex-\npressions, you can use parentheses to override these rules. Since precedence rules vary \nslightly from language to language, we use parentheses and otherwise strive to avoid \ndependence on precedence rules in our code.\n  T y p e  c o n v e r s i o n .  Numbers are automatically promoted to a more inclusive ", "start": 24, "end": 25}, "27": {"text": "to language, we use parentheses and otherwise strive to avoid \ndependence on precedence rules in our code.\n  T y p e  c o n v e r s i o n .  Numbers are automatically promoted to a more inclusive type if no \ninformation is lost. For example, in the expression 1 + 2.5 , the 1 is promoted to the \ndouble value 1.0 and the expression evaluates to the double value 3.5 . A  cast is a type \nname in parentheses within an expression, a directive to convert the following value \ninto a value of that type. For example (int) 3.7 is 3 and (double) 3 is 3.0. Note that \ncasting to an int is truncation instead of rounding\u2014rules for casting within compli-\ncated expressions can be intricate, and casts should be used sparingly and with care. A \nbest practice is to use expressions that involve literals or variables of a single type.\n C o m p a r i s o n s .  The following operators compare two values of the same type and \nproduce a boolean value: equal (==), not equal (!=), less than ( <), less than or equal \n(<=), greater than (>), and g reater than or equal  (>=). These operators are known as \nmixed-type operators because their value is boolean, not the type of the values being \ncompared. An expression with a boolean value is known as a  boolean expression. Such \nexpressions are essential components in conditional and loop statements, as we will see.\nOther primitive types. Java\u2019s int has 232 different values by design, so it can be repre-\nsented in a 32-bit machine word (many machines have 64-bit words nowadays, but the \n32-bit int persists). Similarly, the double standard speci\ufb01es a 64-bit representation. \nThese data-type ", "start": 25, "end": 25}, "28": {"text": "repre-\nsented in a 32-bit machine word (many machines have 64-bit words nowadays, but the \n32-bit int persists). Similarly, the double standard speci\ufb01es a 64-bit representation. \nThese data-type sizes are adequate for typical applications that use integers and real \nnumbers. T o provide \ufb02exibility, Java has \ufb01ve additional primitive data types: \n\u25a0 64-bit integers, with arithmetic operations ( long)\n\u25a0 16-bit integers, with arithmetic operations ( short)\n\u25a0 16-bit characters, with arithmetic operations (char)\n\u25a0 8-bit integers, with arithmetic operations ( byte)\n\u25a0 32-bit single-precision real numbers, again with arithmetic operations ( float)\nWe most often use int and double arithmetic operations in this book, so we do not \nconsider the others (which are very similar) in further detail here. \n131.1 \u25a0 Basic Programming Model\n  \n \n S t a t e m e n t s  A Java program is composed of statements, which de\ufb01ne the computa-\ntion by creating and manipulating variables, assigning data-type values to them, and \ncontrolling the \ufb02ow of execution of such operations. Statements are often organized in \nblocks, sequences of statements within curly braces.\n\u25a0  Declarations create variables of a speci\ufb01ed type and name them with identi\ufb01ers.\n\u25a0  Assignments associate a data-type value (de\ufb01ned by an expression) with a vari-\nable. Java also has several implicit assignment idioms for changing the value of a \ndata-type value relative to its current value, such as incrementing the value of an \ninteger variable. \n\u25a0  Conditionals provide for a simple change in the \ufb02ow of execution\u2014execute the \nstatements in one of two blocks, depending on a speci\ufb01ed condition.\n\u25a0  Loops provide for a more profound change in the \ufb02ow ", "start": 25, "end": 26}, "29": {"text": "a simple change in the \ufb02ow of execution\u2014execute the \nstatements in one of two blocks, depending on a speci\ufb01ed condition.\n\u25a0  Loops provide for a more profound change in the \ufb02ow of execution\u2014execute the \nstatements in a block as long as a given condition is true.\n\u25a0 \n \n \n \nCalls and returns relate to static methods (see page 22), which provide another way \nto change the \ufb02ow of execution and to organize code.\nA program is a sequence of statements, with declarations, assignments, conditionals, \nloops, calls, and returns. Programs typically have a nested structure : a statement among \nthe statements in a block within a conditional or a loop may itself be a conditional or a \nloop. For example, the while loop in rank() contains an if statement. Next, we con-\nsider each of these types of statements in turn.\n  D e c l a r a t i o n s .  A declaration statement associates a variable name with a type at com -\npile time. Java requires us to use declarations to specify the names and types of vari-\nables. By doing so, we are being explicit about any computation that we are specify-\ning. Java is said to be a    strongly typed language, because the Java compiler checks for \nconsistency (for example, it does not permit us to multiply a boolean and a double).  \nDeclarations can appear anywhere before a variable is \ufb01rst used\u2014most often, we put \nthem at the point of \ufb01rst use. The   scope of a variable is the part of the program where it \nis de\ufb01ned. Generally the scope of a variable is composed of the statements that follow \nthe declaration in the same block as the declaration. \n  A s s i g n m e n t s .  An assignment statement associates a data-type value (de\ufb01ned by an ex-\npression) ", "start": 26, "end": 26}, "30": {"text": "statements that follow \nthe declaration in the same block as the declaration. \n  A s s i g n m e n t s .  An assignment statement associates a data-type value (de\ufb01ned by an ex-\npression) with a variable. When we write c = a + b  in Java, we are not expressing \nmathematical equality, but are instead expressing an action: set the value of the vari-\nable c to be the value of a plus the value of b. It is true that c is mathematically equal \nto a + b immediately after the assignment statement has been executed, but the point \nof the statement is to change the value of c (if necessary). The left-hand side of an as-\nsignment statement must be a single variable; the right-hand side can be an arbitrary \nexpression that produces a value of the type. \n14 CHAPTER 1 \u25a0 Fundamentals\n   C o n d i t i o n a l s .  Most computations require different actions for different inputs. One \nway to express these differences in Java is the if statement:\nif (<boolean expression>) { <block statements> }\nThis description introduces a formal notation known as a template that we use occa-\nsionally to specify the format of Java constructs. We put within angle brackets ( < >) \na construct that we have already de\ufb01ned, to indicate that we can use any instance of \nthat construct where speci\ufb01ed. In this case, <boolean expression>  represents an \nexpression that has a boolean value, such as one involving a comparison operation, \nand < block statements> represents a sequence of Java statements. It is possible to \nmake formal de\ufb01nitions of <boolean expression> and <block statements>, but \nwe refrain from going into that level of detail. The meaning of an if statement is self-\nexplanatory: the statement(s) in the block are ", "start": 26, "end": 27}, "31": {"text": "de\ufb01nitions of <boolean expression> and <block statements>, but \nwe refrain from going into that level of detail. The meaning of an if statement is self-\nexplanatory: the statement(s) in the block are to be executed if and only if the boolean \nexpression is true. The   if-else statement:\nif (<boolean expression>) { <block statements> } \nelse                      { <block statements> }\nallows for choosing between two alternative blocks of statements.\n L o o p s .  Many computations are inherently repetitive. The basic Java construct for han-\ndling such computations has the following format:\nwhile (<boolean expression>) { <block statements> }\nThe while statement has the same form as the if statement (the only difference being \nthe use of the keyword while instead of if), but the meaning is quite different. It is an \ninstruction to the computer to behave as follows: if the boolean expression is false, \ndo nothing; if the boolean expression is true, execute the sequence of statements in \nthe block (just as with if) but then check the boolean expression again, execute the se-\nquence of statements in the block again if the boolean expression is true, and continue \nas long as the boolean expression is true. We refer to the statements in the block in a \nloop as the body of the loop.\nBreak and  continue. Some situations call for slightly more complicated control \ufb02ow \nthan provide by the basic if and while statements. Accordingly, Java supports two ad-\nditional statements for use within while loops:\n\u25a0 The  break statement, which immediately exits the loop\n\u25a0 The  continue statement, which immediately begins the next iteration of the \nloop\nWe rarely use these statements in the code in this book (and many prog rammers never \nuse them), but they do considerably simplify code in certain instances.\n151.1 \u25a0 Basic Programming Model\n  \n \nShortcut notations There are several ways to express a ", "start": 27, "end": 28}, "32": {"text": "in the code in this book (and many prog rammers never \nuse them), but they do considerably simplify code in certain instances.\n151.1 \u25a0 Basic Programming Model\n  \n \nShortcut notations There are several ways to express a given computation; we \nseek clear, elegant, and ef\ufb01cient code. Such code often takes advantage of the following \nwidely used shortcuts (that are found in many languages, not just Java).\n I n i t i a l i z i n g  d e c l a r a t i o n s .  We can combine a declaration w ith an assig nment to ini -\ntialize a variable at the same time that it is declared (created). For example, the code \nint i = 1;  creates an int variable named i and assigns it the initial value 1. A best \npractice is to use this mechanism close to \ufb01rst use of the variable (to limit scope).\n I m p l i c i t  a s s i g n m e n t s .  The following shortcuts are available when our purpose is to \nmodify a variable\u2019s value relative to its current value:\n\u25a0 Increment/decrement operators: i++ is the same as i = i + 1 and has the value \ni in an expression. Similarly, i-- is the same as i = i - 1. The code ++i and \n--i are the same except that the expression value is taken after the increment/\ndecrement, not before.\n\u25a0 Other compound operations: Prepending a binary operator to the = in an as-\nsignment is equivalent to using the variable on the left as the \ufb01rst operand. \nFor example, the code i/=2; is equivalent to the code i = i/2; Note that  \ni += 1; has the same effect as i = i+1; (and i++).\n S i n g l e - ", "start": 28, "end": 28}, "33": {"text": "i/=2; is equivalent to the code i = i/2; Note that  \ni += 1; has the same effect as i = i+1; (and i++).\n S i n g l e - s t a t e m e n t  b l o c k s .  If a block of statements in a conditional or a loop has only a \nsingle statement, the curly braces may be omitted.\n F o r  n o t a t i o n .  Many loops follow this scheme: initialize an index variable to some val-\nue and then use a while loop to test a loop continuation condition involving the index \nvariable, where the last statement in the while loop increments the index variable. Y ou \ncan express such loops compactly with Java\u2019s for notation:\nfor (<initialize>; <boolean expression>; <increment>) \n{\n   <block statements>\n}\nThis code is, with only a few exceptions, equivalent to\n<initialize>; \nwhile (<boolean expression>) \n{\n   <block statements>\n   <increment>; \n}\nWe use for loops to support this initialize-and-increment programming idiom.\n16 CHAPTER 1 \u25a0 Fundamentals\n statement examples definition\ndeclaration\nint i; \ndouble c;\ncreate a variable of a speci\ufb01ed type, \nnamed with a given identi\ufb01er\nassignment\na = b + 3; \ndiscriminant = b*b - 4.0*c; assign a data-type value to a variable\ninitializing\ndeclaration\nint i = 1; \ndouble c = 3.141592625;\ndeclaration that also assigns an \ninitial value\nimplicit\nassignment\ni++; \ni += 1;\ni = i + 1;\nconditional (if) if (x < 0) x = -x; execute a statement,\ndepending on boolean expression\nconditional\n(if-else)\nif (x > y) max = x; ", "start": 28, "end": 29}, "34": {"text": "= i + 1;\nconditional (if) if (x < 0) x = -x; execute a statement,\ndepending on boolean expression\nconditional\n(if-else)\nif (x > y) max = x; \nelse       max = y; \nexecute one or the other statement,\ndepending on boolean expression\nloop (while)\nint v = 0; \nwhile (v <= N)\n   v = 2*v; \ndouble t = c;\nwhile (Math.abs(t - c/t) > 1e-15*t)\n   t = (c/t + t) / 2.0;\nexecute statement\nuntil boolean expression is false\nloop (for)\nfor (int i = 1; i <= N; i++)\n   sum += 1.0/i; \nfor (int i = 0; i <= N; i++)\n   StdOut.println(2*Math.PI*i/N);\ncompact version of while statement\ncall int key = StdIn.readInt(); invoke other methods (see page 22) \nreturn return false; return from a method (see page 24)\nJava statements\n171.1 \u25a0 Basic Programming Model\n   A r r a y s  An array stores a sequence of values that are all of the same type. We want \nnot only to store values but also to access each individual value. The method that we \nuse to refer to individual values in an array is numbering and then indexing them. If \nwe have N values, we think of them as being numbered from 0 to N/H110021. Then, we can \nunambiguously specify one of them in Java code by using the notation a[i] to refer to \nthe ith value for any value of i from 0 to N-1. This Java construct is known as a one-\ndimensional array. \nCreating and initializing an array. Making an array in a Java program involves three \ndistinct steps:\n\u25a0 ", "start": 29, "end": 30}, "35": {"text": "for any value of i from 0 to N-1. This Java construct is known as a one-\ndimensional array. \nCreating and initializing an array. Making an array in a Java program involves three \ndistinct steps:\n\u25a0 Declare the array name and type.\n\u25a0 Create the array.\n\u25a0 Initialize the array values.\nTo  d e c l a re  t h e  a r r ay, yo u  n e e d  to  s p e c i f y  a  n a m e  a n d  t h e  t y p e  o f  d a t a  i t  w i l l  co n t a i n . \nTo  c re a te  i t , yo u  n e e d  to  s p e c i f y  i t s  l e n g t h  ( t h e  n u m b e r  o f  v a l u e s ) . Fo r  e x a m p l e , t h e  \n\u201clong form\u201d code shown at right makes \nan array of N numbers of type double, all \ninitialized to 0.0. The \ufb01rst statement is \nthe array declaration. It is just like a dec-\nlaration of a variable of the correspond-\ning primitive type except for the square \nbrackets following the type name, which \nspecify that we are declaring an array. \nThe keyword new in the second state-\nment is a Java directive to create the ar -\nray. The reason that we need to explicitly \ncreate arrays at run time is that the Java \ncompiler cannot know how much space \nto reserve for the array at compile time (as it can for primitive-type values). The for\nstatement initializes the N array values. This code sets all of the array entries to the value \n0.0. When you begin to write code that ", "start": 30, "end": 30}, "36": {"text": "array at compile time (as it can for primitive-type values). The for\nstatement initializes the N array values. This code sets all of the array entries to the value \n0.0. When you begin to write code that uses an array, you must be sure that your code \ndeclares, creates, and initializes it. Omitting one of these steps is a common program-\nming mistake. \nShort form. For economy in code, we often take advantage of Java\u2019s default array ini-\ntialization convention and combine all three steps into a single statement, as in the \n\u201cshort form\u201d code in our example. The code to the left of the equal sign constitutes the \ndeclaration; the code to the right constitutes the creation. The for loop is unnecessary \nin this case because the default initial value of variables of type double in a Java array is \ndeclaration\ncreationdouble[] a;\na = new double[N];\nfor (int i = 0; i < N; i++)\n   a[i] = 0.0;   \ndouble[] a = new double[N];\ninitialization\nDeclaring, creating and initializing an array\nshort form\nint[] a = { 1, 1, 2, 3, 5, 8 };\ninitializing declaration\nlong form\n18 CHAPTER 1 \u25a0 Fundamentals\n 0.0, but it would be required if a nonzero value were desired. The default initial value \nis zero for numeric types and false for type boolean. The third option shown for our \nexample is to specify the initialization values at compile time, by listing literal values \nbetween curly braces, separated by commas. \nUsing an array. Typical array-processing code is shown on page 21. After declaring \nand creating an array, you can refer to any individual value anywhere you would use \na variable name in a program by enclosing an integer index in square brackets after \nthe array name. Once we create ", "start": 30, "end": 31}, "37": {"text": "21. After declaring \nand creating an array, you can refer to any individual value anywhere you would use \na variable name in a program by enclosing an integer index in square brackets after \nthe array name. Once we create an array, its size is \ufb01xed. A program can refer to the \nlength of an array a[] with the code a.length. The last element of an array a[] is \nalways a[a.length-1]. Java does  automatic bounds checking\u2014if you have created an \narray of size N and use an index whose value is less than 0 or greater than N-1, your pro-\ngram will terminate with an ArrayOutOfBoundsException runtime exception. \n   A l i a s i n g .  Note carefully that an array name refers to the whole array \u2014if we assign one \narray name to another, then both refer to the same array, as illustrated in the following \ncode fragment. \nint[] a = new int[N]; \n... \na[i] = 1234; \n... \nint[] b = a; \n... \nb[i] = 5678;  // a[i] is now 5678.\nThis situation is known as aliasing and can lead to subtle bugs. If your intent is to make \na copy of an array, then you need to declare, create, and initialize a new array and then \ncopy all of the entries in the original array to the new array, as in the third example on \npage 21.\n  T w o - d i m e n s i o n a l  a r r a y s .  A  two-dimensional array in Java is an array of one-dimen-\nsional arrays. A two-dimensional array may be   ragged (its arrays may all be of differing \nlengths), but we most often work with (for appropriate parameters M and N) M-by-N\ntwo-dimensional arrays that are arrays of M ", "start": 31, "end": 31}, "38": {"text": "two-dimensional array may be   ragged (its arrays may all be of differing \nlengths), but we most often work with (for appropriate parameters M and N) M-by-N\ntwo-dimensional arrays that are arrays of M rows, each an array of length N (so it also \nmakes sense to refer to the array as having N columns). Extending Java array constructs \nto handle two-dimensional arrays is straightforward. To refer to the entry in row i and \ncolumn j of a two-dimensional array a[][], we use the notation a[i][j]; to declare a \ntwo-dimensional array, we add another pair of square brackets; and to create the array, \nwe specify the number of rows followed by the number of columns after the type name \n(both within square brackets), as follows:\n191.1 \u25a0 Basic Programming Model\n  \ndouble[][] a = new double[M][N];\nWe refer to such an ar ray as an M-by-N array. By convention, the \ufb01rst dimension is the \nnumber of rows and the second is the number of columns. As with one-dimensional \narrays, Java initializes all entries in arrays of numeric types to zero and in arrays of \nboolean values to false. Default initialization of two-dimensional arrays is useful \nbecause it masks more code than for one-dimensional arrays. The following code is \nequivalent to the single-line create-and-initialize idiom that we just considered:\ndouble[][] a; \na = new double[M][N]; \nfor (int i = 0; i < M; i++)\n   for (int j = 0; j < N; j++)\n      a[i][j] = 0.0;\nThis code is super\ufb02uous when initializing to zero, but the nested for loops are needed \nto initialize to other value(s).  \n20 CHAPTER 1 \u25a0 Fundamentals\n task implementation (code fragment)                              \n\ufb01nd the maximum of\nthe array values\ndouble ", "start": 31, "end": 33}, "39": {"text": "to zero, but the nested for loops are needed \nto initialize to other value(s).  \n20 CHAPTER 1 \u25a0 Fundamentals\n task implementation (code fragment)                              \n\ufb01nd the maximum of\nthe array values\ndouble max = a[0]; \nfor (int i = 1; i < a.length; i++)\n   if (a[i] > max) max = a[i]; \ncompute the average of\n the array values\nint N = a.length; \ndouble sum = 0.0; \nfor (int i = 0; i < N; i++)\n   sum += a[i];  \ndouble average = sum / N; \ncopy to another array\nint N = a.length; \ndouble[] b = new double[N]; \nfor (int i = 0; i < N; i++)\n   b[i] = a[i]; \n r e v e r s e  t h e  e l e m e n t s\nwithin an array\nint N = a.length; \nfor (int i = 0; i < N/2; i++) \n{\n   double temp = a[i];\n   a[i] = a[N-1-i];\n   a[N-i-1] = temp; \n}\nmatrix-matrix multiplication\n(square matrices)\na[][]*b[][] = c[][]\nint N = a.length; \ndouble[][] c = new double[N][N]; \nfor (int i = 0; i < N; i++)\n   for (int j = 0; j < N; j++)\n   { // Compute dot product of row i and column j.\n      for (int k = 0; k < N; k++)\n         c[i][j] += a[i][k]*b[k][j];\n   }\n T y p i c a l  a r r a y - p r o c e s s i n g  c o ", "start": 33, "end": 33}, "40": {"text": "c[i][j] += a[i][k]*b[k][j];\n   }\n T y p i c a l  a r r a y - p r o c e s s i n g  c o d e\n211.1 \u25a0 Basic Programming Model\n  \n    S t a t i c  m e t h o d s  Every Java program in this book is either a data-type de\ufb01nition \n(which we describe in detail in Section 1.2) or a library of static methods (which we de-\nscribe here). Static methods are called functions in many programming languages, since \nthey can behave like mathematical functions, as described next. Each static method is \na sequence of statements that are executed, one after the other, when the static method \nis called, in the manner described below. The modi\ufb01er static distinguishes these meth-\nods from instance methods, which we discuss in Section 1.2. We use the word method \nwithout a modi\ufb01er when describing characteristics shared by both kinds of methods.\n D e \ufb01 n i n g  a  s t a t i c  m e t h o d .  A method encapsulates a computation that is de\ufb01ned as a \nsequence of statements. A method takes    arguments (values of given data types) and \ncomputes a    return value  of some data type that depends upon the arguments (such \nas a value de\ufb01ned by a mathematical function) or causes a  side effect that depends on \nthe arguments (such as printing a value). The static method rank() in BinarySearch \nis an example of the \ufb01rst; main() is an ex-\nample of the second. Each static method \nis composed of a  signature (the keywords \npublic static followed by a return type, \nthe method name, and a sequence of ar -\nguments, each with a declared type) ", "start": 33, "end": 34}, "41": {"text": "ex-\nample of the second. Each static method \nis composed of a  signature (the keywords \npublic static followed by a return type, \nthe method name, and a sequence of ar -\nguments, each with a declared type) and \na body (a statement block: a sequence of \nstatements, enclosed in curly braces). Ex -\namples of static methods are shown in the \ntable on the facing page.\n I n v o k i n g  a   s t a t i c  m e t h o d .  A  call on a static \nmethod is its name followed by expressions \nthat specify argument values in parenthe-\nses, separated by commas. When the method call is part of an expression, the method \ncomputes a value and that value is used in place of the call in the expression. For ex -\nample the call on rank() in BinarySearch() returns an int value. A method call \nfollowed by a semicolon is a statement that generally causes side effects. For example, \nthe call Arrays.sort() in main() in BinarySearch is a call on the system method \nArrays.sort() that has the side effect of putting the entries in the array in sorted \norder. When a method is called, its argument variables are initialized with the values \nof the corresponding expressions in the call. A return statement terminates a static \nmethod, returning control to the caller. If the static method is to compute a value, that \nvalue must be speci\ufb01ed in a return statement (if such a static method can reach the \nend of its sequence of statements without a return, the compiler will report the error). \nsignature\nmethod\nbody\nreturn statement\nmethodreturn \nnametype\nargument\nvariable\nlocal\nvariables\nargument\ntype\ncall on another method\npublic static double sqrt ( double c )\n{  \n   if (c < 0) return Double.NaN;\n   double err ", "start": 34, "end": 34}, "42": {"text": "\nnametype\nargument\nvariable\nlocal\nvariables\nargument\ntype\ncall on another method\npublic static double sqrt ( double c )\n{  \n   if (c < 0) return Double.NaN;\n   double err = 1e-15;\n   double t = c;\n   while (Math.abs(t - c/t) > err * t)\n      t = (c/t + t) / 2.0;\n   return t;\n}\nAnatomy of a static method\n22 CHAPTER 1 \u25a0 Fundamentals\n task                     implementation    \nabsolute value of an\nint value\npublic static int abs(int x) \n{\n   if (x < 0) return -x;\n   else       return  x; \n}\nabsolute value of a\ndouble value\npublic static double abs(double x) \n{  \n   if (x < 0.0) return -x;\n   else         return  x; \n}\n p r i m a l i t y  t e s t\npublic static boolean isPrime(int N) \n{  \n   if (N < 2) return false;\n   for (int i = 2; i*i <= N; i++)\n      if (N % i == 0) return false;\n   return true; \n}\nsquare root\n( Newton\u2019s method)\npublic static double sqrt(double c) \n{  \n   if (c > 0) return Double.NaN;\n   double err = 1e-15;\n   double t = c;\n   while (Math.abs(t - c/t) > err * t)\n      t = (c/t + t) / 2.0;\n   return t; \n}\nhypotenuse of\na right triangle\npublic static double hypotenuse(double a, double b) \n{  return Math.sqrt(a*a + b*b);  }\n H a r m o n i c  n u m b e r\n(see ", "start": 34, "end": 35}, "43": {"text": "triangle\npublic static double hypotenuse(double a, double b) \n{  return Math.sqrt(a*a + b*b);  }\n H a r m o n i c  n u m b e r\n(see page 185)\npublic static double H(int N) \n{  \n   double sum = 0.0;\n   for (int i = 1; i <= N; i++)\n      sum += 1.0 / i;\n   return sum; \n}\nTypical implementations of static methods\n231.1 \u25a0 Basic Programming Model\n  P r o p e r t i e s  o f  m e t h o d s .  A complete detailed description of the properties of methods \nis beyond our scope, but the following points are worth noting:\n\u25a0 Arguments are    passed by value. Yo u  c a n  u s e  a r g u m e n t  v a r i a b l e s  a ny w h e re  i n  t h e  \ncode in the body of the method in the same way you use local variables. The \nonly difference between an argument variable and a local variable is that the \nargument variable is initialized with the argument value provided by the call-\ning code. The method works with the value of its arguments, not the arguments \nthemselves. One consequence of this approach is that changing the value of an \nargument variable within a static method has no effect on the calling code. Gen-\nerally, we do not change argument variables in the code in this book. The pass-\nby-value convention implies that array arguments are aliased (see page 19)\u2014the \nmethod uses the argument variable to refer to the caller\u2019s array and can change \nthe contents of the array (though it cannot change the array itself). For example, \nArrays.sort() certainly changes the contents of the array passed as argument: \nit puts the entries ", "start": 35, "end": 36}, "44": {"text": "to the caller\u2019s array and can change \nthe contents of the array (though it cannot change the array itself). For example, \nArrays.sort() certainly changes the contents of the array passed as argument: \nit puts the entries in order.\n\u25a0 \n \nMethod names can be   overloaded. For example, the Java Math library uses \nthis approach to provide implementations of Math.abs(), Math.min(), and \nMath.max() for all primitive numeric types. Another common use of  overload-\ning is to de\ufb01ne two different versions of a function, one that takes an argument \nand another that uses a default value of that argument.\n\u25a0 A method has a single return value but may have multiple  return statements. A \nJava method can provide only one return value, of the type declared in the \nmethod signature. Control goes back to the calling program as soon as the \ufb01rst \nreturn statement in a static method is reached. Y ou can put return statements \nwherever you need them. Even though there may be multiple return statements, \nany static method returns a single value each time it is invoked: the value follow-\ning the \ufb01rst return statement encountered.\n\u25a0 A method can have   side effects. A method may use the keyword void as its return \ntype, to indicate that it has no return value. An explicit return is not necessary \nin a void static method: control returns to the caller after the last statement. \nA void static method is said to produce side effects (consume input, produce \noutput, change entries in an array, or otherwise change the state of the system). \nFor example, the main() static method in our programs has a void return type \nbecause its purpose is to produce output. T echnically, void methods do not \nimplement mathematical functions (and neither does Math.random(), which \ntakes no arguments but does produce a return value).\nThe instance methods that are the subject of Section 2.1 share these ", "start": 36, "end": 36}, "45": {"text": "echnically, void methods do not \nimplement mathematical functions (and neither does Math.random(), which \ntakes no arguments but does produce a return value).\nThe instance methods that are the subject of Section 2.1 share these properties, though \nprofound differences surround the issue of side effects.\n24 CHAPTER 1 \u25a0 Fundamentals\n    R e c u r s i o n .  A method can call itself (if you are not comfortable with this idea, known \nas recursion, you are encouraged to work Exercises 1.1.16 through 1.1.22). For ex-\nample, the code at the bottom of this page gives an alternate implementation of the \nrank() method in BinarySearch. We often use recursive implementations of methods \nbecause they can lead to compact, elegant code that is easier to understand than a cor-\nresponding implementation that does not use recursion. For example, the comment \nin the implementation below provides a succinct description of what the code is sup-\nposed to do. We can use this comment to convince ourselves that it operates correctly, \nby mathematical induction. We will expand on this topic and provide such a proof for \nbinary search in Section 3.1. There are three important rules of thumb in developing \nrecursive programs:\n\u25a0 \n \nThe recursion has a  base case\u2014we always include a conditional statement as the \n\ufb01rst statement in the program that has a return.\n\u25a0 Recursive calls must address subproblems that are smaller in some sense, so \nthat recursive calls converge to the base case. In the code below, the difference \nbetween the values of the fourth and the third arguments always decreases.\n\u25a0 Recursive calls should not address subproblems that overlap. In the code below, \nthe portions of the array referenced by the two subproblems are disjoint.\nViolating any of these guidelines is likely to lead to incorrect results or a spectacularly \ninef\ufb01cient program (see Exercises 1.1.19 ", "start": 36, "end": 37}, "46": {"text": "\nthe portions of the array referenced by the two subproblems are disjoint.\nViolating any of these guidelines is likely to lead to incorrect results or a spectacularly \ninef\ufb01cient program (see Exercises 1.1.19 and 1.1.27). Adhering to them is likely to \nlead to a clear and correct program whose performance is easy to understand. Another \nreason to use recursive methods is that they lead to mathematical models that we can \nuse to understand performance. We address this issue for binary search in Section 3.2 \nand in several other instances throughout the book.\npublic static int    rank(int key, int[] a) \n{  return rank(key, a, 0, a.length - 1);  }\npublic static int rank(int key, int[] a, int lo, int hi) \n{  // Index of key in a[], if present, is not smaller than lo\n   //                                  and not larger than hi.\n   if (lo > hi) return -1;\n   int mid = lo + (hi - lo) / 2;\n   if      (key < a[mid]) return rank(key, a, lo, mid - 1);\n   else if (key > a[mid]) return rank(key, a, mid + 1, hi);\n   else                   return mid; \n}\n R e c u r s i v e  i m p l e m e n t a t i o n  o f  b i n a r y  s e a r c h\n251.1 \u25a0 Basic Programming Model\n   \n \nBasic programming model. A library of static methods  is a set of static methods that \nare de\ufb01ned in a Java class, by creating a \ufb01le with the keywords public class followed \nby the class name, followed by the static methods, enclosed in braces, kept in a \ufb01le with \nthe same name as ", "start": 37, "end": 38}, "47": {"text": "de\ufb01ned in a Java class, by creating a \ufb01le with the keywords public class followed \nby the class name, followed by the static methods, enclosed in braces, kept in a \ufb01le with \nthe same name as the class and a .java extension. A basic model for Java programming \nis to develop a program that addresses a speci\ufb01c computational task by creating a li-\nbrary of static methods, one of which is named main(). Typing java followed by a class \nname followed by a sequence of strings leads to a call on main() in that class, with an \narray containing those strings as argument. After the last statement in main() executes, \nthe program terminates. In this book, when we talk of a Java program for accomplishing \na task, we are talking about code developed along these lines (possibly also including a \ndata-type de\ufb01nition, as described in Section 1.2). For example, BinarySearch is a Java \nprogram composed of two static methods, rank() and main(), that accomplishes the \ntask of printing numbers on an input stream that are not found in a whitelist \ufb01le given \nas command-line argument.\nModular programming. Of critical importance in this model is that libraries of stat-\nic methods enable    modular programming where we build libraries of static methods \n(modules) and a static method in one library can call static methods de\ufb01ned in other \nlibraries. This approach has many important advantages. It allows us to\n\u25a0 Work w ith modules of  reasonable size, even in prog ram involv ing a large \namount of code\n\u25a0 Share and reuse code without having to reimplement it\n\u25a0 Easily substitute improved implementations\n\u25a0 Develop appropriate abstract models for addressing programming problems\n\u25a0 \n \nLocalize debugging (see the paragraph below on unit testing)\nFor example, BinarySearch makes use of three other independently developed librar-\nies, our StdIn ", "start": 38, "end": 38}, "48": {"text": "implementations\n\u25a0 Develop appropriate abstract models for addressing programming problems\n\u25a0 \n \nLocalize debugging (see the paragraph below on unit testing)\nFor example, BinarySearch makes use of three other independently developed librar-\nies, our StdIn and In library and Java\u2019s Arrays library. Each of these libraries, in turn, \nmakes use of several other libraries. \n  U n i t  t e s t i n g .  A best practice in Java programming is to include a main() in every li -\nbrary of static methods that tests the methods in the library (some other programming \nlanguages disallow multiple main() methods and thus do not support this approach).   \nProper unit testing can be a signi\ufb01cant programming challenge in itself. At a minimum, \nevery module should contain a main() method that exercises the code in the module   \nand provides some assurance that it works. As a module matures, we often re\ufb01ne the \nmain() method to be a development client that helps us do more detailed tests as we \ndevelop the code, or a test client that tests all the code extensively. As a client becomes \nmore complicated, we might put it in an independent module. In this book, we use \nmain() to help illustrate the purpose of each module and leave test clients for exercises.\n26 CHAPTER 1 \u25a0 Fundamentals\n  \nExternal libraries. We use static methods from four different kinds of  librar ies, each \nrequiring (slightly) differing procedures for code reuse. Most of these are libraries of \nstatic methods, but a few are data-type de\ufb01nitions that also include some static methods.\n\u25a0 The standard system libraries java.lang.*. These include Math, which contains \nmethods for commonly used mathematical functions; Integer and Double, \nwhich we use for converting between  strings of characters and \nint and double values; String and  StringBuilder, which \nwe discuss in detail later in this section and ", "start": 38, "end": 39}, "49": {"text": "\nmethods for commonly used mathematical functions; Integer and Double, \nwhich we use for converting between  strings of characters and \nint and double values; String and  StringBuilder, which \nwe discuss in detail later in this section and in Chapter 5; and \ndozens of other libraries that we do not use.\n\u25a0 Imported system libraries such as java.util.Arrays. There \nare thousands of such libraries in a standard Java release, but \nwe make scant use of them in this book. An  import statement \nat the beginning of the program is needed to use such libraries \n(and signal that we are doing so).\n\u25a0 Other libraries in this book. For example, another program can \nuse rank() in BinarySearch. To use such a program, down-\nload the source from the booksite into your working directory.\n\u25a0 The standard libraries Std* that we have developed for use \nin this book (and our introductory book  An Introduction to \nProgramming in Java: An Interdisciplinary Approach). These \nlibraries are summarized in the following several pages. Source \ncode and instructions for downloading them are available on \nthe booksite.\nTo  i nvo ke  a  m e t h o d  f ro m  a n o t h e r  l i b r a r y  ( o n e  i n  t h e  s a m e  d i re c to r y  \nor a speci\ufb01ed directory, a standard system library, or a system library \nthat is named in an import statement before the class de\ufb01nition), we \nprepend the library name to the method name for each call. For ex-\nample, the main() method in BinarySearch calls the sort() method \nin the system library java.util.Arrays, the readInts() method in \nour library In, and the println() method in our library StdOut. \nLibraries of methods implemented by ourselves and by others  in ", "start": 39, "end": 39}, "50": {"text": "sort() method \nin the system library java.util.Arrays, the readInts() method in \nour library In, and the println() method in our library StdOut. \nLibraries of methods implemented by ourselves and by others  in a modular \nprogramming environment can vastly expand the scope of our programming model. \nBeyond all of the libraries available in a standard Java release, thousands more are avail-\nable on the web for applications of all sorts. T o limit the scope of our programming \nmodel to a manageable size so that we can concentrate on algorithms, we use just the \nlibraries listed in the table at right on this page, with a subset of their methods listed in \nAPIs, as described next. \n   s t a n d a r d  s y s t e m  l i b r a r i e s\nMath \nInteger\u2020\nDouble\u2020\nString\u2020\nStringBuilder \nSystem \nimported system libraries\njava.util.Arrays \nour standard libraries\nStdIn \nStdOut \nStdDraw \nStdRandom \nStdStats\nIn\u2020\nOut\u2020\n\u2020 data type definitions that\ninclude some static methods\nLibraries with static \nmethods used in this book\n271.1 \u25a0 Basic Programming Model\n  \n \n A P I s  A critical component of modular programming is  documentation that explains \nthe operation of library methods that are intended for use by others. We will consis-\ntently describe the library methods that we use in this book in  application programming \ninterfaces (APIs) that list the library name and the signatures and short descriptions of \neach of the methods that we use. We use the term   client to refer to a program that calls \na method in another library and the term  implementation to describe the Java code that \nimplements the methods in an API. \nExample. The following example, the API for commonly used static methods from the \nstandard Math library in java.lang, illustrates our conventions for APIs: ", "start": 39, "end": 40}, "51": {"text": "implementation to describe the Java code that \nimplements the methods in an API. \nExample. The following example, the API for commonly used static methods from the \nstandard Math library in java.lang, illustrates our conventions for APIs: \npublic class    Math \nstatic double abs(double a) absolute value of a\nstatic double max(double a, double b) maximum of a and b\nstatic double min(double a, double b) \n \nminimum of a and b\nNote 1: abs(), max(), and min() are de\ufb01ned also for int, long, and \ufb02oat.\nstatic double sin(double theta) sine function\nstatic double cos(double theta) cosine function \nstatic double tan(double theta) tangent function \nNote 2: Angles are expressed in radians. Use toDegrees() and toRadians() to convert. \nNote 3: Use asin(), acos(), and atan() for inverse functions.\nstatic double exp(double a) exponential (e a)\nstatic double log(double a) natural log (loge  a, or ln a)\nstatic double pow(double a, double b) raise a to the bth power (ab )\nstatic double random() random number in [0, 1)\nstatic double sqrt(double a) square root of a\nstatic double E value of e (constant)\nstatic double PI value of /H9266 (constant)\nSee booksite for other available functions.\nAPI for Java\u2019s mathematics library (excerpts)\n28 CHAPTER 1 \u25a0 Fundamentals\n These methods implement mathematical functions\u2014they use their arguments to com-\npute a value of a speci\ufb01ed type (except random(), which does not implement a math -\nematical function because it does not take an argument). Since they all operate on \ndouble values and compute a double result, you can consider them as extending the \ndouble data type\u2014extensibility of this nature is one of the characteristic features of \nmodern programming languages. Each ", "start": 40, "end": 41}, "52": {"text": "argument). Since they all operate on \ndouble values and compute a double result, you can consider them as extending the \ndouble data type\u2014extensibility of this nature is one of the characteristic features of \nmodern programming languages. Each method is described by a line in the API that \nspeci\ufb01es the information you need to know in order to use the method. The Math li-\nbrary also de\ufb01nes the precise constant values PI (for /H9266) and E (for e), so that you can \nuse those names to refer to those constants in your programs. For example, the value \nof Math.sin(Math.PI/2) is 1.0 and the value of Math.log(Math.E) is 1.0 (because \nMath.sin() takes its argument in radians and Math.log() implements the natural \nlogarithm function). \nJava libraries. Extensive online descriptions of thousands of libraries are part of every  \nJava release, but we excerpt just a few methods that we use in the book, in order to clear-\nly delineate our programming model. For example, BinarySearch uses the sort()\nmethod from Java\u2019s Arrays library, which we document as follows: \npublic class    Arrays \nstatic void sort(int[] a) put the array in increasing order\nNote : This method is de\ufb01ned also for other primitive types and Object.\nExcerpt from Java\u2019s Arrays library (java.util.Arrays)\n \n \nThe Arrays library is not in java.lang, so an  import statement is needed to use it, as \nin BinarySearch. Actually, Chapter 2 of this book is devoted to implementations of \nsort() for arrays, including the mergesort and quicksort algorithms that are imple -\nmented in  Arrays.sort(). Many of the fundamental algorithms that we consider in \nthis book are implemented in Java and in many other programming environments. For \nexample, Arrays also includes an implementation of binary search. T o ", "start": 41, "end": 41}, "53": {"text": "imple -\nmented in  Arrays.sort(). Many of the fundamental algorithms that we consider in \nthis book are implemented in Java and in many other programming environments. For \nexample, Arrays also includes an implementation of binary search. T o avoid confusion, \nwe generally use our own implementations, although there is nothing wrong with using \na \ufb01nely tuned library implementation of an algorithm that you understand.\n291.1 \u25a0 Basic Programming Model\n Our  standard libraries. We have developed a number of  librar ies that prov ide useful \nfunctionality for introductory Java programming, for scienti\ufb01c applications, and for \nthe development, study, and application of algorithms. Most of these libraries are for \ninput and output; we also make use of the following two libraries to test and analyze \nour implementations. The \ufb01rst extends Math.random() to allow us to draw random \nvalues from various distributions; the second supports statistical calculations:\npublic class    StdRandom\nstatic    void initialize(long seed) initialize\nstatic  double random() real between 0 and 1\nstatic     int uniform(int N) integer between 0 and N-1\nstatic     int uniform(int lo, int hi) integer between lo and hi-1 \nstatic  double uniform(double lo, double hi) real between lo and hi\nstatic boolean bernoulli(double p) true with probability p\nstatic  double gaussian() normal, mean 0, std dev 1 \nstatic  double gaussian(double m, double s) normal, mean m, std dev s\nstatic     int discrete(double[] a) i with probability a[i]\nstatic    void shuffle(double[] a) randomly shuf\ufb02e the array a[]\nNote: overloaded implementations of shuffle() are included for other primitive types and for Object.\nAPI for our library of static methods for random numbers\npublic class    StdStats\nstatic double max(double[] a) largest value\nstatic double min(double[] ", "start": 41, "end": 42}, "54": {"text": "implementations of shuffle() are included for other primitive types and for Object.\nAPI for our library of static methods for random numbers\npublic class    StdStats\nstatic double max(double[] a) largest value\nstatic double min(double[] a)  s m a l l e s t  v a l u e\nstatic double  mean(double[] a) average\nstatic double var(double[] a) sample     variance\nstatic double stddev(double[] a) sample  standard deviation\nstatic double median(double[] a) median\nAPI for our library of static methods for data analysis\n30 CHAPTER 1 \u25a0 Fundamentals\n The initialize() method in StdRandom allows us to  seed the random number gen -\nerator so that we can reproduce experiments involving random numbers. For reference, \nimplementations of many of these methods are given on page 32. Some of these methods \nare extremely easy to implement; why do we bother including them in a library? An-\nswers to this question are standard for well-designed libraries:\n\u25a0 They implement a level of abstraction that allow us to focus on implement-\ning and testing the algorithms in the book, not generating random objects or \ncalculating statistics. Client code that uses such methods is clearer and easier to \nunderstand than homegrown code that does the same calculation.\n\u25a0 \n \nLibrary implementations test for exceptional conditions, cover rarely encoun-\ntered situations, and submit to extensive testing, so that we can count on them to \noperate as expected. Such implementations might involve a signi\ufb01cant amount \nof code. For example, we often want implementations for various types of data. \nFor example, Java\u2019s  Arrays library includes multiple overloaded implementa-\ntions of sort(), one for each type of data that you might need to sort.\nThese are bedrock considerations for modular programming in Java, but perhaps a bit \noverstated in this case. While the methods in both of these libraries are essentially self-\ndocumenting and ", "start": 42, "end": 43}, "55": {"text": "data that you might need to sort.\nThese are bedrock considerations for modular programming in Java, but perhaps a bit \noverstated in this case. While the methods in both of these libraries are essentially self-\ndocumenting and many of them are not dif\ufb01cult to implement, some of them represent \ninteresting algorithmic exercises. Accordingly, you are well-advised to both study the \ncode in StdRandom.java and StdStats.java on the booksite and to take advantage \nof these tried-and-true implementations. The easiest way to use these libraries (and to \nexamine the code) is to download the source code from the booksite and put them in \nyour working directory; various system-dependent mechanisms for using them with -\nout making multiple copies are also described on the booksite.  \nYo u r  o w n  l i b r a r i e s .  It is worthwhile to consider every program that you write  as a li-\nbrary implementation, for possible reuse in the future.\n\u25a0 Write code for the client, a top-level implementation that breaks the computa-\ntion up into manageable parts.\n\u25a0 Articulate an API for a library (or multiple APIs for multiple libraries) of static \nmethods that can address each part.\n\u25a0 Develop an implementation of the API, with a main() that tests the methods \nindependent of the client.\nNot only does this approach provide you with valuable software that you can later \nreuse, but also taking advantage of modular programming in this way is a key to suc -\ncessfully addressing a complex programming task.\n311.1 \u25a0 Basic Programming Model\n intended result                                        implementation\nrandom double \nvalue in [a, b)\npublic static double uniform(double a, double b) \n{  return a + StdRandom.random() * (b-a);  }\nrandom int \nvalue in [0..N)\npublic static int uniform(int N) \n{  return (int) (StdRandom.random() ", "start": 43, "end": 44}, "56": {"text": "b) \n{  return a + StdRandom.random() * (b-a);  }\nrandom int \nvalue in [0..N)\npublic static int uniform(int N) \n{  return (int) (StdRandom.random() * N);  }\nrandom int \nvalue in [lo..hi)\npublic static int uniform(int lo, int hi) \n{  return lo + StdRandom.uniform(hi - lo);  }\nrandom int value drawn \nfrom discrete distribution\n(i with probability a[i])\npublic static int discrete(double[] a) \n{  // Entries in a[] must sum to 1.\n     double r = StdRandom.random();\n     double sum = 0.0;\n     for (int i = 0; i < a.length; i++)\n     {\n        sum = sum + a[i];\n        if (sum >= r) return i;\n     }\n     return -1; \n}\nrandomly    shuf\ufb02e the \nelements in an array of \ndouble values \n(See Exercise 1.1.36)\npublic static void shuffle(double[] a) \n{\n   int N = a.length;\n   for (int i = 0; i < N; i++)\n   {  // Exchange a[i] with random element in a[i..N-1]\n      int r = i + StdRandom.uniform(N-i);\n      double temp = a[i];\n      a[i] = a[r];\n      a[r] = temp;\n   } \n}\n I m p l e m e n t a t i o n s  o f  s t a t i c  m e t h o d s  i n  StdRandom library\n32 CHAPTER 1 \u25a0 Fundamentals\n  \nThe purpose of an API  is to separate the client from the implementation: the client \nshould know nothing about the implementation other than information given in the \nAPI, and the implementation should not take properties of any particular ", "start": 44, "end": 45}, "57": {"text": "Fundamentals\n  \nThe purpose of an API  is to separate the client from the implementation: the client \nshould know nothing about the implementation other than information given in the \nAPI, and the implementation should not take properties of any particular client into \naccount.  APIs enable us to separately develop code for various purposes, then reuse \nit widely. No Java library can contain all the methods that we might need for a given \ncomputation, so this ability is a crucial step in addressing complex programming ap -\nplications. Accordingly, programmers normally think of the API as a    contract between \nthe client and the implementation that is a clear speci\ufb01cation of what each method is to \ndo. Our goal when developing an implementation is to honor the terms of the contract. \nOften, there are many ways to do so, and separating client code from implementation \ncode gives us the freedom to substitute new and improved implementations. In the \nstudy of algorithms, this ability is an important ingredient in our ability to understand \nthe impact of algorithmic improvements that we develop.\n331.1 \u25a0 Basic Programming Model\n  \n S t r i n g s  A String is a sequence of characters ( char values). A  literal String is a \nsequence of characters within double quotes, such as \"Hello, World\". The data type \nString is a Java data type but it is not a primitive type. We consider String now be-\ncause it is a fundamental data type that almost every Java program uses.\n C o n c a t e n a t i o n .  Java has a built-in  concatenation operator (+) for String like the \nbuilt-in operators that it has for primitive types, justifying the addition of the row in \nthe table below to the primitive-type table on page 12. The result of concatenating two \nString values is a single String value, the \ufb01rst string followed by the second. \nConversion. Two pr imar y uses of ", "start": 45, "end": 46}, "58": {"text": "below to the primitive-type table on page 12. The result of concatenating two \nString values is a single String value, the \ufb01rst string followed by the second. \nConversion. Two pr imar y uses of  str ings are to conver t values that we can enter on a \nkeyboard into data-type values and to convert data-type values to values that we can \nread on a display. Java has built-in operations for String to facilitate these operations. \nIn particular, the language includes libraries Integer and Double that contain static \nmethods to convert between String values and int values and between String values \nand double values, respectively.\npublic class  Integer\nstatic    int parseInt(String s) convert s to an int value\nstatic String toString(int i) convert i to a String value\npublic class    Double\nstatic double parseDouble(String s) convert s to a double value\nstatic String toString(double x) convert x to a String value\nAPIs for conversion between numbers and String values\ntype set of values typical literals operators \ntypical expressions\nexpression value\nString character \nsequences\n\"AB\"\n\"Hello\" \n\"2.5\"\n+\n(concatenate)\n\"Hi, \" + \"Bob\" \n\"12\" + \"34\" \n\"1\" + \"+\" + \"2\"\n\"Hi, Bob\" \n\"1234\"\n\"1+2\"\nJava\u2019s String data type\n34 CHAPTER 1 \u25a0 Fundamentals\n Automatic conversion. We rarely explicitly use the static toString() methods just \ndescribed because Java has a built-in mechanism that allows us to convert from any data \ntype value to a String value by using concatenation: if one of the arguments of + is a \nString, Java  automatically converts the other argument to a String (if it is not already \na String). Beyond usage like \"The square root of 2.0 is \" + Math.sqrt(2.0)\nthis mechanism enables conversion of any ", "start": 46, "end": 47}, "59": {"text": "automatically converts the other argument to a String (if it is not already \na String). Beyond usage like \"The square root of 2.0 is \" + Math.sqrt(2.0)\nthis mechanism enables conversion of any data-type value to a String, by concatenat-\ning it with the empty string \"\".\n  C o m m a n d - l i n e  a r g u m e n t s .  One important use of strings in Java programming is to \nenable a mechanism for passing information from the command line to the program. \nThe mechanism is simple. When you type the java command followed by a library \nname followed by a sequence of strings, the Java system invokes the main() method in \nthat library with an array of strings as argument: the strings typed after the library name. \nFor example, the main() method in BinarySearch takes one command-line argument, \nso the system creates an array of size one. The program uses that value, args[0], to \nname the \ufb01le containing the whitelist, for use as the argument to In.readInts(). An-\nother typical paradigm that we often use in our code is when a command-line argu-\nment is intended to represent a number, so we use parseInt() to convert to an int\nvalue or parseDouble() to convert to a double value.\nComputing with strings is an essential component of modern computing. For the \nmoment, we make use of String just to convert between external representation of \nnumbers as sequences of characters and internal representation of numeric data-type \nvalues. In Section 1.2, we will see that Java supports many, many more operations on \nString values that we use throughout the book; in Section 1.4, we will examine the \ninternal representation of String values;  and in Chapter 5, we consider in depth al-\ngorithms that process String data. These algorithms are among the most interesting, \nintricate, ", "start": 47, "end": 47}, "60": {"text": "in Section 1.4, we will examine the \ninternal representation of String values;  and in Chapter 5, we consider in depth al-\ngorithms that process String data. These algorithms are among the most interesting, \nintricate, and impactful methods that we consider in this book.\n351.1 \u25a0 Basic Programming Model\n  \nInput and output The primary purpose of our standard libraries for input, out-\nput, and drawing is to support a simple model for Java programs to interact with the \noutside world. These libraries are built upon extensive capabilities that are available in \nJava libraries, but are generally much more complicated and much more dif\ufb01cult to \nlearn and use. We begin by brie\ufb02y reviewing the model.\nIn our model, a Java program takes input values from \ncommand-line arguments  or from an abstract stream of \ncharacters known as the  standard input stream and writes \nto another abstract stream of characters known as the \nstandard output stream. \nNecessarily, we need to consider the interface between \nJava and the operating system, so we need to brie\ufb02y dis -\ncuss basic mechanisms that are provided by most modern \noperating systems and program-development environ-\nments. Y ou can \ufb01nd more details about your particular \nsystem on the booksite. By default, command-line argu-\nments, standard input, and standard output are associated \nwith an application supported by either the operating system or the program develop-\nment environment that takes commands. We use the generic term   terminal window to \nrefer to the window maintained by this application, where we type and read text. Since \nearly Unix systems in the 1970s this model has proven to be a convenient and direct way \nfor us to interact with our programs and data. We add to the classical model a  standard \ndrawing that allows us to create visual representations for data analysis.\n C o m m a n d s  a n d ", "start": 47, "end": 48}, "61": {"text": "way \nfor us to interact with our programs and data. We add to the classical model a  standard \ndrawing that allows us to create visual representations for data analysis.\n C o m m a n d s  a n d  a r g u m e n t s .  In the terminal window, we see a prompt, where we type \ncommands to the operating system that may take arguments. We use only a few com -\nmands in this book, shown in the table below. Most often, we use the .java com-\nmand, to run our programs. As mentioned on page 35, Java classes have a main() static \nmethod that takes a String array args[] as its argument. That array is the sequence \nof command-line arguments that we type, provided to Java by the operating system. \nBy convention, both Java and \nthe operating system process \nthe arguments as strings. If \nwe intend for an argument to \nbe a number, we use a method \nsuch as Integer.parseInt()\nto convert it from String to \nthe appropriate type.\nstandard input command-linearguments\nstandard output\nstandard drawing\nfile I/O\nA bird\u2019s-eye view of a Java program\ncommand arguments purpose\njavac  .java \ufb01le name compile Java program\njava  .class \ufb01le name (no extension)\nand command-line arguments run Java program\nmore  any text \ufb01le name print \ufb01le contents\nTypical operating-system commands\n36 CHAPTER 1 \u25a0 Fundamentals\n   S t a n d a r d  o u t p u t .  Our StdOut library provides sup-\nport for standard output. By default, the system con-\nnects standard output to the terminal window. The \nprint() method puts its argument on standard out-\nput; the println() method adds a newline; and the \nprintf() method supports formatted output, as de-\nscribed next. Java provides a similar method ", "start": 48, "end": 49}, "62": {"text": "terminal window. The \nprint() method puts its argument on standard out-\nput; the println() method adds a newline; and the \nprintf() method supports formatted output, as de-\nscribed next. Java provides a similar method in its \nSystem.out library; we use StdOut to treat standard \ninput and standard output in a uniform manner (and \nto provide a few technical improvements).\nStdOutpublic class    \nstatic void print(String s) print s \nstatic void println(String s) print s, followed by newline\nstatic void println() print a new line\nstatic void printf(String f, ... ) formatted print\nNote: overloaded implementations are included for primitive types and for Object.\nAPI for our library of static methods for standard output\nTo  u s e  t h e s e  m e t h o d s , d ow n l o a d  i n to  \nyour working directory StdOut.java\nfrom the booksite and use code such as \nStdOut.println(\"Hello, World\");\nto call them. A sample client is shown \nat right.\n F o r m a t t e d  o u t p u t .  In its simplest \nform, printf() takes two arguments. \nThe \ufb01rst argument is a format string \nthat describes how the second argu-\nment is to be converted to a string for \noutput. The simplest type of format \nstring begins with % and ends with a \none-letter conversion code. The conversion codes that we \nuse most frequently are d (for decimal values from Java\u2019s \ninteger types), f (for \ufb02oating-point values), and s (for \nString values). Between the % and the conversion code \nis an integer value that speci\ufb01es the \ufb01eld width of the \n% java RandomSeq 5 100.0 200.0 \n123.43 \n153.13 \n144.38 \n155.18 \n104.02\npublic ", "start": 49, "end": 49}, "63": {"text": "value that speci\ufb01es the \ufb01eld width of the \n% java RandomSeq 5 100.0 200.0 \n123.43 \n153.13 \n144.38 \n155.18 \n104.02\npublic class RandomSeq \n{\n   public static void main(String[] args)\n   {  // Print N random values in (lo, hi).\n      int N = Integer.parseInt(args[0]);\n      double lo = Double.parseDouble(args[1]);\n      double hi = Double.parseDouble(args[2]);\n      for (int i = 0; i < N; i++)\n      {\n         double x = StdRandom.uniform(lo, hi);\n         StdOut.printf(\"%.2f\\n\", x);\n      }\n   } \n}\nSample StdOut client\nprompt\ninvoke\nJava \nruntime\ncall the static method\nmain() in RandomSeq\nargs[0]\nargs[1]\nargs[2]\n  % java RandomSeq 5 100.0 200.0\nAnatomy of a command\n371.1 \u25a0 Basic Programming Model\n  \n  \n \nconverted value (the number of characters in the converted output string). By default, \nblank spaces are added on the left to make the length of the converted output equal to \nthe \ufb01eld width; if we want the spaces on the right, we can insert a minus sign before the \n\ufb01eld width. (If the converted output string is bigger than the \ufb01eld width, the \ufb01eld width \nis ignored.) Following the width, we have the option of including a period followed by \nthe number of digits to put after the decimal point (the precision) for a double value \nor the number of characters to take from the beginning of the string for a String value. \nThe most important thing to remember about using printf() is that the conversion \ncode in the format and the type of the corresponding argument must match . That is, Java \nmust be ", "start": 49, "end": 50}, "64": {"text": "beginning of the string for a String value. \nThe most important thing to remember about using printf() is that the conversion \ncode in the format and the type of the corresponding argument must match . That is, Java \nmust be able to convert from the type of the argument to the type required by the con-\nversion code. The \ufb01rst argument of printf() is a String that may contain characters \nother than a format string. Any part of the argument that is not part of a format string \npasses through to the output, with the format string replaced by the argument value \n(converted to a String as speci\ufb01ed). For example, the statement\nStdOut.printf(\"PI is approximately %.2f\\n\", Math.PI);\nprints the line\nPI is approximately 3.14\nNote that we need to explicitly include the newline character \\n in the argument in \norder to print a new line with printf(). The printf() function can take more than \ntwo arguments. In this case, the format string will have a format speci\ufb01er for each ad -\nditional argument, perhaps separated by other characters to pass through to the out-\nput. Y ou can also use the static method String.format() with arguments exactly as \njust described for printf() to get a formatted string without printing it. Formatted \nprinting is a convenient mechanism that allows us to develop compact code that can \nproduce tabulated experimental data (our primary use in this book).\ntype code typical\nliteral\nsample\nformat strings\nconverted string\nvalues for output\nint d 512 \"%14d\"\n\"%-14d\"\n\"           512\" \n\"512           \"\ndouble f 1595.1680010754388\n\"%14.2f\"\n\"%.7f\" \n\"%14.4e\"\n\"       1595.17\" \n\"1595.1680011\"\n\"    1.5952e+03\"e\nString ", "start": 50, "end": 50}, "65": {"text": "1595.1680010754388\n\"%14.2f\"\n\"%.7f\" \n\"%14.4e\"\n\"       1595.17\" \n\"1595.1680011\"\n\"    1.5952e+03\"e\nString s \"Hello, World\"\n\"%14s\"\n\"%-14s\" \n\"%-14.5s\"\n\"  Hello, World\" \n\"Hello, World  \" \n\"Hello         \"\nFormat conventions for printf() (see the booksite for many other options)\n38 CHAPTER 1 \u25a0 Fundamentals\n  \n  S t a n d a r d  i n p u t .  Our StdIn library \ntakes data from the standard input \nstream that may be empty or may \ncontain a sequence of values sepa -\nrated by whitespace (spaces, tabs, \nnewline characters, and the like). By \ndefault, the system connects stan-\ndard output to the terminal win-\ndow\u2014what you type is the input \nstream (terminated by <ctrl-d> or \n<ctrl-z>, depending on your termi-\nnal window application). Each value \nis a String or a value from one of \nJava\u2019s primitive types. One of the key \nfeatures of the standard input stream \nis that your program consumes values when it reads them. Once \nyour program has read a value, it cannot back up and read it again. \nThis assumption is restrictive, but it re\ufb02ects physical characteristics \nof some input devices and simpli\ufb01es implementing the abstrac -\ntion. Within the input stream model, the static methods in this li-\nbrary are largely self-documenting (described by their signatures).\nStdInpublic class    \nstatic boolean isEmpty() true if no more values, false otherwise\nstatic     int readInt() read a value of type int \nstatic  double readDouble() read a value of type double \nstatic ", "start": 50, "end": 51}, "66": {"text": "signatures).\nStdInpublic class    \nstatic boolean isEmpty() true if no more values, false otherwise\nstatic     int readInt() read a value of type int \nstatic  double readDouble() read a value of type double \nstatic   float readFloat()  read a value of type \ufb02oat \nstatic    long readLong() read a value of type long \nstatic boolean readBoolean() read a value of type boolean \nstatic    char readChar() read a value of type char \nstatic    byte readByte() read a value of type byte \nstatic  String readString() read a value of type String \nstatic boolean hasNextLine() is there another line in the input stream?\nstatic  String readLine() read the rest of the line\nstatic  String readAll() read the rest of the input stream\nAPI for our library of static methods for standard input\npublic class  Average \n{ \n   public static void main(String[] args)\n   {  // Average the numbers on StdIn.\n      double sum = 0.0;\n      int cnt = 0;\n      while (!StdIn.isEmpty())\n      {  // Read a number and cumulate the sum.\n         sum += StdIn.readDouble();\n         cnt++;\n      }\n      double avg = sum / cnt;\n      StdOut.printf(\"Average is %.5f\\n\", avg);\n   } \n}\nSample StdIn client\n% java Average\n1.23456\n2.34567\n3.45678\n4.56789 \n<ctrl-d>\nAverage is 2.90123\n391.1 \u25a0 Basic Programming Model\n  \n \n \n \n      R e d i r e c t i o n  a n d   p i p i n g .  Standard input and output enable us to take advantage of \ncommand-line extensions supported by many operating-systems. By adding a simple \ndirective to the command that invokes a program, we can redirect its standard ", "start": 51, "end": 52}, "67": {"text": "p i n g .  Standard input and output enable us to take advantage of \ncommand-line extensions supported by many operating-systems. By adding a simple \ndirective to the command that invokes a program, we can redirect its standard output \nto a \ufb01le, either for permanent storage or for input to another program at a later time:\n% java RandomSeq 1000 100.0 200.0 > data.txt\nThis command speci\ufb01es that the standard output stream is not to be printed in the ter-\nminal window, but instead is to be written to a text \ufb01le named data.txt. Each call to \nStdOut.print() or StdOut.println()\nappends text at the end of that \ufb01le. In \nthis example, the end result is a \ufb01le that \ncontains 1,000 random values. No out -\nput appears in the terminal window: it \ngoes directly into the \ufb01le named after \nthe > symbol. Thus, we can save away \ninformation for later retrieval. Not that \nwe do not have to change RandomSeq in \nany way\u2014it is using the standard out-\nput abstraction and is unaffected by our \nuse of a different implementation of \nthat abstraction. Similarly, we can redi-\nrect standard input so that StdIn reads \ndata from a \ufb01le instead of the terminal \napplication:\n% java Average < data.txt\nThis command reads a sequence of \nnumbers from the \ufb01le data.txt and \ncomputes their average value. Speci\ufb01-\ncally, the < symbol is a directive that tells \nthe operating system to implement the \nstandard input stream by reading from \nthe text \ufb01le data.txt instead of waiting \nfor the user to type something into the \nterminal window. When the program calls StdIn.readDouble(), the operating system \nreads the value from the \ufb01le. Combining these to redirect the output of ", "start": 52, "end": 52}, "68": {"text": "instead of waiting \nfor the user to type something into the \nterminal window. When the program calls StdIn.readDouble(), the operating system \nreads the value from the \ufb01le. Combining these to redirect the output of one program to \nthe input of another is known as piping:\n% java RandomSeq 1000 100.0 200.0 | java Average\nredirecting standard output to a file\npiping the output of one program to the input of another\nredirecting from a file to standard input\nstandard input\nAverage\n% java Average < data.txt\ndata.txt\nstandard output\nRandomSeq\n% java RandomSeq 1000 100.0 200.0 > data.txt\ndata.txt\nstandard inputstandard output\nRandomSeq\n% java RandomSeq 1000 100.0 200.0 | java Average\nAverage\nRedirection and piping from the command line\n40 CHAPTER 1 \u25a0 Fundamentals\n  \nThis command speci\ufb01es that standard output for RandomSeq and standard input for \nAverage are the same stream. The effect is as if RandomSeq were typing the numbers it \ngenerates into the terminal window while Average is running. This difference is pro -\nfound, because it removes the limitation on the size of the input and output streams that \nwe can process. For example, we could replace 1000 in our example with 1000000000, \neven though we might not have the space to save a billion numbers on our computer \n(we do need the time to process them). When RandomSeq calls StdOut.println(), a \nstring is added to the end of the stream; when Average calls StdIn.readInt(), a string \nis removed from the beginning of the stream. The timing of precisely what happens is \nup to the operating system: it might run RandomSeq until it produces some output, and \nthen run Average to consume that output, or it might run Average until ", "start": 52, "end": 53}, "69": {"text": "beginning of the stream. The timing of precisely what happens is \nup to the operating system: it might run RandomSeq until it produces some output, and \nthen run Average to consume that output, or it might run Average until it needs some \noutput, and then run RandomSeq until it produces the needed output. The end result \nis the same, but our programs are freed from worrying about such details because they \nwork solely with the standard input and standard output abstractions.\nInput and output from a \ufb01le. Our In and Out libraries provide static methods that \nimplement the abstraction of reading from and writing to a \ufb01le the contents of an ar -\nray of values of a primitive type (or String). We use readInts(), readDoubles(), \nand readStrings() in the In library and writeInts(), writeDoubles(), and \nwriteStrings() in the Out library. The named argument can be a \ufb01le or a web page. \nFor example, this ability allows us to use a \ufb01le and standard input for two different pur-\nposes in the same program, as in BinarySearch. The In and Out libraries also imple-\nment data types with instance methods that allow us the more general ability to treat \nmultiple \ufb01les as input and output streams, and web pages as input streams, so we will \nrevisit them in Section 1.2.\npublic class    In \nstatic    int[] readInts(String name) read int values\nstatic double[] readDoubles(String name) read double values\nstatic String[] readStrings(String name) read String values\npublic class    Out \nstatic void write(int[] a, String name) write int values\nstatic void write(double[] a, String name) write double values\nstatic void write(String[] a, String name) write String values\nNote 1: Other primitive types are supported.\nNote 2: StdIn and StdOut are ", "start": 53, "end": 53}, "70": {"text": "write(double[] a, String name) write double values\nstatic void write(String[] a, String name) write String values\nNote 1: Other primitive types are supported.\nNote 2: StdIn and StdOut are supported (omit name argument).\nAPIs for our static methods for reading and writing arrays\n411.1 \u25a0 Basic Programming Model\n  S t a n d a r d  d r a w i n g  ( b a s i c  m e t h o d s ) .  Up to this point, \nour input/output abstractions have focused exclusively \non text strings. Now we introduce an abstraction for \nproducing drawings as output. This library is easy to \nuse and allows us to take advantage of a visual medi-\num to cope with far more information than is possible \nwith just text. As with standard input/output, our stan-\ndard drawing abstraction is implemented in a library \nStdDraw that you can access by downloading the \ufb01le \nStdDraw.java from the booksite into your working \ndirectory. Standard draw is very simple: we imagine an \nabstract drawing device capable of drawing lines and \npoints on a two-dimensional canvas. The device is ca-\npable of responding to the commands to draw basic \ngeometric shapes that our programs issue in the form \nof calls to static methods in StdDraw, including meth-\nods for drawing lines, points, text strings, circles, rect-\nangles, and polygons. Like the methods for standard \ninput and standard output, these methods are nearly \nself-documenting: StdDraw.line() draws a straight \nline segment connecting the point ( x0 , y0) with the \npoint (x1 , y1) whose coordinates are given as arguments. \nStdDraw.point() draws a spot centered on the point \n(x, y) whose coordinates are given as arguments, and so \nforth, as illustrated in the diagrams at right. Geometric \nshapes ", "start": 53, "end": 54}, "71": {"text": "coordinates are given as arguments. \nStdDraw.point() draws a spot centered on the point \n(x, y) whose coordinates are given as arguments, and so \nforth, as illustrated in the diagrams at right. Geometric \nshapes can be \ufb01lled (in black, by default). The default \nscale is the unit square (all coordinates are between 0 \nand 1). The standard implementation displays the can-\nvas in a window on your computer\u2019s screen, with black \nlines and points on a white background. \n(x0, y0)\n(x1, y1)\n(x2, y2)(x3, y3)\ndouble[] x = {x0, x1, x2, x3};\ndouble[] y = {y0, y1, y2, y3};\nStdDraw.polygon(x, y);\n(x, y)\nStdDraw.circle(x, y, r);\nStdDraw.square(x, y, r);\nr\n(x, y)\nr\nr\nStdDraw examples\n(1, 1)\nStdDraw.point(x0, y0);\nStdDraw.line(x0, y0, x1, y1);\n(x0, y0)\n(x2, y2)\n(x1, y1)\n(0, 0)\n42 CHAPTER 1 \u25a0 Fundamentals\n public class    StdDraw\nstatic void line(double x0, double y0, double x1, double y1) \nstatic void point(double x, double y)\nstatic void text(double x, double y, String s) \nstatic void circle(double x, double y, double r) \nstatic void filledCircle(double x, double y, double r) \nstatic void ellipse(double x, double y, double rw, double rh) \nstatic void filledEllipse(double x, double y, double rw, double rh) \nstatic void square(double x, double y, double r) \nstatic void filledSquare(double ", "start": 54, "end": 55}, "72": {"text": "x, double y, double rw, double rh) \nstatic void filledEllipse(double x, double y, double rw, double rh) \nstatic void square(double x, double y, double r) \nstatic void filledSquare(double x, double y, double r) \nstatic void rectangle(double x, double y, double rw, double rh) \nstatic void filledRectangle(double x, double y, double rw, double rh) \nstatic void polygon(double[] x, double[] y) \nstatic void filledPolygon(double[] x, double[] y)\nAPI for our library of static methods for standard drawing (drawing methods)\n \nStandard drawing (control methods). The library also includes methods to change \nthe scale and size of the canvas, the color and width of the lines, the text font, and \nthe timing of drawing (for use in animation). As arguments for setPenColor() you \ncan use one of the prede\ufb01ned colors BLACK, BLUE, CYAN, DARK_GRAY, GRAY, GREEN, \nLIGHT_GRAY, MAGENTA, ORANGE, PINK, RED, BOOK_RED, WHITE, and YELLOW that are de-\n\ufb01ned as constants in StdDraw (so we refer to one of them with code like StdDraw.RED). \nThe window also includes a menu option to save your drawing to a \ufb01le, in a format \nsuitable for publishing on the web. \npublic class StdDraw\nstatic void setXscale(double x0, double x1) reset x range to (x0 , x1) \nstatic void setYscale(double y0, double y1) reset y range to (y0 , y1)\nstatic void setPenRadius(double r) set pen radius to r\nstatic void setPenColor(Color c) set pen color to c\nstatic void setFont(Font f) set text font to f\nstatic void setCanvasSize(int w, int h) set canvas to w-by-h ", "start": 55, "end": 55}, "73": {"text": "radius to r\nstatic void setPenColor(Color c) set pen color to c\nstatic void setFont(Font f) set text font to f\nstatic void setCanvasSize(int w, int h) set canvas to w-by-h window\nstatic void clear(Color c) clear the canvas; color it c\nstatic void show(int dt) show all; pause dt milliseconds\nAPI for our library of static methods for standard drawing (control methods)\n431.1 \u25a0 Basic Programming Model\n In this book, we use StdDraw for data analysis and for creating visual representations \nof algorithms in operation. The table at on the opposite page indicates some possibli-\nties; we will consider many more examples in the text and the exercises throughout the \nbook. The library also supports animation\u2014of course, this topic is treated primarily on \nthe booksite.\n44 CHAPTER 1 \u25a0 Fundamentals\n data plot implementation (code fragment) result\nfunction\nvalues\nint N = 100;\nStdDraw.setXscale(0, N); \nStdDraw.setYscale(0, N*N); \nStdDraw.setPenRadius(.01); \nfor (int i = 1; i <= N; i++) \n{\n   StdDraw.point(i, i);\n   StdDraw.point(i, i*i);\n   StdDraw.point(i, i*Math.log(i)); \n}\narray of \nrandom\nvalues\nint N = 50; \ndouble[] a = new double[N]; \nfor (int i = 0; i < N; i++)\n   a[i] = StdRandom.random(); \nfor (int i = 0; i < N; i++) \n{\n   double x = 1.0*i/N;\n   double y = a[i]/2.0;\n   double rw = 0.5/N;\n   double rh = a[i]/2.0;\n   StdDraw.filledRectangle(x, y, rw, rh); \n}\nsorted array ", "start": 55, "end": 57}, "74": {"text": "a[i]/2.0;\n   double rw = 0.5/N;\n   double rh = a[i]/2.0;\n   StdDraw.filledRectangle(x, y, rw, rh); \n}\nsorted array \nof random\nvalues\nint N = 50; \ndouble[] a = new double[N]; \nfor (int i = 0; i < N; i++)\n   a[i] = StdRandom.random();\nArrays.sort(a); \nfor (int i = 0; i < N; i++) \n{\n   double x = 1.0*i/N;\n   double y = a[i]/2.0;\n   double rw = 0.5/N;\n   double rh = a[i]/2.0;\n   StdDraw.filledRectangle(x, y, rw, rh); \n}\nStdDraw plotting examples\n451.1 \u25a0 Basic Programming Model\n   B i n a r y  s e a r c h  The sample Java program that we started with, shown on the facing \npage, is based on the famous, effective, and widely used binary search algorithm. This \nexample is a prototype of the way in which we will examine new algorithms throughout \nthe book. As with all of the programs we consider, it is both a precise de\ufb01nition of the \nmethod and a complete Java implementation that you can download from the booksite. \nBinary search. We w ill study the binar y search algor ithm in detail in Section 3.2 , \nbut a brief description is appropriate here. The algorithm is implemented in the static \nmethod rank(), which takes an integer key and \na sorted array of int values as arguments and re-\nturns the index of the key if it is present in the \narray, -1 otherwise. It accomplishes this task by \nmaintaining variables lo and hi such that the key \nis in a[lo..hi] if ", "start": 57, "end": 58}, "75": {"text": "re-\nturns the index of the key if it is present in the \narray, -1 otherwise. It accomplishes this task by \nmaintaining variables lo and hi such that the key \nis in a[lo..hi] if it is in the array, then entering \ninto a loop that tests the middle entry in the in-\nterval (at index mid). If the key is equal to a[mid], \nthe return value is mid; otherwise the method cuts \nthe interval size about in half, looking at the left \nhalf if the key is less than a[mid] and at the right \nhalf if the key is greater than a[mid].  The process \nterminates when the key is found or the interval is \nempty. Binary search is effective because it needs \nto examine just a few ar -\nray entries (relative to the \nsize of the array) to \ufb01nd \nthe key (or determine that \nit is not there).\nDevelopment client. For every algorithm implementation, \nwe include a development client main() that you can use with \nsample input \ufb01les provided in the book and on the booksite \nto learn about the algorithm and to test its performance. In \nthis example, the client reads integers from the \ufb01le named on \nthe command line, then prints any integers on standard input \nthat do not appear in the \ufb01le. We use small test \ufb01les such as \nthose shown at right to demonstrate this behavior, and as the \nbasis for traces and examples such as those at left above. We \nuse large test \ufb01les to model real-world applications and to test \nperformance (see page 48).\n10 11 12 16 18 23 29 33 48 54 57 68 77 84 98\n10 11 12 16 18 23 29 33 48 54 57 68 77 84 ", "start": 58, "end": 58}, "76": {"text": "23 29 33 48 54 57 68 77 84 98\n10 11 12 16 18 23 29 33 48 54 57 68 77 84 98\n10 11 12 16 18 23 29 33 48 54 57 68 77 84 98\nsuccessful search for 23\nlo mid hi\nlo mid hi\nlo mid hi\n10 11 12 16 18 23 29 33 48 54 57 68 77 84 98\n10 11 12 16 18 23 29 33 48 54 57 68 77 84 98\n10 11 12 16 18 23 29 33 48 54 57 68 77 84 98\n10 11 12 16 18 23 29 33 48 54 57 68 77 84 98\n10 11 12 16 18 23 29 33 48 54 57 68 77 84 98\nBinary search in an ordered array\nunsuccessful search for 50\nlo mid hi\nlo mid hi\nlo mid\nhi lo \nhi\nlo mid hi\n84\n48\n68\n10\n18\n98\n12\n23\n54\n57\n48\n33\n16\n77\n11\n29\ntinyW.txt\n23\n50\n10\n99\n18\n23\n98\n84\n11\n10\n48\n77\n13\n54\n98\n77\n77\n68\ntinyT.txt\nSmall test files for\nBinarySearch test client\nnot in\ntinyW.txt\n46 CHAPTER 1 \u25a0 Fundamentals\n  B i n a r y  S e a r c h\nimport ", "start": 58, "end": 59}, "77": {"text": "hi\n84\n48\n68\n10\n18\n98\n12\n23\n54\n57\n48\n33\n16\n77\n11\n29\ntinyW.txt\n23\n50\n10\n99\n18\n23\n98\n84\n11\n10\n48\n77\n13\n54\n98\n77\n77\n68\ntinyT.txt\nSmall test files for\nBinarySearch test client\nnot in\ntinyW.txt\n46 CHAPTER 1 \u25a0 Fundamentals\n  B i n a r y  S e a r c h\nimport java.util.Arrays;\npublic class  BinarySearch \n{\n   public static int rank(int key, int[] a)\n   {  // Array must be sorted.\n      int lo  = 0;\n      int hi = a.length - 1;\n      while (lo <= hi)\n      {  // Key is in a[lo..hi] or not present.\n         int mid = lo + (hi - lo) / 2;\n         if      (key < a[mid]) hi = mid - 1;\n         else if (key > a[mid]) lo = mid + 1;\n         else                   return mid;\n      }\n      return -1;\n   } \n   public static void main(String[] args)\n   {\n      int[] whitelist = In.readInts(args[0]);\n      Arrays.sort(whitelist);\n      while (!StdIn.isEmpty())\n      {  // Read key, print if not in whitelist.\n         int key = StdIn.readInt();\n         if (rank(key, whitelist) < 0)\n            StdOut.println(key);\n      }\n   }\n}\nThis program takes the name of a whitelist \ufb01le (a sequence of integers) as argument and \ufb01lters any \nentry that is on the whitelist from standard input, leaving only integers that are not on the whitelist \non standard output. It uses the binary search algorithm, implemented in the static method ", "start": 59, "end": 59}, "78": {"text": "integers) as argument and \ufb01lters any \nentry that is on the whitelist from standard input, leaving only integers that are not on the whitelist \non standard output. It uses the binary search algorithm, implemented in the static method rank(), \nto accomplish the task ef\ufb01ciently. See Sec-\ntion 3.1 for a full discussion of the binary \nsearch algorithm, its correctness, its per -\nformance analysis, and its applications.\n% java BinarySearch tinyW.txt < tinyT.txt \n50 \n99 \n13\n471.1 \u25a0 Basic Programming Model  \n  W h i t e l i s t i n g .  When possible, our development clients are intended to mirror practical \nsituations and demonstrate the need for the algorithm at hand. In this case, the process \nis known as whitelisting. Speci\ufb01cally, imagine a credit card company that needs to check \nwhether customer transactions are for a valid account. T o do so, it can\n\u25a0  Keep customers account numbers in a \ufb01le, which we refer to as a whitelist.\n\u25a0 Produce the account number associated with each transaction in the standard \ninput stream. \n\u25a0 \n \nUse the test client to put onto standard output the numbers that are not associat-\ned with any customer. Presumably the company would refuse such transactions. \nIt would not be unusual for a big company with millions of customers to have to pro -\ncess millions of transactions or more. To model this situation, we provide on the book-\nsite the \ufb01les largeW.txt (1 million integers) and largeT.txt (10 million integers).\nPerformance. A working program is often not suf\ufb01cient. For example, a much simpler \nimplementation of rank(), which does not even require the array to be sorted, is to \ncheck every entry, as follows:\npublic static int rank(int key, int[] a) \n{\n   for (int i ", "start": 59, "end": 60}, "79": {"text": "much simpler \nimplementation of rank(), which does not even require the array to be sorted, is to \ncheck every entry, as follows:\npublic static int rank(int key, int[] a) \n{\n   for (int i = 0; i < a.length; i++)\n      if (a[i] == key) return i;\n   return -1; \n}\nGiven this simple and easy-to-understand solution, why do we use mergesort and bi-\nnary search? If you work Exercise 1.1.38, you will see that your computer is too slow \nto run this brute-force implementation of rank() for large numbers of inputs (say, 1 \nmillion whitelist entries and 10 million transactions). Solving the whitelist problem for \na large number of inputs is not feasible without ef\ufb01cient algorithms such as binary search \nand mergesort. Good performance is often of critical importance, so we lay the ground-\nwork for studying performance in Section 1.4 and analyze the performance character-\nistics of all of our algorithms (including binary search, in Section 3.1 and mergesort, \nin Section 2.2). \nIn the present context, our goal in thoroughly outlining our programming model \nis to ensure that you can run code like BinarySearch on your computer, use it on test \ndata like ours, and modify it to adapt to various situations (such as those described in \nthe exercises at the end of this section), in order to best understand its applicability. \nThe programming model that we have sketched is designed to facilitate such activities, \nwhich are crucial to our approach to studying  algorithms.\n48 CHAPTER 1 \u25a0 Fundamentals\n 489910\n 18940\n774392\n490636\n125544\n407391\n115771\n992663\n923282\n176914\n217904\n571222\n519039\n395667\n ...\n  \n944443\n293674\n572153\n600579\n499569\n984875\n763178\n295754\n ", "start": 60, "end": 61}, "80": {"text": "18940\n774392\n490636\n125544\n407391\n115771\n992663\n923282\n176914\n217904\n571222\n519039\n395667\n ...\n  \n944443\n293674\n572153\n600579\n499569\n984875\n763178\n295754\n 44696\n207807\n138910\n903531\n140925\n699418\n759984\n199694\n774549\n635871\n161828\n805380\n ...\n  \n% java BinarySearch largeW.txt < largeT.txt\n499569\n984875\n295754\n207807\n140925\n161828\n ...\nlargeW.txt largeT.txt\nLarge files for BinarySearch test client\nnot in\nlargeW.txt\n1,000,000\nint values\n3,675,966\nint values\n10,000,000\nint values\n491.1 \u25a0 Basic Programming Model\n  \nPerspective In this section, we have described a \ufb01ne and complete programming \nmodel that served (and still serves) many programmers for many decades. Modern \nprogramming, however, goes one step further. This next level is called data abstraction, \nsometimes known as object-oriented programming, and is the subject of the next sec -\ntion. Simply put, the idea behind data abstraction is to allow a program to de\ufb01ne data \ntypes (sets of values and sets of operations on those values), not just static methods that \noperate on prede\ufb01ned data types.\nObject-oriented programming has come into widespread use in recent decades, and \ndata abstraction is central to modern program development. We embrace data abstrac-\ntion in this book for three primary reasons:\n\u25a0 \n \nIt enables us to expand our ability to reuse code through modular programming. \nFor example, our sorts in Chapter 2 and binary search and other algorithms in \nChapter 3 allow clients to make use of the same code for any type of ", "start": 61, "end": 62}, "81": {"text": "us to expand our ability to reuse code through modular programming. \nFor example, our sorts in Chapter 2 and binary search and other algorithms in \nChapter 3 allow clients to make use of the same code for any type of data (not \njust integers), including one de\ufb01ned by the client.\n\u25a0 \n \nIt provides a convenient mechanism for building so-called linked data structures \nthat provide more \ufb02exibility than arrays and are the basis of ef\ufb01cient algorithms \nin many settings.\n\u25a0  \n \n  \nIt enables us to precisely de\ufb01ne the algorithmic challenges that we face. For ex-\nample, our union-\ufb01nd algorithms in Section 1.5, our priority-queue algorithms \nin Section 2.4, and our symbol-table algorithms in Chapter 3 are all oriented \ntoward de\ufb01ning data structures that enable ef\ufb01cient implementations of a set of \noperations. This challenge aligns perfectly with data abstraction.\nDespite all of these considerations, our focus remains on the study of algorithms. In \nthis context, we proceed to consider next the essential features of object-oriented pro-\ngramming that are relevant to our mission.\n50 CHAPTER 1 \u25a0 Fundamentals\n Q&A\n \n \nQ. What is Java bytecode?\nA. A low-level version of your program that runs on the  Java virtual machine. This level \nof abstraction makes it easier for the developers of Java to ensure that our programs run \non a broad variety of devices.\nQ. It seems wrong that Java should just let ints over\ufb02ow and give bad values. Shouldn\u2019t \nJava automatically check for   over\ufb02ow?\nA. This issue is a contentious one among programmers. The short answer is that the \nlack of such checking is one reason such types are called  primitive data types. A little \nknowledge can go a long way in avoiding such problems. We use the int type for small \nnumbers ", "start": 62, "end": 63}, "82": {"text": "short answer is that the \nlack of such checking is one reason such types are called  primitive data types. A little \nknowledge can go a long way in avoiding such problems. We use the int type for small \nnumbers (less than ten decimal digits), and the long type when values run into the bil-\nlions or more. \nQ. What is the value of Math.abs(-2147483648)?\nA. -2147483648. This strange (but true) result is a typical example of the effects of \ninteger over\ufb02ow.\nQ. How can I initialize a double variable to in\ufb01nity?\nA. Java has built-in constants available for this purpose: Double.POSITIVE_INFINITY\nand Double.NEGATIVE_INFINITY.\nQ. Can you compare a double to an int?\nA. Not without doing a type conversion, but remember that Java usually does the req-\nuisite type conversion automatically. For example, if x is an int with the value 3, then \nthe expression (x < 3.1) is true\u2014Java converts x to double (because 3.1 is a double\nliteral) before performing the comparison.\nQ. What happens if I use a variable before initializing it to a value?\nA. Java will report a compile-time error if there is any path through your code that \nwould lead to use of an uninitialized variable.\nQ. What are the values of 1/0 and 1.0/0.0 as Java expressions?\nA. The \ufb01rst generates a runtime exception for  division by zero (which stops your pro-\ngram because the value is unde\ufb01ned); the second has the value Infinity.\n511.1 \u25a0 Basic Programming Model\n  \nQ. Can you use < and > to compare String variables?\nA. No. Those operators are de\ufb01ned only for primitive types. See page 80.\nQ. What is the result of division and ", "start": 63, "end": 64}, "83": {"text": "Programming Model\n  \nQ. Can you use < and > to compare String variables?\nA. No. Those operators are de\ufb01ned only for primitive types. See page 80.\nQ. What is the result of division and remainder for negative integers?\nA. The quotient a/b rounds toward 0; the remainder a % b is de\ufb01ned such that (a / \nb) * b + a % b  is always equal to a. For example, -14/3 and 14/-3 are both -4, but \n-14 % 3 is -2 and 14 % -3 is 2.\nQ. Why do we say (a && b) and not (a & b)?\nA. The operators &, |, and ^ are  bitwise logical operations for integer types that do and, \nor, and exclusive or (respectively) on each bit position. Thus the value of 10&6 is 14 and \nthe value of 10^6 is 12. We use these operators rarely (but occasionally) in this book. \nThe operators && and || are valid only in boolean expressions are included separately \nbecause of  short-circuiting: an expression is evaluated left-to-right and the evaluation \nstops when the value is known.\nQ. Is ambiguity in nested if statements a problem?\nA. Ye s . In  Jav a , w h e n  yo u  w r i te\nif <expr1> if <expr2> <stmntA> else <stmntB>\nit is equivalent to\nif <expr1> { if <expr2> <stmntA> else <stmntB> }\neven if you might have been thinking\nif <expr1> { if <expr2> <stmntA> } else <stmntB>\nUsing explicit braces is a good way to avoid this  dangling else pitfall.\nQ. What is the ", "start": 64, "end": 64}, "84": {"text": "thinking\nif <expr1> { if <expr2> <stmntA> } else <stmntB>\nUsing explicit braces is a good way to avoid this  dangling else pitfall.\nQ. What is the difference between a for loop and its while formulation?\nA. The code in the for loop header is considered to be in the same block as the for\nloop body. In a typical for loop, the incrementing variable is not available for use in \nlater statements; in the corresponding while loop, it is. This distinction is often a rea-\nson to use a while instead of a for loop. \nQ. Some Java programmers use int a[] instead of int[] a to declare arrays. What\u2019s \nthe difference?\nQ&A (continued)\n52 CHAPTER 1 \u25a0 Fundamentals\n A. In Java, both are legal and equivalent. The former is how arrays are declared in C. \nThe latter is the preferred style in Java since the type of the variable int[] more clearly \nindicates that it is an array of integers.\nQ. Why do array indices start at  0 instead of 1?\nA. This convention originated with machine-language programming, where the ad-\ndress of an array element would be computed by adding the index to the address of the \nbeginning of an array. Starting indices at 1 would entail either a waste of space at the \nbeginning of the array or a waste of time to subtract the 1.\nQ. If a[] is an array, why does StdOut.println(a) print out a hexadecimal integer, \nsuch as @f62373 , instead of the elements of the array?\nA. Good question. It is printing out the memory address of the array, which, unfortu-\nnately, is rarely what you want.\nQ. Why are we not using the standard Java libraries for input and graphics?\nA. We are using them, but we prefer to work ", "start": 64, "end": 65}, "85": {"text": "the array, which, unfortu-\nnately, is rarely what you want.\nQ. Why are we not using the standard Java libraries for input and graphics?\nA. We are using them, but we prefer to work with simpler abstract models. The Java \nlibraries behind StdIn and StdDraw are built for production programming, and the \nlibraries and their APIs are a bit unwieldy. T o get an idea of what they are like, look at \nthe code in StdIn.java and StdDraw.java. \nQ. Can my program reread data from standard input?\nA. No. You only get one shot at it, in the same way that you cannot undo println().\nQ. What happens if my program attempts to read after standard input is exhausted?\nA. Yo u  w i l l  g e t  a n  e r ro r. StdIn.isEmpty() allows you to avoid such an error by check-\ning whether there is more input available.\nQ. What does this error message mean?\n      Exception in thread \"main\" java.lang.NoClassDefFoundError: StdIn\nA. Yo u  p ro b a b l y  f o r g o t  t o  p u t  StdIn.java in your working directory.\nQ. Can a static method take another static method as an argument in Java?\nA. No. Good question, since many other languages do support this capability.\n531.1 \u25a0 Basic Programming Model\n EXERCISES\n1.1.1 Give the value of each of the following expressions:\na. ( 0 + 15 ) / 2\nb. 2.0e-6 * 100000000.1\nc.  true && false || true && true\n1.1.2 Give the type and value of each of the following expressions:\na. (1 + 2.236)/2\nb. 1 + ", "start": 65, "end": 66}, "86": {"text": "100000000.1\nc.  true && false || true && true\n1.1.2 Give the type and value of each of the following expressions:\na. (1 + 2.236)/2\nb. 1 + 2 + 3 + 4.0\nc. 4.1 >= 4\nd. 1 + 2 + \"3\"\n1.1.3 Write a program that takes three integer command-line arguments and prints \nequal if all three are equal, and not equal otherwise.\n1.1.4 What (if anything) is wrong with each of the following statements?\na.  if (a > b) then c = 0;\nb.  if a > b { c = 0; }\nc.  if (a > b) c = 0;\nd.  if (a > b) c = 0 else b = 0;\n1.1.5 Write a code fragment that prints true if the double variables x and y are both \nstrictly between 0 and 1 and false otherwise.\n1.1.6 What does the following program print?\nint f = 0; \nint g = 1; \nfor (int i = 0; i <= 15; i++) \n{\n   StdOut.println(f);\n   f = f + g;\n   g = f - g; \n}\n54 CHAPTER 1 \u25a0 Fundamentals\n 1.1.7 Give the value printed by each of the following code fragments:\na.   double t = 9.0;\n    while (Math.abs(t - 9.0/t) > .001)\n       t = (9.0/t + t) / 2.0;\n    StdOut.printf(\"%.5f\\n\", t);\nb.   int sum = 0;\n    for (int i = 1; i < 1000; ", "start": 66, "end": 67}, "87": {"text": "(9.0/t + t) / 2.0;\n    StdOut.printf(\"%.5f\\n\", t);\nb.   int sum = 0;\n    for (int i = 1; i < 1000; i++)\n        for (int j = 0; j < i; j++)\n            sum++;\n    StdOut.println(sum);\nc.    int sum = 0;\n    for (int i = 1; i < 1000; i *= 2)\n       for (int j = 0; j < N; j++)\n           sum++;\n    StdOut.println(sum);\n1.1.8 What do each of the following print?\na. System.out.println('b');\nb. System.out.println('b' + 'c');\nc. System.out.println((char) ('a' + 4));\nExplain each outcome.\n1.1.9 Write a code fragment that puts the binary representation of a positive integer N\ninto a String s.\nSolution: Java has a built-in method Integer.toBinaryString(N) for this job, but \nthe point of the exercise is to see how such a method might be implemented. Here is a \nparticularly concise solution:\nString s = \"\"; \nfor (int n = N; n > 0; n /= 2)\n   s = (n % 2) + s;\n551.1 \u25a0 Basic Programming Model\n 1.1.10 What is wrong with the following code fragment?\nint[] a; \nfor (int i = 0; i < 10; i++)\n   a[i] = i * i; \nSolution: It does not allocate memory for a[] with new. This code results in a \nvariable a might not have been initialized compile-time error.\n1.1.11 Write a code fragment that prints the contents of a two-dimensional boolean \narray, using * to represent true and a space to represent false. ", "start": 67, "end": 68}, "88": {"text": "in a \nvariable a might not have been initialized compile-time error.\n1.1.11 Write a code fragment that prints the contents of a two-dimensional boolean \narray, using * to represent true and a space to represent false. Include row and column \nnumbers.\n1.1.12 What does the following code fragment print?\nint[] a = new int[10]; \nfor (int i = 0; i < 10; i++)\n   a[i] = 9 - i; \nfor (int i = 0; i < 10; i++)\n   a[i] = a[a[i]]; \nfor (int i = 0; i < 10; i++)\n   System.out.println(i); \n1.1.13 Write a code fragment to print the  transposition (rows and columns changed) \nof a two-dimensional array with M rows and N columns.\n1.1.14 Write a static method lg() that takes an int value N as argument and returns \nthe largest int not larger than the base-2 logarithm of N. Do not use Math.\n1.1.15 Write a static method histogram() that takes an array a[] of int values and \nan integer M as arguments and returns an array of length M whose ith entry is the num-\nber of times the integer i appeared in the argument array. If the values in a[] are all \nbetween 0 and M\u20131, the sum of the values in the returned array should be equal to \na.length.\n1.1.16  Give the value of exR1(6):\npublic static String exR1(int n) \n{\n   if (n <= 0) return \"\";\n   return exR1(n-3) + n + exR1(n-2) + n; \n}\nEXERCISES  (continued)\n56 CHAPTER 1 \u25a0 Fundamentals\n 1.1.17 Criticize ", "start": 68, "end": 69}, "89": {"text": "exR1(n-3) + n + exR1(n-2) + n; \n}\nEXERCISES  (continued)\n56 CHAPTER 1 \u25a0 Fundamentals\n 1.1.17 Criticize the following recursive function:\npublic static String exR2(int n) \n{\n   String s = exR2(n-3) + n + exR2(n-2) + n;\n   if (n <= 0) return \"\";\n   return s; \n}\nAnswer : The base case will never be reached. A call to exR2(3) will result in calls to \nexR2(0), exR2(-3), exR3(-6), and so forth until a   StackOverflowError occurs.\n1.1.18 Consider the following recursive function:\npublic static int mystery(int a, int b) \n{\n   if (b == 0)     return 0;\n   if (b % 2 == 0) return mystery(a+a, b/2);\n   return mystery(a+a, b/2) + a; \n}\nWhat are the values of  mystery(2, 25) and mystery(3, 11)? Given positive integers \na and b, describe what value mystery(a, b) computes. Answer the same question, but \nreplace + with * and replace return 0 with return 1.\n1.1.19  Run the following program on your computer:\npublic class    Fibonacci \n{\n   public static long F(int N)\n   {\n      if (N == 0) return 0;\n      if (N == 1) return 1;\n      return F(N-1) + F(N-2);\n   }\n   public static void main(String[] args)\n   {\n      for (int N = 0; N < 100; N++)\n         StdOut.println(N + \" \" + F(N));\n   } \n}\n571.1 ", "start": 69, "end": 69}, "90": {"text": "F(N-2);\n   }\n   public static void main(String[] args)\n   {\n      for (int N = 0; N < 100; N++)\n         StdOut.println(N + \" \" + F(N));\n   } \n}\n571.1 \u25a0 Basic Programming Model\n What is the largest value of N for which this program takes less 1 hour to compute the \nvalue of F(N)? Develop a better implementation of F(N) that saves computed values in \nan array.\n1.1.20 Write a recursive static method that computes the value of ln (N !)\n1.1.21 Write a program that reads in lines from standard input with each line contain-\ning a name and two integers and then uses printf() to print a table with a column of \nthe names, the integers, and the result of dividing the \ufb01rst by the second, accurate to \nthree decimal places. Y ou could use a program like this to tabulate batting averages for \nbaseball players or grades for students.\n1.1.22  Write a version of BinarySearch that uses the recursive rank() given on page \n25 and traces the method calls. Each time the recursive method is called, print the argu-\nment values lo and hi, indented by the depth of the recursion. Hint: Add an argument \nto the recursive method that keeps track of the depth. \n1.1.23 Add to the BinarySearch test client the ability to respond to a second argu-\nment: + to print numbers from standard input that are not in the whitelist, - to print \nnumbers that are in the whitelist.\n1.1.24  Give the sequence of values of p and q that are computed when Euclid\u2019s algo-\nrithm is used to compute the greatest common divisor of 105 and 24. Extend the code \ngiven on page 4 to develop a program  Euclid that takes two integers from the command \nline ", "start": 69, "end": 70}, "91": {"text": "Euclid\u2019s algo-\nrithm is used to compute the greatest common divisor of 105 and 24. Extend the code \ngiven on page 4 to develop a program  Euclid that takes two integers from the command \nline and computes their greatest common divisor, printing out the two arguments for \neach call on the recursive method. Use your program to compute the greatest common \ndivisor or 1111111 and 1234567.\n1.1.25  Use mathematical induction to prove that Euclid\u2019s algorithm computes the \ngreatest common divisor of any pair of nonnegative integers p and q.\nEXERCISES  (continued)\n58 CHAPTER 1 \u25a0 Fundamentals\n CREATIVE PROBLEMS\n1.1.26  Sorting three numbers. Suppose that the variables a, b, c, and t are all of the \nsame numeric primitive type. Show that the following code puts a, b, and c in ascending \norder:\nif (a > b) { t = a; a = b; b = t; } \nif (a > c) { t = a; a = c; c = t; } \nif (b > c) { t = b; b = c; c = t; }\n1.1.27     Binomial distribution. Estimate the number of recursive calls that would be \nused by the code\npublic static double binomial(int N, int k, double p) \n{\n   if ((N == 0) || (k < 0)) return 1.0;\n   return (1.0 - p)*binomial(N-1, k) + p*binomial(N-1, k-1); \n}\nto compute binomial(100, 50). Develop a better implementation that is based on \nsaving computed values in an array.\n1.1.28  Remove duplicates. Modify the test client in BinarySearch to remove any du-\nplicate ", "start": 70, "end": 71}, "92": {"text": "compute binomial(100, 50). Develop a better implementation that is based on \nsaving computed values in an array.\n1.1.28  Remove duplicates. Modify the test client in BinarySearch to remove any du-\nplicate keys in the whitelist after the sort.\n1.1.29  Equal keys. Add to BinarySearch a static method rank() that takes a key and \na sorted array of int values (some of which may be equal) as arguments and returns the \nnumber of elements that are smaller than the key and a similar method count() that \nreturns the number of elements equal to the key. Note : If i and j are the values returned \nby rank(key, a) and count(key, a) respectively, then a[i..i+j-1] are the values in \nthe array that are equal to key.\n1.1.30  Array exercise. Write a code fragment that creates an N-by-N boolean array \na[][] such that a[i][j] is true if i and j are relatively prime (have no common fac-\ntors), and false otherwise.  \n1.1.31  Random connections. Write a program that takes as command-line arguments \nan integer N and a double value p (between 0 and 1), plots N equally spaced dots of size \n.05 on the circumference of a circle, and then, with probability p for each pair of points, \ndraws a gray line connecting them.\n591.1 \u25a0 Basic Programming Model\n  \n1.1.32  Histogram. Suppose that the standard input stream is a sequence of double\nvalues. Write a program that takes an integer N and two double values l and r from the \ncommand line and uses StdDraw to plot a histogram of the count of the numbers in the \nstandard input stream that fall in each of the N intervals de\ufb01ned by dividing (l , r) into \nN equal-sized intervals.\n1.1.33 ", "start": 71, "end": 72}, "93": {"text": "StdDraw to plot a histogram of the count of the numbers in the \nstandard input stream that fall in each of the N intervals de\ufb01ned by dividing (l , r) into \nN equal-sized intervals.\n1.1.33  Matrix library. Write a library  Matrix that implements the following API:\npublic class  Matrix \nstatic     double dot(double[] x, double[] y) vector dot product\nstatic double[][] mult(double[][] a, double[][] b) matrix-matrix product\nstatic double[][] transpose(double[][] a) transpose\nstatic   double[] mult(double[][] a, double[] x) matrix-vector product\nstatic   double[] mult(double[] y, double[][] a) vector-matrix product\n \nDevelop a test client that reads values from standard input and tests all the methods.\n1.1.34   Filtering. Which of the following require saving all the values from standard \ninput (in an array, say), and which could be implemented as a \ufb01lter using only a \ufb01xed \nnumber of variables and arrays of \ufb01xed size (not dependent on N)? For each, the input \ncomes from standard input and consists of N real numbers between 0 and 1.\n\u25a0 Print the maximum and minimum numbers.\n\u25a0 Print the median of the numbers.\n\u25a0 Print the k th smallest value, for k less than 100.\n\u25a0 Print the sum of the squares of the numbers.\n\u25a0 Print the average of the N numbers.\n\u25a0 Print the percentage of numbers greater than the average.\n\u25a0 Print the N numbers in increasing order.\n\u25a0 Print the N numbers in random order.\nCREATIVE PROBLEMS  (continued)\n60 CHAPTER 1 \u25a0 Fundamentals\n EXPERIMENTS\n \n \n \n1.1.35  Dice simulation. The following code computes the exact probability distribu-\ntion for the sum of two dice:\nint SIDES = 6; \ndouble[] dist = new double[2*SIDES+1]; ", "start": 72, "end": 73}, "94": {"text": "\n1.1.35  Dice simulation. The following code computes the exact probability distribu-\ntion for the sum of two dice:\nint SIDES = 6; \ndouble[] dist = new double[2*SIDES+1]; \nfor (int i = 1; i <= SIDES; i++)\n   for (int j = 1; j <= SIDES; j++)\n      dist[i+j] += 1.0;\nfor (int k = 2; k <= 2*SIDES; k++)\n   dist[k] /= 36.0; \nThe value dist[i] is the probability that the dice sum to k. Run experiments to vali-\ndate this calculation simulating N dice throws, keeping track of the frequencies of oc-\ncurrence of each value when you compute the sum of two random integers between 1 \nand 6. How large does N have to be before your empirical results match the exact results \nto three decimal places?\n1.1.36    Empirical shuf\ufb02e check. Run computational experiments to check that our \nshuf\ufb02ing code on page 32 works as advertised. Write a program ShuffleTest that takes \ncommand-line arguments M and N, does N shuf\ufb02es of an array of size M that is initial-\nized with a[i] = i before each shuf\ufb02e, and prints an M-by-M table such that row i\ngives the number of times i wound up in position j for all j. All entries in the array \nshould be close to N/M.\n1.1.37  Bad shuf\ufb02ing. Suppose that you choose a random integer between 0 and N-1\nin our shuf\ufb02ing code instead of one between i and N-1. Show that the resulting order is \nnot equally likely to be one of the N! possibilities. ", "start": 73, "end": 73}, "95": {"text": "between 0 and N-1\nin our shuf\ufb02ing code instead of one between i and N-1. Show that the resulting order is \nnot equally likely to be one of the N! possibilities. Run the test of the previous exercise \nfor this version.\n1.1.38    Binary search versus brute-force search. Write a program BruteForceSearch\nthat uses the brute-force search method given on page 48 and compare its running time \non your computer with that of BinarySearch for largeW.txt and largeT.txt. \n611.1 \u25a0 Basic Programming Model\n  \n1.1.39  Random matches. Write a BinarySearch client that takes an int value T as \ncommand-line argument and runs T trials of the following experiment for N = 103, 104, \n105, and 106: generate two arrays of N randomly generated positive six-digit int values, \nand \ufb01nd the number of values that appear in both arrays. Print a table giving the average \nvalue of this quantity over the T trials for each value of N. \nEXPERIMENTS  (continued)\n62 CHAPTER 1 \u25a0 Fundamentals\n This page intentionally left blank \n 1.2   DATA ABSTRACTION\n \n \n \nA data type is a set of values and a set of operations on those values. So far, we have \ndiscussed in detail Java\u2019s primitive data types: for example, the values of the primitive \ndata type int are integers between /H11002231 and 231 /H11002 1; the operations of int include +, *, \n-, /, %, <, and >. In principle, we could write all of our programs using only the built-in \nprimitive types, but it is much more convenient to write programs at a higher level of \nabstraction. In this section, we focus on the process of de\ufb01ning and using data types, \nwhich is known as ", "start": 73, "end": 76}, "96": {"text": "\nprimitive types, but it is much more convenient to write programs at a higher level of \nabstraction. In this section, we focus on the process of de\ufb01ning and using data types, \nwhich is known as data abstraction (and supplements the function abstraction style that \nis the basis of SECTION 1.1). \nProgramming in Java is largely based on building data types known as  reference types\nwith the familiar Java  class. This style of programming is known as object-oriented \nprogramming, as it revolves around the concept of an object, an entity that holds a data \ntype value. With Java\u2019s primitive types we are largely con\ufb01ned to programs that operate \non numbers, but with reference types we can write programs that operate on strings, \npictures, sounds, any of hundreds of other abstractions that are available in Java\u2019s stan-\ndard libraries or on our booksite. Even more signi\ufb01cant than libraries of prede\ufb01ned \ndata types is that the range of data types available in Java programming is open-ended, \nbecause you can de\ufb01ne your own data types to implement any abstraction whatsoever. \nAn    abstract data type (ADT) is a data type whose representation is hidden from the \nclient. Implementing an ADT as a Java class is not very different from implementing a \nfunction library as a set of static methods. The primary difference is that we associate \ndata with the function implementations and we hide the representation of the data \nfrom the client. When using an ADT, we focus on the operations speci\ufb01ed in the API and \npay no attention to the data representation; when implementing an ADT, we focus on \nthe data, then implement operations on that data. \nAbstract data types are important because they support encapsulation in program \ndesign. In this book, we use them as a means to\n\u25a0 Precisely specify problems in the form of APIs ", "start": 76, "end": 76}, "97": {"text": "then implement operations on that data. \nAbstract data types are important because they support encapsulation in program \ndesign. In this book, we use them as a means to\n\u25a0 Precisely specify problems in the form of APIs for use by diverse clients\n\u25a0 Describe algorithms and data structures as API implementations\nOur primary reason for studying different algorithms for the same task is that perfor -\nmance characteristics differ. Abstract data types are an appropriate framework for the \nstudy of algorithms because they allow us to put knowledge of algorithm performance \nto immediate use: we can substitute one algorithm for another to improve performance \nfor all clients without changing any client code. \n64\n Using abstract data types You do not need to know how a data t y pe is imple -\nmented in order to be able to use it , so we begin by describing how to write programs \nthat use a simple data type named Counter whose values are a name and a nonnega-\ntive integer and whose operations are create and initialize to zero, increment by one, and \nexamine the current value. This abstraction is useful in many contexts. For example, it \nwould be reasonable to use such a data type in electronic voting software, to ensure that \nthe only thing that a voter can do is increment a chosen candidate\u2019s tally by one. Or, \nwe might use a Counter to keep track of fundamental operations when analyzing the \nperformance of algorithms. T o use a Counter, you need to learn our mechanism for \nspecifying the operations de\ufb01ned in the data type and the Java language mechanisms \nfor creating and manipulating data-type values. Such mechanisms are critically im-\nportant in modern programming, and we use them throughout this book, so this \ufb01rst \nexample is worthy of careful attention.\nAPI for an abstract data type. To  s p e c i f y  t h e  b e h av i o r  o f  a n  a b s ", "start": 76, "end": 77}, "98": {"text": "\nexample is worthy of careful attention.\nAPI for an abstract data type. To  s p e c i f y  t h e  b e h av i o r  o f  a n  a b s t r a c t  d a t a  t y p e , w e  u s e  \nan    instance application programming interface (API), which is a list of    constructors and    \nmethods (operations), with an informal description of the effect of each, as in this API \nfor Counter:\n public class    Counter\nCounter(String id) create a counter named id\nvoid increment() increment the counter by one\nint tally() number of increments since creation\nString toString() string representation\n A n  A P I  f o r  a  c o u n t e r\n \nEven though the basis of a data-type de\ufb01nition is a set of values, the role of the values \nis not visible from the API, only the operations on those values. Accordingly, an ADT \nde\ufb01nition has many similarities with a library of static methods (see page 24):  \n\u25a0 Both are implemented as a Java class.\n\u25a0  Instance methods may take zero or more arguments of a speci\ufb01ed type, sepa-\nrated by commas and enclosed in parentheses.\n\u25a0  They may provide a return value of a speci\ufb01ed type or no return value (signi\ufb01ed \nby void).\nAnd there are three signi\ufb01cant differences:\n\u25a0 Some entries in the API have the same name as the class and lack a return type. \nSuch entries are known as constructors and play a special role. In this case, \nCounter has a constructor that takes a String argument.\n651.2 \u25a0 Data Abstraction\n \u25a0  Instance methods lack the static modi\ufb01er. They are not static methods\u2014their \npurpose is to operate on data type values.\n\u25a0 ", "start": 77, "end": 78}, "99": {"text": "has a constructor that takes a String argument.\n651.2 \u25a0 Data Abstraction\n \u25a0  Instance methods lack the static modi\ufb01er. They are not static methods\u2014their \npurpose is to operate on data type values.\n\u25a0 \n \nSome instance methods are present so as to adhere to Java conventions\u2014we \nrefer to such methods as   inherited methods and shade them gray in the API.\nAs with APIs for libraries of static methods, an API for an abstract data type is a con -\ntract with all clients and, therefore, the starting point both for developing any client \ncode and for developing any data-type implementation. In this case, the API tells us \nthat to use Counter, we have available the Counter() constructor, the increment()\nand tally() instance methods, and the inherited toString() method. \nInherited methods. Var ious Java conventions enable a data t y pe to take advantage of  \nbuilt-in language mechanisms by including speci\ufb01c methods in the API. For example, \nall Java data types inherit a   toString() method that returns a String representation \nof the data-type values. Java calls this method when any data-type value is to be concat-\nenated with a String value with the + operator. The default implementation is not par-\nticularly useful (it gives a string representation of the memory address of the data-type \nvalue), so we often provide an implementation that  overrides the default, and include \ntoString() in the API whenever we do so. Other examples of such methods include \nequals(), compareTo(), and hashCode() (see page 101).\nClient code. As with modular programming based on static methods, the API allows \nus to write client code without knowing details of the implementation (and to write \nimplementation code without knowing details of any particular client). The mecha-\nnisms introduced on page 28 for organizing programs as independent modules are use-\nful for all Java classes, and thus are effective for modular programming with ", "start": 78, "end": 78}, "100": {"text": "write \nimplementation code without knowing details of any particular client). The mecha-\nnisms introduced on page 28 for organizing programs as independent modules are use-\nful for all Java classes, and thus are effective for modular programming with ADTs as \nwell as for libraries of static methods. Accordingly, we can use an ADT in any program \nprovided that the source code is in a .java \ufb01le in the same directory, or in the standard \nJava library, or accessible through an  import statement, or through one of the   classpath \nmechanisms described on the booksite. All of the bene\ufb01ts of modular programming \nfollow. By encapsulating all the code that implements a data type within a single Java \nclass, we enable the development of client code at a higher level of abstraction. T o de-\nvelop client code, you need to be able to declare variables, create objects to hold data-\ntype values, and provide access to the values for instance methods to operate on them. \nThese processes are different from the corresponding processes for primitive types, \nthough you will notice many similarities.\n66 CHAPTER 1 \u25a0 Fundamentals\n     O b j e c t s .  Naturally, you can declare that a variable heads is to be associated with data \nof type Counter with the code\nCounter heads;\nbut how can you assign values or specify operations? The  answer to this question in-\nvolves a fundamental concept in data abstraction: an object is an entity that can take on \na data-type value. Objects are characterized by three essential prop-\nerties:  state,  identity, and  behavior. The state of an object is a value \nfrom its data type. The identity of an object distinguishes one object \nfrom another. It is useful to think of an object\u2019s identity as the place \nwhere its value is stored in memory. The behavior of an object is the \neffect of data-type operations. The implementation ", "start": 78, "end": 79}, "101": {"text": "distinguishes one object \nfrom another. It is useful to think of an object\u2019s identity as the place \nwhere its value is stored in memory. The behavior of an object is the \neffect of data-type operations. The implementation has the sole re-\nsponsibility for maintaining an object\u2019s identity, so that client code \ncan use a data type without regard to the representation of its state \nby conforming to an API that describes an object\u2019s behavior. An ob-\nject\u2019s state might be used to provide information to a client or cause \na side effect or be changed by one of its data type\u2019s operations, but \nthe details of the representation of the data-type value are not rel-\nevant to client code. A    reference is a mechanism for accessing an ob-\nject. Java nomenclature makes clear the distinction from primitive \ntypes (where variables are associated with values) by using the term \nreference types for nonprimitive types. The details of implementing \nreferences vary in Java implementations, but it is useful to think of a \nreference as a memory address, as shown at right (for brevity, we use \nthree-digit memory addresses in the diagram).\n   C r e a t i n g  o b j e c t s .  Each data-type value is stored in an object. T o \ncreate (or instantiate) an individual object, we invoke a constructor \nby using the keyword new, followed by the class name, followed by \n() (or a list of argument values enclosed in parentheses, if the con-\nstructor takes arguments). A constructor has no return type because \nit always returns a reference to an object of its data type. Each time \nthat a client uses   new(), the system\n\u25a0 Allocates  memory space for the object\n\u25a0 Invokes the constructor to initialize its value\n\u25a0 Returns a reference to the object\nIn client code we typically create objects in an initializing declaration that associates a \nvariable ", "start": 79, "end": 79}, "102": {"text": "the system\n\u25a0 Allocates  memory space for the object\n\u25a0 Invokes the constructor to initialize its value\n\u25a0 Returns a reference to the object\nIn client code we typically create objects in an initializing declaration that associates a \nvariable with the object, as we often do with variables of primitive types. Unlike primi-\ntive types, variables are associated with references to objects, not the data-type values \n460        \nheads     460  \nreference\n460        \nheads     460 \n612        \ntails     612 \nidentity\nof heads\nidentity\nof tails\nidentity\n(details hidden\nObject representation\none Counter object\ntwo Counter objects\n671.2 \u25a0 Data Abstraction\n  \nthemselves. We can create any num-\nber of objects from the same class\u2014\neach object has its own identity \nand may or may not store the same \nvalue as another object of the same \ntype. For example, the code\nCounter heads = new Counter(\"heads\"); \nCounter tails = new Counter(\"tails\");\ncreates two different Counter objects. In an abstract data type, details of the representa-\ntion of the value are hidden from client code. Y ou might assume that the value associ-\nated with each Counter object is a String name and an int tally, but you cannot write \ncode that depends on any speci\ufb01c representation (or even know whether that assumption \nis true\u2014perhaps the tally is a long value). \n  I n v o k i n g   i n s t a n c e  m e t h o d s .  The purpose of an instance method is to operate on data-\ntype values, so the Java language includes a special mechanism to invoke instance meth-\nods that emphasizes a connection to an object. Speci\ufb01cally, we invoke an instance meth-\nod by writing a variable name that refers to an object, \nfollowed by a period, followed by an instance method \nname, ", "start": 79, "end": 80}, "103": {"text": "a connection to an object. Speci\ufb01cally, we invoke an instance meth-\nod by writing a variable name that refers to an object, \nfollowed by a period, followed by an instance method \nname, followed by 0 or more arguments, enclosed in \nparentheses and separated by commas. An instance   \nmethod might change the data-type value or just exam-\nine the data-type value. Instance methods have all of \nthe properties of static methods that we considered on \npage 24\u2014arguments are passed by value, method names \ncan be overloaded, they may have a return value, and \nthey may cause side effects\u2014but they have an addi-\ntional property that characterizes them: each invoca-\ntion is associated with an object. For example, the code \n    heads.increment();\ninvokes the instance method increment() to operate \non the Counter object heads (in this case the opera-\ntion involves incrementing the tally), and the code \n    heads.tally() - tails.tally();\ninvokes the instance method tally() twice, \ufb01rst to \noperate on the Counter object heads and then to op -\nerate on the Counter object tails (in this case the \nStdOut.println( heads );\ninvoke heads.toString()\nheads.tally() - tails.tally()\ninvoke an instance method\nthat accesses the object\u2019s value\nheads.increment();\nobject name\ndeclaration\nobject name\ninvoke an instance method\nthat changes the object\u2019s value\nheads = new Counter (\"heads\");\ninvoke a constructor (create an object)\nInvoking instance methods\nvia automatic type conversion (toString())\nas an expression\nas a statement (void return value)\nwith new (constructor)\nCounter heads;\ncall on constructor\nto create an object\ndeclaration to associate\nvariable with object reference\nCounter heads  =  new Counter(\"heads\");\nCreating an object\n68 CHAPTER 1 \u25a0 Fundamentals\n operation involves returning the tally as an int value). As ", "start": 80, "end": 81}, "104": {"text": "create an object\ndeclaration to associate\nvariable with object reference\nCounter heads  =  new Counter(\"heads\");\nCreating an object\n68 CHAPTER 1 \u25a0 Fundamentals\n operation involves returning the tally as an int value). As these examples illustrate, you \ncan use calls on instance methods in client code in the same way as you use calls on stat-\nic methods\u2014as statements ( void methods) or values in expressions (methods that re-\nturn a value). The primary purpose of stat-\nic methods is to implement functions; the \nprimary purpose of non-static (instance) \nmethods is to implement data-type opera-\ntions. Either type of method may appear in \nclient code, but you can easily distinguish \nbetween them, because a static method \ncall starts with a class name (uppercase, by \nconvention) and a non-static method call \nalways starts with an object name (lower -\ncase, by convention). These differences are \nsummarized in the  table at right.\n  Using objects. Declarations give us variable names for objects that we can use in code \nnot just to create objects and invoke instance methods, but also in the same way as we \nuse variable names for integers, \ufb02oating-point numbers, and other primitive types. T o \ndevelop client code for a given data type, w:\n\u25a0 Declare variables of the type, for use in referring to objects\n\u25a0 Use the keyword new to invoke a constructor that creates objects of the type\n\u25a0 \n \nUse the object name to invoke instance methods, either as statements or within \nexpressions\nFor example, the class Flips shown at the top of the next page is a Counter client that \ntakes a command-line argument T and simulates T coin \ufb02ips (it is also a StdRandom cli-\nent). Beyond these direct uses, we can use variables associated with objects in the same \nway as we use variables associated with primitive-type values: \n\u25a0 In ", "start": 81, "end": 81}, "105": {"text": "coin \ufb02ips (it is also a StdRandom cli-\nent). Beyond these direct uses, we can use variables associated with objects in the same \nway as we use variables associated with primitive-type values: \n\u25a0 In assignment statements\n\u25a0 To  p a s s  o r  re t u r n  o b j e c t s  f ro m  m e t h o d s\n\u25a0 To  c re a te  a n d  u s e  a r r ay s  o f  o b j e c t .\nUnderstanding the behavior of each of these types of uses requires thinking in terms of \nreferences, not values, as you will see when we consider them, in turn. \nAssignment statements. An assignment statement with a reference type creates a copy \nof the reference. The assignment statement does not create a new object, just another \nreference to an existing object. This situation is known as  aliasing: both variables refer \nto the same object. The effect of aliasing is a bit unexpected, because it is different for \nvariables holding values of a primitive type. Be sure that you understand the difference. \ninstance method static method\nsample call head.increment() Math.sqrt(2.0)\ninvoked with object name class name\nparameters reference to object \nand argument(s) argument(s)\nprimary \npurpose\nexamine or change \nobject value\ncompute return \nvalue\nInstance methods versus static methods\n691.2 \u25a0 Data Abstraction\n If x and y are variables of a primitive type, then the as-\nsignment x = y copies the value of y to x. For reference \ntypes, the reference is copied (not the value). Aliasing is a \ncommon source of bugs in Java programs, as illustrated \nby the following example:\nCounter c1 = new Counter(\"ones\"); \nc1.increment();\nCounter c2 = c1; \nc2.increment(); \nStdOut.println(c1);\nWith ", "start": 81, "end": 82}, "106": {"text": "a \ncommon source of bugs in Java programs, as illustrated \nby the following example:\nCounter c1 = new Counter(\"ones\"); \nc1.increment();\nCounter c2 = c1; \nc2.increment(); \nStdOut.println(c1);\nWith a typical toString() implementation this code   \nwould print the string \"2 ones\" which may or may not \nbe what was intended and is counterintuitive at \ufb01rst. Such \nbugs are common in programs written by people without \nmuch experience in using objects (that may be you, so pay \nattention here!). Changing the state of an object impacts \nall code involving aliased variables referencing that ob-\nject. We are used to thinking of two different variables of \nprimitive types as being independent, but that intuition \ndoes not carry over to variables of reference types.\npublic class  Flips \n{\n   public static void main(String[] args)\n   {\n      int T = Integer.parseInt(args[0]);\n      Counter heads = new Counter(\"heads\");\n      Counter tails = new Counter(\"tails\");\n      for (int t = 0; t < T; t++)\n         if (StdRandom.bernoulli(0.5))\n              heads.increment();\n         else tails.increment();\n      StdOut.println(heads);\n      StdOut.println(tails);\n      int d = heads.tally() - tails.tally();\n      StdOut.println(\"delta: \" + Math.abs(d));\n   } \n}\nCounter client that simulates T coin flips\n% java Flips 10 \n5 heads \n5 tails \ndelta: 0\n% java Flips 10 \n8 heads \n2 tails \ndelta: 6\n% java Flips 1000000 \n499710 heads \n500290 tails \ndelta: 580\nCounter c1; \nc1 = new Counter(\"ones\");\nc1.increment();\nCounter c2 = c1;\nc2.increment();\n811        \n       2\n  c2 ", "start": 82, "end": 82}, "107": {"text": "\n500290 tails \ndelta: 580\nCounter c1; \nc1 = new Counter(\"ones\");\nc1.increment();\nCounter c2 = c1;\nc2.increment();\n811        \n       2\n  c2    811 \n  c1    811 references to\nsame object\nreference to\n\"ones\"\nAliasing\n70 CHAPTER 1 \u25a0 Fundamentals\n  \n \n  O b j e c t s  a s  a r g u m e n t s .  Yo u  c a n  p a s s  o b j e c t s  a s  arguments to methods. This ability typi-\ncally simpli\ufb01es client code. For example, when we use a Counter as an argument, we are \nessentially passing both a name and a tally, but need only specify one variable. When \nwe call a method with arguments, the effect in Java is as if each argument value were \nto appear on the right-hand side of an assignment statement with the corresponding \nargument name on the left. That is, Java passes a copy of the argument value from the \ncalling program to the method. This arrangement is known as    pass by value (see page \n24). One important consequence is that the method cannot change the value of a caller\u2019s \nvariable. For primitive types, this policy is what we expect (the two variables are inde-\npendent), but each time that we use a reference type as a method argument we create \nan alias, so we must be cautious. In other words, the convention is to pass the reference\nby value (make a copy of it) but to pass the object by  reference. For example, if we pass \na reference to an object of type Counter, the method cannot change the original refer-\nence (make it point to a different Counter), but it can change the value of the object, \nfor example by using the reference ", "start": 82, "end": 83}, "108": {"text": "pass \na reference to an object of type Counter, the method cannot change the original refer-\nence (make it point to a different Counter), but it can change the value of the object, \nfor example by using the reference to call increment().\n O b j e c t s  a s  r e t u r n  v a l u e s .  Naturally, you can also use an object as a return value from \na method. The method might return an object passed to it as an argument, as in the \nexample below, or it might create an object and return a reference to it. This capa -\nbility is important because \nJava methods allow only one \nreturn value\u2014using objects \nenables us to write code that, \nin effect, returns multiple \nvalues.\npublic class  FlipsMax \n{\n   public static Counter max(Counter x, Counter y)\n   {\n      if (x.tally() > y.tally()) return x;\n      else                       return y;\n   }\n   public static void main(String[] args)\n   {\n      int T = Integer.parseInt(args[0]);\n      Counter heads = new Counter(\"heads\");\n      Counter tails = new Counter(\"tails\");\n      for (int t = 0; t < T; t++)\n         if (StdRandom.bernoulli(0.5))\n              heads.increment();\n         else tails.increment();\n      if (heads.tally() == tails.tally()) \n           StdOut.println(\"Tie\");\n      else StdOut.println(max(heads, tails) + \" wins\");\n   } \n}\nExample of a static method with object arguments and return values\n% java FlipsMax 1000000 \n500281 tails wins\n711.2 \u25a0 Data Abstraction\n   A r r a y s  a r e  o b j e c t s .  In Java, every value of any nonprimitive type is an object. In par ", "start": 83, "end": 84}, "109": {"text": "wins\n711.2 \u25a0 Data Abstraction\n   A r r a y s  a r e  o b j e c t s .  In Java, every value of any nonprimitive type is an object. In par -\nticular, arrays are objects. As with strings, there is special language support for certain \noperations on arrays: declarations, initialization, and indexing. As with any other ob-\nject, when we pass an array to a method or use an array variable on the right hand side \nof an assignment statement, we are making a copy of the array reference, not a copy \nof the array. This convention is appropriate for the typical case where we expect the \nmethod to be able to modify the array, by rearranging its entries, as, for example, in \njava.util.Arrays.sort() or the shuffle() method that we considered on page 32.\n  A r r a y s   o f  o b j e c t s .  Array entries can be of any type, as we have already seen: args[] in \nour main() implementations is an array of String objects. When we create an array of \nobjects, we do so in two steps:\n\u25a0 Create the array, using the bracket syntax for array constructors.\n\u25a0 \n \n \nCreate each object in the array, using a standard constructor for each.\nFor example, the code below simulates rolling a die, using an array of Counter objects \nto keep track of the number of occurrences of each possible value.  An array of objects \nin Java is an array of references to objects, not the objects themselves. If the objects are \nlarge, then we may gain ef\ufb01ciency by not having to move them around, just their refer-\nences. If they are small, we may lose ef\ufb01ciency by having to follow a reference each time \nwe need to get to some information. \npublic class  Rolls \n{\n   public static ", "start": 84, "end": 84}, "110": {"text": "just their refer-\nences. If they are small, we may lose ef\ufb01ciency by having to follow a reference each time \nwe need to get to some information. \npublic class  Rolls \n{\n   public static void main(String[] args)\n   {\n      int T = Integer.parseInt(args[0]);\n      int SIDES = 6;\n      Counter[] rolls = new Counter[SIDES+1];\n      for (int i = 1; i <= SIDES; i++)\n         rolls[i] = new Counter(i + \"'s\");\n      for (int t = 0; t < T; t++)\n      {\n         int result = StdRandom.uniform(1, SIDES+1);\n         rolls[result].increment();\n      }\n      for (int i = 1; i <= SIDES; i++)\n         StdOut.println(rolls[i]);\n   } \n}\nCounter client that simulates T rolls of a die\n% java Rolls 1000000 \n167308 1's \n166540 2's \n166087 3's \n167051 4's \n166422 5's \n166592 6's\n72 CHAPTER 1 \u25a0 Fundamentals\n    \nWith this focus on objects , writing code that embraces data abstraction (de\ufb01ning \nand using data types, with data-type values held in objects) is widely referred to as \nobject-oriented programming. The basic concepts that we have just covered are the start-\ning point for object-oriented programming, so it is worthwhile to brie\ufb02y summarize \nthem. A data type is a set of values and a set of operations de\ufb01ned on those values. We \nimplement data types in independent Java class modules and write client programs \nthat use them. An object is an entity that can take on a data-type value or an  instance of \na data type. Objects are characterized by three essential properties: state, identity, and \nbehavior.  A ", "start": 84, "end": 85}, "111": {"text": "programs \nthat use them. An object is an entity that can take on a data-type value or an  instance of \na data type. Objects are characterized by three essential properties: state, identity, and \nbehavior.  A data-type implementation supports clients of the data type as follows:\n\u25a0 Client code can create objects (establish identity) by using the new construct to \ninvoke a constructor that creates an object, initializes its instance variables, and \nreturns a reference to that object.\n \u25a0 Client code can manipulate data-type values (control an object\u2019s behavior, pos-\nsibly changing its state) by using a variable associated with an object to invoke \nan instance method that operates on that object\u2019s instance variables. \n\u25a0 \n \nClient code can manipulate objects by creating arrays of objects and passing them \nand returning them to methods, in the same way as for primitive-type values, \nexcept that variables refer to references to values, not the values themselves.\nThese capabilities are the foundation of a \ufb02exible, modern, and widely useful program-\nming style that we will use as the basis for studying algorithms in this book.\n731.2 \u25a0 Data Abstraction\n Examples of abstract data types The Java language has thousands of built-in \nADTs, and we have de\ufb01ned many other ADTs to facilitate the study of algorithms. In-\ndeed, every Java program that we write is a data-type implementation (or a library of \nstatic methods). T o control complexity, we will speci\ufb01cally cite APIs for any ADT that \nwe use in this book (not many, actually). \nIn this section, we introduce as examples several data types, with some examples \nof client code. In some cases, we present excerpts of APIs that may contain dozens of \ninstance methods or more. We articulate these APIs to present real-world examples, to \nspecify the instance methods that we will use in the book, and to emphasize that you ", "start": 85, "end": 86}, "112": {"text": "we present excerpts of APIs that may contain dozens of \ninstance methods or more. We articulate these APIs to present real-world examples, to \nspecify the instance methods that we will use in the book, and to emphasize that you \ndo not need to know the details of an ADT implementation in order to be able to use it. \nFor reference, the data types that we use and develop in this book are shown on the \nfacing page. These fall into several different categories:\n\u25a0 Standard system ADTs in java.lang.*, which can be used in any Java program.\n\u25a0 Java ADTs in libraries such as java.awt, java.net, and java.io, which can also \nbe used in any Java program, but need an import statement.\n\u25a0 Our I/O ADTs that allow us to work with multiple input/output streams similar \nto StdIn and StdOut.\n\u25a0 Data-oriented ADTs whose primary purpose is to facilitate organizing and pro-\ncessing data by encapsulating the representation. We describe several examples \nfor applications in computational geometry and information processing later in \nthis section and use them as examples in client code later on.\n\u25a0 Collection ADTs whose primary purpose is to facilitate manipulation collections \nof data of the same. We describe the basic Bag, Stack, and Queue types in Sec-\ntion 1.3, PQ types in Chapter 2, and the ST and SET types in Chapters 3 and 5.\n\u25a0 Opertions-oriented ADTs that we use to analyze algorithms, as described in Sec-\ntion 1.4 and Section 1.5.\n\u25a0 \n \nADTs for graph algorithms, including both data-oriented ADTs that focus on \nencapsulating representations of various kinds of graphs and operations-orient-\ned ADTs that focus on providing speci\ufb01cations for graph-processing algorithms.\nThis list does not include the dozens of types that we consider in exercises, which may \nbe found in the index. Also, as described ", "start": 86, "end": 86}, "113": {"text": "operations-orient-\ned ADTs that focus on providing speci\ufb01cations for graph-processing algorithms.\nThis list does not include the dozens of types that we consider in exercises, which may \nbe found in the index. Also, as described on page 90, we often distinguish multiple imple-\nmentations of various ADTs with a descriptive pre\ufb01x. As a group, the ADTs that we \nuse demonstrate that organizing and understanding the data types that you use is an \nimportant factor in modern programming.\nA typical application might use only \ufb01ve to ten of these ADTs. A prime goal in the \ndevelopment and organization of the ADTs in this book is to enable programmers to \neasily take advantage of a relatively small set of them in developing client code.\n74 CHAPTER 1 \u25a0 Fundamentals\n standard Java system types in java.lang\nInteger int wrapper\nDouble double wrapper\nString indexed chars\nStringBuilder builder for strings\nother Java types\njava.awt.Color colors\njava.awt.Font fonts\njava.net.URL URLs\njava.io.File \ufb01les\nour standard I/O types\nIn input stream\nOut output stream\nDraw drawing\ndata-oriented types  for client examples\nPoint2D point in the plane\nInterval1D 1D interval\nInterval2D 2D interval\nDate date\nTransaction transaction\ntypes for the analysis of algorithms\nCounter counter\nAccumulator accumulator\nVisualAccumulator visual version\nStopwatch stopwatch\ncollection types\nStack pushdown stack\nQueue FIFO queue\nBag bag\nMinPQ MaxPQ priority queue\nIndexMinPQ IndexMinPQ priority queue (indexed )\nST symbol table\nSET set\nStringST symbol table (string keys )\ndata-oriented graph types\nGraph graph\nDigraph directed graph\nEdge edge (weighted )\nEdgeWeightedGraph graph (weighted )\nDirectedEdge edge (directed, weighted )\nEdgeWeightedDigraph graph (directed, weighted ", "start": 86, "end": 87}, "114": {"text": ")\ndata-oriented graph types\nGraph graph\nDigraph directed graph\nEdge edge (weighted )\nEdgeWeightedGraph graph (weighted )\nDirectedEdge edge (directed, weighted )\nEdgeWeightedDigraph graph (directed, weighted )\noperations-oriented graph types\nUF dynamic connectivity\nDepthFirstPaths DFS path searcher\nCC connected components\nBreadthFirstPaths BFS path search\nDirectedDFS DFS digraph path search\nDirectedBFS BFS digraph path search\nTransitiveClosure all paths\nTopological topological order\nDepthFirstOrder DFS order\nDirectedCycle cycle search\nSCC strong components\nMST minimum spanning tree\nSP shortest paths\nSelected ADTs used in this book\n751.2 \u25a0 Data Abstraction\n  G e o m e t r i c  o b j e c t s .  A natural example of object-oriented programming is design-\ning data types for geometric objects. For example, the APIs on the facing page de\ufb01ne \nabstract data types for three familiar \ngeometric objects: Point2D (points \nin the plane), Interval1D (intervals \non the line), and Interval2D (two-\ndimensional intervals in the plane, or \naxis-aligned rectangles). As usual, the \nAPIs are essentially self-documenting \nand lead immediately to easily under-\nstood client code such as the example \nat left, which reads the boundaries \nof an Interval2D and an integer T\nfrom the command line, generates T\nrandom points in the unit square, and \ncounts the number of points that fall \nin the interval (an estimate of the area \nof the rectangle). For dramatic effect, \nthe client also draws the interval and \nthe points that fall outside the inter -\nval. This computation is a model for \na method that reduces the problem \nof computing the area and volume \nof geometric shapes to the problem \nof determining whether a point falls \nwithin the shape or not ", "start": 87, "end": 88}, "115": {"text": "outside the inter -\nval. This computation is a model for \na method that reduces the problem \nof computing the area and volume \nof geometric shapes to the problem \nof determining whether a point falls \nwithin the shape or not (a less dif\ufb01cult but not trivial prob -\nlem). Of course, we can de\ufb01ne APIs for other geometric ob-\njects such as line segments, triangles, polygons, circles, and \nso forth, though implementing operations on them can be \nchallenging. Several examples are addressed in the exercises \nat the end of this section. \nPrograms that process geometric objects  have wide \napplication in computing with models of the natural world, \nin scienti\ufb01c computing, video games, movies, and many \nother applications. The development and study of such pro-\ngrams and applications has blossomed into a far-reaching \n\ufb01eld of study known as  computational geometry , which is a \npublic static void main(String[] args) \n{\n   double xlo = Double.parseDouble(args[0]);\n   double xhi = Double.parseDouble(args[1]);\n   double ylo = Double.parseDouble(args[2]);\n   double yhi = Double.parseDouble(args[3]);\n   int T = Integer.parseInt(args[4]);\n   Interval1D x = new Interval1D(xlo, xhi);\n   Interval1D y = new Interval1D(ylo, yhi);\n   Interval2D box = new Interval2D(x, y);\n   box.draw();\n   Counter c = new Counter(\u201chits\u201d);\n   for (int t = 0; t < T; t++)\n   {\n      double x = Math.random();\n      double y = Math.random();\n      Point p = new Point(x, y);\n      if (box.contains(p)) c.increment();\n      else                 p.draw();\n   }\n   StdOut.println(c);\n   StdOut.println(box.area()); \n}\nInterval2D test client\n% ", "start": 88, "end": 88}, "116": {"text": "p = new Point(x, y);\n      if (box.contains(p)) c.increment();\n      else                 p.draw();\n   }\n   StdOut.println(c);\n   StdOut.println(box.area()); \n}\nInterval2D test client\n% java Interval2D .2 .5 .5 .6 10000\n297 hits\n.03\n76 CHAPTER 1 \u25a0 Fundamentals\n   \nfertile area of examples for the application of the algorithms that we address in this \nbook, as you will see in examples throughout the book. In the present context, our \ninterest is to suggest that abstract data types that directly represent geometric abstrac-\ntions are not dif\ufb01cult to de\ufb01ne and can lead to simple and clear client code. This idea is \nreinforced in several exercises at the end of this section and on the booksite.\npublic class    Point2D \nPoint2D(double x, double y) create a point\ndouble x() x coordinate\ndouble y() y coordinate\ndouble r() radius ( polar coordinates)\ndouble theta() angle (polar coordinates)\ndouble distTo(Point2D that) Euclidean distance from this point to that\nvoid draw() draw the point on StdDraw\n A n  A P I  f o r  p o i n t s  i n  t h e  p l a n e\npublic class  Interval1D \nInterval1D(double lo, double hi) create an interval\ndouble length() length of the interval\nboolean contains(double x) does the interval contain x?\nboolean intersects(Interval1D that) does the interval intersect that?\nvoid draw() draw the interval on StdDraw \nAn API for intervals on the line\npublic class  Interval2D\nInterval2D(Interval1D x, Interval1D y) create a 2D interval\ndouble area() area of the 2D interval\nboolean contains(Point p) does ", "start": 88, "end": 89}, "117": {"text": "line\npublic class  Interval2D\nInterval2D(Interval1D x, Interval1D y) create a 2D interval\ndouble area() area of the 2D interval\nboolean contains(Point p) does the 2D interval contain p?\nboolean intersects(Interval2D that) does the 2D interval intersect that?\nvoid draw() draw the 2D interval on StdDraw \nAn API for two dimensional intervals in the plane\n771.2 \u25a0 Data Abstraction\n Information processing Whether it be a bank processing millions of credit card trans-\nactions or a web analytics company processing billions of touchpad taps or a scien-\nti\ufb01c research group processing millions of experimental observations, a great many \napplications are centered around processing and organizing information. Abstract data \ntypes provide a natural mechanism for organizing the information. Without getting \ninto details, the two APIs on the facing page suggest a typical approach for a commer -\ncial application. The idea is to de\ufb01ne data types that allow us to keep information in \nobjects that correspond to things in the real world. A date is a day, a month, and a year \nand a transaction is a customer, a date, and an amount. These two are just examples: we \nmight also de\ufb01ne data types that can hold detailed information for customers, times, \nlocations, goods and services, or whatever. Each data type consists of constructors that \ncreate objects containing the data and methods for use by client code to access it. T o \nsimplify client code, we provide two constructors for each type, one that presents the \ndata in its appropriate type and another that parses a string to get the data (see Exer-\ncise 1.2.19 for details). As usual, there is no reason for client code to know the rep -\nresentation of the data. Most often, the reason to organize the data in this way is to \ntreat ", "start": 89, "end": 90}, "118": {"text": "Exer-\ncise 1.2.19 for details). As usual, there is no reason for client code to know the rep -\nresentation of the data. Most often, the reason to organize the data in this way is to \ntreat the data associated with an object as a single entity: we can maintain arrays of \nTransaction values, use  Date values as a argument or a return value for a method, and \nso forth. The focus of such data types is on encapsulating the data, while at the same \ntime enabling the development of client code that does not depend on the representa -\ntion of the data. We do not dwell on organizing information in this way, except to take \nnote that doing so and including the inherited methods toString(), compareTo(), \nequals(), and hashCode() allows us to take advantage of algorithm implementations \nthat can process any type of data . We will discuss inherited methods in more detail \non page 100.  For example, we have already noted Java\u2019s convention that  enables clients \nto print a string representation of every value if we include toString() implemen-\ntation in a data type. We consider conventions corresponding to the other inherited \nmethods in Section 1.3, Section 2.5, Section 3.4, and Section 3.5, using Date and \nTransaction as examples. Section 1.3 gives classic examples of data types and a Java \nlanguage mechanism known as parameterized types, or generics, that takes advantage of \nthese conventions, and Chapter 2 and Chapter 3 are also devoted to taking advantage \nof generic types and inherited methods to develop implementations of sorting and \nsearching algorithms that are effective for any type of data. \nWhenever you have data of different types  that logically belong together, it is \nworthwhile to contemplate de\ufb01ning an ADT as in these examples. The ability to do so \nhelps to organize the data, can ", "start": 90, "end": 90}, "119": {"text": "\nWhenever you have data of different types  that logically belong together, it is \nworthwhile to contemplate de\ufb01ning an ADT as in these examples. The ability to do so \nhelps to organize the data, can greatly simplify client code in typical applications, and \nis an important step on the road to data abstraction.\n78 CHAPTER 1 \u25a0 Fundamentals\n public class  Date implements Comparable<Date> \nDate(int month, int day, int year) create a date\nDate(String date) create a date (parse constructor)\nint month() month\nint day() day\nint year() year\nString toString() string representation\nboolean equals(Object that) is this the same date as that?\nint compareTo(Date that) compare this date to that\nint hashCode() hash code\npublic class  Transaction implements Comparable<Transaction> \nTransaction(String who, Date when, double amount)\nTransaction(String transaction) create a transaction (parse constructor)\nString who() customer name\nDate when() date\ndouble amount() amount\nString toString() string representation\nboolean equals(Object that) is this the same transaction as that?\nint compareTo(Transaction that) compare this transaction to that\nint hashCode() hash code\n S a m p l e  A P I s  f o r  c o m m e r c i a l  a p p l i c a t i o n s  ( d a t e s  a n d  t r a n s a c t i o n s )\n791.2 \u25a0 Data Abstraction\n   S t r i n g s .  Java\u2019s String is an important and useful ADT. A String is an indexed se-\nquence of char values. String has dozens of instance methods, including the following: \npublic class    String\nString() create an empty string\nint length() length of the string\nint charAt(int i) ith character\nint indexOf(String p) first occurrence of p (-1 ", "start": 90, "end": 92}, "120": {"text": "methods, including the following: \npublic class    String\nString() create an empty string\nint length() length of the string\nint charAt(int i) ith character\nint indexOf(String p) first occurrence of p (-1 if none)\nint indexOf(String p, int i)  first occurrence of p after i (-1 if none)\nString concat(String t) this string with t appended\nString substring(int i, int j) substring of this string (ith to j-1st chars)\nString[] split(String delim) strings between occurrences of delim\nint compareTo(String t) string comparison\nboolean equals(String t) is this string\u2019s value the same as t\u2019s ?\nint hashCode() hash code\n J a v a  String API (partial list of methods)\nString values are similar to arrays of characters, but the two are not the same. Ar -\nrays have built-in Java language syntax for accessing a character; String has instance \nmethods for indexed access, length, and many other operations. On the other hand, \nString has special language support for initialization and concatenation: instead of \ncreating and initializing a string with a constructor, we can use a string  literal; instead \nof invoking the method concat() we can use the  + operator. We do not need to con-\nsider the details of the implementation, though \nunderstanding performance characteristics of \nsome of the methods is important when develop-\ning string-processing algorithms, as you will see \nin Chapter 5. Why not just use arrays of charac-\nters instead of String values? The answer to this \nquestion is the same as for any ADT: to simplify \nand clarify client code . With String, we can write \nclear and simple client code that uses numerous \nconvenient instance methods without regard to \nthe way in which strings are represented (see fac-\ning page). Even this short list contains powerful \noperations that require advanced algorithms such \nString a = \"now ", "start": 92, "end": 92}, "121": {"text": "code that uses numerous \nconvenient instance methods without regard to \nthe way in which strings are represented (see fac-\ning page). Even this short list contains powerful \noperations that require advanced algorithms such \nString a = \"now is \";\nString b = \"the time \";\nString c = \"to\"\na.length()\na.charAt(4)\na.concat(c)\na.indexOf(\"is\")\na.substring(2, 5)\na.split(\" \")[0]\na.split(\" \")[1]\nb.equals(c)\n7\ni\n\"now is to\"\n4\n\"w i\"\n\"now\"\n\"is\"\nfalse\ncall value\nExamples of string operations\n80 CHAPTER 1 \u25a0 Fundamentals\n task                                   implementation\nis the string\na  palindrome?\npublic static boolean isPalindrome(String s) \n{  \n   int N = s.length();\n   for (int i = 0; i < N/2; i++)\n      if (s.charAt(i) != s.charAt(N-1-i))\n         return false;\n   return true; \n}\nextract \ufb01le name\nand extension from a \ncommand-line\nargument\nString s = args[0]; \nint dot = s.rank(\".\");\nString base      = s.substring(0, dot);\nString extension = s.substring(dot + 1, s.length());\nprint all lines in\n standard input that\n contain a string\nspeci\ufb01ed on the\ncommand line\nString query = args[0]; \nwhile (!StdIn.isEmpty()) \n{\n   String s = StdIn.readLine();\n   if (s.contains(query)) StdOut.println(s); \n}\ncreate an array\nof the strings on StdIn \ndelimited by whitespace\nString input = StdIn.readAll(); \nString[] words = input.split(\"\\\\s+\");\ncheck whether an array \nof strings is in\n alphabetical order\npublic boolean isSorted(String[] a) \n{\n   for (int i = 1; i < a.length; i++)\n ", "start": 92, "end": 93}, "122": {"text": "\nString[] words = input.split(\"\\\\s+\");\ncheck whether an array \nof strings is in\n alphabetical order\npublic boolean isSorted(String[] a) \n{\n   for (int i = 1; i < a.length; i++)\n   {\n      if (a[i-1].compareTo(a[i]) > 0)\n         return false;\n   }\n   return true; \n}\n T y p i c a l  s t r i n g - p r o c e s s i n g  c o d e\n811.2 \u25a0 Data Abstraction\n  \n \nas those considered in Chapter 5. For example, the argument of split() can be a  \n  r e g u l a r  e x p r e s s i o n (see Section 5.4)\u2014the split() example on page 81 uses the argu-\nment \"\\\\s+\", which means \u201cone or more tabs, spaces, newlines, or returns. \u201d\n    I n p u t  a n d  o u t p u t  r e v i s i t e d .  A disadvantage of the StdIn, StdOut, and StdDraw stan-\ndard libraries of Section 1.1 is that they restrict us to working with just one input \ufb01le, \none output \ufb01le, and one drawing for any given program. With object-oriented pro -\ngramming, we can de\ufb01ne similar mechanisms that allow us to work with multiple input \nstreams, output streams, and drawings within one program. Speci\ufb01cally, our standard \nlibary includes the data types In, Out, and  Draw with the APIs shown on the facing page, \nWhen invoked with a constructor having a String argument, In and Out will \ufb01rst try \nto \ufb01nd a \ufb01le in the current directory of your computer that has that name. If it cannot \ndo so, ", "start": 93, "end": 94}, "123": {"text": "\nWhen invoked with a constructor having a String argument, In and Out will \ufb01rst try \nto \ufb01nd a \ufb01le in the current directory of your computer that has that name. If it cannot \ndo so, it will assume the argu-\nment to be a website name and \nwill try to connect to that web -\nsite (if no such website exists, it \nwill issue a runtime exception). \nIn either case, the speci\ufb01ed \ufb01le \nor website becomes the source/\ntarget of the input/output for \nthe stream object thus created, \nand the read*() and print*()\nmethods will refer to that \ufb01le or \nwebsite. (If you use the no-argu-\nment constructor, then you ob-\ntain the standard streams.) This \narrangement makes it possible \nfor a single program to process \nmultiple \ufb01les and drawings. Y ou also can assign such \nobjects to variables, pass them as arguments or re-\nturn values from methods, create arrays of them, and \nmanipulate them just as you manipulate objects of \nany type. The program Cat shown at left is a sample \nclient of In and Out that uses multiple input streams \nto concatenate several input \ufb01les into a single out -\nput \ufb01le. The In and Out classes also contain static \nmethods for reading \ufb01les containing values that are \nall int, double, or String types into an array (see \npage 126 and Exercise 1.2.15).\npublic class  Cat \n{\n   public static void main(String[] args)\n   {  // Copy input files to out (last argument).\n      Out out = new Out(args[args.length-1]);\n      for (int i = 0; i < args.length - 1; i++)\n      {  // Copy input file named on ith arg to out.\n         In in = new In(args[i]);\n         String s ", "start": 94, "end": 94}, "124": {"text": "Out(args[args.length-1]);\n      for (int i = 0; i < args.length - 1; i++)\n      {  // Copy input file named on ith arg to out.\n         In in = new In(args[i]);\n         String s = in.readAll();\n         out.println(s);\n         in.close();\n      }\n      out.close();\n   } \n} \nA sample In and Out client\n% more in1.txt \nThis is\n% more in2.txt \na tiny \ntest.\n% java Cat in1.txt in2.txt out.txt\n% more out.txt \nThis is \na tiny \ntest.\n82 CHAPTER 1 \u25a0 Fundamentals\n public class    In \nIn() create an input stream from standard input\nIn(String name) create an input stream from a \ufb01le or website\nboolean isEmpty() true if no more input, false otherwise\nint readInt() read a value of type int\ndouble readDouble() read a value of type double\n...\nvoid close() close the input stream\nNote: all operations supported by StdIn are also supported for In objects.\nAPI for our data type for input streams\npublic class    Out\nOut() create an output stream to standard output\nOut(String name) create an output stream to a \ufb01le\nvoid print(String s) append s to the output stream\nvoid println(String s) append s and a newline to the output stream\nvoid println() append a newline to the output stream\nvoid printf(String f, ...) formatted print to the output steam\nvoid close() close the output stream\nNote: all operations supported by StdOut are also supported for Out objects.\nAPI for our data type for output streams\npublic class    Draw \nDraw()\nvoid line(double x0, double y0, double x1, double y1) \nvoid point(double x, double y)\n...\nNote: all operations supported by StdDraw are also supported for Draw objects.\nAPI for ", "start": 94, "end": 95}, "125": {"text": "\nDraw()\nvoid line(double x0, double y0, double x1, double y1) \nvoid point(double x, double y)\n...\nNote: all operations supported by StdDraw are also supported for Draw objects.\nAPI for our data type for drawings\n831.2 \u25a0 Data Abstraction\n  \n \n \n \nImplementing an abstract data type. As with libraries of static methods, we \nimplement ADTs with a Java class, putting the code in a \ufb01le with the same name as \nthe class, followed by the .java extension. The \ufb01rst statements in the \ufb01le declare    in-\nstance variables that de\ufb01ne the data-type values. Following the instance variables are the \nconstructor and the    instance methods that implement operations on data-type values. \nInstance methods may be  public (speci\ufb01ed in the API) or  private (used to organize the \ncomputation and not available to clients). A data-type de\ufb01nition may have multiple \nconstructors and may also include de\ufb01nitions of static methods. In particular, a unit-\ntest client main() is normally useful for testing and debugging. As a \ufb01rst example, we \nconsider an implementation of the Counter ADT that we de\ufb01ned on page 65. A full \nannotated implementation is shown on the facing page, for reference as we discuss its \nconstituent parts. Every ADT implementation that you will develop has the same basic \ningredients as this simple example. \n I n s t a n c e  v a r i a b l e s .  To  d e \ufb01 n e  d a t a - t y p e  \nvalues (the state of each object), we de-\nclare instance variables  in much the same \nway as we declare local variables. There is a \ncritical distinction between instance vari-\nables and the local variables within a static \nmethod or a ", "start": 95, "end": 96}, "126": {"text": "state of each object), we de-\nclare instance variables  in much the same \nway as we declare local variables. There is a \ncritical distinction between instance vari-\nables and the local variables within a static \nmethod or a block that you are accustomed to: there is just one value corresponding to \neach local variable at a given time, but there are numerous values corresponding to each \ninstance variable (one for each object that is an instance of the data type). There is no \nambiguity with this arrangement, because each time that we access an instance variable, \nwe do so with an object name\u2014that object is the one whose value we are accessing. \nAlso, each declaration is quali\ufb01ed by a  visibility modi\ufb01er. In ADT implementations, we \nuse private, using a Java language mechansim to enforce the idea that the representa-\ntion of an ADT is to be hidden from the client, and also  final, if the value is not to be \nchanged once it is initialized. Counter has two instance variables: a String value name\nand an int value count. If we were to use public instance variables (allowed in Java) \nthe data type would, by de\ufb01nition, not be abstract, so we do not do so.\n  C o n s t r u c t o r s .  Every Java class has at least one constructor that establishes an object\u2019s \nidentity. A constructor is like a static method, but it can refer directly to instance vari -\nables and has no return value. Generally, the purpose of a constructor is to initialize \nthe instance variables. Every constructor creates an object and provides to the client a \nreference to that object. Constructors always share the same name as the class. We can \noverload the name and have multiple constructors with different signatures, just as \nwith methods. If no other constructor is de\ufb01ned, a default no-argument constructor ", "start": 96, "end": 96}, "127": {"text": "always share the same name as the class. We can \noverload the name and have multiple constructors with different signatures, just as \nwith methods. If no other constructor is de\ufb01ned, a default no-argument constructor is \n   public class Counter\n   {\n      private final String name;\n      private int count;  \n   ...\n   } \nInstance variables in ADTs are  private\ninstance\nvariable\ndeclarations\n84 CHAPTER 1 \u25a0 Fundamentals\n public class Counter\n{\n   private final String name;\n   private int count;\n   public Counter(String id)\n   { name = id; }\n   public void increment()\n   { count++; }\n   public int tally()\n   { return count; }\n   public String toString()\n   { return count + \" \" + name; }\n   public static void main(String[] args)\n   {\n      Counter heads = new Counter(\"heads\");\n      Counter tails = new Counter(\"tails\");\n      heads.increment();\n      heads.increment();\n      tails.increment();\n      StdOut.println(heads + \" \" + tails);\n      StdOut.println(heads.tally() + tails.tally() );\n   }\n}\nAnatomy of a class that defines a data type \ninstance\nvariables\ninstance\nmethods\nconstructor\ntest client\ninvoke\nconstructor\ninvoke\nmethod\nautomatically invoke\ntoString()\ninstance\nvariable\n name\ncreate\nand\ninitialize\nobjects\nobject\nname\nclass\nname\n851.2 \u25a0 Data Abstraction\n implicit, has no arguments, and initializes instance values to default values. The default \nvalues of instance variables are 0 for primitive numeric types, false for boolean, and \nnull for reference types. These defaults \nmay be changed by using initializing \ndeclarations for instance variables. Java \nautomatically invokes a constructor \nwhen a client program uses the keyword \nnew. Overloaded constructors are typi -\ncally used to initialize instance variables \nto client-supplied values other than the \ndefaults. ", "start": 96, "end": 98}, "128": {"text": "for instance variables. Java \nautomatically invokes a constructor \nwhen a client program uses the keyword \nnew. Overloaded constructors are typi -\ncally used to initialize instance variables \nto client-supplied values other than the \ndefaults.  For example, Counter has a \none-argument constructor that initial-\nizes the name instance variable to the \nvalue given as argument (leaving the \ncount instance variable to be initialized \nto the default value 0). \n  I n s t a n c e  m e t h o d s .  To  i m p l e m e n t  d a t a - t y p e  i n s t a n ce  m e t h o d s  ( t h e  behavior of each \nobject), we implement instance methods with code that is precisely like the code that you \nlearned in Section 1.1 to implement static methods (functions). Each instance method \nhas a return type, a    signature (which speci\ufb01es its name and the types and names of its \nparameter variables), and a body (which \nconsists of a sequence of statements, in -\ncluding a  return statement that provides \na value of the return type back to the cli -\nent). When a client invokes a method, the \nparameter values (if any) are initialized \nwith client values, the statements are ex -\necuted until a return value is computed, \nand the value is returned to the client, \nwith the same effect as if the method in -\nvocation in the client were replaced with that value. All of this action is the same as for \nstatic methods, but there is one critical distinction for instance methods: they can access \nand perform operations on instance variables. How do we specify which object\u2019s instance \nvariables we want to use? If you think about this question for a moment, you will see \nthe logical answer: a reference ", "start": 98, "end": 98}, "129": {"text": "they can access \nand perform operations on instance variables. How do we specify which object\u2019s instance \nvariables we want to use? If you think about this question for a moment, you will see \nthe logical answer: a reference to a variable in an instance method refers to the value \nfor the object that was used to invoke the method. When we say heads.increment() the \ncode in increment() is referring to the instance variables for heads. In other words, \nmethod\nname\nreturnvisibility \ntypemodifier signature\ninstance variable name\nAnatomy of an instance method\npublic void increment()\n{ count++; }\npublic class Counter\n{\n   private final String name;\n   private int count;\n   ...\n   ...\n}\ncode to initialize instance variables\n(count initialized to 0 by default)\nvisibility\nmodifier NO return\ntype\nconstructor name\n(same as class name)\nsignature\nparameter\nvariable\nAnatomy of a constructor\n   public  Counter ( String id )\n   { name = id; }\n86 CHAPTER 1 \u25a0 Fundamentals\n object-oriented programming adds one critically important additional way to use vari-\nables in a Java program:\n\u25a0 to invoke an instance method that operates on the object\u2019s values. \nThe difference from working solely with static methods is semantic (see the Q&A), \nbut has reoriented the way that modern programmers think about developing code in \nmany situations. As you will see, it also dovetails well with the study of algorithms and \ndata structures.\n  S c o p e .  In summary, the Java code that we write to implement instance methods uses \nthree kinds of variables:\n\u25a0 Parameter variables\n\u25a0 Local variables\n\u25a0 \n \nInstance variables\nThe \ufb01rst two of these are the same as for static methods: parameter variables are spec -\ni\ufb01ed in the method signature and initialized with client values when the method is \ncalled, and local variables are declared and initialized within the method body. The \nscope ", "start": 98, "end": 99}, "130": {"text": "same as for static methods: parameter variables are spec -\ni\ufb01ed in the method signature and initialized with client values when the method is \ncalled, and local variables are declared and initialized within the method body. The \nscope of parameter variables is the entire method; the scope of local variables is the \nfollowing statements in the block where they are de\ufb01ned. Instance variables are com-\npletely different: they hold data-type values for objects in a class, and their scope is the \nentire class (whenever there is an ambiguity, you can use the   this pre\ufb01x to identify in-\nstance variables). Understanding the distinctions among these three kinds of variables \nin instance methods is a key to success in object-oriented programming.\npublic class Example\n{\n   private int var;\n   ...\n   private void method1()\n   {\n      int var;\n      ...  var       ...\n      ...  this.var  ...\n   }\n   \n   private void method2()\n   {\n      ...  var       ...\n   }\n   ...\n}\nScope of instance and local variables in an instance method\ninstance\nvariable\nrefers to local variable, NOT instance variable\nrefers to instance variable\nrefers to instance variable\nlocal variable\n871.2 \u25a0 Data Abstraction\n  \nAPI, clients, and  implementations. These are the basic components that you need \nto understand to be able to build and use abstract data types in Java. Every ADT im -\nplementation that we will consider will be a Java class with private instance variables, \nconstructors, instance methods, and a client. To fully understand a data type, we need \nthe API, typical client code, and an implementation, summarized for Counter on the \nfacing page. T o emphasize the separation of client and implementation, we normally \npresent each client as a separate class containing a static method main() and reserve \ntest client\u2019s main() in the data-type de\ufb01nition for minimal unit testing ", "start": 99, "end": 100}, "131": {"text": "o emphasize the separation of client and implementation, we normally \npresent each client as a separate class containing a static method main() and reserve \ntest client\u2019s main() in the data-type de\ufb01nition for minimal unit testing and develop-\nment (calling each instance method at least once). In each data type that we develop, \nwe go through the same steps. Rather than thinking about what action we need to take \nnext to accomplish a computational goal (as we did when \ufb01rst learning to program), we \nthink about the needs of a client, then accommodate them in an ADT, following these \nthree steps:\n\u25a0 \n \n \nSpecify an API. The purpose of the API is to separate clients from implementa-\ntions, to enable modular programming. We have two goals when specifying an \nAPI. First, we want to enable clear and correct client code. Indeed, it is a good \nidea to write some client code before \ufb01nalizing the API to gain con\ufb01dence that \nthe speci\ufb01ed data-type operations are the ones that clients need. Second, we \nwant to be able to implement the operations. There is no point specifying opera-\ntions that we have no idea how to implement.\n\u25a0  Implement a Java class that meets the API speci\ufb01cations. First we choose the \ninstance variables, then we write constructors and the instance methods. \n\u25a0 Develop multiple test clients, to validate the design decisions made in the \ufb01rst \ntwo steps.\nWhat operations do clients need to perform, and what data-type values can best sup -\nport those operations? These basic decisions are at the heart of every implementation \nthat we develop.\n88 CHAPTER 1 \u25a0 Fundamentals\n public class    Counter \n{\n   private final String name;\n   private int count;\n   public Counter(String id)\n   { name = id; }\n   public void increment()\n   { count++; }\n   public int tally()\n ", "start": 100, "end": 101}, "132": {"text": "Fundamentals\n public class    Counter \n{\n   private final String name;\n   private int count;\n   public Counter(String id)\n   { name = id; }\n   public void increment()\n   { count++; }\n   public int tally()\n   { return count; }\n   public String toString()\n   { return count + \" \" + name; }\n}\n A n  a b s t r a c t  d a t a  t y p e  f o r  a  s i m p l e  c o u n t e r\nAPI\ntypical client\napplicationimplementation\npublic class Flips \n{\n   public static void main(String[] args)\n   {\n      int T = Integer.parseInt(args[0]);\n      Counter heads = new Counter(\"heads\");\n      Counter tails = new Counter(\"tails\");\n      for (int t = 0; t < T; t++)\n         if (StdRandom.bernoulli(0.5))\n              heads.increment();\n         else tails.increment();\n      StdOut.println(heads);\n      StdOut.println(tails);\n      int d = heads.tally() - tails.tally();\n      StdOut.println(\"delta: \" + Math.abs(d));\n   } \n}\npublic class Counter\nCounter(String id) create a counter named id \nvoid increment() increment the counter\nint tally() number of increments since creation\nString toString() string representation\n% java Flips 1000000 \n500172 heads \n499828 tails \ndelta: 344\n891.2 \u25a0 Data Abstraction\n More ADT implementations As with any programming concept, the best way \nto understand the power and utility of ADTs is to consider carefully more examples \nand more implementations. There will be ample opportunity for you to do so, as much \nof this book is devoted to ADT implementations, but a few more simple examples will \nhelp us lay the groundwork for addressing them.\nDate. Shown on the facing page ", "start": 101, "end": 102}, "133": {"text": "ample opportunity for you to do so, as much \nof this book is devoted to ADT implementations, but a few more simple examples will \nhelp us lay the groundwork for addressing them.\nDate. Shown on the facing page are two implementations of the Date ADT that we con-\nsidered on page 79. T o reduce clutter, we omit the parsing constructor (which is described \nin Exercise 1.2.19) and the inherited methods equals() (see page 103), compareTo() (see \npage 247),  and hashCode() (see Exercise 3.4.22). The straightforward implementation \non the left maintains the day, month, and year as instance variables, so that the instance \nmethods can just return the appropriate value; the more space-ef\ufb01cient implementa-\ntion on the right uses only a single int value to represent a date, using a mixed-radix \nnumber that represents the date with day d, month m, and year y as 512y + 32m + d. \nOne way that a client might notice the difference between these implementations is by \nviolating implicit assumptions: the second implementation depends for its correctness \non the day being between 0 and 31, the month being between 0 and 15, and the year be-\ning positive (in practice, both implementations should check that months are between \n1 and 12, days are between 1 and 31, and that dates such as June 31 and February 29, \n2009, are illegal, though that requires a bit more work).  This example highlights the \nidea that we rarely fully specify implementation requirements in an API (we normally \ndo the best we can, and could do better here). Another way that a client might notice the \ndifference between the two implementations is performance: the implementation on the \nright uses less space to hold data-type values at the cost of more time to provide ", "start": 102, "end": 102}, "134": {"text": "can, and could do better here). Another way that a client might notice the \ndifference between the two implementations is performance: the implementation on the \nright uses less space to hold data-type values at the cost of more time to provide them to \nthe client in the agreed form (one or two arithmetic operations are needed). Such trad-\neoffs are common: one client may prefer one of the implementations and another client \nmight prefer the other, so we need to accommodate both. Indeed, one of the recurring \nthemes of this book is that we need to understand the space and time requirements of \nvarious implementations and their suitability for use by various clients. One of the key \nadvantages of using data abstraction in our implementations is that we can normally \nchange from one implementation to another without changing any client code.\n M a i n t a i n i n g   m u l t i p l e  i m p l e m e n t a t i o n s .  Multiple implementations of the same API \ncan present maintainence and nomenclature issues. In some cases, we simply want to \nreplace an old implementation with an improved one. In others, we may need to main-\ntain two implementations, one suitable for some clients, the other suitable for others. \nIndeed, a prime goal of this book is to consider in depth several implementations of \neach of a number of fundamental ADTs, generally with different performance charac-\nteristics. In this book, we often compare the performance of a single client using two \n90 CHAPTER 1 \u25a0 Fundamentals\n public static void main(String[] args) \n{\n   int m = Integer.parseInt(args[0]);\n   int d = Integer.parseInt(args[1]);\n   int y = Integer.parseInt(args[2]);\n   Date date = new Date(m, d, y);\n   StdOut.println(date); \n}\npublic class    Date \n{\n   private final int month;\n   private final ", "start": 102, "end": 103}, "135": {"text": "Integer.parseInt(args[1]);\n   int y = Integer.parseInt(args[2]);\n   Date date = new Date(m, d, y);\n   StdOut.println(date); \n}\npublic class    Date \n{\n   private final int month;\n   private final int day;\n   private final int year;\n   public Date(int m, int d, int y)\n   {  month = m; day = d; year = y; }\n   public int month()\n   {  return month;  }\n   public int day()\n   {  return day;  }\n   public int year()\n   {  return day;  }\n   public String toString()\n   {  return month() + \"/\" + day()\n                     + \"/\" + year();  }\n}\npublic class Date \n{\n   private final int value;\n   public Date(int m, int d, int y)\n   { value = y*512 + m*32 + d; }\n   public int month()\n   { return (value / 32) % 16; }\n   public int day()\n   { return value % 32; }\n   public int year()\n   { return value / 512; }\n   public String toString()\n   {  return month() + \"/\" + day()\n                     + \"/\" + year();  }\n}\n% java Date 12 31 1999 \n12/31/1999\n public class Date\nDate(int month, int day, int year) create a date\nint month() month\nint day() day\nint year() year\nString toString() string representation\ntest client\nimplementation\napplication\nalternate implementation\nAPI\n A n  a b s t r a c t  d a t a  t y p e  t o  e n c a p s u l a t e  d a t e s ,  w i t h  t w o  i m p l e m e n t a t i o n s\n911.2 ", "start": 103, "end": 103}, "136": {"text": "t o  e n c a p s u l a t e  d a t e s ,  w i t h  t w o  i m p l e m e n t a t i o n s\n911.2 \u25a0 Data Abstraction\n different implementations of the same API. For this reason, we generally adopt an in-\nformal naming convention where we: \n\u25a0 \n \nIdentify different implementations of the same API by prepending a descrip-\ntive modi\ufb01er. For example, we might name our Date implementations on the \nprevious page BasicDate and SmallDate, and we might wish to develop a \nSmartDate implementation that can validate that dates are legal.\n\u25a0  \n \n \nMaintain a reference implementation with no pre\ufb01x that makes a choice that \nshould be suitable for most clients. That is, most clients should just use Date.\nIn a large system, this solution is not ideal, as it might involve changing client code. For \nexample, if we were to develop a new implementation ExtraSmallDate, then our only \noptions are to change client code or to make it the reference implementation for use by \nall clients. Java has various advanced language mechanisms for maintaining multiple \nimplementations without needing to change client code, but we use them sparingly \nbecause their use is challenging (and even controversial) even for experts, especially in \nconjuction with other advanced language features that we do value (generics and itera-\ntors). These issues are important (for example, ignoring them led to the celebrated Y2K \nproblem at the turn of the millennium, because many programs used their own imple-\nmentations of the date abstraction that did not take into account the \ufb01rst two digits of \nthe year), but detailed consideration of these issues would take us rather far a\ufb01eld from \nthe study of algorithms.\n A c c u m u l a t o r .  The accumulator API ", "start": 103, "end": 104}, "137": {"text": "\ufb01rst two digits of \nthe year), but detailed consideration of these issues would take us rather far a\ufb01eld from \nthe study of algorithms.\n A c c u m u l a t o r .  The accumulator API shown on the facing page de\ufb01nes an abstract data \ntype that provides to clients the ability to maintain a running average of data values. For \nexample, we use this data type frequently in this book to process experimental results \n(see Section 1.4). The implementation is straightforward: it maintains a int instance \nvariable counts the number of data values seen so far and a double instance variable \nthat keeps track of the sum of the values seen so far; to compute the average it divides \nthe sum by the count. Note that the implementation does not save the data values\u2014it \ncould be used for a huge number of them (even on a device that is not capable of \nholding that many), or a huge number of accumulators could be used on a big system. \nThis performance characteristic is subtle and might be speci\ufb01ed in the API, because \nan implementation that does save the values might cause an application to run out of \nmemory. \n92 CHAPTER 1 \u25a0 Fundamentals\n public class TestAccumulator \n{\n   public static void main(String[] args)\n   {\n      int T = Integer.parseInt(args[0]);\n      Accumulator a = new Accumulator();\n      for (int t = 0; t < T; t++)\n         a.addDataValue(StdRandom.random());\n      StdOut.println(a);\n   } \n}\npublic class  Accumulator \n{\n   private double total;\n   private int N;\n   public void addDataValue(double val)\n   {\n       N++;\n       total += val;\n   }\n   public double mean()\n   {  return total/N;  }\n   public String toString()\n   { return \"Mean (\" + N + \" values): \"\n                 + String.format(\"%7.5f\", ", "start": 104, "end": 105}, "138": {"text": "{\n       N++;\n       total += val;\n   }\n   public double mean()\n   {  return total/N;  }\n   public String toString()\n   { return \"Mean (\" + N + \" values): \"\n                 + String.format(\"%7.5f\", mean()); }\n}\n% java TestAccumulator 1000 \nMean (1000 values): 0.51829\n% java TestAccumulator 1000000 \nMean (1000000 values): 0.49948\n% java TestAccumulator 1000000 \nMean (1000000 values): 0.50014\nAn abstract data type for accumulating data values\n public class  Accumulator\nAccumulator() create an accumulator\nvoid addDataValue(double val) add a new data value\ndouble mean() mean of all data values\nString toString() string representation\ntypical client\nimplementation\napplication\nAPI\n931.2 \u25a0 Data Abstraction\n Visual accumulator. The visual accumulator  implementation shown on the facing \npage extends Accumulator to present a useful side effect: it draws on StdDraw all the \ndata (in gray) and the running average (in red). \nThe easiest way to do so is to add a constructor \nthat provides the number of points to be plotted \nand the maximum value, for rescaling the plot. \nVisualAccumulator is not technically an imple-\nmentation of the Accumulator API (its construc-\ntor has a different signature and it causes a differ-\nent prescribed side effect). Generally, we are \ncareful to fully specify APIs and are loath to make \nany changes in an API once articulated, as it might \ninvolve changing an unknown amount of client (and implementation) code, but add-\ning a constructor to gain functionality can sometimes be defended because it involves \nchanging the same line in client code that we change when changing a class name. In \nthis example, if we have ", "start": 105, "end": 106}, "139": {"text": "(and implementation) code, but add-\ning a constructor to gain functionality can sometimes be defended because it involves \nchanging the same line in client code that we change when changing a class name. In \nthis example, if we have developed a client that uses an Accumulator and perhaps has \nmany calls to addDataValue() and avg(), we can enjoy the bene\ufb01ts of \nVisualAccumulator by just changing one line of client code.\nVisual accumulator plot\nheight of gray dot\nis the data point value\nheight of Nth red dot from the left\nis the average of the heights\nof  the leftmost N gray dots\napplication\n% java TestVisualAccumulator 2000\nMean (2000 values): 0.509789\n94 CHAPTER 1 \u25a0 Fundamentals\n public class TestVisualAccumulator \n{\n   public static void main(String[] args)\n   {\n      int T = Integer.parseInt(args[0]);\n      VisualAccumulator a = new VisualAccumulator(T, 1.0);\n      for (int t = 0; t < T; t++)\n         a.addDataValue(StdRandom.random());\n      StdOut.println(a);\n   } \n}\npublic class VisualAccumulator \n{\n   private double total;\n   private int N;\n   public VisualAccumulator(int trials, double max)\n   {\n      StdDraw.setXscale(0, trials);\n      StdDraw.setYscale(0, max);\n      StdDraw.setPenRadius(.005);\n   }\n   public void addDataValue(double val)\n   {\n      N++;\n      total += val;\n      StdDraw.setPenColor(StdDraw.DARK_GRAY);\n      StdDraw.point(N, val);\n      StdDraw.setPenColor(StdDraw.RED);\n      StdDraw.point(N, total/N);\n   }\n   public double mean()\n   public String toString()\n   // Same as Accumulator.\n}\n A n  a b s t r ", "start": 106, "end": 107}, "140": {"text": "StdDraw.setPenColor(StdDraw.RED);\n      StdDraw.point(N, total/N);\n   }\n   public double mean()\n   public String toString()\n   // Same as Accumulator.\n}\n A n  a b s t r a c t  d a t a  t y p e  f o r  a c c u m u l a t i n g  d a t a  v a l u e s  ( v i s u a l  v e r s i o n )\n public class    VisualAccumulator\nVisualAccumulator(int trials, double max)\nvoid addDataValue(double val) add a new data value\ndouble avg() average of all data values\nString toString() string representation\ntypical client\nimplementation\nAPI\n951.2 \u25a0 Data Abstraction\n  \n  D a t a - t y p e  d e s i g n  An abstract data type is a data type whose representation is hid -\nden from the client. This idea has had a powerful effect on modern programming. The \nvarious examples that we have considered give us the vocabulary to address advanced \ncharacteristics of ADTs and their implementation as Java classes. Many of these topics \nare, on the surface, tangential to the study of algorithms, so it is safe for you to skim \nthis section and refer to it later in the context of speci\ufb01c implementation problems. Our \ngoal is to put important information related to designing data types in one place for \nreference and to set the stage for implementations throughout this book. \n   E n c a p s u l a t i o n .  A hallmark of object-oriented programming is that it enables us to \nencapsulate data types within their implementations, to facilitate separate development \nof clients and data type implementations. Encapsulation enables modular program-\nming, allowing us to\n\u25a0 Independently develop of client and implementation code\n\u25a0 Substitute improved implementations without ", "start": 107, "end": 108}, "141": {"text": "\nencapsulate data types within their implementations, to facilitate separate development \nof clients and data type implementations. Encapsulation enables modular program-\nming, allowing us to\n\u25a0 Independently develop of client and implementation code\n\u25a0 Substitute improved implementations without affecting clients\n\u25a0 Support programs not yet written (the API is a guide for any future client)\nEncapsulation also isolates data-type operations, which leads to the possibility of\n\u25a0 Limiting the potential for error\n\u25a0 Adding consistency checks and other debugging tools in implementations\n\u25a0 Clarifying client code\nAn encapsulated data type can be used by any client, so it extends the Java language. \nThe programming style that we are advocating is predicated on the idea of breaking \nlarge programs into small modules that can be developed and debugged independently. \nThis approach improves the resiliency of our software by limiting and localizing the ef-\nfects of making changes, and it promotes code reuse by making it possible to substitute \nnew implementations of a data type to improve performance, accuracy, or memory \nfootprint. The same idea works in many settings. We often reap the bene\ufb01ts of encap -\nsulation when we use system libraries. New versions of the Java system often include \nnew implementations of various data types or static method libraries, but the APIs do \nnot change. In the context of the study of algorithms and data structures, there is strong \nand constant motivation to develop better algorithms because we can improve perfor-\nmance for all clients by substituting an improved ADT implementation without chang-\ning the code of any client. The key to success in modular programming is to maintain \nindependence among modules. We do so by insisting on the API being the only point of \ndependence between client and implementation. You do not need to know how a data \ntype is implemented in order to use it and you can assume that a client knows nothing but \nthe API when implementing a data type. Encapsulation is ", "start": 108, "end": 108}, "142": {"text": "\ndependence between client and implementation. You do not need to know how a data \ntype is implemented in order to use it and you can assume that a client knows nothing but \nthe API when implementing a data type. Encapsulation is the key to attaining both of \nthese advantages.\n96 CHAPTER 1 \u25a0 Fundamentals\n  \n D e s i g n i n g  A P I s .  One of the most important and most challenging steps in building \nmodern software is designing APIs. This task takes practice, careful deliberation, and \nmany iterations, but any time spent designing a good API is certain to be repaid in time \nsaved debugging or code reuse. Articulating an API might seem to be overkill when \nwriting a small program, but you should consider writing every program as though you \nwill need to reuse the code someday. Ideally, an API would clearly articulate behavior \nfor all possible inputs, including side effects, and then we would have software to check \nthat implementations meet the speci\ufb01cation. Unfortunately, a fundamental result from \ntheoretical computer science known as the  speci\ufb01cation problem implies that this goal \nis actually impossible to achieve. Brie\ufb02y, such a speci\ufb01cation would have to be written \nin a formal language like a programming language, and the problem of determining \nwhether two programs perform the same computation is known, mathematically, to be \nundecidable. Therefore, our APIs are brief English-language descriptions of the set of \nvalues in the associated abstract data type along with a list of constructors and instance \nmethods, again with brief English-language descriptions of their purpose, including \nside effects.  T o validate the design, we always include examples of client code in the text \nsurrounding our APIs. Within this broad outline, there are numerous pitfalls that every \nAPI design is susceptible to: \n\u25a0 \n \nAn API may be too hard to implement, ", "start": 108, "end": 109}, "143": {"text": "we always include examples of client code in the text \nsurrounding our APIs. Within this broad outline, there are numerous pitfalls that every \nAPI design is susceptible to: \n\u25a0 \n \nAn API may be too hard to implement, implying implementations that are dif-\n\ufb01cult or impossible to develop.\n\u25a0 An API may be too hard to use, leading to client code that is more complicated \nthan it would be without the API. \n\u25a0 An API may be too narrow, omitting methods that clients need.\n\u25a0 \n \n  \nAn API may be too wide, including a large number of methods not needed \nby any client. This pitfall is perhaps the most common, and one of the most \ndif\ufb01cult to avoid. The size of an API tends to grow over time because it is not \ndif\ufb01cult to add methods to an existing API, but it is dif\ufb01cult to remove methods \nwithout breaking existing clients.\n\u25a0 An API may be too general, providing no useful abstractions. \n\u25a0  An API may be too speci\ufb01c, providing abstractions so detailed or so diffuse as to \nbe useless. \n\u25a0 \n \nAn API may be too dependent on a particular representation, therefore not serv-\ning the purpose of freeing client code from the details of using that representa-\ntion. This pitfall is also dif\ufb01cult to avoid, because the representation is certainly \ncentral to the development of the implementation.\nThese considerations are sometimes summarized in yet another motto: provide to cli-\nents the methods they need and no others. \n971.2 \u25a0 Data Abstraction\n Algorithms and abstract data types. Data abstraction is naturally suited to the study \nof algorithms, because it helps us provide a framework within which we can precisely \nspecify both what an algorithm needs to accomplish and how a client can make use of \nan algorithm. Typically, in this book, an algorithm is an implementation of an instance \nmethod in an abstract data ", "start": 109, "end": 110}, "144": {"text": "which we can precisely \nspecify both what an algorithm needs to accomplish and how a client can make use of \nan algorithm. Typically, in this book, an algorithm is an implementation of an instance \nmethod in an abstract data type. For example, our whitelisting example at the begin -\nning of the chapter is naturally cast as an ADT client, based on the following operations:\n\u25a0 Construct a SET from an array of given values.\n\u25a0 Determine whether a given value is in the set.\nThese operations are encapsulated in the StaticSETofInts ADT, shown on the facing \npage along with Whitelist, a typical client. StaticSETofInts is a special case of the \nmore general and more useful symbol table ADT that is the focus of Chapter 3.  Binary \nsearch is one of several algorithms that we study that is suitable for implementing these \nADTs. By comparison with the BinarySearch implementation on page 47, this imple-\nmentation leads to clearer and more useful client code. For example, StaticSETofInts\nenforces the idea that the array must be sorted before rank() is called. With the abstract \ndata type, we separate the client from the implementation making it easier for any client \nto bene\ufb01t from the ingenuity of the binary search algorithm, just by following the API \n(clients of rank() in BinarySearch have to know to sort the array \ufb01rst). Whitelisting is \none of many clients that can take advantage of binary search. \nEvery Java program  is a set of \nstatic methods and/or a data type \nimplementation. In this book, we \nfocus primarily on abstract data \ntype implementations such as \nStaticSETofInts, where the focus \nis on operations and the representa-\ntion of the data is hidden from the \nclient. As this example illustrates, \ndata abstraction enables us to\n\u25a0 Precisely specify what algorithms can provide for ", "start": 110, "end": 110}, "145": {"text": "\nStaticSETofInts, where the focus \nis on operations and the representa-\ntion of the data is hidden from the \nclient. As this example illustrates, \ndata abstraction enables us to\n\u25a0 Precisely specify what algorithms can provide for clients\n\u25a0 Separate algorithm implementations from the client code\n\u25a0 Develop layers of abstraction, where we make use of well-understood algorithms \nto develop other algorithms\nThese are desirable properties of any approach to describing algorithms, whether it be \nan English-language description or pseudo-code. By embracing the Java class mecha-\nnism in support of data abstraction, we have little to lose and much to gain: working \ncode that we can test and use to compare performance for diverse clients.\n% java Whitelist largeW.txt < largeT.txt \n499569 \n984875 \n295754 \n207807 \n140925 \n161828\n ...\napplication\n98 CHAPTER 1 \u25a0 Fundamentals\n public class    Whitelist \n{\n   public static void main(String[] args)\n   {\n      int[] w = In.readInts(args[0]);\n      StaticSETofInts set = new StaticSETofInts(w);\n      while (!StdIn.isEmpty())\n      {  // Read key, print if not in whitelist. \n         int key = StdIn.readInt();\n         if (set.rank(key) == -1)\n            StdOut.println(key);\n      }\n   } \n}\nimport java.util.Arrays;\npublic class StaticSETofInts \n{\n   private int[] a;\n   public StaticSETofInts(int[] keys)\n   {\n      a = new int[keys.length];\n      for (int i = 0; i < keys.length; i++)\n         a[i] = keys[i]; // defensive copy\n      Arrays.sort(a);\n   }\n   public boolean contains(int key)\n   {  return rank(key) != -1;  }\n   private int rank(int key)\n   {  // Binary search.\n      int ", "start": 110, "end": 111}, "146": {"text": "defensive copy\n      Arrays.sort(a);\n   }\n   public boolean contains(int key)\n   {  return rank(key) != -1;  }\n   private int rank(int key)\n   {  // Binary search.\n      int lo  = 0;\n      int hi = a.length - 1;\n      while (lo <= hi)\n      {  // Key is in a[lo..hi] or not present.\n         int mid = lo + (hi - lo) / 2;\n         if      (key < a[mid]) hi = mid - 1;\n         else if (key > a[mid]) lo = mid + 1;\n         else                   return mid;\n      }\n      return -1;\n   } \n}\n B i n a r y  s e a r c h  r e c a s t  a s  a n  o b j e c t - o r i e n t e d  p r o g r a m  ( a n  A D T  f o r  s e a r c h  i n  a  s e t  o f  i n t e g e r s )\ntypical client\nimplementation\nAPI public class    StaticSETofInts\nStaticSETofInts(int[] a) create a set from the values in a[]\nboolean contains(int key) is key in the set?\n991.2 \u25a0 Data Abstraction\n     I n t e r f a c e  i n h e r i t a n c e .  Java provides language support for de\ufb01ning relationships \namong objects, known as inheritance. These mechanisms are widely used by software \ndevelopers, so you will study them in detail if you take a course in software engineer -\ning. The \ufb01rst inheritance mechanism that we consider is known as    subtyping, which \nallows us to specify a relationship between otherwise unrelated classes by specifying in \nan interface ", "start": 111, "end": 112}, "147": {"text": "detail if you take a course in software engineer -\ning. The \ufb01rst inheritance mechanism that we consider is known as    subtyping, which \nallows us to specify a relationship between otherwise unrelated classes by specifying in \nan interface a set of common methods that each implementing class must contain. An \ninterface is nothing more than a list of instance methods. For example, instead of using \nour informal API, we might have articulated an interface for Date: \npublic  interface Datable \n{\n   int month();\n   int day();\n   int year(); \n}\nand then referred to the interface in our implementation code\npublic class Date implements Datable \n{\n   // implementation code (same as before) \n}\nso that the Java compiler will check that it matches the interface. Adding the code \nimplements Datable to any class that implements month(), day(), and year() pro-\nvides a guarantee to any client that an object of that class can invoke those methods. \nThis arrangement is known as interface inheritance\u2014an implementing class inherits the \ninterface. Interface inheritance allows us to write client programs that can manipulate \nobjects of any type that implements \nthe interface (even a type to be creat-\ned in the future), by invoking meth-\nods in the interface. We might have \nused interface inheritance in place of \nour more informal APIs, but chose \nnot to do so to avoid dependence on \nspeci\ufb01c high-level language mecha -\nnisms that are not critical to the \nunderstanding of algorithms and \nto avoid the extra baggage of inter -\nface \ufb01les. But there are a few situa-\ntions where Java conventions make \ninterface methods section\ncomparison\njava.lang.Comparable compareTo() 2.1\njava.util.Comparator compare() 2.5\niteration\njava.lang.Iterable iterator() 1.3\njava.util.Iterator\nhasNext() \nnext() \nremove()\n1.3\n J a v a  i n ", "start": 112, "end": 112}, "148": {"text": "2.1\njava.util.Comparator compare() 2.5\niteration\njava.lang.Iterable iterator() 1.3\njava.util.Iterator\nhasNext() \nnext() \nremove()\n1.3\n J a v a  i n t e r f a c e s  u s e d  i n  t h i s  b o o k\n100 CHAPTER 1 \u25a0 Fundamentals\n  \nit worthwhile for us to take advantage of interfaces: we use them for comparison and for \niteration, as detailed in the table at the bottom of the previous page, and will consider \nthem in more detail when we cover those concepts. \n  I m p l e m e n t a t i o n  i n h e r i t a n c e .  Java also supports another inheritence mechanism \nknown as subclassing, which is a powerful technique that enables a programmer to \nchange behavior and add functionality without rewriting an entire class from scratch. \nThe idea is to de\ufb01ne a new class (    subclass, or derived class) that inherits instance meth-\nods and instance variables from another class (   superclass, or base class ). The subclass \ncontains more methods than the superclass. Moreover, the subclass can rede\ufb01ne or \n  override methods in the superclass. Subclassing is widely used by systems programmers \nto build so-called  extensible libraries\u2014one programmer (even you) can add methods to \na library built by another programmer (or, perhaps, a team of systems programmers), \neffectively reusing the code in a potentially huge library. For example, this approach is \nwidely used in the development of graphical user interfaces, so that the large amount of \ncode required to provide all the facilities that users expect (drop-down menus, cut-and-\npaste, access to \ufb01les, and so forth) can be reused. The use of subclassing is controversial \namong systems and applications programmers ", "start": 112, "end": 113}, "149": {"text": "required to provide all the facilities that users expect (drop-down menus, cut-and-\npaste, access to \ufb01les, and so forth) can be reused. The use of subclassing is controversial \namong systems and applications programmers (its advantages over interface inheri-\ntance are debatable), and we avoid it in this book because it generally works against \nencapsulation. Certain vestiges of the approach are built in to Java and therefore un-\navoidable: speci\ufb01cally, every class is a subtype of Java\u2019s Object class. This structure \nenables the \u201cconvention\u201d that every class includes an implementation of    getClass(), \ntoString(), equals(),  hashCode(), and several other methods that we do not use in \nthis book. Actually, every class inherits these methods from  Object through subclassing, \nso any client can use them for any object. We usually override toString(), equals(), \nhashCode() in new classes because the default Object implementation generally does \nnot lead to the desired behavior. We now will consider toString() and equals(); we \ndiscuss hashCode() in Section 3.4.\nmethod purpose section\nClass getClass() what class is this object? 1.2\nString toString() string representation of this object 1.1\nboolean equals(Object that) is this object equal to that? 1.2\nint hashCode() hash code for this object 3.4\n I n h e r i t e d  m e t h o d s  f r o m  Object used in this book\n1011.2 \u25a0 Data Abstraction\n  \n  S t r i n g  c o n v e r s i o n .  By convention, every Java type inherits  toString() from Object, \nso any client can invoke toString() for any object. This convention is the basis for Ja-\nva\u2019s automatic conversion of one operand of the concatenation operator + to a String\nwhenever ", "start": 113, "end": 114}, "150": {"text": "type inherits  toString() from Object, \nso any client can invoke toString() for any object. This convention is the basis for Ja-\nva\u2019s automatic conversion of one operand of the concatenation operator + to a String\nwhenever the other operand is a String. If an object\u2019s data type does not include an \nimplementation of toString(), then the default implementation in Object is invoked, \nwhich is normally not helpful, since it typically returns a string representation of the \nmemory address of the object. Accordingly, we generally include implementations of \ntoString() that override the default in every class that we develop, as highlighted for \nDate on the facing page. As illustrated in this code, toString() implementations are \noften quite simple, implicitly (through +) using toString() for each instance variable.\n  W r a p p e r  t y p e s .  Java supplies built-in reference types known as wrapper types, one for \neach of the primitive types:         Boolean, Byte, Character, Double, Float, Integer, Long, \nand Short correspond to boolean, byte, char, double, float, int, long, and short, \nrespectively. These classes consist primarily of static methods such as parseInt() but \nthey also include the inherited instance methods toString(), compareTo(), equals(), \nand  hashCode(). Java automatically converts from primitive types to wrapper types \nwhen warranted, as described on page 122. For example, when an int value is concat-\nenated with a String, it is converted to an Integer that can invoke toString().\n  E q u a l i t y .  What does it mean for two objects to be equal? If we test equality with \n(a == b) where a and b are reference variables of the same type, we are testing whether \nthey have the same identity : whether the references are equal. Typical clients would \nrather be able to test whether the data-type values  (object state) are the same, ", "start": 114, "end": 114}, "151": {"text": "variables of the same type, we are testing whether \nthey have the same identity : whether the references are equal. Typical clients would \nrather be able to test whether the data-type values  (object state) are the same, or to \nimplement some type-speci\ufb01c rule. Java gives us a head start by providing implementa-\ntions both for standard types such as Integer, Double, and String and for more com-\nplicated types such as File and URL. When using these types of data, you can just use the \nbuilt-in implementation.  For example, if x and y are String values, then x.equals(y)\nis true if and only if x and y have the same length and are identical in each character \nposition. When we de\ufb01ne our own data types, such as Date or Transaction, we need \nto override equals(). Java\u2019s convention is that equals() must be an  equivalence rela-\ntion. It must be\n\u25a0 Re\ufb02exive : x.equals(x) is true.\n\u25a0 Symmetric : x.equals(y) is true if and only if y.equals(x). \n\u25a0      Transitive : if x.equals(y) and y.equals(z) are true, then so is x.equals(z).\nIn addition, it must take an Object as argument and satisfy the following properties. \n\u25a0 Consistent : multiple invocations of x.equals(y) consistently return the same \nvalue, provided neither object is modi\ufb01ed. \n\u25a0 Not null : x.equals(null) returns false. \n102 CHAPTER 1 \u25a0 Fundamentals\n  These are natural de\ufb01nitions, but ensuring that these properties hold, adhering to Java \nconventions, and avoiding unnecessary work in an implementation can be tricky, as il-\nlustrated for Date below. It takes the following step-by-step approach:\n\u25a0 If the reference to this object is the same as the reference to the argument object, \nreturn ", "start": 114, "end": 115}, "152": {"text": "work in an implementation can be tricky, as il-\nlustrated for Date below. It takes the following step-by-step approach:\n\u25a0 If the reference to this object is the same as the reference to the argument object, \nreturn true. This test saves the work of doing all the other checks in this case.\n\u25a0 If the argument is null, return false, to adhere to the convention (and to avoid \nfollowing a null reference in code to follow).\n\u25a0 If the objects are not from the same class, return false. To determine an object\u2019s \nclass, we use  getClass(). Note that we can use == to tell us whether two objects \nof type Class are equal because getClass() is guaranteed to return the same \nreference for all objects in any given class.\n\u25a0 Cast the argument \nfrom Object to Date\n(this cast must succeed \nbecause of the previous \ntest).\n\u25a0 \n \nReturn false if any \ninstance variables do \nnot match. For other \nclasses, some other \nde\ufb01nition of equality \nmight be appropriate. \nFor example, we might \nregard two Counter\nobjects as equal if their \ncount instance variables \nare equal. \nThis implementation is a \nmodel that you can use to \nimplement equals() for any \ntype that you implement. \nOnce you have implemented \none equals(), you will not \n\ufb01nd it dif\ufb01cult to implement \nanother.\npublic class  Date \n{\n   private final int month;\n   private final int day;\n   private final int year;\n   public Date(int m, int d, int y)\n   {  month = m; day = d; year = y; }\n   public int month()\n   {  return month;  }\n   public int day()\n   {  return day;  }\n   public int year()\n   {  return year;  }\n   public String toString()\n   {  return month() + \"/\" + day() + \"/\" + year(); ", "start": 115, "end": 115}, "153": {"text": "public int day()\n   {  return day;  }\n   public int year()\n   {  return year;  }\n   public String toString()\n   {  return month() + \"/\" + day() + \"/\" + year();  }\n   public boolean    equals(Object x)\n   {\n      if (this == x) return true;\n      if (x == null) return false;\n      if (this.getClass() != x.getClass()) return false;\n      Date that = (Date) x;\n      if (this.day != that.day)            return false;\n      if (this.month != that.month)        return false;\n      if (this.year != that.year)          return false;\n      return true;\n   }\n}\n O v e r r i d i n g  toString() and equals() in a data-type definition \n1031.2 \u25a0 Data Abstraction\n  \n  M e m o r y  m a n a g e m e n t .  The ability to assign a new value to a reference variable cre-\nates the possibility that a program may have created an object that can no longer be \nreferenced. For example, consider the three assignment statements in the \ufb01gure at left. \nAfter the third assignment statement, not only do a and b refer to the same Date object \n(1/1/2011), but also there is no longer a reference to the Date object that was created \nand used to initialize b. The only reference to that object \nwas in the variable b, and this reference was overwritten \nby the assignment, so there is no way to refer to the object \nagain. Such an object is said to be    orphaned. Objects are \nalso orphaned when they go out of scope. Java programs \ntend to create huge numbers of objects (and variables that \nhold primitive data-type values), but only have a need for a \nsmall number of them at any given point in time. Accord-\ningly, ", "start": 115, "end": 116}, "154": {"text": "out of scope. Java programs \ntend to create huge numbers of objects (and variables that \nhold primitive data-type values), but only have a need for a \nsmall number of them at any given point in time. Accord-\ningly, programming languages and systems need mecha-\nnisms to allocate memory for data-type values during the \ntime they are needed and to free the memory when they \nare no longer needed (for an object, sometime after it is \norphaned). Memory management turns out to be easier \nfor primitive types because all of the information needed \nfor  memory allocation is known at compile time. Java (and \nmost other systems) takes care of reserving space for vari-\nables when they are declared and freeing that space when \nthey go out of scope. Memory management for objects is \nmore complicated: the system can allocate memory for an \nobject when it is created, but cannot know precisely when \nto free the memory associated with each object because \nthe dynamics of a program in execution determines when \nobjects are orphaned. In many languages (such as   C and \nC++) the programmer is responsible for both allocating \nand freeing memory. Doing so is tedious and notoriously \nerror-prone. One of Java\u2019s most signi\ufb01cant features is its ability to automatically man-\nage memory. The idea is to free the programmers from the responsibility of managing \nmemory by keeping track of orphaned objects and returning the memory they use to \na pool of free memory. Reclaiming memory in this way is known as   garbage collection. \nOne of Java\u2019s characteristic features is its policy that references cannot be modi\ufb01ed. \nThis policy enables Java to do ef\ufb01cient automatic garbage collection. Programmers still \ndebate whether the overhead of automatic garbage collection justi\ufb01es the convenience \nof not having to worry about memory management. \nDate a = new Date(12, 31, 1999);\nDate ", "start": 116, "end": 116}, "155": {"text": "collection. Programmers still \ndebate whether the overhead of automatic garbage collection justi\ufb01es the convenience \nof not having to worry about memory management. \nDate a = new Date(12, 31, 1999);\nDate b = new Date(1, 1, 2011);\nb = a;\n811       1\n812       1\n813    2011\n  b     811\n  a     811\n655      12\n656      31\n657    1999\nNew Year\u2019s\n Eve 1999\nNew Year\u2019s\n Day 2011\norphaned\nobject\nreferences to\nsame object\nAn orphaned object\n104 CHAPTER 1 \u25a0 Fundamentals\n  \nImmutability. An  immutable data type, such as Date, has the property that the value \nof an object never changes once constructed. By contrast, a mutable data type, such as \nCounter or Accumulator, manipulates object values that are intended to change. Java\u2019s \nlanguage support for helping to enforce immutability is the  final modi\ufb01er. When you \ndeclare a variable to be final, you are promising to assign it a value only once, either \nin an initializer or in the constructor. Code that could modify the value of a final\nvariable leads to a compile-time error. In our code, we use the modi\ufb01er final with \ninstance variables whose values never change. This policy serves as documentation that \nthe value does not change, prevents accidental changes, and makes programs easier \nto debug. For example, you do not have to include a final value in a trace, since you \nknow that its value never changes. A data type such as Date whose instance variables \nare all primitive and final is immutable (in code that does not use implementation \ninheritence, our convention). Whether to make a data type immutable is an important \ndesign decision and ", "start": 116, "end": 117}, "156": {"text": "A data type such as Date whose instance variables \nare all primitive and final is immutable (in code that does not use implementation \ninheritence, our convention). Whether to make a data type immutable is an important \ndesign decision and depends on the application at hand. For data \ntypes such as Date, the purpose of the abstraction is to encap -\nsulate values that do not change so that we can use them in as-\nsignment statements and as arguments and return values from \nfunctions in the same way as we use primitive types (without hav-\ning to worry about their values changing). A programmer imple-\nmenting a Date client might reasonably expect to write the code \nd = d0 for two Date variables, in the same way as for double or \nint values. But if Date were mutable and the value of d were to \nchange after the assignment d = d0, then the value of d0 would also change (they are \nboth references to the same object)! On the other hand, for data types such as Counter\nand Accumulator, the very purpose of the abstraction is to encapsulate values as they \nchange. Y ou have already encountered this distinction as a client programmer, when \nusing Java arrays (mutable) and Java\u2019s String data type (immutable). When you pass \na String to a method, you do not worry about that method changing the sequence of \ncharacters in the String, but when you pass an array to a method, the method is free to \nchange the contents of the array. String objects are immutable because we generally do \nnot want String values to change, and Java arrays are mutable because we generally do\nwant array values to change. There are also situations where we want to have mutable \nstrings (that is the purpose of Java\u2019s  StringBuilder class) and where we want to have \nimmutable arrays (that is the purpose of the Vector class that we consider later in this \nsection). Generally, immutable types ", "start": 117, "end": 117}, "157": {"text": "mutable \nstrings (that is the purpose of Java\u2019s  StringBuilder class) and where we want to have \nimmutable arrays (that is the purpose of the Vector class that we consider later in this \nsection). Generally, immutable types are easier to use and harder to misuse than muta-\nble types because the scope of code that can change their values is far smaller. It is easier \nto debug code that uses immutable types because it is easier to guarantee that variables \nin client code that uses them remain in a consistent state. When using mutable types, \nmutable immutable\nCounter Date\nJava arrays String\nMutable/immutable examples\n1051.2 \u25a0 Data Abstraction\n you must always be concerned about where and when their values change. The down-\nside of immutability is that a new object must be created for every value. This expense is \nnormally manageable because Java garbage collectors are typically optimized for such \nsituations. Another downside of immutability stems from the fact that, unfortunately, \nfinal guarantees immutability only when instance variables are primitive types, not \nreference types. If an instance variable of a reference type has the final modi\ufb01er, the \nvalue of that instance variable (the reference to an object) will never change\u2014it will \nalways refer to the same object\u2014but the value of the object itself can change. For ex-\nample, this code does not implement an immutable type:\npublic class Vector \n{\n   private final double[] coords;\n   public Vector(double[] a)\n   {  coords = a; }\n   ... \n}\nA client program could create a  Vector by specifying the entries in an array, and then \n(bypassing the API) change the elements of the Vector after construction:\ndouble[] a = { 3.0, 4.0 };\nVector vector = new Vector(a); \na[0] = 0.0;  // Bypasses the public API.\nThe instance variable ", "start": 117, "end": 118}, "158": {"text": "construction:\ndouble[] a = { 3.0, 4.0 };\nVector vector = new Vector(a); \na[0] = 0.0;  // Bypasses the public API.\nThe instance variable coords[] is private and final, but Vector is mutable because \nthe client holds a reference to the data. Immutability needs to be taken into account in \nany data-type design, and whether a data type is immutable should be speci\ufb01ed in the \nAPI, so that clients know that object values will not change. In this book, our primary \ninterest in immutability is for use in certifying the correctness of our algorithms. For \nexample, if the type of data used for a binary search algorithm were mutable, then cli -\nents could invalidate our assumption that the array is sorted for binary search. \n106 CHAPTER 1 \u25a0 Fundamentals\n Design by contract. To  co n c l u d e , w e  b r i e \ufb02 y  d i s c u s s  Jav a  l a n g u a g e  m e c h a n i s m s  t h a t  \nenables you to verify assumptions about your program as it is running. We use two Java \nlanguage mechanisms for this purpose:\n\u25a0 Exceptions, which generally handle unforeseen errors outside our control\n\u25a0 \n \n \nAssertions, which verify assumptions that we make within code we develop\nLiberal use of both exceptions and assertions is good programming practice. We use \nthem sparingly in the book for economy, but you will \ufb01nd them throughout the code \non the booksite. This code aligns with a substantial amount of the surrounding com-\nmentary about each algorithm in the text that has to do with exceptional conditions \nand with asserted invariants.\nExceptions and errors.  Exceptions and  errors are disruptive events that occur while a \nprogram is running, often to ", "start": 118, "end": 119}, "159": {"text": "com-\nmentary about each algorithm in the text that has to do with exceptional conditions \nand with asserted invariants.\nExceptions and errors.  Exceptions and  errors are disruptive events that occur while a \nprogram is running, often to signal an error. The action taken is known as  throwing an \nexception or throwing an error. We have already encountered exceptions thrown by Java \nsystem methods in the course of learning basic features of Java:  StackOverflowError, \n  ArithmeticException,    ArrayIndexOutOfBoundsException,    OutOfMemoryError, \nand    NullPointerException are typical examples. Y ou can also create your own ex-\nceptions. The simplest kind is a RuntimeException that terminates execution of the \nprogram and prints an error message\nthrow new    RuntimeException(\"Error message here.\");\nA general practice known as  fail fast programming suggests that an error is more easily \npinpointed if an exception is thrown as soon as an error is discovered (as opposed to \nignoring the error and deferring the exception to sometime in the future). \nassertion is a boolean expression that you are af\ufb01rming is true at that Assertions. An    \npoint in the program. If the expression is false, the program will terminate and re -\nport an error message. We use assertions both to gain con\ufb01dence in the correctness of \nprograms and to document intent. For example, suppose that you have a computed \nvalue that you might use to index into an array. If this value were negative, it would \ncause an ArrayIndexOutOfBoundsException sometime later. But if you write the code \nassert index >= 0; you can pinpoint the place where the error occurred. You can \nalso add an optional detail message such as\nassert index >= 0 : \"Negative index in method X\"; \nto help you locate the bug. By default, assertions are disabled. Y ou can enable them from \nthe command line by using the -enableassertions \ufb02ag (-ea ", "start": 119, "end": 119}, "160": {"text": "0 : \"Negative index in method X\"; \nto help you locate the bug. By default, assertions are disabled. Y ou can enable them from \nthe command line by using the -enableassertions \ufb02ag (-ea for short). Assertions are \nfor debugging: your program should not rely on assertions for normal operation since \nthey may be disabled. When you take a course in systems programming, you will learn \nto use assertions to ensure that your code never terminates in a system error or goes into \n1071.2 \u25a0 Data Abstraction\n an in\ufb01nite loop. One model, known as the design-by-contract model of programming \nexpresses the idea. The designer of a data type expresses a precondition (the condition \nthat the client promises to satisfy when calling a method), a postcondition (the condi-\ntion that the implementation promises to achieve when returning from a method), and \nside effects (any other change in state that the method could cause). During develop-\nment, these conditions can be tested with assertions. \nSummary. The language mechanisms discussed throughout this section illustrate that \neffective data-type design leads to nontrivial issues that are not easy to resolve. Ex-\nperts are still debating the best ways to support some of the design ideas that we are \ndiscussing. Why does Java not allow functions as arguments? Why does Matlab copy \narrays passed as arguments to functions? As mentioned early in Chapter 1, it is a slip-\npery slope from complaining about features in a programming language to becoming \na programming-language designer. If you do not plan to do so, your best strategy is \nto use widely available languages. Most systems have extensive libraries that you cer -\ntainly should use when appropriate, but you often can simplify your client code and \nprotect yourself by building abstractions that can easily transport to other languages. \nYo u r  m a i n  g o a l  i s  t o ", "start": 119, "end": 120}, "161": {"text": "appropriate, but you often can simplify your client code and \nprotect yourself by building abstractions that can easily transport to other languages. \nYo u r  m a i n  g o a l  i s  t o  d e ve l o p  d a t a  t y p e s  s o  t h a t  m o s t  o f  yo u r  wo r k  i s  d o n e  a t  a  l e ve l  o f  \nabstraction that is appropriate to the problem at hand.\nThe table on the facing page summarizes the various kinds of Java classes that we \nhave considered. \n108 CHAPTER 1 \u25a0 Fundamentals\n kind of class examples characteristics\nstatic methods Math StdIn StdOut no instance variables\nimmutable\nabstract data type\nDate Transaction \nString Integer\ninstance variables all private\ninstance variables all final \ndefensive copy for reference types\nNote: these are necessary but not suf\ufb01cient.\nmutable\nabstract data type\nCounter Accumulator instance variables all private\nnot all instance variables final\nabstract data type\nwith I/O side effects\nVisualAccumulator \nIn Out Draw\ninstance variables all private\ninstance methods do I/O\nJava classes (data-type implementations)\n1091.2 \u25a0 Data Abstraction\n Q & A\n \n \nQ. Why bother with data abstraction?\nA. It helps us produce reliable and correct code. For example, in the 2000 presidential \nelection, Al Gore received \u201316,022 votes on an electronic voting machine in Volusia \nCounty, Florida\u2014the tally was clearly not properly encapsulated in the voting machine \nsoftware!\nQ. Why the distinction between  primitive and reference types? Why not just have refer-\nence types?\nA. Performance. Java provides the reference types Integer, Double, and so forth that \ncorrespond to primitive types that can be used by programmers who prefer to ignore ", "start": 120, "end": 122}, "162": {"text": "and reference types? Why not just have refer-\nence types?\nA. Performance. Java provides the reference types Integer, Double, and so forth that \ncorrespond to primitive types that can be used by programmers who prefer to ignore \nthe distinction. Primitive types are closer to the types of data that are supported by \ncomputer hardware, so programs that use them usually run faster than programs that \nuse corresponding reference types.\nQ. Do data types have to be abstract?\nA. No. Java also allows  public and    protected to allow some clients to refer directly \nto instance variables. As described in the text, the advantages of allowing client code to \ndirectly refer to data are greatly outweighed by the disadvantages of dependence on a \nparticular representation, so all instance variables are private in our code. We also oc-\ncasionally use private instance methods to share code among public methods. \nQ. What happens if I forget to use new when creating an object?\nA. To  Jav a , i t  l o o k s  a s  t h o u g h  yo u  w a n t  to  c a l l  a  s t a t i c  m e t h o d  w i t h  a  re t u r n  v a l u e  o f  t h e  \nobject type. Since you have not de\ufb01ned such a method, the error message is the same as \nanytime you refer to an unde\ufb01ned symbol. If you compile the code\nCounter c = Counter(\"test\");\nyou get this error message: \ncannot find symbol \nsymbol  : method Counter(String)\nYo u  g e t  t h e  s a m e  k i n d  o f  e r ro r  m e s s a g e  i f  yo u  p rov i d e  t h e ", "start": 122, "end": 122}, "163": {"text": "t h e  s a m e  k i n d  o f  e r ro r  m e s s a g e  i f  yo u  p rov i d e  t h e  w ro n g  n u m b e r  o f  a r g u m e n t s  \nto a constructor.\n110 CHAPTER 1 \u25a0 Fundamentals\n Q. What happens if I forget to use new when creating an array of objects?\nA. Yo u  n e e d  t o  u s e  new for each object that you create, so when you create an array of \nN objects, you need to use new N+1 times: once for the array and once for each of the \nobjects. If you forget to create the array:\nCounter[] a; \na[0] = new Counter(\"test\");\nyou get the same error message that you would get when trying to assign a value to any \nuninitialized variable:\nvariable a might not have been initialized\n      a[0] = new Counter(\"test\"); \n      ^\nbut if you forget to use new when creating an object within the array and then try to use \nit to invoke a method:\nCounter[] a = new Counter[2]; \na[0].increment();\nyou get a NullPointerException. \nQ. Why not write StdOut.println(x.toString()) to print objects?\nA. That code works \ufb01ne, but Java saves us the trouble of writing it by automatically \ninvoking the toString() method for any object, since println() has a method that \ntakes an Object as argument. \n Q .  What is a  pointer ? \nA. Good question. Perhaps that should be NullReferenceException. Like a Java ref-\nerence, you can think of a pointer as a machine address. In many programming lan -\nguages, the pointer is a primitive data type that programmers can manipulate in ", "start": 122, "end": 123}, "164": {"text": "that should be NullReferenceException. Like a Java ref-\nerence, you can think of a pointer as a machine address. In many programming lan -\nguages, the pointer is a primitive data type that programmers can manipulate in many \nways. But programming with pointers is notoriously error-prone, so operations pro-\nvided for pointers need to be carefully designed to help programmers avoid errors. \nJava takes this point of view to an extreme (that is favored by many modern program-\nming-language designers). In Java, there is only one way to create a reference (new) and \nonly one way to change a reference (with an assignment statement). That is, the only \nthings that a programmer can do with references are to create them and copy them. In \n1111.2 \u25a0 Data Abstraction\n  \n \n \nprogramming-language jargon, Java references are known as  safe pointers, because Java \ncan guarantee that each reference points to an object of the speci\ufb01ed type (and it can \ndetermine which objects are not in use, for garbage collection). Programmers used to \nwriting code that directly manipulates pointers think of Java as having no pointers at \nall, but people still debate whether it is really desirable to have unsafe pointers.\nQ. Where can I \ufb01nd more details on how Java implements references and does garbage \ncollection?\nA. One Java system might differ completely from another. For example, one natural \nscheme is to use a pointer (machine address); another is to use a  handle (a pointer to \na pointer). The former gives faster access to data; the latter provides for better garbage \ncollection.\nQ. What exactly does it mean to import a name?\nA.  Not much: it just saves some typing. You could type java.util.Arrays instead of \nArrays everywhere in your code instead of using the import statement.\nQ. What is the problem with implementation inheritance?\nA. Subtyping makes modular programming more dif\ufb01cult ", "start": 123, "end": 124}, "165": {"text": "just saves some typing. You could type java.util.Arrays instead of \nArrays everywhere in your code instead of using the import statement.\nQ. What is the problem with implementation inheritance?\nA. Subtyping makes modular programming more dif\ufb01cult for two reasons. First, any \nchange in the superclass affects all subclasses. The subclass cannot be developed inde-\npendently of the superclass; indeed, it is completely dependent on the superclass. This \nproblem is known as the  fragile base class  problem. Second, the subclass code, hav -\ning access to instance variables, can subvert the intention of the superclass code. For \nexample, the designer of a class like Counter for a voting system may take great care \nto make it so that Counter can only increment the tally by one (remember Al Gore\u2019s \nproblem). But a subclass, with full access to the instance variable, can change it to any \nvalue whatever. \nQ. How do I make a class immutable?\nA. To  e n s u re  i m m u t a b i l i t y  o f  a  d a t a  t y p e  t h a t  i n c l u d e s  a n  i n s t a n ce  v a r i a b l e  o f  a  m u-\ntable type, we need to make a local copy, known as a  defensive copy. And that may not be \nenough. Making the copy is one challenge; ensuring that none of the instance methods \nchange values is another.\nQ. What is   null?\nQ & A (continued)\n112 CHAPTER 1 \u25a0 Fundamentals\n  \nA. It is a literal value that refers to no object. Invoking a method using the null ref-\nerence is meaningless and results in a  NullPointerException. If you get this error \nmessage, check to make sure that your constructor properly initializes all of its ", "start": 124, "end": 125}, "166": {"text": "that refers to no object. Invoking a method using the null ref-\nerence is meaningless and results in a  NullPointerException. If you get this error \nmessage, check to make sure that your constructor properly initializes all of its instance \nvariables.\nQ. Can I have a static method in a class that implements a data type?\nA. Of course. For example, all of our classes have main(). Also, it is natural to consider \nadding static methods for operations that involve multiple objects where none of them \nnaturally suggests itself as the one that should invoke the method. For example, we \nmight de\ufb01ne a static method like the following within Point:\npublic static double distance(Point a, Point b) \n{  \n   return a.distTo(b); \n}\nOften, including such methods can serve to clarify client code.\nQ. Are there other kinds of variables besides parameter, local, and instance variables?\nA. If you include the keyword static in a class declaration (outside of any type) it \ncreates a completely different type of variable, known as a   static variable. Like instance \nvariables, static variables are accessible to every method in the class; however, they are \nnot associated with any object. In older programming languages, such variables are \nknown as  global variables, because of their global scope. In modern programming, we \nfocus on limiting scope and therefore rarely use such variables. When we do, we will call \nattention to them.\nQ. What is a   deprecated method?\nA. A method that is no longer fully supported, but kept in an API to maintain compat-\nibility. For example, Java once included a method Character.isSpace(), and pro -\ngrammers wrote programs that relied on using that method\u2019s behavior. When the de-\nsigners of Java later wanted to support additional Unicode whitespace characters, they \ncould not change the behavior of isSpace() without breaking client programs, so, \ninstead, they added a ", "start": 125, "end": 125}, "167": {"text": "that method\u2019s behavior. When the de-\nsigners of Java later wanted to support additional Unicode whitespace characters, they \ncould not change the behavior of isSpace() without breaking client programs, so, \ninstead, they added a new method, Character.isWhiteSpace(), and deprecated the \nold method. As time wears on, this practice certainly complicates APIs. Sometimes, en-\ntire classes are deprecated. For example, Java deprecated its  java.util.Date in order \nto better support internationalization.\n1131.2 \u25a0 Data Abstraction\n EXERCISES\n \n1.2.1 Write a Point2D client that takes an integer value N from the command line, \ngenerates N random points in the unit square, and computes the distance separating \nthe closest pair of points.\n1.2.2 Write an Interval1D client that takes an int value N as command-line argu-\nment, reads N intervals (each de\ufb01ned by a pair of double values) from standard input, \nand prints all pairs that intersect.\n1.2.3 Write an Interval2D client that takes command-line arguments N, min, and max\nand generates N random 2D intervals whose width and height are uniformly distributed \nbetween min and max in the unit square. Draw them on StdDraw and print the number \nof pairs of intervals that intersect and the number of intervals that are contained in one \nanother.\n1.2.4 What does the following code fragment print?\nString string1 = \"hello\"; \nString string2 = string1; \nstring1 = \"world\"; \nStdOut.println(string1); \nStdOut.println(string2);\n1.2.5 What does the following code fragment print?\nString s = \"Hello World\";\ns.toUpperCase();\ns.substring(6, 11); \nStdOut.println(s);\nAnswer : \"Hello World\" . String objects are  immutable\u2014string methods return \na new String object with the ", "start": 125, "end": 126}, "168": {"text": "s = \"Hello World\";\ns.toUpperCase();\ns.substring(6, 11); \nStdOut.println(s);\nAnswer : \"Hello World\" . String objects are  immutable\u2014string methods return \na new String object with the appropriate value (but they do not change the value \nof the object that was used to invoke them). This code ignores the objects returned \nand just prints the original string. T o print \"WORLD\", use s = s.toUpperCase() and \ns = s.substring(6, 11).\n1.2.6 A string s is a  circular rotation of a string t if it matches when the characters \nare circularly shifted by any number of positions; e.g., ACTGACG is a circular shift of \nTGACGAC, and vice versa. Detecting this condition is important in the study of genomic \nsequences. Write a program that checks whether two given strings s and t are circular \n114 CHAPTER 1 \u25a0 Fundamentals\n shifts of one another. Hint : The solution is a one-liner with indexOf(), length(), and \nstring concatenation.\n1.2.7 What does the following recursive function return?\npublic static String mystery(String s) \n{\n   int N = s.length();\n   if (N <= 1) return s;\n   String a = s.substring(0, N/2);\n   String b = s.substring(N/2, N);\n   return mystery(b) + mystery(a); \n}\n1.2.8 Suppose that a[] and b[] are each integer arrays consisting of millions of inte-\ngers. What does the follow code do? Is it reasonably ef\ufb01cient?\nint[] t = a; a = b; b = t;\nAnswer. It swaps them. It could hardly be more ef\ufb01cient because it does so by copying \nreferences, so that it is not necessary to copy millions of elements.\n1.2.9 Instrument BinarySearch (page ", "start": 126, "end": 127}, "169": {"text": "t;\nAnswer. It swaps them. It could hardly be more ef\ufb01cient because it does so by copying \nreferences, so that it is not necessary to copy millions of elements.\n1.2.9 Instrument BinarySearch (page 47) to use a Counter to count the total number \nof keys examined during all searches and then print the total after all searches are com-\nplete. Hint : Create a Counter in main() and pass it as an argument to rank().\n1.2.10 Develop a class VisualCounter that allows both increment and decrement \noperations. Take two arguments N and max in the constructor, where N speci\ufb01es the \nmaximum number of operations and max speci\ufb01es the maximum absolute value for \nthe counter. As a side effect, create a plot showing the value of the counter each time its \ntally changes.\n1.2.11 Develop an implementation SmartDate of our Date API that raises an excep-\ntion if the date is not legal.\n1.2.12 Add a method dayOfTheWeek() to SmartDate that returns a String value \nMonday, Tuesday, Wednesday, Thursday, Friday, Saturday, or Sunday, giving the ap-\npropriate day of the week for the date. Y ou may assume that the date is in the 21st \ncentury.\n1151.2 \u25a0 Data Abstraction\n 1.2.13  Using our implementation of Date as a model (page 91), develop an implementa-\ntion of Transaction. \n1.2.14 Using our implementation of equals() in Date as a model (page 103), develop \nimplementations of equals() for Transaction.\nEXERCISES  (continued)\n116 CHAPTER 1 \u25a0 Fundamentals\n CREATIVE PROBLEMS\n1.2.15    File input. Develop a possible implementation of the static readInts() meth-\nod from In (which we use for various test clients, ", "start": 127, "end": 129}, "170": {"text": "1 \u25a0 Fundamentals\n CREATIVE PROBLEMS\n1.2.15    File input. Develop a possible implementation of the static readInts() meth-\nod from In (which we use for various test clients, such as binary search on page 47) that \nis based on the split() method in String. \nSolution:\npublic static int[] readInts(String name) \n{\n   In in = new In(name);\n   String input = StdIn.readAll();\n    String[] words = input.split(\"\\\\s+\");\n   int[] ints = new int[words.length;\n   for int i = 0; i < word.length; i++)\n      ints[i] = Integer.parseInt(words[i]);\n   return ints; \n}\nWe w ill consider a different implementation in Section 1.3 (see page 126).\n1.2.16    Rational numbers. Implement an immutable data type Rational for rational \nnumbers that supports addition, subtraction, multiplication, and division.\npublic class  Rational \nRational(int numerator. int denominator) \nRational plus(Rational b) sum of this number and b\nRational minus(Rational b) difference of this number and b\nRational times(Rational b) product of this number and b\nRational divides(Rational b) quotient of this number and b\nboolean equals(Rational that) is this number equal to that ?\nString toString() string representation\nYo u  d o  n o t  h ave  t o  wo r r y  a b o u t  t e s t i n g  f o r  ove r \ufb02 ow  ( s e e  Exercise 1.2.17), but use as \ninstance variables two long values that represent the numerator and denominator to \nlimit the possibility of over\ufb02ow. Use Euclid\u2019s algorithm (see page 4) to ensure that the \nnumerator and denominator never have any common ", "start": 129, "end": 129}, "171": {"text": "variables two long values that represent the numerator and denominator to \nlimit the possibility of over\ufb02ow. Use Euclid\u2019s algorithm (see page 4) to ensure that the \nnumerator and denominator never have any common factors. Include a test client that \nexercises all of your methods.\n1171.2 \u25a0 Data Abstraction\n 1.2.17    Robust implementation of rational numbers. Use assertions to develop an im -\nplementation of Rational (see Exercise 1.2.16) that is immune to over\ufb02ow.\n1.2.18    Var iance for accumulator. Validate that the follow ing code, which adds the \nmethods var() and stddev() to Accumulator, computes both the mean and variance \nof the numbers presented as arguments to addDataValue():\npublic class Accumulator \n{\n   private double m;\n   private double s;\n   private int N;\n   public void addDataValue(double x)\n   {\n      N++;\n      s = s + 1.0 * (N-1) / N * (x - m) * (x - m);\n      m = m + (x - m) / N;\n   }\n   public double mean()\n   {  return m;  }\n   public double var()\n   {  return s/(N - 1);  }\n   public double stddev()\n   {  return Math.sqrt(this.var());  }\n}\nThis implementation is less susceptible to roundoff error than the straightforward im-\nplementation based on saving the sum of the squares of the numbers.\nCREATIVE PROBLEMS  (continued)\n118 CHAPTER 1 \u25a0 Fundamentals\n 1.2.19    Parsing. Develop the parse constructors for your Date and Transaction im-\nplementations of Exercise 1.2.13 that take a single String argument to specify the \ninitialization values, using the formats given in the table below.\nPartial solution:\npublic Date(String date) \n{\n ", "start": 129, "end": 131}, "172": {"text": "Transaction im-\nplementations of Exercise 1.2.13 that take a single String argument to specify the \ninitialization values, using the formats given in the table below.\nPartial solution:\npublic Date(String date) \n{\n   String[] fields = date.split(\"/\");\n   month = Integer.parseInt(fields[0]);\n   day   = Integer.parseInt(fields[1]);\n   year  = Integer.parseInt(fields[2]); \n}\ntype format example\nDate integers separated by slashes 5/22/1939\nTransaction customer, date, and amount, \nseparated by whitespace\nTuring 5/22/1939 11.99\n F o r m a t s  f o r  p a r s i n g\n1191.2 \u25a0 Data Abstraction\n 1.3 BAGS, QUEUES, AND STACKS\n \nSeveral fundamental data types involve  collections of objects. Speci\ufb01cally, the set \nof values is a collection of objects, and the operations revolve around adding, remov-\ning, or examining objects in the collection. In this section, we consider three such data \ntypes, known as the bag, the queue, and the stack. They differ in the speci\ufb01cation of \nwhich object is to be removed or examined next.\nBags, queues, and stacks are fundamental and broadly useful. We use them in imple-\nmentations throughout the book. Beyond this direct applicability, the client and imple-\nmentation code in this section serves as an introduction to our general approach to the \ndevelopment of data structures and algorithms.\nOne goal of this section is to emphasize the idea that the way in which we represent \nthe objects in the collection directly impacts the ef\ufb01ciency of the various operations. \nFor collections, we design data structures for representing the collection of objects that \ncan support ef\ufb01cient implementation of the requisite operations.\nA second goal of this section is ", "start": 131, "end": 132}, "173": {"text": "the ef\ufb01ciency of the various operations. \nFor collections, we design data structures for representing the collection of objects that \ncan support ef\ufb01cient implementation of the requisite operations.\nA second goal of this section is to introduce generics and iteration, basic Java con -\nstructs that substantially simplify client code. These are advanced programming-lan-\nguage mechanisms that are not necessarily essential to the understanding of algorithms, \nbut their use allows us to develop client code (and implementations of algorithms) that \nis more clear, compact, and elegant than would otherwise be possible.\nA third goal of this section is to introduce and show the importance of linked data \nstructures. In particular, a classic data structure known as the linked list enables im-\nplementation of bags, queues, and stacks that achieve ef\ufb01ciencies not otherwise pos-\nsible. Understanding linked lists is a key \ufb01rst step to the study of algorithms and data \nstructures.\nFor each of the three types, we consider APIs and sample client programs, then \nlook at possible representations of the data type values and implementations of the \ndata-type operations. This scenario repeats (with more complicated data structures) \nthroughout this book. The implementations here are models of implementations later \nin the book and worthy of careful study. \n120\n  \nAPIs As usual, we begin our discussion of abstract data types for collections by de -\n\ufb01ning their APIs, shown below. Each contains a no-argument constructor, a method to \nadd an item to the collection, a method to test whether the collection is empty, and a \nmethod that returns the size of the collection. Stack and Queue each have a method to \nremove a particular item from the collection. Beyond these basics, these APIs re\ufb02ect two \nJava features that we will describe on the next few pages: generics and iterable collections.\nAPIs for fundamental generic iterable collections\npublic class  Queue<Item> implements Iterable<Item> \nQueue() ", "start": 132, "end": 133}, "174": {"text": "basics, these APIs re\ufb02ect two \nJava features that we will describe on the next few pages: generics and iterable collections.\nAPIs for fundamental generic iterable collections\npublic class  Queue<Item> implements Iterable<Item> \nQueue() create an empty queue\nvoid enqueue(Item item) add an item\nItem dequeue() remove the least recently added item\nboolean isEmpty() is the queue empty?\nint size() number of items in the queue\npublic class  Stack<Item> implements Iterable<Item> \nStack() create an empty stack\nvoid push(Item item) add an item\nItem pop() remove the most recently added item\nboolean isEmpty() is the stack empty?\nint size() number of items in the stack\npublic class    Bag<Item> implements Iterable<Item>\nBag() create an empty bag\nvoid add(Item item) add an item\nboolean isEmpty() is the bag empty?\nint size() number of items in the bag\nFIFO queue\n P u s h d o w n  ( L I F O )  s t a c k\nBag\n1211.3 \u25a0 Bags, Queues, and Stacks\n  \n \n   G e n e r i c s .  An essential characteristic of collection ADTs is that we should be able to use \nthem for any type of data. A speci\ufb01c Java mechanism known as generics, also known \nas   parameterized types, enables this capability. The impact of generics on the program-\nming language is suf\ufb01ciently deep that they are not found in many languages (including \nearly versions of Java), but our use of them in the present context involves just a small \nbit of extra Java syntax and is easy to understand. The notation <Item> after the class \nname in each of our APIs de\ufb01nes the name Item as a    type parameter, a symbolic place-\nholder for some  concrete type to be used by the client. Y ou can ", "start": 133, "end": 134}, "175": {"text": "<Item> after the class \nname in each of our APIs de\ufb01nes the name Item as a    type parameter, a symbolic place-\nholder for some  concrete type to be used by the client. Y ou can read Stack<Item> as \n\u201cstack of items.\u201d When implementing Stack, we do not know the  concrete type of Item, \nbut a client can use our stack for any type of data, including one de\ufb01ned long after we \ndevelop our implementation. The client code provides a  concrete type when the stack \nis created: we can replace Item with the name of any reference data type (consistently, \neverywhere it appears). This provides exactly the capability that we need. For example, \nyou can write code such as\nStack<String> stack = new Stack<String>(); \nstack.push(\"Test\"); \n...\nString next = stack.pop();\nto use a stack for String objects and code such as\nQueue<Date> queue = new Queue<Date>(); \nqueue.enqueue(new Date(12, 31, 1999)); \n...\nDate next = queue.dequeue();\nto use a queue for Date objects. If you try to add a Date (or data of any other type than \nString) to stack or a String (or data of any other type than Date) to queue, you will \nget a compile-time error. Without generics, we would have to de\ufb01ne (and implement) \ndifferent APIs for each type of data we might need to collect; with generics, we can use \none API (and one implementation) for all types of data, even types that are imple-\nmented in the future. As you will soon see, generic types lead to clear client code that is \neasy to understand and debug, so we use them throughout this book. \n       A u t o b o x i n g .  Type parameters have to be instantiated as reference types, so Java has \nspecial mechanisms to ", "start": 134, "end": 134}, "176": {"text": "\neasy to understand and debug, so we use them throughout this book. \n       A u t o b o x i n g .  Type parameters have to be instantiated as reference types, so Java has \nspecial mechanisms to allow generic code to be used with primitive types. Recall that \nJava\u2019s wrapper types are reference types that correspond to primitive types: Boolean, \nByte, Character, Double, Float, Integer, Long, and Short correspond to boolean, \nbyte, char, double, float, int, long, and short, respectively. Java automatically con-\nverts between these reference types and the corresponding primitive types\u2014in assign -\nments, method arguments, and arithmetic/logic expressions. In the present context, \n122 CHAPTER 1 \u25a0 Fundamentals\n this conversion is helpful because it enables us to use generics with primitive types, as \nin the following code:\nStack<Integer> stack = new Stack<Integer>(); \nstack.push(17);      // auto-boxing (int -> Integer) \nint i = stack.pop(); // auto-unboxing (Integer -> int)\nAutomatically casting a primitive type to a wrapper type is known as autoboxing, and \nautomatically casting a wrapper type to a primitive type is known as auto-unboxing. \nIn this example, Java automatically casts (autoboxes) the primitive value 17 to be of \ntype Integer when we pass it to the push() method. The pop() method returns an \nInteger, which Java casts (auto-unboxes) to an int before assigning it to the variable i. \n   I t e r a b l e  c o l l e c t i o n s .  For many applications, the client\u2019s requirement is just to process \neach of the items in some way, or to iterate through the items in the collection. This \nparadigm is so important that it has achieved \ufb01rst-class status in Java and many other \nmodern languages (the programming language itself ", "start": 134, "end": 135}, "177": {"text": "the items in some way, or to iterate through the items in the collection. This \nparadigm is so important that it has achieved \ufb01rst-class status in Java and many other \nmodern languages (the programming language itself has speci\ufb01c mechanisms to sup-\nport it, not just the libraries). With it, we can write clear and compact code that is free \nfrom dependence on the details of a collection\u2019s implementation. For example, suppose \nthat a client maintains a collection of transactions in a Queue, as follows:\nQueue<Transaction> collection = new Queue<Transaction>();\nIf the collection is iterable, the client can print a transaction list with a single statement:\nfor (Transaction t : collection) \n{  StdOut.println(t);  }\nThis construct is known as the   foreach statement: you can read the for statement as for \neach transaction t in the collection, execute the following block of code. This client code \ndoes not need to know anything about the representation or the implementation of the \ncollection; it just wants to process each of the items in the collection. The same for loop \nwould work with a Bag of transactions or any other iterable collection. We could hardly \nimagine client code that is more clear and compact. As you will see, supporting this \ncapability requires extra effort in the implementation, but this effort is well worthwhile.\nIt is interesting to note that the only differences between the APIs for Stack and \nQueue are their names and the names of the methods. This observation highlights the \nidea that we cannot easily specify all of the characteristics of a data type in a list of \nmethod signatures. In this case, the true speci\ufb01cation has to do with the English-lan-\nguage descriptions that specify the rules by which an item is chosen to be removed (or \nto be processed next in the foreach statement). Differences in these rules are profound, \npart of the API, and certainly of critical importance in developing client ", "start": 135, "end": 135}, "178": {"text": "that specify the rules by which an item is chosen to be removed (or \nto be processed next in the foreach statement). Differences in these rules are profound, \npart of the API, and certainly of critical importance in developing client code.\n1231.3 \u25a0 Bags, Queues, and Stacks\n  \n \nBags. A bag is a collection where removing items is not supported\u2014its purpose is to \nprovide clients with the ability to collect items and then to iterate through the collected \nitems (the client can also test if a bag is empty and \ufb01nd its number of items). The order \nof iteration is unspeci\ufb01ed and should be immaterial to the client. T o appreciate the con-\ncept, consider the idea of an avid marble collector, who might put marbles in a bag, one \nat a time, and periodically process all the marbles to look \nfor one having some particular characteristic. With our \nBag API, a client can add items to a bag and process them \nall with a foreach statement whenever needed. Such a cli-\nent could use a stack or a queue, but one way to emphasize \nthat the order in which items are processed is immaterial \nis to use a Bag. The class Stats at right illustrates a typi-\ncal Bag client. The task is simply to compute the average \nand the sample standard deviation of the double values \non standard input. If there are N numbers on standard in-\nput, their average is computed by adding the numbers and \ndividing by N; their sample standard deviation is comput-\ned by adding the squares of the difference between each \nnumber and the average, dividing by N\u20131, and taking the \nsquare root. The order in which the numbers are consid-\nered is not relevant for either of these calculations, so we \nsave them in a Bag and use the foreach construct to com-\npute each sum. Note : It is possible to compute the ", "start": 135, "end": 136}, "179": {"text": "order in which the numbers are consid-\nered is not relevant for either of these calculations, so we \nsave them in a Bag and use the foreach construct to com-\npute each sum. Note : It is possible to compute the standard \ndeviation without saving all the numbers (as we did for the \naverage in Accumulator\u2014see Exercise 1.2.18). Keeping \nthe all numbers in a Bag is required for more complicated \nstatistics. \nOperations on a bag\na bag of\nmarbles\nprocess each marble m\n(in any order)\nadd(  )\nfor (Marble m : bag)\nadd(  )\n124 CHAPTER 1 \u25a0 Fundamentals\n public class    Stats \n{\n   public static void main(String[] args)\n   {\n      Bag<Double> numbers = new Bag<Double>();\n      while (!StdIn.isEmpty())\n         numbers.add(StdIn.readDouble());\n      int N = numbers.size();\n      double sum = 0.0;\n      for (double x : numbers)\n         sum += x;\n      double mean = sum/N;\n      sum = 0.0;\n      for (double x : numbers)\n         sum += (x - mean)*(x - mean);\n      double std = Math.sqrt(sum/(N-1));\n      StdOut.printf(\"Mean: %.2f\\n\", mean);\n      StdOut.printf(\"Std dev: %.2f\\n\", std);\n   } \n}\n% java Stats \n100 \n99 \n101 \n120 \n98 \n107 \n109 \n81 \n101 \n90\nMean: 100.60 \nStd dev: 10.51\ntypical Bag client\napplication\n1251.3 \u25a0 Bags, Queues, and Stacks\n  \n \n  F I F O  q u e u e s .  A FIFO queue (or just a queue) is a collection that is based on the  \ufb01rst-\nin-\ufb01rst-out (FIFO) policy. ", "start": 136, "end": 138}, "180": {"text": "I F O  q u e u e s .  A FIFO queue (or just a queue) is a collection that is based on the  \ufb01rst-\nin-\ufb01rst-out (FIFO) policy. The policy of doing tasks in the same order that they arrive \nis one that we encounter frequently in everyday life: \nfrom people waiting in line at a theater, to cars wait-\ning in line at a toll booth, to tasks waiting to be ser-\nviced by an application on your computer. One bed-\nrock principle of any service policy is the perception \nof fairness. The \ufb01rst idea that comes to mind when \nmost people think about fairness is that whoever \nhas been waiting the longest should be served \ufb01rst. \nThat is precisely the FIFO discipline. Queues are a \nnatural model for many everyday phenomena, and \nthey play a central role in numerous applications. \nWhen a client iterates through the items in a queue \nwith the foreach construct, the items are processed \nin the order they were added to the queue. A typi-\ncal reason to use a queue in an application is to save \nitems in a collection while at the same time preserv-\ning their relative order : they come out in the same \norder in which they were put in. For example, the \nclient below is a possible implementation of the \nreadDoubles() static method from our In class. \nThe problem that this method solves for the client \nis that the client can get numbers from a \ufb01le into an \narray without knowing the \ufb01le size ahead of time. We enqueue the numbers from the \ufb01le, \nuse the size() method from Queue to \ufb01nd the size needed for the array, create the ar-\nray, and then dequeue the num-\nbers to move them to the array. \nA queue is appropriate because \nit puts the numbers into the ar-\nray in the order ", "start": 138, "end": 138}, "181": {"text": "the size needed for the array, create the ar-\nray, and then dequeue the num-\nbers to move them to the array. \nA queue is appropriate because \nit puts the numbers into the ar-\nray in the order in which they \nappear in the \ufb01le (we might use \na Bag if that order is immateri-\nal). This code uses autoboxing \nand auto-unboxing to convert \nbetween the client\u2019s double \nprimitive type and and the \nqueue\u2019s Double wrapper type.\nA typical FIFO queue\nqueue of customersserver\nenqueue\nfirst in line\nleaves queue\nnew  arrival\n at the end\nnew  arrival\nat the end\nnext in line\nleaves queue\n0 1 2\n0 1 2 3\n3 4\n4\n3\nenqueue\n4\ndequeue\n0\ndequeue\n1\n0 1 2\n10\n1\n2 3\n2 3 4\npublic static int[] readInts(String name) \n{\n   In in = new In(name);\n   Queue<Integer> q = new Queue<Integer>();\n   while (!in.isEmpty())\n       q.enqueue(in.readInt());\n   int N = q.size();\n   int[] a = new int[N];\n   for (int i = 0; i < N; i++)\n      a[i] = q.dequeue();\n   return a; \n}\n S a m p l e Queue client\n126 CHAPTER 1 \u25a0 Fundamentals\n  \n    P u s h d o w n  s t a c k s .  A pushdown stack (or just a stack) is \na collection that is based on the  last-in-\ufb01rst-out (LIFO) \npolicy. When you keep your mail in a pile on your desk, \nyou are using a stack. You pile pieces of new mail on the \ntop when they arrive and take each piece of mail ", "start": 138, "end": 139}, "182": {"text": "last-in-\ufb01rst-out (LIFO) \npolicy. When you keep your mail in a pile on your desk, \nyou are using a stack. You pile pieces of new mail on the \ntop when they arrive and take each piece of mail from \nthe top when you are ready to read it. People do not \nprocess as many papers as they did in the past, but the \nsame organizing principle underlies several of the ap-\nplications that you use regularly on your computer. For \nexample, many people organize their email as a stack\u2014\nthey  push messages on the top when they are received \nand  pop them from the top when they read them, with \nmost recently received \ufb01rst (last in, \ufb01rst out). The ad-\nvantage of this strategy is that we see interesting email as \nsoon as possible; the disadvantage is that some old email \nmight never get read if we never empty the stack. Y ou \nhave likely encountered another common example of a \nstack when sur\ufb01ng the web. When you click a hyperlink, \nyour browser displays the new page (and pushes onto a \nstack). Y ou can keep clicking on hyperlinks to visit new \npages, but you can always revisit the previous page by \nclicking the back button (popping it from the stack). \nThe LIFO policy offered by a stack provides just the be-\nhavior that you expect. When a client iterates through \nthe items in a stack with the foreach construct, the items \nare processed in the reverse of the order in \nwhich they were added. A typical reason to \nuse a stack iterator in an application is to save \nitems in a collection while at the same time \nreversing their relative order  . For example, \nthe client Reverse at right reverses the or -\nder of the integers on standard input, again \nwithout having to know ahead of time how \nmany there ", "start": 139, "end": 139}, "183": {"text": "same time \nreversing their relative order  . For example, \nthe client Reverse at right reverses the or -\nder of the integers on standard input, again \nwithout having to know ahead of time how \nmany there are. The importance of stacks in \ncomputing is fundamental and profound, \nas indicated in the detailed example that we \nconsider next. \nOperations on a pushdown stack\na stack of\ndocuments\nnew (black) one\ngoes on top\nremove the\nblack one\nfrom the top\nremove the\ngray one\nfrom the top\nnew (gray) one\ngoes on toppush(     )\npush(     )\n = pop()\n = pop()\npublic class    Reverse \n{\n   public static void main(String[] args)\n   {\n      Stack<Integer> stack;\n      stack = new Stack<Integer>();\n      while (!StdIn.isEmpty())\n         stack.push(StdIn.readInt());\n      for (int i : stack)\n         StdOut.println(i);\n   } \n}\nSample Stack client\n1271.3 \u25a0 Bags, Queues, and Stacks\n  \n   A r i t h m e t i c  e x p r e s s i o n  e v a l u a t i o n .  As another example of a stack client, we consider \na classic example that also demonstrates the utility of generics. Some of the \ufb01rst pro-\ngrams that we considered in Section 1.1 involved computing the value of arithmetic \nexpressions like this one:\n( 1 + ( ( 2 + 3 ) * ( 4 * 5 ) ) )\nIf you multiply 4 by 5, add 3 to 2, multiply the result, and then add 1, you get  the value \n101. But how does the Java system do this calculation? Without going into the details of \nhow the Java system is built, we can address the essential ideas by ", "start": 139, "end": 140}, "184": {"text": "result, and then add 1, you get  the value \n101. But how does the Java system do this calculation? Without going into the details of \nhow the Java system is built, we can address the essential ideas by writing a Java program \nthat can take a string as input (the expression) and produce the number represented by \nthe expression as output. For simplicity, we begin with the following explicit recursive \nde\ufb01nition: an arithmetic expression is either a number, or a left parenthesis followed by \nan arithmetic expression followed by an operator followed by another arithmetic ex-\npression followed by a right parenthesis. For simplicity, this de\ufb01nition is for fully paren-\nthesized arithmetic expressions, which specify precisely which operators apply to which \noperands\u2014you are a bit more familiar with expressions such as 1 + 2 * 3, where we \noften rely on precedence rules instead of parentheses. The same basic mechanisms that \nwe consider can handle precedence rules, but we avoid that complication. For speci -\n\ufb01city, we support the familiar binary operators *, +, -, and /, as well as a square-root \noperator sqrt that takes just one argument. We could easily allow more operators and \nmore kinds of operators to embrace a large class of familiar mathematical expressions, \ninvolving trigonometric, exponential, and logarithmic functions. Our focus is on un-\nderstanding how to interpret the string of parentheses, operators, and numbers to en-\nable performing in the proper order the low-level arithmetic operations that are avail-\nable on any computer. Precisely how can we convert an arithmetic expression\u2014a string \nof characters\u2014to the value that it represents? A remarkably simple algorithm that was \ndeveloped by  E. W. Dijkstra in the 1960s uses two stacks (one for operands and one for \noperators) to do this job. An expression consists of parentheses, operators, and oper-\nands (numbers). ", "start": 140, "end": 140}, "185": {"text": "E. W. Dijkstra in the 1960s uses two stacks (one for operands and one for \noperators) to do this job. An expression consists of parentheses, operators, and oper-\nands (numbers). Proceeding from left to right and taking these entities one at a time, \nwe manipulate the stacks according to four possible cases, as follows:\n\u25a0 Push operands onto the operand stack.\n\u25a0 Push operators onto the operator stack.\n\u25a0 Ignore left parentheses.\n\u25a0 On encountering a right parenthesis, pop an operator, pop the requisite number \nof operands, and push onto the operand stack the result of applying that opera-\ntor to those operands.\nAfter the \ufb01nal right parenthesis has been processed, there is one value on the stack, \nwhich is the value of the expression. This method may seem mysterious at \ufb01rst, but it \n128 CHAPTER 1 \u25a0 Fundamentals\n  D i j k s t r a \u2019 s  T w o - S t a c k  A l g o r i t h m  f o r  E x p r e s s i o n  E v a l u a t i o n\npublic class  Evaluate \n{\n   public static void main(String[] args)\n   {\n      Stack<String> ops  = new Stack<String>();\n      Stack<Double> vals = new Stack<Double>();\n      while (!StdIn.isEmpty()) \n      {  // Read token, push if operator.\n         String s = StdIn.readString();\n         if      (s.equals(\"(\"))               ;\n         else if (s.equals(\"+\"))    ops.push(s);\n         else if (s.equals(\"-\"))    ops.push(s);\n         else if (s.equals(\"*\"))    ops.push(s);\n         else if (s.equals(\"/\"))    ops.push(s);\n         else if (s.equals(\"sqrt\")) ops.push(s);\n         else if (s.equals(\")\"))\n         {  // Pop, evaluate, and push result ", "start": 140, "end": 141}, "186": {"text": "else if (s.equals(\"/\"))    ops.push(s);\n         else if (s.equals(\"sqrt\")) ops.push(s);\n         else if (s.equals(\")\"))\n         {  // Pop, evaluate, and push result if token is \")\".\n            String op = ops.pop();\n            double v = vals.pop();\n            if      (op.equals(\"+\"))    v = vals.pop() + v;\n            else if (op.equals(\"-\"))    v = vals.pop() - v;\n            else if (op.equals(\"*\"))    v = vals.pop() * v;\n            else if (op.equals(\"/\"))    v = vals.pop() / v;\n            else if (op.equals(\"sqrt\")) v = Math.sqrt(v);\n            vals.push(v);\n         }  // Token not operator or paren: push double value.\n         else vals.push(Double.parseDouble(s));\n      }\n      StdOut.println(vals.pop());\n   } \n}\nThis Stack client uses two stacks to evaluate arithmetic expressions, illustrating an essential compu-\ntational process: interpreting a string as a program and executing that program to compute the de-\nsired result. With generics, we can use the code in a single Stack implementation to implement one \nstack of String values and another stack of Double \nvalues. For simplicity, this code assumes that the expres-\nsion is fully parenthesized, with numbers and characters \nseparated by whitespace.\n% java Evaluate \n( 1 + ( ( 2 + 3 ) * ( 4 * 5 ) ) ) \n101.0\n% java Evaluate \n( ( 1 + sqrt ( 5.0 ) ) / 2.0 )\n1.618033988749895\n1291.3 \u25a0 Bags, Queues, and Stacks is easy to convince yourself that it computes the proper value: any time the algorithm \nencounters a subexpression consisting of two operands separated by an operator, all \nsurrounded by parentheses, it leaves the ", "start": 141, "end": 142}, "187": {"text": "Queues, and Stacks is easy to convince yourself that it computes the proper value: any time the algorithm \nencounters a subexpression consisting of two operands separated by an operator, all \nsurrounded by parentheses, it leaves the result of performing that operation on those \noperands on the operand stack. The result is the same as if that value had appeared in \nthe input instead of the subexpression, so we can think of replacing the subexpression \nby the value to get an expression that would yield the same result. We can apply this \nargument again and again until we get a single value. For example, the algorithm com-\nputes the same value for all of these expressions:\n( 1 + ( ( 2 + 3 ) * ( 4 * 5 ) ) ) \n( 1 + ( 5 * ( 4 * 5 ) ) ) \n( 1 + ( 5 * 20 ) ) \n( 1 + 100 ) \n101\nEvaluate on the previous page is an implementation of this algorithm. This code is a \nsimple example of an  interpreter: a program that interprets the computation speci\ufb01ed \nby a given string and performs the computation to arrive at the result. \n130 CHAPTER 1 \u25a0 Fundamentals\n 1  \n 1\n+\n 1\n + \n 1 \n + \n 1 2 \n +\n 1 2 \n + + \n 1 2 3 \n + + \n 1 5 \n + \n 1 5\n + *\n 1 5\n + *\n 1 5 4 \n + * \n 1 5 4 \n + * * \n 1 5 4 5 \n + * * \n 1 5 20 \n + * \n 1 100 \n + \n101  \n(1+((2+3)*(4*5)))\n1+((2+3)*(4*5)))\n+((2+3)*(4*5)))\n((2+3)*(4*5)))\n(2+3)*(4*5)))\n2+3)*(4*5)))\n+3)*(4*5)))\n3)*(4*5)))\n)*(4*5)))\n*(4*5)))\n(4*5)))\n4*5)))\n*5)))\n5)))\n)))\n))\n)\nTrace ", "start": 142, "end": 143}, "188": {"text": "5 \n + * * \n 1 5 20 \n + * \n 1 100 \n + \n101  \n(1+((2+3)*(4*5)))\n1+((2+3)*(4*5)))\n+((2+3)*(4*5)))\n((2+3)*(4*5)))\n(2+3)*(4*5)))\n2+3)*(4*5)))\n+3)*(4*5)))\n3)*(4*5)))\n)*(4*5)))\n*(4*5)))\n(4*5)))\n4*5)))\n*5)))\n5)))\n)))\n))\n)\nTrace of Dijkstra\u2019s two-stack arithmetic expression-evaluation algorithm\nleft parenthesis: ignore\noperand: push onto operand stack\noperator: push onto operator stack\nright parenthesis: pop operator \nand operands and push result\noperand\nstack\noperator\nstack\n1311.3 \u25a0 Bags, Queues, and Stacks\n Implementing collections To  a d d re s s  t h e  i s s u e  o f  i m p l e m e n t i n g  Bag, Stack and \nQueue, we begin with a simple classic implementation, then address improvements that \nlead us to implementations of the APIs articulated on page 121. \n  F i x e d - c a p a c i t y  s t a c k .  As a strawman, we consider an abstract data type for a \ufb01xed-\ncapacity stack of strings, shown on the opposite page. The API differs from our Stack\nAPI: it works only for String values, it requires the client to specify a capacity, and it \ndoes not support iteration. The primary choice in developing an API implementation is \nto choose a representation for the data. For  FixedCapacityStackOfStrings, an obvious \nchoice is to use an  array of String values. Pursuing this choice leads to the implemen-\ntation ", "start": 143, "end": 144}, "189": {"text": "implementation is \nto choose a representation for the data. For  FixedCapacityStackOfStrings, an obvious \nchoice is to use an  array of String values. Pursuing this choice leads to the implemen-\ntation shown at the bottom on the opposite page, which could hardly be simpler (each \nmethod is a one-liner). The instance variables are an array a[] that holds the items in \nthe stack and an integer N that counts the number of items in the stack. T o remove an \nitem, we decrement N and then return a[N]; to insert a new item, we set a[N] equal to \nthe new item and then increment N. These operations preserve the following properties:\n\u25a0 The items in the array are in their insertion order.\n\u25a0 The stack is empty when N is 0.\n\u25a0 The top of the stack (if it is nonempty) is at a[N-1].\nAs usual, thinking in terms of invariants of this sort is the easiest way to verify that an \nimplementation operates as intended. Be sure that you fully understand this implemen -\ntation. The best way to do so is to examine a trace of the stack contents for a sequence of \noperations, as illustrated at left for the test client, \nwhich reads strings from standard input and push-\nes each string onto a stack, unless it is \"-\", when it \npops the stack and prints the result. The primary \nperformance characteristic of this implementation \nis that the push and pop operations take time inde-\npendent of the stack size . For many applications, it \nis the method of choice because of its simplicity. \nBut it has several drawbacks that limit its potential \napplicability as a general-purpose tool, which we \nnow address. With a moderate amount of effort \n(and some help from Java language mechanisms), \nwe can develop an implementation that is broadly \nuseful. This effort is worthwhile because the im-\nplementations ", "start": 144, "end": 144}, "190": {"text": "tool, which we \nnow address. With a moderate amount of effort \n(and some help from Java language mechanisms), \nwe can develop an implementation that is broadly \nuseful. This effort is worthwhile because the im-\nplementations that we develop serve as a model for \nimplementations of other, more powerful, abstract \ndata types throughout the book.\nStdIn\n(push)\nStdOut \n(pop) N\na[] \n0 1 2 3 4\n0\nto 1 to\nbe 2 to be\nor 3 to be or\nnot 4 to be or not\nto 5 to be or not to\n- to 4 to be or not to\nbe 5 to be or not be\n- be 4 to be or not be\n- not 3 to be or not be\nthat 4 to be or that be\n- that 3 to be or that be\n- or 2 to be or that be\n- be 1 to be or that be\nis 2 to is or not to\nTrace of FixedCapacityStackOfStrings test client\n132 CHAPTER 1 \u25a0 Fundamentals\n public class FixedCapacityStackOfStrings \n{\n   private String[] a; // stack entries\n   private int N;      // size\n   public FixedCapacityStackOfStrings(int cap)\n   {  a = new String[cap];  }\n   public boolean isEmpty() {  return N == 0; }\n   public int size()        {  return N; }\n   public void push(String item)\n   {  a[N++] = item; }\n   public String pop()\n   {  return a[--N]; }\n}\npublic static void main(String[] args) \n{\n   FixedCapacityStackOfStrings s;\n   s = new FixedCapacityStackOfStrings(100);\n   while (!StdIn.isEmpty())\n   {\n      String item = StdIn.readString();\n      if (!item.equals(\"-\"))\n ", "start": 144, "end": 145}, "191": {"text": "args) \n{\n   FixedCapacityStackOfStrings s;\n   s = new FixedCapacityStackOfStrings(100);\n   while (!StdIn.isEmpty())\n   {\n      String item = StdIn.readString();\n      if (!item.equals(\"-\"))\n           s.push(item);\n      else if (!s.isEmpty()) StdOut.print(s.pop() + \" \");\n   }\n   StdOut.println(\"(\" + s.size() + \" left on stack)\"); \n}\n% more tobe.txt \nto be or not to - be - - that - - - is\n% java FixedCapacityStackOfStrings < tobe.txt \nto be not that or be (2 left on stack)\nAn abstract data type for a fixed-capacity stack of strings\ntest client\napplication\nimplementation\nAPI public class    FixedCapacityStackOfStrings\nFixedCapacityStackOfStrings(int cap) create an empty stack of capacity cap \nvoid push(String item) add a string\nString pop() remove the most recently added string\nboolean isEmpty() is the stack empty?\nint size() number of strings on the stack\n1331.3 \u25a0 Bags, Queues, and Stacks\n  \n \n \n \n     G e n e r i c s . The \ufb01rst drawback of FixedCapacityStackOfStrings is that it works only \nfor String objects. If we want a stack of double values, we would need to develop \nanother class with similar code, essentially replacing String with double everywhere. \nThis is easy enough but becomes burdensome when we consider building a stack of \nTransaction values or a queue of Date values, and so forth. As discussed on page 122, \nJava\u2019s parameterized types (generics) are speci\ufb01cally designed to address this situation, \nand we saw several examples of client code (on pages 125, 126, 127, and 129). But how \ndo we implement a generic stack? The code on the facing page shows the details. It imple-\nments ", "start": 145, "end": 146}, "192": {"text": "\nand we saw several examples of client code (on pages 125, 126, 127, and 129). But how \ndo we implement a generic stack? The code on the facing page shows the details. It imple-\nments a class FixedCapacityStack that differs from FixedCapacityStackOfStrings\nonly in the code highlighted in red\u2014we replace every occurrence of String with Item\n(with one exception, discussed below) and declare the class with the following \ufb01rst line \nof code:\npublic class FixedCapacityStack<Item>\nThe name Item is a  type parameter, a symbolic placeholder for some  concrete type to be \nused by the client. Y ou can read  FixedCapacityStack<Item> as stack of items, which is \nprecisely what we want. When implementing FixedCapacityStack, we do not know \nthe  actual type of Item, but a client can use our stack for any type of data by providing a \n c o n c r e t e  t y p e  w h e n  t h e  s t a c k  i s  c r e a t e d .  C o n c r e t e  t y p e s  m u s t  b e  r e f e r e n c e  t y p e s ,  b u t  c l i-\nents can depend on autoboxing to convert primitive types to their corresponding wrap-\nper types. Java uses the type parameter  Item to check for type mismatch errors\u2014even \nthough no  concrete type is yet known, variables of type Item must be assigned values \nof type Item, and so forth. But there is one signi\ufb01cant hitch in this story: We would like \nto implement the constructor in FixedCapacityStack with the code\na = new Item[cap];\nwhich calls for creation of a generic array. For historical and technical reasons beyond \nour scope,   generic ", "start": 146, "end": 146}, "193": {"text": "this story: We would like \nto implement the constructor in FixedCapacityStack with the code\na = new Item[cap];\nwhich calls for creation of a generic array. For historical and technical reasons beyond \nour scope,   generic array creation is disallowed in Java. Instead, we need to use a cast:\na = (Item[]) new Object[cap];\nThis code produces the desired effect (though the Java compiler gives a warning, which \nwe can safely ignore), and we use this idiom throughout the book (the Java system li -\nbrary implementations of similar abstract data types use the same idiom).  \n134 CHAPTER 1 \u25a0 Fundamentals\n public class FixedCapacityStack<Item> \n{\n   private Item[] a;   // stack entries\n   private int N;      // size\n   public FixedCapacityStack(int cap)\n   {  a = (Item[]) new Object[cap];  }\n   public boolean isEmpty() {  return N == 0; }\n   public int size()        {  return N; }\n   public void push(Item item)\n   {  a[N++] = item; }\n   public Item pop()\n   {  return a[--N]; }\n}\npublic static void main(String[] args) \n{\n   FixedCapacityStack<String> s;\n   s = new FixedCapacityStack<String>(100);\n   while (!StdIn.isEmpty())\n   {\n      String item = StdIn.readString();\n      if (!item.equals(\"-\"))\n           s.push(item);\n      else if (!s.isEmpty()) StdOut.print(s.pop() + \" \");\n   }\n   StdOut.println(\"(\" + s.size() + \" left on stack)\"); \n}\n% more tobe.txt \nto be or not to - be - - that - - - is\n% java FixedCapacityStack < tobe.txt \nto be not that or be (2 left on stack)\nAn abstract data type for a fixed-capacity generic stack\ntest client\napplication\nimplementation\nAPI ", "start": 146, "end": 147}, "194": {"text": "that - - - is\n% java FixedCapacityStack < tobe.txt \nto be not that or be (2 left on stack)\nAn abstract data type for a fixed-capacity generic stack\ntest client\napplication\nimplementation\nAPI public class    FixedCapacityStack<Item>\nFixedCapacityStack(int cap) create an empty stack of capacity cap\nvoid push(Item item) add an item\nItem pop() remove the most recently added item\nboolean isEmpty() is the stack empty?\nint size() number of items on the stack\n1351.3 \u25a0 Bags, Queues, and Stacks\n     Array resizing. Choosing an array to represent the stack contents implies that clients \nmust estimate the maximum size of the stack ahead of time. In Java, we cannot change \nthe size of an array once created, so the stack always uses space proportional to that \nmaximum. A client that chooses a large capacity risks wasting a large amount of mem-\nory at times when the collection is empty or nearly empty. For example, a transaction \nsystem might involve billions of items and thousands of collections of them. Such a \nclient would have to allow for the possibility that each of those collections could hold \nall of those items, even though a typical constraint in such systems is that each item \ncan appear in only one collection. Moreover, every client risks over\ufb02ow if the collection \ngrows larger than the array. For this reason, push() needs code to test for a full stack \n,and we should have an isFull() method in the API to allow clients to test for that \ncondition. We omit that code, because our desire is to relieve the client from having to \ndeal with the concept of a full stack, as articulated in our original Stack API. Instead, \nwe modify the array implementation to dynamically adjust the size of the array a[] so \nthat it is both suf\ufb01ciently large to hold all of the items and not so large as ", "start": 147, "end": 148}, "195": {"text": "our original Stack API. Instead, \nwe modify the array implementation to dynamically adjust the size of the array a[] so \nthat it is both suf\ufb01ciently large to hold all of the items and not so large as to waste an \nexcessive amount of space. Achieving these goals turns out to be remarkably easy. First, \nwe implement a method that moves a stack into an array of a different size:\nprivate void resize(int max) \n{  // Move stack of size N <= max to a new array of size max.\n   Item[] temp = (Item[]) new Object[max];\n   for (int i = 0; i < N; i++)\n      temp[i] = a[i];\n   a = temp; \n}\nNow, in push(), we check whether the array is too small. In particular, we check wheth-\ner there is room for the new item in the array by checking whether the stack size N is \nequal to the array size a.length. If there is no room, we double the size of the array. \nThen we simply insert the new item with the code a[N++] = item, as before:\npublic void push(String item) \n{  // Add item to top of stack.\n   if (N == a.length) resize(2*a.length);\n   a[N++] = item; \n}\nSimilarly, in pop(), we begin by deleting the item, then we halve the array size if it is \ntoo large. If you think a bit about the situation, you will see that the appropriate test \nis whether the stack size is less than one-fourth the array size. After the array is halved, \nit will be about half full and can accommodate a substantial number of push() and \npop() operations before having to change the size of the array again. \n136 CHAPTER 1 \u25a0 Fundamentals\n  \n  \n \npublic String pop() \n{  // Remove item from top of stack.\n ", "start": 148, "end": 149}, "196": {"text": "substantial number of push() and \npop() operations before having to change the size of the array again. \n136 CHAPTER 1 \u25a0 Fundamentals\n  \n  \n \npublic String pop() \n{  // Remove item from top of stack.\n   String item = a[--N];\n   a[N] = null;  // Avoid loitering (see text).\n   if (N > 0 && N == a.length/4) resize(a.length/2);\n   return item; \n}\nWith this implementation, the stack never over\ufb02ows and never becomes less than one-\nquarter full (unless the stack is empty, when the array size is 1). We will address the \nperformance analysis of this approach in more detail in Section 1.4.\n   Loitering. Java\u2019s garbage collection policy is to reclaim the memory associated with \nany objects that can no longer be accessed. In our pop() implementations, the refer -\nence to the popped item remains in the array. The item is effectively an  orphan\u2014it will \nbe never be accessed again\u2014but the Java garbage collector has no way to know this \nuntil it is overwritten. Even when the client is done with the item, the reference in the \narray may keep it alive. This condition (holding a reference to an item that is no longer \nneeded) is known as  loitering. In this case, loitering is easy to avoid, by setting the array \nentry corresponding to the popped item to null, thus overwriting the unused refer -\nence and making it possible for the system to reclaim the memory associated with the \npopped item when the client is \ufb01nished with it.\npush() pop() N a.length\na[]\n0 1 2 3 4 5 6 7\n0 1 null\nto 1 1 to\nbe 2 2 to be\nor 3 4 to be or null\nnot ", "start": 149, "end": 149}, "197": {"text": "1 2 3 4 5 6 7\n0 1 null\nto 1 1 to\nbe 2 2 to be\nor 3 4 to be or null\nnot 4 4 to be or not\nto 5 8 to be or not to null null null\n- to 4 8 to be or not null null null null\nbe 5 8 to be or not be null null null\n- be 4 8 to be or not null null null null\n- not 3 8 to be or null null null null null\nthat 4 8 to be or that null null null null\n- that 3 8 to be or null null null null null\n- or 2 4 to be null null\n- be 1 2 to null\nis 2 2 to is\nTrace of array resizing during a sequence of push() and pop() operations\n1371.3 \u25a0 Bags, Queues, and Stacks\n        I t e r a t i o n . As mentioned earlier in this section, one of the fundamental operations on \ncollections is to process each item by iterating through the collection using Java\u2019s foreach\nstatement. This paradigm leads to clear and compact code that is free from dependence \non the details of a collection\u2019s implementation. T o consider the task of implementing \niteration, we start with a snippet of client code that prints all of the items in a collection \nof strings, one per line:\nStack<String> collection = new Stack<String>(); \n... \nfor (String s : collection)\n   StdOut.println(s); \n...\nNow, this foreach statement is shorthand for a while construct (just like the for state-\nment itself). It is essentially equivalent to the following while statement:\nIterator<String> i = collection.iterator(); \nwhile (i.hasNext()) \n{\n   String s = i.next();\n   StdOut.println(s); ", "start": 149, "end": 150}, "198": {"text": "(just like the for state-\nment itself). It is essentially equivalent to the following while statement:\nIterator<String> i = collection.iterator(); \nwhile (i.hasNext()) \n{\n   String s = i.next();\n   StdOut.println(s); \n}\nThis code exposes the ingredients that we need to implement in any iterable collection:\n\u25a0 The collection must implement an iterator() method that returns an \nIterator object.\n\u25a0 \n \nThe Iterator class must include two methods: hasNext() (which returns a \nboolean value) and next() (which returns a generic item from the collection).\nIn Java, we use the interface mechanism to express the idea that a class implements \na speci\ufb01c method (see page 100). For iterable collections, the necessary interfaces are al-\nready de\ufb01ned for us in Java. To make a class iterable, the \ufb01rst step is to add the phrase \nimplements Iterable<Item> to its declaration, matching the interface\npublic interface    Iterable<Item> \n{\n   Iterator<Item> iterator(); \n}\n(which is in java.lang.Iterable), and to add a method iterator() to the class that \nreturns an Iterator<Item>. Iterators are generic, so we can use our parameterized \ntype Item to allow clients to iterate through objects of whatever type is provided by our \nclient. For the array representation that we have been using, we need to iterate through \n138 CHAPTER 1 \u25a0 Fundamentals\n  \nan array in reverse order, so we name the iterator ReverseArrayIterator and add this \nmethod:\npublic Iterator<Item> iterator() \n{  return new ReverseArrayIterator();  }\nWhat is an iterator? An object from a class that implements the methods hasNext()\nand next(), as de\ufb01ned in the following interface (which is in java.util.Iterator):\npublic interface    Iterator<Item> \n{\n    boolean hasNext();\n    Item next();\n    void remove(); \n}\nAlthough the interface speci\ufb01es a remove() method, ", "start": 150, "end": 151}, "199": {"text": "the following interface (which is in java.util.Iterator):\npublic interface    Iterator<Item> \n{\n    boolean hasNext();\n    Item next();\n    void remove(); \n}\nAlthough the interface speci\ufb01es a remove() method, we always use an empty method \nfor remove() in this book, because interleaving iteration with operations that modify \nthe data structure is best avoided. For  ReverseArrayIterator, these methods are all \none-liners, implemented in a nested class within our stack class:\nprivate class ReverseArrayIterator implements Iterator<Item> \n{\n   private int i = N;\n   public boolean hasNext() {  return i > 0;   }\n   public Item next()       {  return a[--i];  }\n   public void remove()     {                  } \n}\nNote that this nested class can access the instance variables of the enclosing class, in \nthis case a[] and N (this ability is the main reason we use nested classes for iterators). \nTe c h n i c a l l y, to  co n f o r m  to  t h e  Iterator speci\ufb01cation, we should throw exceptions \nin two cases: an    UnsupportedOperationException if a client calls remove() and a \n  NoSuchElementException if a client calls next() when i is 0. Since we only use itera-\ntors in the foreach construction where these conditions do not arise, we omit this code. \nOne crucial detail remains: we have to include\nimport java.util.Iterator;\nat the beginning of the program because (for historical reasons) Iterator is not part \nof java.lang (even though Iterable is part of java.lang). Now a client using the \nforeach statement for this class will get behavior equivalent to the common for loop for \narrays, but does not need to be aware of the array representation (an implementation \n1391.3 \u25a0 Bags, Queues, and Stacks\n detail). This arrangement is of critical importance for implementations ", "start": 151, "end": 152}, "200": {"text": "common for loop for \narrays, but does not need to be aware of the array representation (an implementation \n1391.3 \u25a0 Bags, Queues, and Stacks\n detail). This arrangement is of critical importance for implementations of fundamen-\ntal data types like the collections that we consider in this book and those included in \nJava libraries. For example, it frees us to switch to a totally different representation \nwithout having to change any client code . More important, taking the client\u2019s point of \nview, it allows clients to use iteration without having to know any details of the class \nimplementation.\nAlgorithm 1.1 is an implementation of our Stack API that resizes the array, allows \nclients to make stacks for any type of data, and supports client use of foreach to iterate \nthrough the stack items in LIFO order. This implementation is based on Java language \nnuances involving Iterator and Iterable, but there is no need to study those nuances \nin detail, as the code itself is not complicated and can be used as a template for other \ncollection implementations.\nFor example, we can implement the Queue API by maintaining two indices as in-\nstance variables, a variable head for the beginning of the queue and a variable tail for \nthe end of the queue. T o remove an item, use head to access it and then increment head; \nto insert an item, use tail to store it, and then increment tail. If incrementing an \nindex brings it past the end of the array, reset it to 0. Developing the details of checking \nwhen the queue is empty and when the array is full and needs resizing is an interesting \nand worthwhile programming exercise (see Exercise 1.3.14).\nIn the context of the study of algorithms, Algorithm 1.1 is signi\ufb01cant because \nit almost (but not quite) achieves optimum performance goals for any collection \nimplementation:\n\u25a0 Each operation should require time independent of the collection ", "start": 152, "end": 152}, "201": {"text": "of the study of algorithms, Algorithm 1.1 is signi\ufb01cant because \nit almost (but not quite) achieves optimum performance goals for any collection \nimplementation:\n\u25a0 Each operation should require time independent of the collection size.\n\u25a0 \n \nThe space used should always be within a constant factor of the collection size.\nThe \ufb02aw in  ResizingArrayStack is that some push and pop operations require resiz-\ning: this takes time proportional to the size of the stack. Next, we consider a way to cor-\nrect this \ufb02aw, using a fundamentally different way to structure data. \nStdIn\n(enqueue)\nStdOut\n(dequeue) N head tail\na[] \n0 1 2 3 4 5 6 7\n5 0 5 to be or not to\n- to 4 1 5 to be or not to\nbe 5 1 6 to be or not to be\n- be 4 2 6 to be or not to be\n- or 3 3 6 to be or that to be\nTrace of ResizingArrayQueue test client\n140 CHAPTER 1 \u25a0 Fundamentals\n ALGORITHM 1.1   Pushdown (LIFO) stack (resizing array implementation)\nimport java.util.Iterator; \npublic class  ResizingArrayStack<Item> implements Iterable<Item> \n{\n   private Item[] a = (Item[]) new Object[1];  // stack items\n   private int N = 0;                          // number of items\n   public boolean isEmpty()  {  return N == 0; }\n   public int size()         {  return N;      }\n   private void resize(int max)\n   {  // Move stack to a new array of size max.\n      Item[] temp = (Item[]) new Object[max];\n      for (int i = 0; i < N; i++)\n         temp[i] ", "start": 152, "end": 153}, "202": {"text": "max)\n   {  // Move stack to a new array of size max.\n      Item[] temp = (Item[]) new Object[max];\n      for (int i = 0; i < N; i++)\n         temp[i] = a[i];\n      a = temp;\n   }\n   public void push(Item item)\n   {  // Add item to top of stack.\n      if (N == a.length) resize(2*a.length);\n      a[N++] = item;\n   }\n   public Item pop()\n   {  // Remove item from top of stack.\n      Item item = a[--N];\n      a[N] = null;  // Avoid loitering (see text).\n      if (N > 0 && N == a.length/4) resize(a.length/2);\n      return item;\n   }\n   public Iterator<Item> iterator()\n    {  return new ReverseArrayIterator();  }\n   private class ReverseArrayIterator implements Iterator<Item>\n   {  // Support LIFO iteration.\n      private int i = N;\n      public boolean hasNext() {  return i > 0;   }\n      public    Item next()    {  return a[--i];  }\n      public    void remove()  {                  }\n   } \n}\nThis generic, iterable implementation of our Stack API is a model for collection ADTs that keep \nitems in an array. It resizes the array to keep the array size within a constant factor of the stack size.\n1411.3 \u25a0 Bags, Queues, and Stacks  \n  L i n k e d  l i s t s  Now we consider the use of a fundamental data structure that is an ap-\npropriate choice for representing the data in a collection ADT implementation. This is \nour \ufb01rst example of building a data structure that is not directly supported by the Java \nlanguage. Our implementation serves as a model for the code that we use for building \nmore complex data structures ", "start": 153, "end": 154}, "203": {"text": "ADT implementation. This is \nour \ufb01rst example of building a data structure that is not directly supported by the Java \nlanguage. Our implementation serves as a model for the code that we use for building \nmore complex data structures throughout the book, so you should read this section \ncarefully, even if you have experience working with linked lists.\nDefinition. A  linked list is a recursive data structure that is either empty (null) or a \nreference to a node having a generic item and a reference to a linked list. \n \n \n  \n \nThe node in this de\ufb01nition is an abstract entity that might hold any kind of data, in ad-\ndition to the node reference that characterizes its role in building linked lists. As with a \nrecursive program, the concept of a recursive data structure can be a bit mindbending \nat \ufb01rst, but is of great value because of its simplicity.\n  N o d e  r e c o r d .  With object-oriented programming, implementing linked lists is not dif-\n\ufb01cult. We start with a nested class that de\ufb01nes the node abstraction:\nprivate class  Node \n{\n   Item item;\n   Node next; \n}\nA Node has two instance variables: an Item (a parameterized type) and a Node. We \nde\ufb01ne Node within the class where we want to use it, and make it private because it \nis not for use by clients. As with any data type, we create an object of type Node by in-\nvoking the (no-argument) constructor with new Node(). The result is a reference to a \nNode object whose instance variables are both initialized to the value null. The Item is \na placeholder for any data that we might want to structure with a linked list (we will use \nJava\u2019s generic mechanism so that it can represent any reference type); the instance vari-\nable of type Node characterizes the linked nature of ", "start": 154, "end": 154}, "204": {"text": "placeholder for any data that we might want to structure with a linked list (we will use \nJava\u2019s generic mechanism so that it can represent any reference type); the instance vari-\nable of type Node characterizes the linked nature of the data structure. T o emphasize \nthat we are just using the Node class to structure the data, we de\ufb01ne no methods and \nwe refer directly to the instance variables in code: if first is a variable associated with \nan object of type Node, we can refer to the instance variables with the code first.item\nand first.next. Classes of this kind are sometimes called records. They do not imple-\nment abstract data types because we refer directly to instance variables. However, Node\nand its client code are in the same class in all of our implementations and not accessible \nby clients of that class, so we still enjoy the bene\ufb01ts of data abstraction.\n142 CHAPTER 1 \u25a0 Fundamentals\n  \nBuilding a linked list. Now, from the recursive de\ufb01nition, we can represent a linked \nlist with a variable of type Node simply by ensuring that its value is either null or a ref-\nerence to a Node whose next \ufb01eld is a reference to a linked list. For example, to build a \nlinked list that contains the items to, be, and or, we create a Node for each item:\nNode first  = new Node();\nNode second = new Node(); \nNode third  = new Node();\nand set the item \ufb01eld in each of the nodes to the \ndesired value (for simplicity, these examples assume \nthat Item is String):\nfirst.item  = \"to\"; \nsecond.item = \"be\"; \nthird.item  = \"or\";\nand set the next \ufb01elds to build the linked list:\nfirst.next  = second; \nsecond.next = third;\n(Note that third.next remains null, the value it \nwas initialized to at the time of creation.)As ", "start": 154, "end": 155}, "205": {"text": "\"or\";\nand set the next \ufb01elds to build the linked list:\nfirst.next  = second; \nsecond.next = third;\n(Note that third.next remains null, the value it \nwas initialized to at the time of creation.)As a result, \nthird is a linked list (it is a reference to a node that \nhas a reference to null, which is the null reference \nto an empty linked list), and second is a linked list \n(it is a reference to a node that has a reference to \nthird, which is a linked list), and first is a linked \nlist (it is a reference to a node that has a reference to \nsecond, which is a linked list). The code that we will \nexamine does these assignment statements in a dif -\nferent order, depicted in the diagram on this page.\nA linked list represents a sequence of items. In the example just considered, first\nrepresents the sequence to be or. We can also use an array to represent a sequence of \nitems. For example, we could use\nString[] s = { \"to\", \"be\", \"or\" };\nto represent the same sequence of strings. The difference is that it is easier to insert \nitems into the sequence and to remove items from the sequence with linked lists. Next, \nwe consider code to accomplish these tasks.\n        \nor\nnull        \nbe\nNode third  = new Node();\nthird.item  = \"or\";\nsecond.next = third;\n        \nto\n        \nbe\nNode second = new Node();\nsecond.item = \"be\";\nfirst.next  = second;\n        \nto\nNode first  = new Node();\nfirst.item  = \"to\";\n        \nto\nfirst\nsecondfirst\nsecond\nthird\nfirst\nnull\nnull\nLinking together a list\n1431.3 \u25a0 Bags, Queues, and Stacks\n When tracing code that uses linked lists and other linked structures, we use a visual \nrepresentation ", "start": 155, "end": 156}, "206": {"text": "\nto\nfirst\nsecondfirst\nsecond\nthird\nfirst\nnull\nnull\nLinking together a list\n1431.3 \u25a0 Bags, Queues, and Stacks\n When tracing code that uses linked lists and other linked structures, we use a visual \nrepresentation where\n\u25a0 We draw a rectang le to represent each object\n\u25a0 We put the values of  instance var iables w ithin the rectang le\n\u25a0 \n \nWe use ar rows that point to the referenced objects to depict references\nThis visual representation captures the essential characteristic of linked lists. For econ-\nomy, we use the term links to refer to node references. For simplicity, when item values \nare strings (as in our examples), we put the string within the object rectangle rather \nthan the more accurate rendition depicting the string object and the character array \nthat we discussed in Section 1.2. This visual representation allows us to focus on the \nlinks.\n I n s e r t  a t  t h e  b e g i n n i n g .  First, suppose that you want to insert a new node into a linked \nlist. The easiest place to do so is at the beginning of the list. For example, to insert the \nstring not at the beginning of a given linked list whose \ufb01rst node is first, we save \nfirst in oldfirst, assign to first a new Node, and assign its item \ufb01eld to not and its \nnext \ufb01eld to oldfirst. This code for inserting a node at the beginning of a linked list \ninvolves just a few assignment statements, so the amount of time that it takes is inde-\npendent of the length of the list. \n        \nor        \nbe\nInserting a new node at the beginning of a linked list\nfirst = new Node();\nNode oldfirst = first;\ntofirst\n        \nor        \nbe\nto\noldfirst\noldfirst\n        \nfirst\nsave ", "start": 156, "end": 156}, "207": {"text": "\nor        \nbe\nInserting a new node at the beginning of a linked list\nfirst = new Node();\nNode oldfirst = first;\ntofirst\n        \nor        \nbe\nto\noldfirst\noldfirst\n        \nfirst\nsave a link to the list\ncreate a new node for the beginning\nset the instance variables in the new node\nfirst.item = \"not\";\nfirst.next = oldfirst;\n        \norbe\nto        \nnotfirst\nnull\nnull\nnull\n144 CHAPTER 1 \u25a0 Fundamentals\n  \n \nRemove from the beginning. Next, suppose that you \nwant to remove the \ufb01rst node from a list. This op-\neration is even easier: simply assign to first the value \nfirst.next. Normally, you would retrieve the value of \nthe item (by assigning it to some variable of type Item) \nbefore doing this assignment, because once you change \nthe value of first, you may not have any access to the \nnode to which it was referring. Typically, the node ob-\nject becomes an  orphan, and the Java memory manage-\nment system eventually reclaims the memory it occupies. \nAgain, this operation just involves one assignment statement, so its running time is \nindependent of the length of the list.\n I n s e r t  a t  t h e  e n d .  How do we add a node to the end of a linked list? T o do so, we need \na link to the last node in the list, because that node\u2019s link has to be changed to refer -\nence a new node containing the item to be inserted. Maintaining an extra link is not \nsomething that should be taken lightly in linked-list code, because every method that \nmodi\ufb01es the list needs code to check whether that variable needs to be modi\ufb01ed (and \nto make the necessary modi\ufb01cations). For \nexample, the code that we just examined ", "start": 156, "end": 157}, "208": {"text": "\nmodi\ufb01es the list needs code to check whether that variable needs to be modi\ufb01ed (and \nto make the necessary modi\ufb01cations). For \nexample, the code that we just examined for \nremoving the \ufb01rst node in the list might in -\nvolve changing the reference to the last node \nin the list, since when there is only one node \nin the list, it is both the \ufb01rst one and the last \none! Also, this code does not work (it follows \na null link) in the case that the list is empty. \nDetails like these make linked-list code noto-\nriously dif\ufb01cult to debug. \nInsert/remove at other positions. In sum -\nmary, we have shown that we can implement \nthe following operations on linked lists with \njust a few instructions, provided that we have \naccess to both a link first to the \ufb01rst ele-\nment in the list and a link last to the last \nelement in the list:\n\u25a0 Insert at the beginning.\n\u25a0 Remove from the beginning.\n\u25a0 Insert at the end.\n        \nor        \nbe\ntofirst\nfirst = first.next;\n        \norbe\nto\nfirst\nnull\nnull\nRemoving the first node in a linked list\n        \nor        \nbe\nInserting a new node at the end of a linked list\nNode last = new Node();\nlast.item = \"not\";\nNode oldlast = last;\ntofirst\n        \nor        \nbe\nto\noldlast\noldlast\nlast\n        \nsave a link to the last node\ncreate a new node for the end\nlink the new node to the end of the list\noldlast.next = last;\n        \nnot\nnot\nor\nbe        \ntofirst\nnull\nnull\nnull\nnull\nlast\nlastfirst\noldlast\n1451.3 \u25a0 Bags, Queues, and Stacks\n Other operations, such as the ", "start": 157, "end": 158}, "209": {"text": "\nnot\nnot\nor\nbe        \ntofirst\nnull\nnull\nnull\nnull\nlast\nlastfirst\noldlast\n1451.3 \u25a0 Bags, Queues, and Stacks\n Other operations, such as the following, are not so easily handled:\n\u25a0 Remove a given node.\n\u25a0 \n \n \nInsert a new node before a given node.\nFor example, how can we remove the last node from a list? The link last is no help, \nbecause we need to set the link in the previous node in the list (the one with the same \nvalue as last) to null. In the absence of any other information, the only solution is to \ntraverse the entire list looking for the node that links to last (see below and Exercise \n1.3.19). Such a solution is undesirable because it takes time proportional to the length \nof the list. The standard solution to enable arbitrary insertions and deletions is to use \na   doubly-linked list, where each node has two links, one in each direction. We leave the \ncode for these operations as an exercise (see Exercise 1.3.31). We do not need doubly \nlinked lists for any of our implementations.\n T r a v e r s a l .  To  e x a m i n e  e ve r y  i te m  i n  a n  a r r ay, w e  u s e  f a m i l i a r  co d e  l i ke  t h e  f o l l ow i n g  \nloop for processing the items in an array a[]:\nfor (int i = 0; i < N; i++) \n{\n   // Process a[i]. \n}\nThere is a corresponding idiom for examining the items in a linked list: We initialize a \nloop index variable x to reference the \ufb01rst Node of the linked list. Then we ", "start": 158, "end": 158}, "210": {"text": "\n{\n   // Process a[i]. \n}\nThere is a corresponding idiom for examining the items in a linked list: We initialize a \nloop index variable x to reference the \ufb01rst Node of the linked list. Then we \ufb01nd the item \nassociated with x by accessing x.item, and then update x to refer to the next Node in the \nlinked list, assigning to it the value of x.next and repeating this process until x is null\n(which indicates that we have reached the end of the linked list). This process is known \nas traversing the list and is succinctly expressed in code like the following loop for pro-\ncessing the items in a linked list whose \ufb01rst item is associated with the variable first:\nfor (Node x = first; x != null; x = x.next) \n{\n   // Process x.item. \n}\nThis idiom is as natural as the standard idiom for iterating through the items in an ar-\nray. In our implementations, we use it as the basis for iterators for providing client code \nthe capability of iterating through the items, without having to know the details of the \nlinked-list implementation.\n146 CHAPTER 1 \u25a0 Fundamentals\n   S t a c k  i m p l e m e n t a t i o n .  Given these preliminaries, developing an implementation for \nour Stack API is straightforward, as shown in Algorithm 1.2 on page 149. It maintains \nthe stack as a linked list, with the top of the stack at the beginning, referenced by an \ninstance variable first. Thus, to push() an item, we add it to the beginning of the \nlist, using the code discussed on page 144 and to pop() an item, we remove it from the \nbeginning of the list, using the code discussed on page 145. T o implement size(), we keep \ntrack of the number of items in an instance variable ", "start": 158, "end": 159}, "211": {"text": "and to pop() an item, we remove it from the \nbeginning of the list, using the code discussed on page 145. T o implement size(), we keep \ntrack of the number of items in an instance variable N, incrementing N when we push \nand decrementing N when we pop. T o implement isEmpty() we check whether first\nis null (alternatively, we could check whether N is 0). The implementation uses the \ngeneric type Item\u2014you can think of the code <Item> after the class name as meaning \nthat any occurrence of Item in the implementation will be replaced by a client-supplied \ndata-type name (see page 134). For now, we omit the code to support iteration, which we \nconsider on page 155. A trace for the test client that we have been using is shown on the \nnext page. This use of linked lists achieves our optimum design goals:\n\u25a0 It can be used for any type of data.\n\u25a0 The space required is always proportional to the size of the collection.\n\u25a0 \n \n \nThe time per operation is always independent of the size of the collection.\nThis implementation is a prototype for many algorithm implementations that we con-\nsider. It de\ufb01nes the linked-list data structure and implements the client methods push()\nand pop() that achieve the speci\ufb01ed effect with just a few lines of code. The algorithms \nand data structure go hand in hand. In this case, the code for the algorithm implemen-\ntations is quite simple, but the properties of the data structure are not at all elemen-\ntary, requiring explanations on the past several pages. This interaction between data \nstructure de\ufb01nition and algorithm implementation is typical and is our focus in ADT \nimplementations throughout this book.\npublic static void main(String[] args) \n{  // Create a stack and push/pop strings as directed on StdIn.\n   Stack<String> s = ", "start": 159, "end": 159}, "212": {"text": "typical and is our focus in ADT \nimplementations throughout this book.\npublic static void main(String[] args) \n{  // Create a stack and push/pop strings as directed on StdIn.\n   Stack<String> s = new Stack<String>();\n   while (!StdIn.isEmpty())\n   {\n      String item = StdIn.readString();\n      if (!item.equals(\"-\"))\n           s.push(item);\n      else if (!s.isEmpty()) StdOut.print(s.pop() + \" \");\n   }\n   StdOut.println(\"(\" + s.size() + \" left on stack)\"); \n}\n T e s t  c l i e n t  f o r  Stack\n1471.3 \u25a0 Bags, Queues, and Stacks\n to\n        \n        \nto\nbe\n        \nto        \nbe\nor\nnull\nnull\nnull\n        \nbe        \nor\nnot\nto\n        \nor        \nnot\nto\nnull\nbe\n        \nbe        \norto not\n        \nor        \nnot\nbe\n        \nbe        \norbe not\n        \nto        \nbenot\nor\nnull\n        \nbe        \nor\nthat\n        \nto        \nbethat or\nnull\n                \ntoor be\n        \nbe to\n        \nto\nto\nStdIn StdOut\nbe\nor\nnot\nto\n-\nbe\n-\n-\nthat\n-\n-\n-\nis is\nto\nnull\nto\nnull\nto\nnull\nto\nnull\nbe to\nnull\nTrace of Stack development client\n148 CHAPTER 1 \u25a0 Fundamentals\n ALGORITHM 1.2   Pushdown stack (linked-list implementation)\npublic class  Stack<Item> implements Iterable<Item> \n{\n   private Node first; // top of stack (most recently added node)\n   private int N;      // number of items\n   private  class Node\n   {  // nested class to define nodes\n      Item item;\n      Node next;\n   }\n   public boolean isEmpty() {  return first == null; }  // Or: N == 0.\n   public ", "start": 159, "end": 161}, "213": {"text": "class Node\n   {  // nested class to define nodes\n      Item item;\n      Node next;\n   }\n   public boolean isEmpty() {  return first == null; }  // Or: N == 0.\n   public int size()        {  return N; }\n   public void push(Item item)\n   {  // Add item to top of stack.\n      Node oldfirst = first;\n      first = new Node();\n      first.item = item;\n      first.next = oldfirst;\n      N++;\n   }\n   public Item pop()\n   {  // Remove item from top of stack.\n      Item item = first.item;\n      first = first.next;\n      N--;\n      return item;\n   }\n   // See page 155 for iterator() implementation.\n   // See page 147 for test client main().\n}\nThis generic Stack implementation is based on a linked-list data structure. It can be used to create \nstacks containing any type of data. T o support \niteration, add the highlighted code described \nfor Bag on page 155. % more tobe.txt \nto be or not to - be - - that - - - is\n% java Stack < tobe.txt \nto be not that or be (2 left on stack)\n1491.3 \u25a0 Bags, Queues, and Stacks   Q u e u e  i m p l e m e n t a t i o n .  An implementation of our Queue API based on the linked-\nlist data structure is also straightforward, as shown in Algorithm 1.3 on the facing \npage. It maintains the queue as a linked list in order from least recently to most recently \nadded items, with the beginning of the queue referenced by an instance variable first\nand the end of the queue referenced by an instance variable last. Thus, to enqueue()\nan item, we add it to the end of the list (using the code discussed on page 145, augmented \nto ", "start": 161, "end": 162}, "214": {"text": "variable first\nand the end of the queue referenced by an instance variable last. Thus, to enqueue()\nan item, we add it to the end of the list (using the code discussed on page 145, augmented \nto set both first and last to refer to the new node when the list is empty) and to \ndequeue() an item, we remove it from the beginning of the list (using the same code \nas for pop() in Stack, augmented to update last when the list becomes empty). The \nimplementations of size() and isEmpty() are the same as for Stack. As with Stack\nthe implementation uses the generic type parameter Item, and we omit the code to \nsupport iteration, which we consider in our Bag implementation on page 155.  A develop-\nment client similar to the one we used for Stack is shown below, and the trace for this \nclient is shown on the following page. This implementation uses the same data struc-\nture as does Stack\u2014a linked list\u2014but it implements different algorithms for adding \nand removing items, which make the difference between LIFO and FIFO for the client. \nAgain, the use of linked lists achieves our optimum design goals: it can be used for any \ntype of data, the space required is proportional to the number of items in the collection, \nand the time required per operation is always independent of the size of the collection. \npublic static void main(String[] args) \n{  // Create a queue and enqueue/dequeue strings.\n   Queue<String> q = new Queue<String>();\n   while (!StdIn.isEmpty())\n   {\n      String item = StdIn.readString();\n      if (!item.equals(\"-\"))\n           q.enqueue(item);\n      else if (!q.isEmpty()) StdOut.print(q.dequeue() + \" \");\n   }\n   StdOut.println(\"(\" + q.size() + \" left on queue)\"); \n}\n T e s t  c l i e n t  f o r  Queue\n% ", "start": 162, "end": 162}, "215": {"text": "StdOut.print(q.dequeue() + \" \");\n   }\n   StdOut.println(\"(\" + q.size() + \" left on queue)\"); \n}\n T e s t  c l i e n t  f o r  Queue\n% more tobe.txt \nto be or not to - be - - that - - - is\n% java Queue < tobe.txt \nto be or not to be (2 left on queue)\n150 CHAPTER 1 \u25a0 Fundamentals\n ALGORITHM 1.3   FIFO queue\npublic class  Queue<Item> implements Iterable<Item> \n{\n   private Node first; // link to least recently added node\n   private Node last;  // link to most recently added node\n   private int N;      // number of items on the queue\n   private  class Node\n   {  // nested class to define nodes\n      Item item;\n      Node next;\n   }\n   public boolean isEmpty() {  return first == null;  }  // Or: N == 0.\n   public int size()        {  return N;  }\n   public void enqueue(Item item)\n   {  // Add item to the end of the list.\n      Node oldlast = last;\n      last = new Node();\n      last.item = item;\n      last.next = null;\n      if (isEmpty()) first = last;\n      else           oldlast.next = last;\n      N++;\n   }\n   public Item dequeue()\n   {  // Remove item from the beginning of the list.\n      Item item = first.item;\n      first = first.next;\n      if (isEmpty()) last = null;\n      N--;\n      return item;\n   }\n   // See page 155 for iterator() implementation.\n   // See page 150 for test client main().\n}\nThis generic Queue implementation is based on a linked-list data structure. It can be used to create \nqueues containing any type of data. T o support iteration, add the highlighted code described for ", "start": 162, "end": 163}, "216": {"text": "150 for test client main().\n}\nThis generic Queue implementation is based on a linked-list data structure. It can be used to create \nqueues containing any type of data. T o support iteration, add the highlighted code described for Bag\non page 155. \n1511.3 \u25a0 Bags, Queues, and Stacks to\n        \n        \nbe\nto\n        \nor        \nbe\nto\nnull\nnull\nnull\n        \nor        \nbe\nto\nnot\n        \nor        \nbe\nto\nnull\nnot\n        \nnot        \norto be\n        \nnot        \nor\nbe\n        \nto        \nnotbe or\n        \nbe        \ntoor not\nnull\n        \nbe        \nto\nnot\n        \nthat        \nbenot to\nnull\n        \n        \nthatto be\n        \nbe that\n        \nis\nto\nStdIn StdOut\nbe\nor\nnot\nto\n-\nbe\n-\n-\nthat\n-\n-\n-\nis that\nto\nnull\nbe\nnull\nthat\nnull\nto\nnull\nto be\nnull\nnull\nnull\nnull\nTrace of Queue development client\n152 CHAPTER 1 \u25a0 Fundamentals\n  \nLinked lists are a fundamental alternative to arrays for structuring a collection \nof data. From a historical perspective, this alternative has been available to program-\nmers for many decades. Indeed, a landmark in the history of programming languages \nwas the development of LISP by John McCarthy in the 1950s, where linked lists are the \nprimary structure for programs and data. Programming with linked lists presents all \nsorts of challenges and is notoriously dif\ufb01cult to debug, as you can see in the exercises. \nIn modern code, the use of safe pointers, automatic garbage collection (see page 111), and \nADTs allows us to encapsulate list-processing code in just a few classes such as the ones \npresented here.\n1531.3 \u25a0 Bags, Queues, and Stacks\n  B a g  i m p l e m e n t a ", "start": 163, "end": 166}, "217": {"text": "list-processing code in just a few classes such as the ones \npresented here.\n1531.3 \u25a0 Bags, Queues, and Stacks\n  B a g  i m p l e m e n t a t i o n .  Implementing our Bag API using a linked-list data structure is \nsimply a matter of changing the name of push() in Stack to add() and removing the \nimplementation of pop(), as shown in Algorithm 1.4 on the facing page (doing the \nsame for Queue would also be effective but requires a bit more code). This implemen-\ntation also highlights the code needed to make Stack, Queue, and Bag all iterable, by \ntraversing the list. For Stack the list is in LIFO order; for Queue it is in FIFO order; and \nfor Bag it happens to be in LIFO order, but the order is not relevant. As detailed in the \nhighlighted code in Algorithm 1.4, to implement iteration in a collection, the \ufb01rst step \nis to include\n  import java.util.Iterator;\nso that our code can refer to Java\u2019s Iterator interface. The second step is to add \nimplements  Iterable<Item>\nto the class declaration, a promise to provide an iterator() method. The iterator()\nmethod itself simply returns an object from a class that implements the Iterator\ninterface:\npublic Iterator<Item> iterator() \n{  return new ListIterator();  }\nThis code is a promise to implement a class that implements the hasNext(), next(), \nand remove() methods that are called when a client uses the foreach construct. T o \nimplement these methods, the nested class ListIterator in Algorithm 1.4 maintains \nan instance variable current that keeps track of the current node on the list. Then the \nhasNext() method tests if current is null, and the next() method saves a reference \nto the current item, updates current to refer to the ", "start": 166, "end": 166}, "218": {"text": "variable current that keeps track of the current node on the list. Then the \nhasNext() method tests if current is null, and the next() method saves a reference \nto the current item, updates current to refer to the next node on the list, and returns \nthe saved reference. \n154 CHAPTER 1 \u25a0 Fundamentals\n ALGORITHM 1.4   Bag\nimport java.util.Iterator;\npublic class  Bag<Item> implements Iterable<Item> \n{\n   private Node first;  // first node in list\n   private  class Node\n   {\n       Item item;\n       Node next;\n   }\n   public void add(Item item)\n   {  // same as push() in Stack\n      Node oldfirst = first;\n      first = new Node();\n      first.item = item;\n      first.next = oldfirst;\n   }\n   public Iterator<Item> iterator()\n   {  return new ListIterator();  }\n   private class ListIterator implements Iterator<Item>\n   {\n       private Node current = first;\n       public boolean hasNext()\n       {  return current != null;  }\n       public void remove() { }\n       public Item next()\n       {\n           Item item = current.item;\n           current = current.next; \n           return item;\n       }\n   } \n}\nThis Bag implementation maintains a linked list of the items provided in calls to add(). Code for \nisEmpty() and size() is the same as in Stack and is omitted. The iterator traverses the list, main-\ntaining the current node in current. We can make Stack and Queue iterable by adding the code \nhighlighted in red to Algorithms 1.1 and 1.2, because they use the same underlying data structure \nand Stack and Queue maintain the list in LIFO and FIFO order, respectively.\n1551.3 \u25a0 Bags, Queues, and Stacks Overview The implementations of bags, queues, and stacks that support generics \nand iteration that we have considered in this section provide ", "start": 166, "end": 168}, "219": {"text": "in LIFO and FIFO order, respectively.\n1551.3 \u25a0 Bags, Queues, and Stacks Overview The implementations of bags, queues, and stacks that support generics \nand iteration that we have considered in this section provide a level of abstraction that \nallows us to write compact client programs that manipulate collections of objects. De-\ntailed understanding of these ADTs is important as an introduction to the study of al-\ngorithms and data structures for three reasons. First, we use these data types as building \nblocks in higher-level data structures throughout this book. Second, they illustrate the \ninterplay between data structures and algorithms and the challenge of simultaneously \nachieving natural performance goals that may con\ufb02ict. Third, the focus of several of \nour implementations is on ADTs that support more powerful operations on collections \nof objects, and we use the implementations here as starting points.\nData structures. We now have two ways to represent collections of  objects, ar rays and \nlinked lists. Arrays are built in to Java; linked lists are easy to build with standard Java \nrecords. These two alternatives, often referred to as    sequential allocation and    linked al-\nlocation, are fundamental. Later in the book, we develop ADT implementations that \ncombine and extend these basic structures \nin numerous ways. One important exten-\nsion is to data structures with multiple \nlinks. For example, our focus in Sections \n3.2 and 3.3 is on data structures known as \nbinary trees that are built from nodes that \neach have two links. Another important \nextension is to compose data structures: \nwe can have a bag of stacks, a queue of ar-\nrays, and so forth. For example, our focus \nin Chapter 4 is on graphs, which we rep-\nresent as arrays of bags. It is very easy to de\ufb01ne data structures of arbitrary complexity \nin this way: one important reason for our focus on abstract ", "start": 168, "end": 168}, "220": {"text": "\nin Chapter 4 is on graphs, which we rep-\nresent as arrays of bags. It is very easy to de\ufb01ne data structures of arbitrary complexity \nin this way: one important reason for our focus on abstract data types is an attempt to \ncontrol such complexity.\ndata structure advantage disadvantage\narray\nindex provides \nimmediate access \nto any item\nneed to know size \non initialization\nlinked list uses space \nproportional to size\nneed reference to \naccess an item\nFundamental data structures\n156 CHAPTER 1 \u25a0 Fundamentals\n Our treatment of BAGS, queues, and STACKS  in this section is a prototypical ex -\nample of the approach that we use throughout this book to describe data structures \nand algorithms. In approaching a new applications domain, we identify computational \nchallenges and use data abstraction to address them, proceeding as follows:\n\u25a0 Specify an API.\n\u25a0 Develop client code with reference to speci\ufb01c applications.\n\u25a0 \n \nDescribe a data structure (representation of the set of values) that can serve as \nthe basis for the instance variables in a class that will implement an ADT that \nmeets the speci\ufb01cation in the API.\n\u25a0 Describe algorithms (approaches to implementing the set of operations) that \ncan serve as the basis for implementing the instance methods in the class.\n\u25a0 Analyze the performance characteristics of the algorithms.\nIn the next section, we consider this last step in detail, as it often dictates which algo -\nrithms and implementations can be most useful in addressing real-world applications.\ndata structure section ADT representation\nparent-link tree 1.5 UnionFind array of integers\nbinary search tree 3.2, 3.3 BST two links per node\nstring 5.1 String  array, offset, and length\nbinary heap 2.4 PQ array of objects\nhash table\n(separate chaining) 3.4 SeparateChainingHashST ", "start": 168, "end": 169}, "221": {"text": "BST two links per node\nstring 5.1 String  array, offset, and length\nbinary heap 2.4 PQ array of objects\nhash table\n(separate chaining) 3.4 SeparateChainingHashST arrays of linked lists\nhash table\n(linear probing) 3.4 LinearProbingHashST two arrays of objects\ngraph adjacency lists 4.1, 4.2 Graph array of Bag objects\ntrie 5.2 TrieST node with array of links\nternary search trie 5.3 TST three links per node\nExamples of data structures developed in this book\n1571.3 \u25a0 Bags, Queues, and Stacks\n Q&A\nQ. Not all programming languages have generics, even early versions of Java. What are \nthe alternatives?\nA. One alternative is to maintain a different implementation for each type of data, as \nmentioned in the text. Another is to build a stack of Object values, then cast to the \ndesired type in client code for pop(). The problem with this approach is that type mis-\nmatch errors cannot be detected until run time. But with generics, if you write code to \npush an object of the wrong type on the stack, like this:\nStack<Apple> stack = new Stack<Apple>(); \nApple  a = new Apple(); \n...\nOrange b = new Orange(); \n... \nstack.push(a); \n... \nstack.push(b);     // compile-time error\nyou will get a compile-time error:\npush(Apple) in Stack<Apple> cannot be applied to (Orange)\nThis ability to discover such errors at compile time is reason enough to use generics. \nQ. Why does Java disallow   generic arrays?\nA. Experts still debate this point. Y ou might need to become one to understand it! For \nstarters, learn about       covariant arrays and type erasure.\nQ. How do I create an array of stacks ", "start": 169, "end": 170}, "222": {"text": "arrays?\nA. Experts still debate this point. Y ou might need to become one to understand it! For \nstarters, learn about       covariant arrays and type erasure.\nQ. How do I create an array of stacks of strings?\nA. Use a cast, such as the following: \nStack<String>[] a = (Stack<String>[]) new Stack[N];\nWarning : This cast, in client code, is different from the one described on page 134. Y ou \nmight have expected to use Object instead of Stack. When using generics, Java \nchecks for type safety at compile time, but throws away that information at run time, \nso it is left with Stack<Object>[] or just Stack[], for short, which we must cast to \nStack<String>[].\nQ. What happens if my program calls pop() for an empty stack?\n158 CHAPTER 1 \u25a0 Fundamentals\n  \n \nA. It depends on the implementation. For our implementation on page 149, you will get a \n  NullPointerException. In our implementations on the booksite, we throw a runtime \nexception to help users pinpoint the error. Generally, including as many such checks as \npossible is wise in code that is likely to be used by many people.\nQ. Why do we care about resizing arrays, when we have linked lists?\nA. We w ill see several examples of  ADT implementations that need to use ar -\nrays to perform other operations that are not easily supported with linked lists. \nResizingArrayStack is a model for keeping their memory usage under control.\nQ. Why declare Node as a     nested class? Why private?\nA. By declaring the nested class Node to be private, we restrict access to methods and \ninstance variables within the enclosing class. One characteristic of a private nested \nclass is that its instance variables can be directly accessed from within the enclosing \nclass but nowhere else, so there is no need to declare the instance variables public or \nprivate. Note ", "start": 170, "end": 171}, "223": {"text": "enclosing class. One characteristic of a private nested \nclass is that its instance variables can be directly accessed from within the enclosing \nclass but nowhere else, so there is no need to declare the instance variables public or \nprivate. Note for experts : A nested class that is not static is known as an inner class, so \ntechnically our Node classes are inner classes, though the ones that are not generic could \nbe static.\nQ.  When I type javac Stack.java to run Algorithm 1.2 and similar programs, I \n\ufb01nd Stack.class and a \ufb01le Stack$Node.class. What is the purpose of that second \none?\nA. That \ufb01le is for the inner  class Node. Java\u2019s naming convention is to use $ to separate \nthe name of the outer class from the inner class.\nQ. Are there Java libraries for stacks and queues?\nA. Ye s  a n d  n o. Jav a  h a s  a  b u i l t - i n  l i b r a r y  c a l l e d   java.util.Stack, but you should \navoid using it when you want a stack. It has several additional operations that are not \nnormally associated with a stack, e.g., getting the ith element. It also allows adding an \nelement to the bottom of the stack (instead of the top), so it can implement a queue! \nAlthough having such extra operations may appear to be a bonus, it is actually a curse. \nWe use data t y pes not just as librar ies of  all the operations we can imag ine, but also as \na mechanism to precisely specify the operations we need. The prime bene\ufb01t of doing so \nis that the system can prevent us from performing operations that we do not actually \n1591.3 \u25a0 Bags, Queues, and Stacks\n  \n \nwant. The java.util.Stack API ", "start": 171, "end": 172}, "224": {"text": "bene\ufb01t of doing so \nis that the system can prevent us from performing operations that we do not actually \n1591.3 \u25a0 Bags, Queues, and Stacks\n  \n \nwant. The java.util.Stack API is an example of a   wide interface, which we generally \nstrive to avoid.\nQ. Should a client be allowed to insert null items onto a stack or queue?\nA. This question arises frequently when implementing collections in Java. Our imple-\nmentation (and Java\u2019s stack and queue libraries) do permit the insertion of null values.\nQ. What should the Stack iterator do if the client calls push() or pop() during iterator?\nA. Throw a   java.util.ConcurrentModificationException to make it a  fail-fast it-\nerator. See 1.3.50.\nQ. Can I use a  foreach loop with arrays?\nA. Ye s  ( e ve n  t h o u g h  a r r ay s  d o  n o t  i m p l e m e n t  t h e  Iterable interface). The following \none-liner prints out the command-line arguments:\npublic static void main(String[] args) \n{  for (String s : args) StdOut.println(s);  }\nQ. Can I use a  foreach loop with strings?\nA. No. String does not implement  Iterable.\nQ. Why not have a single Collection data type that implements methods to add items, \nremove the most recently inserted, remove the least recently inserted, remove random, \niterate, return the number of items in the collection, and whatever other operations we \nmight desire? Then we could get them all implemented in a single class that could be \nused by many clients.\nA. Again, this is an example of a  wide interface. Java has such implementations in its \njava.util.ArrayList and  java.util.LinkedList classes. One reason to avoid them \nis that it ", "start": 172, "end": 172}, "225": {"text": "\nused by many clients.\nA. Again, this is an example of a  wide interface. Java has such implementations in its \njava.util.ArrayList and  java.util.LinkedList classes. One reason to avoid them \nis that it there is no assurance that all operations are implemented ef\ufb01ciently. Through-\nout this book, we use APIs as starting points for designing ef\ufb01cient algorithms and data \nstructures, which is certainly easier to do for interfaces with just a few operations as \nopposed to an interface with many operations. Another reason to insist on narrow in-\nterfaces is that they enforce a certain discipline on client programs, which makes client \ncode much easier to understand. If one client uses Stack<String> and another uses \nQueue<Transaction>, we have a good idea that the LIFO discipline is important to the \n\ufb01rst and the FIFO discipline is important to the second.\nQ & A (continued)\n160 CHAPTER 1 \u25a0 Fundamentals\n EXERCISES\n1.3.1 Add a method isFull() to FixedCapacityStackOfStrings.\n1.3.2 Give the output printed by java Stack for the input\nit was - the best - of times - - - it  was - the - -\n1.3.3 Suppose that a client performs an intermixed sequence of (stack) push and pop\noperations. The push operations put the integers 0 through 9 in order onto the stack; \nthe pop operations print out the return values. Which of the following sequence(s) \ncould not occur?\na. 4 3 2 1 0 9 8 7 6 5\nb. 4 6 8 7 5 3 2 9 0 1 \nc. 2 5 6 7 4 8 9 3 1 0\nd. 4 3 2 1 0 5 ", "start": 172, "end": 173}, "226": {"text": "2 9 0 1 \nc. 2 5 6 7 4 8 9 3 1 0\nd. 4 3 2 1 0 5 6 7 8 9\ne. 1 2 3 4 5 6 9 8 7 0\nf. 0 4 6 5 3 8 1 7 2 9\ng. 1 4 7 9 8 6 5 3 0 2\nh. 2 1 4 3 6 5 8 7 9 0\n1.3.4 Write a stack client Parentheses that reads in a text stream from standard input \nand uses a stack to determine whether its parentheses are properly balanced. For ex-\nample, your program should print true for [()]{}{[()()]()} and false for [(]). \n1.3.5 What does the following code fragment print when N is 50? Give a high-level \ndescription of what it does when presented with a positive integer N.\nStack<Integer> stack = new Stack<Integer>(); \nwhile (N > 0) \n{\n   stack.push(N % 2);\n   N = N / 2; \n} \nfor (int d : stack) StdOut.print(d); \nStdOut.println();\nAnswer : Prints the binary representation of N (110010 when N is 50).\n1611.3 \u25a0 Bags, Queues, and Stacks\n   \n \n1.3.6 What does the following code fragment do to the queue q?\nStack<String> stack = new Stack<String>(); \nwhile (!q.isEmpty())\n   stack.push(q.dequeue()); \nwhile (!stack.isEmpty())\n   q.enqueue(stack.pop());\n1.3.7 Add a method peek() to Stack that returns the most recently inserted item on \nthe ", "start": 173, "end": 174}, "227": {"text": "Stack<String>(); \nwhile (!q.isEmpty())\n   stack.push(q.dequeue()); \nwhile (!stack.isEmpty())\n   q.enqueue(stack.pop());\n1.3.7 Add a method peek() to Stack that returns the most recently inserted item on \nthe stack (without popping it).\n1.3.8 Give the contents and size of the array for DoublingStackOfStrings with the \ninput\nit was - the best - of times - - - it was - the - -\n1.3.9 Write a program that takes from standard input an expression without left pa -\nrentheses and prints the equivalent in\ufb01x expression with the parentheses inserted. For \nexample, given the input:\n1 + 2 ) * 3 - 4 ) * 5 - 6 ) ) ) \nyour program should print\n( ( 1 + 2 ) * ( ( 3 - 4 ) * ( 5 - 6 ) )\n1.3.10 Write a \ufb01lter InfixToPostfix that converts an arithmetic expression from in-\n\ufb01x to post\ufb01x.\n1.3.11 Write a program EvaluatePostfix that takes a post\ufb01x expression from stan -\ndard input, evaluates it, and prints the value. (Piping the output of your program from \nthe previous exercise to this program gives equivalent behavior to Evaluate.\n1.3.12 Write an iterable Stack client that has a static method copy() that takes a stack \nof strings as argument and returns a copy of the stack. Note : This ability is a prime \nexample of the value of having an iterator, because it allows development of such func-\ntionality without changing the basic API.\n1.3.13 Suppose that a client performs an intermixed sequence of (queue) enqueue and \ndequeue operations. The enqueue operations put the integers 0 through 9 in order onto \nEXERCISES  (continued)\n162 CHAPTER 1 ", "start": 174, "end": 174}, "228": {"text": "Suppose that a client performs an intermixed sequence of (queue) enqueue and \ndequeue operations. The enqueue operations put the integers 0 through 9 in order onto \nEXERCISES  (continued)\n162 CHAPTER 1 \u25a0 Fundamentals\n  \nthe queue; the dequeue operations print out the return value. Which of the following \nsequence(s) could not occur?\na. 0 1 2 3 4 5 6 7 8 9\nb. 4 6 8 7 5 3 2 9 0 1\nc. 2 5 6 7 4 8 9 3 1 0\nd. 4 3 2 1 0 5 6 7 8 9\n1.3.14  Develop a class ResizingArrayQueueOfStrings that implements the queue \nabstraction with a \ufb01xed-size array, and then extend your implementation to use array \nresizing to remove the size restriction.\n1.3.15 Write a Queue client that takes a command-line argument k and prints the kth \nfrom the last string found on standard input (assuming that standard input has k or \nmore strings).\n1.3.16  Using readInts() on page 126 as a model, write a static method readDates() for \nDate that reads dates from standard input in the format speci\ufb01ed in the table on page 119 \nand returns an array containing them. \n1.3.17  Do Exercise 1.3.16 for Transaction.\n1631.3 \u25a0 Bags, Queues, and Stacks\n LINKED-LIST EXERCISES\n \n \nThis list of exercises is intended to give you experience in working with linked lists. Sugges-\ntion: make drawings using the visual representation described in the text.\n1.3.18 Suppose x is a linked-list ", "start": 174, "end": 176}, "229": {"text": "EXERCISES\n \n \nThis list of exercises is intended to give you experience in working with linked lists. Sugges-\ntion: make drawings using the visual representation described in the text.\n1.3.18 Suppose x is a linked-list node and not the last node on the list. What is the ef-\nfect of the following code fragment?\nx.next = x.next.next;\nAnswer : Deletes from the list the node immediately following x.\n1.3.19  Give a code fragment that removes the last node in a linked list whose \ufb01rst node \nis first.\n1.3.20 Write a method delete() that takes an int argument k and deletes the kth ele-\nment in a linked list, if it exists.\n1.3.21 Write a method find() that takes a linked list and a string key as arguments \nand returns true if some node in the list has key as its item \ufb01eld, false otherwise.\n1.3.22 Suppose that x is a linked list Node. What does the following code fragment do?\nt.next = x.next;\nx.next = t;     \nAnswer : Inserts node t immediately after node x.\n1.3.23 Why does the following code fragment not do the same thing as in the previous \nquestion?\nx.next = t;\nt.next = x.next;\nAnswer : When it comes time to update t.next, x.next is no longer the original node \nfollowing x, but is instead t itself!\n1.3.24 Write a method removeAfter() that takes a linked-list Node as argument and \nremoves the node following the given one (and does nothing if the argument or the next \n\ufb01eld in the argument node is null).\n1.3.25 Write a method insertAfter() that takes two linked-list Node arguments and \ninserts the second after the \ufb01rst on its list (and does nothing if either argument is null).\n164 CHAPTER 1 \u25a0 ", "start": 176, "end": 176}, "230": {"text": "null).\n1.3.25 Write a method insertAfter() that takes two linked-list Node arguments and \ninserts the second after the \ufb01rst on its list (and does nothing if either argument is null).\n164 CHAPTER 1 \u25a0 Fundamentals\n    \n1.3.26 Write a method remove() that takes a linked list and a string key as arguments \nand removes all of the nodes in the list that have key as its item \ufb01eld.\n1.3.27 Write a method max() that takes a reference to the \ufb01rst node in a linked list as \nargument and returns the value of the maximum key in the list. Assume that all keys are \npositive integers, and return 0 if the list is empty. \n1.3.28 Develop a recursive solution to the previous question.\n1.3.29 Write a Queue implementation that uses a  circular linked list, which is the same \nas a linked list except that no links are null and the value of last.next is first when-\never the list is not empty. Keep only one Node instance variable (last).\n1.3.30 Write a function that takes the \ufb01rst Node in a linked list as argument and (de-\nstructively)   reverses the list, returning the \ufb01rst Node in the result.\nIterative solution : To accomplish this task, we maintain references to three consecutive \nnodes in the linked list, reverse, first, and second. At each iteration, we extract the \nnode first from the original linked list and insert it at the beginning of the reversed \nlist. We maintain the invariant that first is the \ufb01rst node of what\u2019s left of the original \nlist, second is the second node of what\u2019s left of the original list, and reverse is the \ufb01rst \nnode of the resulting reversed list.\npublic Node reverse(Node x) \n{\n   Node first   = x;\n   Node reverse = null;\n   while (first ", "start": 176, "end": 177}, "231": {"text": "what\u2019s left of the original list, and reverse is the \ufb01rst \nnode of the resulting reversed list.\npublic Node reverse(Node x) \n{\n   Node first   = x;\n   Node reverse = null;\n   while (first != null)\n   {\n      Node second = first.next;\n      first.next  = reverse;\n      reverse     = first;\n      first       = second;\n   }\n   return reverse; \n}\nWhen writing code involving linked lists, we must always be careful to properly handle \nthe exceptional cases (when the linked list is empty, when the list has only one or two \n1651.3 \u25a0 Bags, Queues, and Stacks\n  \nnodes) and the boundary cases (dealing with the \ufb01rst or last items). This is usually \nmuch trickier than handling the normal cases.\nRecursive solution : Assuming the linked list has N nodes, we recursively reverse the last \nN \u2013 1 nodes, and then carefully append the \ufb01rst node to the end.\npublic Node reverse(Node first) \n{\n   if (first == null) return null;\n   if (first.next == null) return first;\n   Node second = first.next;\n   Node rest = reverse(second);\n   second.next = first;\n   first.next  = null;\n   return rest; \n}\n1.3.31  Implement a nested class DoubleNode for building doubly-linked lists, where \neach node contains a reference to the item preceding it and the item following it in the \nlist (null if there is no such item). Then implement static methods for the following \ntasks: insert at the beginning, insert at the end, remove from the beginning, remove \nfrom the end, insert before a given node, insert after a given node, and remove a given \nnode.\nLINKED-LIST EXERCISES  (continued)\n166 CHAPTER 1 \u25a0 Fundamentals\n CREATIVE PROBLEMS\n1.3.32      Steque. ", "start": 177, "end": 179}, "232": {"text": "a given node, and remove a given \nnode.\nLINKED-LIST EXERCISES  (continued)\n166 CHAPTER 1 \u25a0 Fundamentals\n CREATIVE PROBLEMS\n1.3.32      Steque. A stack-ended queue or steque is a data type that supports push, pop, and \nenqueue. Articulate an API for this ADT. Develop a linked-list-based implementation.\n1.3.33      Deque. A double-ended queue or deque (pronounced \u201cdeck\u201d) is like a stack or \na queue but supports adding and removing items at both ends. A deque stores a collec-\ntion of items and supports the following API:\npublic class    Deque<Item> implements Iterable<Item> \nDeque() create an empty deque\nboolean isEmpty() is the deque empty?\nint size() number of items in the deque\nvoid pushLeft(Item item) add an item to the left end\nvoid pushRight(Item item) add an item to the right end\nItem popLeft() remove an item from the left end\nItem popRight() remove an item from the right end\nAPI for a generic double-ended queue\nWrite a class  Deque that uses a doubly-linked list to implement this API and a class \nResizingArrayDeque that uses a resizing array.\n1.3.34   Random bag. A random bag stores a collection of items and supports the fol -\nlowing API:\npublic class    RandomBag<Item> implements Iterable<Item> \nRandomBag() create an empty random bag\nboolean isEmpty() is the bag empty?\nint size() number of items in the bag\nvoid add(Item item) add an item\nAPI for a generic random bag\nWrite a class RandomBag that implements this API. Note that this API is the same as for \nBag, except for the adjective random, which indicates that the iteration should provide \n1671.3 \u25a0 Bags, Queues, and Stacks\n the items ", "start": 179, "end": 180}, "233": {"text": "implements this API. Note that this API is the same as for \nBag, except for the adjective random, which indicates that the iteration should provide \n1671.3 \u25a0 Bags, Queues, and Stacks\n the items in random order (all N !  permutations equally likely, for each iterator). Hint : \nPut the items in an array and randomize their order in the iterator\u2019s constructor.\n1.3.35  Random queue. A random queue stores a collection of items and supports the \nfollowing API:\npublic class    RandomQueue<Item> \nRandomQueue() create an empty random queue\nboolean isEmpty() is the queue empty?\nvoid enqueue(Item item) add an item\nItem dequeue() remove and return a random item\n(sample without replacement)\nItem sample() return a random item, but do not remove\n(sample with replacement)\nAPI for a generic random queue\nWrite a class RandomQueue that implements this API. Hint : Use an array representation \n(with resizing). T o remove an item, swap one at a random position (indexed 0 through \nN-1) with the one at the last position (index N-1). Then delete and return the last ob-\nject, as in ResizingArrayStack. Write a client that deals bridge hands (13 cards each) \nusing RandomQueue<Card>.\n1.3.36  Random iterator. Write an iterator for RandomQueue<Item> from the previous \nexercise that returns the items in random order. \n1.3.37   Josephus problem. In the Josephus problem from antiquity, N people are in dire \nstraits and agree to the following strategy to reduce the population. They arrange them-\nselves in a circle (at positions numbered from 0 to N\u20131) and proceed around the circle, \neliminating every Mth person until only one person is left. Legend has it that Josephus \n\ufb01gured out where to sit to avoid being eliminated. Write ", "start": 180, "end": 180}, "234": {"text": "to N\u20131) and proceed around the circle, \neliminating every Mth person until only one person is left. Legend has it that Josephus \n\ufb01gured out where to sit to avoid being eliminated. Write a Queue client Josephus that \ntakes N and M from the command line and prints out the order in which people are \neliminated (and thus would show Josephus where to sit in the circle).\n% java Josephus 7 2 \n1 3 5 0 4 2 6\nCREATIVE PROBLEMS  (continued)\n168 CHAPTER 1 \u25a0 Fundamentals\n 1.3.38  Delete kth element. Implement a class that supports the following API:\npublic class    GeneralizedQueue<Item>\nGeneralizedQueue() create an empty queue\nboolean isEmpty() is the queue empty?\nvoid insert(Item x) add an item\nItem delete(int k) delete and return the kth least recently inserted item\nAPI for a generic generalized queue\n \nFirst, develop an implementation that uses an array implementation, and then develop \none that uses a linked-list implementation. Note : the algorithms and data structures \nthat we introduce in Chapter 3 make it possible to develop an implementation that \ncan guarantee that both insert() and delete() take time prortional to the logarithm \nof the number of items in the queue\u2014see Exercise 3.5.27.\n1.3.39    Ring buffer. A ring buffer, or  circular queue, is a FIFO data structure of a \ufb01xed \nsize N. It is useful for transferring data between asynchronous processes or for storing \nlog \ufb01les. When the buffer is empty, the consumer waits until data is deposited; when the \nbuffer is full, the producer waits to deposit data. Develop an API for a RingBuffer and \nan implementation that uses an array representation (with circular wrap-around).\n1.3.40    Move-to-front. ", "start": 180, "end": 181}, "235": {"text": "when the \nbuffer is full, the producer waits to deposit data. Develop an API for a RingBuffer and \nan implementation that uses an array representation (with circular wrap-around).\n1.3.40    Move-to-front. Read in a sequence of characters from standard input and \nmaintain the characters in a linked list with no duplicates. When you read in a previ-\nously unseen character, insert it at the front of the list. When you read in a duplicate \ncharacter, delete it from the list and reinsert it at the beginning. Name your program \nMoveToFront: it implements the well-known move-to-front strategy, which is useful for \ncaching, data compression, and many other applications where items that have been \nrecently accessed are more likely to be reaccessed.\n1.3.41  Copy a queue. Create a new constructor so that\nQueue<Item> r = new Queue<Item>(q);\nmakes r a reference to a new and independent copy of the queue q. Yo u  s h o u l d  b e  a b l e  \nto push and pop from either q or r without in\ufb02uencing the other. Hint : Delete all of the \nelements from q and add these elements to both q and r.\n1691.3 \u25a0 Bags, Queues, and Stacks\n  \n1.3.42  Copy a stack. Create a new constructor for the linked-list implementation of \nStack so that\nStack<Item> t = new Stack<Item>(s);\nmakes t a reference to a new and independent copy of the stack s. \n1.3.43  Listing \ufb01les. A folder is a list of \ufb01les and folders. Write a program that takes the \nname of a folder as a command-line argument and prints out all of the \ufb01les contained \nin that folder, with the contents of each folder recursively listed (indented) under that \nfolder\u2019s ", "start": 181, "end": 182}, "236": {"text": "program that takes the \nname of a folder as a command-line argument and prints out all of the \ufb01les contained \nin that folder, with the contents of each folder recursively listed (indented) under that \nfolder\u2019s name. Hint : Use a queue, and see java.io.File.\n1.3.44  Text editor buffer. Develop a data type for a buffer in a text editor that imple-\nments the following API:\npublic class    Buffer\nBuffer() create an empty buffer\nvoid insert(char c) insert c at the cursor position\nchar delete() delete and return the character at the cursor\nvoid left(int k) move the cursor k positions to the left\nvoid right(int k) move the cursor k positions to the right\nint size() number of characters in the buffer\nAPI for a text buffer\nHint : Use two stacks.\n1.3.45  Stack generability. Suppose that we have a sequence of intermixed push and \npop operations as with our test stack client, where the integers 0, 1, ..., N-1 in that \norder (push directives) are intermixed with N minus signs ( pop directives). Devise an \nalgorithm that determines whether the intermixed sequence causes the stack to under-\n\ufb02ow. (Y ou may use only an amount of space independent of N\u2014you cannot store the \nintegers in a data structure.) Devise a linear-time algorithm that determines whether a \ngiven permutation can be generated as output by our test client (depending on where \nthe pop directives occur).\nCREATIVE PROBLEMS  (continued)\n170 CHAPTER 1 \u25a0 Fundamentals\n  Solution: The stack does not over\ufb02ow unless there exists an integer k such that the \ufb01rst \nk pop operations occur before the \ufb01rst k push operations. If a given permutation can be \ngenerated, it is uniquely generated as follows: if the next integer in the output permuta-\ntion ", "start": 182, "end": 183}, "237": {"text": "k such that the \ufb01rst \nk pop operations occur before the \ufb01rst k push operations. If a given permutation can be \ngenerated, it is uniquely generated as follows: if the next integer in the output permuta-\ntion is in the top of the stack, pop it; otherwise, push it onto the stack.\n1.3.46  Forbidden triple for stack generability. Prove that a permutation can be gener -\nated by a stack (as in the previous question) if and only if it has no forbidden triple (a, b, \nc) such that a < b < c with c \ufb01rst, a second, and b third (possibly with other intervening \nintegers between c and a and between a and b).\nPartial solution: Suppose that there is a forbidden triple (a, b, c). Item c is popped before \na and b, but a and b are pushed before c. Thus, when c is pushed, both a and b are on \nthe stack. Therefore, a cannot be popped before b.\n1.3.47      Catenable queues, stacks, or steques. Add an extra operation catenation that (de-\nstructively) concatenates two queues, stacks, or steques (see Exercise 1.3.32). Hint : Use \na circular linked list, maintaining a pointer to the last item.\n1.3.48  Two stacks w ith a deque.  Implement two stacks with a single deque so that each \noperation takes a constant number of deque operations (see Exercise 1.3.33).\n1.3.49  Queue with three stacks. Implement a queue with three stacks so that each \nqueue operation takes a constant (worst-case) number of stack operations. Warning : \nhigh degree of dif\ufb01culty.\n1.3.50       Fail-fast iterator. Modify the iterator code in Stack to immediately throw a \n  java.util.ConcurrentModificationException ", "start": 183, "end": 183}, "238": {"text": "(worst-case) number of stack operations. Warning : \nhigh degree of dif\ufb01culty.\n1.3.50       Fail-fast iterator. Modify the iterator code in Stack to immediately throw a \n  java.util.ConcurrentModificationException if the client modi\ufb01es the collection \n(via push() or pop()) during iteration? b).\nSolution: Maintain a counter that counts the number of push() and pop() operations. \nWhen creating an iterator, store this value as an Iterator instance variable. Before \neach call to hasNext() and next(), check that this value has not changed since con -\nstruction of the iterator; if it has, throw the exception.\n1711.3 \u25a0 Bags, Queues, and Stacks\n 1.4  ANALYSIS OF ALGORITHMS\nAS people gain experience using computers, they use them to solve dif\ufb01cult prob-\nlems or to process large amounts of data and are invariably led to questions like these:\nHow long will my program take?\nWhy does my program run out of memory?\nYo u  ce r t a i n l y  h ave  a s ke d  yo u r s e l f  t h e s e  q u e s t i o n s , p e r h a p s  w h e n  re b u i l d i n g  a  m u s i c  o r  \nphoto library, installing a new application, working with a large document, or work-\ning with a large amount of experimental data. The questions are much too vague to \nbe answered precisely\u2014the answers depend on many factors such as properties of the \nparticular computer being used, the particular data being processed, and the particular \nprogram that is doing the job (which implements some algorithm). All of these factors \nleave us with a daunting amount of information to analyze. \nDespite these challenges, the path to developing useful answers to these basic ques-\ntions ", "start": 183, "end": 184}, "239": {"text": "the particular \nprogram that is doing the job (which implements some algorithm). All of these factors \nleave us with a daunting amount of information to analyze. \nDespite these challenges, the path to developing useful answers to these basic ques-\ntions is often remarkably straightforward, as you will see in this section. This process is \nbased on the scienti\ufb01c method, the commonly accepted body of techniques used by sci-\nentists to develop knowledge about the natural world. We apply mathematical analysis\nto develop concise models of costs and do experimental studies to validate these models.\n  S c i e n t i \ufb01 c  m e t h o d  The very same approach that scientists use to understand the \nnatural world is effective for studying the  running time of programs:\n\u25a0 Observe some feature of the natural world, generally with precise measurements.\n\u25a0 Hypothesize a model that is consistent with the observations.\n\u25a0 Predict events using the hypothesis.\n\u25a0 Ver ify the predictions by making further observations.\n\u25a0 Validate by repeating until the hypothesis and observations agree.\nOne of the key tenets of the scienti\ufb01c method is that the experiments we design must \nbe reproducible, so that others can convince themselves of the validity of the hypothesis. \nHypotheses must also be falsi\ufb01able, so that we can know for sure when a given hypoth-\nesis is wrong (and thus needs revision). As Einstein famously is reported to have said \n(\u201cNo amount of experimentation can ever prove me right; a single experiment can prove \nme wrong\u201d), we can never know for sure that any hypothesis is absolutely correct; we \ncan only validate that it is consistent with our observations. \n172\n  \n O b s e r v a t i o n s  Our \ufb01rst challenge is to determine how to make quantitative mea-\nsurements of the running time of our programs. This task is far easier than in the natu-\nral sciences. ", "start": 184, "end": 185}, "240": {"text": "v a t i o n s  Our \ufb01rst challenge is to determine how to make quantitative mea-\nsurements of the running time of our programs. This task is far easier than in the natu-\nral sciences. We do not have to send a rocket to Mars or kill laboratory animals or split \nan atom\u2014we can simply run the program. Indeed, every time you run a program, you \nare performing a scienti\ufb01c experiment that relates the program to the natural world \nand answers one of our core questions: How long will my program take?\nOur \ufb01rst qualitative observation about most programs is that there is a    problem size\nthat characterizes the dif\ufb01culty of the computational task. Normally, the problem size \nis either the size of the input or the value of a command-line argument. Intuitively, the \nrunning time should increase with problem size, but the question of by how much  it \nincreases naturally comes up every time we develop and run a program. \nAnother qualitative observation for many programs is that the running time is rela-\ntively insensitive to the input itself; it depends primarily on the problem size. If this \nrelationship does not hold, we need to take steps to better understand and perhaps \nbetter control the running time\u2019s sensitivity to the input. But it does often hold, so we \nnow focus on the goal of better quantifying the relationship between problem size and \nrunning time. \n E x a m p l e .  As a running example, we will work with the program ThreeSum shown \nhere, which counts the number of triples in a \ufb01le of N integers that sum to 0 (assum-\ning that over\ufb02ow plays no role). This \ncomputation may seem contrived to you, \nbut it is deeply related to numerous fun-\ndamental computational tasks (for exam-\nple, see Exercise 1.4.26). ", "start": 185, "end": 185}, "241": {"text": "over\ufb02ow plays no role). This \ncomputation may seem contrived to you, \nbut it is deeply related to numerous fun-\ndamental computational tasks (for exam-\nple, see Exercise 1.4.26). As a test input, \nconsider the \ufb01le 1Mints.txt from the \nbooksite, which contains 1 million ran-\ndomly generated int values. The second, \neighth, and tenth entries in 1Mints.txt \nsum to 0. How many more such triples \nare there in the \ufb01le? ThreeSum can tell us, \nbut can it do so in a reasonable amount \nof time? What is the relationship between \nthe problem size N and running time \nfor ThreeSum? As a \ufb01rst experiment, try \nrunning ThreeSum on your computer \nfor the \ufb01les 1Kints.txt, 2Kints.txt, \n4Kints.txt, and 8Kints.txt on the \npublic class  ThreeSum \n{\n   public static int count(int[] a) \n   {  // Count triples that sum to 0.\n      int N = a.length;\n      int cnt = 0;\n      for (int i = 0; i < N; i++)\n         for (int j = i+1; j < N; j++)\n            for (int k = j+1; k < N; k++)\n               if (a[i] + a[j] + a[k] == 0)\n                  cnt++;\n      return cnt;\n   }\n   public static void main(String[] args) \n   {\n      int[] a = In.readInts(args[0]);\n      StdOut.println(count(a));\n   } \n}\nGiven N, how long will this program take?\n1731.4 \u25a0 Analysis of Algorithms\n booksite that contain the \ufb01rst 1,000, 2,000, 4,000, ", "start": 185, "end": 186}, "242": {"text": "StdOut.println(count(a));\n   } \n}\nGiven N, how long will this program take?\n1731.4 \u25a0 Analysis of Algorithms\n booksite that contain the \ufb01rst 1,000, 2,000, 4,000, and 8,000 integers \nfrom 1Mints.txt, respectively. You can quickly determine that there are \n70 triples that sum to 0 in 1Kints.txt and that there are 528 triples that \nsum to 0 in 2Kints.txt. The program takes substantially more time \nto determine that there are 4,039 triples that sum to 0 in 4Kints.txt, \nand as you wait for the program to \ufb01nish for 8Kints.txt, you will \ufb01nd \nyourself asking the question How long will my program take ? As you will \nsee, answering this question for this program turns out to be easy. In-\ndeed, you can often come up with \na fairly accurate prediction while \nthe program is running.\n   S t o p w a t c h .  Reliably measuring \nthe exact  running time of a given \nprogram can be dif\ufb01cult. Fortu-\nnately, we are usually happy with \nestimates. We want to be able to \ndistinguish programs that will \n\ufb01nish in a few seconds or a few \nminutes from those that might \nrequire a few days or a few months or more, and \nwe want to know when one program is twice as \nfast as another for the same task. Still, we need \naccurate measurements to generate experimental \ndata that we can use to formulate and to check \nthe validity of hypotheses about the relationship \nbetween running time and problem size. For this \npurpose, we use the Stopwatch data type shown \non the facing page. Its elapsedTime() method \nreturns the elapsed time since it was created, in \nseconds. ", "start": 186, "end": 186}, "243": {"text": "hypotheses about the relationship \nbetween running time and problem size. For this \npurpose, we use the Stopwatch data type shown \non the facing page. Its elapsedTime() method \nreturns the elapsed time since it was created, in \nseconds. The implementation is based on using \nthe Java system\u2019s currentTimeMillis() method, \nwhich gives the current time in milliseconds, to \nsave the time when the constructor is invoked, \nthen uses it again to compute the elapsed time \nwhen elapsedTime() is invoked. \n% java ThreeSum 1000 1Kints.txt\n70\n% java ThreeSum 2000 2Kints.txt\n% java ThreeSum 4000 4Kints.txt\n528\n4039\ntick tick tick\nObserving the running time of a program\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick ", "start": 186, "end": 186}, "244": {"text": "tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\ntick tick tick tick tick tick tick tick\n% more 1Mints.txt\n 324110 \n-442472\n 626686 \n-157678\n 508681\n 123414\n -77867\n155091\n 129801\n287381\n 604242\n 686904 \n-247109\n  77867\n 982455 \n-210707 \n-922943 \n-738817\n  85168\n 855430\n ...\n174 CHAPTER 1 \u25a0 Fundamentals\n public static void main(String[] args) \n{\n   int N = Integer.parseInt(args[0]);\n   int[] a = new int[N];\n   for (int i = 0; i < N; i++)\n      a[i] = StdRandom.uniform(-1000000, 1000000);\n   Stopwatch timer = new Stopwatch();\n   int cnt = ThreeSum.count(a);\n   double time = timer.elapsedTime();\n   StdOut.println(cnt + \" triples \" + time); \n}\npublic class  Stopwatch \n{\n   private final long start;\n   public Stopwatch()\n   {  start = System.currentTimeMillis();  }\n   public double elapsedTime()\n   {\n      long now = System.currentTimeMillis();\n      return (now - start) / 1000.0;\n   }\n}\n% java Stopwatch 1000 \n51 triples 0.488 seconds\n% java Stopwatch 2000 \n516 triples 3.855 seconds\nAn abstract data type for a stopwatch\ntypical client\napplication\nimplementation\nAPI public class  Stopwatch\nStopwatch() create a stopwatch\ndouble elapsedTime() return elapsed time since creation\n1751.4 \u25a0 Analysis of Algorithms\n  \n \n \n \n \nAnalysis of experimental data. The program    DoublingTest on the ", "start": 186, "end": 188}, "245": {"text": "client\napplication\nimplementation\nAPI public class  Stopwatch\nStopwatch() create a stopwatch\ndouble elapsedTime() return elapsed time since creation\n1751.4 \u25a0 Analysis of Algorithms\n  \n \n \n \n \nAnalysis of experimental data. The program    DoublingTest on the facing page is a \nmore sophisticated Stopwatch client that produces experimental data for ThreeSum. It \ngenerates a sequence of random input arrays, doubling the array size at each step, and \nprints the running times of ThreeSum.count() for each input size. These experiments \nare certainly reproducible\u2014you can also run them on your own computer, as many \ntimes as you like. When you run DoublingTest, you will \ufb01nd yourself in a prediction-\nveri\ufb01cation cycle: it prints several lines very quickly, but then slows down considerably. \nEach time it prints a line, you \ufb01nd yourself wondering how long it will be until it prints \nthe next line. Of course, since you have a different computer from ours, the actual run-\nning times that you get are likely to be different from those shown for our computer. \nIndeed, if your computer is twice as fast as ours, your running times will be about half \nours, which leads immediately to the well-founded hypothesis that running times on \ndifferent computers are likely to differ by a constant factor. Still, you will \ufb01nd yourself \nasking the more detailed question How long will my program take, as a function of the \ninput size? To  h e l p  a n s w e r  t h i s  q u e s t i o n , w e  p l o t  t h e  d a t a . T h e  d i a g r a m s  a t  t h e  b o t to m  o f  \nthe facing page show the result of plotting the data, both on a normal and on a log-log \nscale, ", "start": 188, "end": 188}, "246": {"text": "i a g r a m s  a t  t h e  b o t to m  o f  \nthe facing page show the result of plotting the data, both on a normal and on a log-log \nscale, with the problem size N on the x-axis and the running time T(N ) on the y-axis. \nThe log-log plot immediately leads to a hypothesis about the running time\u2014the data \n\ufb01ts a straight line of slope 3 on the log-log plot. The equation of such a line is\nlg(T(N )) = 3 lg N + lg a\n(where a is a constant) which is equivalent to\nT(N ) = a N 3\nthe running time, as a function of the input size, as desired. We can use one of our data \npoints to solve for a\u2014for example, T(8000) = 51.1 = a 8000 3, so a = 9.98/H1100310 \u201311\u2014and \nthen use the equation\nT(N ) = 9.98/H1100310 \u201311 N 3\nto predict running times for large N. Informally, we are checking the hypothesis \nthat the data points on the      log-log plot fall close to this line. Statistical methods are \navailable for doing a more careful analysis to \ufb01nd estimates of a and the exponent \nb, but our quick calculations suf\ufb01ce to estimate running time for most purposes. For \nexample, we can estimate the running time on our computer for N = 16,000 to be \nabout 9.98/H1100310 \u201311 16000 3 = 408.8 seconds, or about 6.8 minutes (the actual time was   \n409.3 seconds). While waiting for your computer to print the line for N = 16,000 in \nDoublingTest, you might use this method to predict when it will \ufb01nish, ", "start": 188, "end": 188}, "247": {"text": "6.8 minutes (the actual time was   \n409.3 seconds). While waiting for your computer to print the line for N = 16,000 in \nDoublingTest, you might use this method to predict when it will \ufb01nish, then check the \nresult by waiting to see if your prediction is true.\n176 CHAPTER 1 \u25a0 Fundamentals\n 1K\n.1\n.2\n.4\n.8\n1.6\n3.2\n6.4\n12.8\n25.6\n51.2\nAnalysis of experimental data (the running time of ThreeSum.count())\nlog-log plotstandard plot\nlgNproblem size N\n2K 4K 8K\nlg (T(N))\nrunning time T(N)\n1K\n10\n20\n30\n40\n50\n2K 4K 8K\nstraight line\nof slope 3\npublic class  DoublingTest \n{\n   public static double timeTrial(int N)\n   {  // Time ThreeSum.count() for N random 6-digit ints.\n      int MAX = 1000000;\n      int[] a = new int[N];\n      for (int i = 0; i < N; i++)\n         a[i] = StdRandom.uniform(-MAX, MAX);\n      Stopwatch timer = new Stopwatch();\n      int cnt = ThreeSum.count(a);\n      return timer.elapsedTime();\n   }\n   public static void main(String[] args)\n   {  // Print table of running times.\n      for (int N = 250; true; N += N)\n      {  // Print time for problem size N.\n         double time = timeTrial(N);\n         StdOut.printf(\"%7d %5.1f\\n\", N, time);\n      }\n   } \n}\n p r o g r a m  t o  p e r f o r m  e x p e r i m e n t s ", "start": 188, "end": 189}, "248": {"text": "%5.1f\\n\", N, time);\n      }\n   } \n}\n p r o g r a m  t o  p e r f o r m  e x p e r i m e n t s results of experiments\n% java DoublingTest\n    250   0.0\n    500   0.0\n   1000   0.1\n   2000   0.8\n   4000   6.4\n   8000  51.1 \n...\n1771.4 \u25a0 Analysis of Algorithms\n   \nSo far, this process mirrors the process scientists use when trying to understand \nproperties of the real world. A straight line in a log-log plot is equivalent to the hy-\npothesis that the data \ufb01ts the equation T(N ) = a N b . Such a \ufb01t is known as a  power law. \nA great many natural and synthetic phenomena are described by power laws, and it is \nreasonable to hypothesize that the running time of a program does, as well. Indeed, for \nthe analysis of algorithms, we have mathematical models that strongly support this and \nsimilar hypotheses, to which we now turn.\n M a t h e m a t i c a l  m o d e l s  In the early days of computer science,  D. E. Knuth postu-\nlated that, despite all of the complicating factors in understanding the running times of \nour programs, it is possible, in principle, to build a mathematical model to describe the \nrunning time of any program. Knuth\u2019s basic insight is simple: the total running time of \na program is determined by two primary factors:\n\u25a0 The cost of executing each statement\n\u25a0 The frequency of execution of each statement\nThe former is a property of the computer, the Java compiler and the operating system; \nthe latter is a property of the program ", "start": 189, "end": 190}, "249": {"text": "primary factors:\n\u25a0 The cost of executing each statement\n\u25a0 The frequency of execution of each statement\nThe former is a property of the computer, the Java compiler and the operating system; \nthe latter is a property of the program and the input. If we know both for all instruc-\ntions in the program, we can multiply them together and sum for all instructions in the \nprogram to get the running time.\nThe primary challenge is to determine the frequency of execution of the statements. \nSome statements are easy to analyze: for example, the statement that sets cnt to 0 in \nThreeSum.count() is executed exactly once. Others require higher-level reasoning: for \nexample, the if statement in ThreeSum.count() is executed precisely \nN (N/H110021)(N/H110022)/6 \ntimes (the number of ways to pick three different numbers from the input array\u2014see \nExercise 1.4.1). Others depend on the input data: for example the number of times the \ninstruction cnt++ in ThreeSum.count() is executed is precisely the number of triples \nthat sum to 0 in the input, which could range from 0 of them to all of them. In the case \nof DoublingTest, where we generate the numbers randomly, it is possible to do a prob-\nabilistic analysis to determine the expected value of this quantity (see Exercise 1.4.40).\n   T i l d e  a p p r o x i m a t i o n s .  Frequency analyses of this sort can lead to complicated and \nlengthy mathematical expressions. For example, consider the count just considered of \nthe number of times the if statement in ThreeSum is executed:\nN (N/H110021)(N/H110022)/6 = N 3/6 /H11002 N 2/2 /H11001 N/3 \n178 CHAPTER 1 \u25a0 Fundamentals\n  \nAs is typical ", "start": 190, "end": 191}, "250": {"text": "(N/H110021)(N/H110022)/6 = N 3/6 /H11002 N 2/2 /H11001 N/3 \n178 CHAPTER 1 \u25a0 Fundamentals\n  \nAs is typical in such expressions, the terms after \nthe leading term are relatively small (for exam-\nple, when N = 1,000 the value of /H11002 N 2/2 /H11001 N/3 \n499,667 is certainly insigni\ufb01cant by com-\nparison with N 3/6 /H11015 166,666,667). T o allow us \nto ignore insigni\ufb01cant terms and therefore sub-\nstantially simplify the mathematical formulas \nthat we work with, we often use a mathemati-\ncal device known as the tilde notation (~). This \nnotation allows us to work with tilde approxi-\nmations, where we throw away low-order terms \nthat complicate formulas and represent a negli-\ngible contribution to values of interest:\nDefinition. We w r ite ~f (N ) to represent \nany function that, when divided by f (N ), \napproaches 1 as N grows, and we write \ng(N ) ~ f (N ) to indicate that g(N )/f (N ) \napproaches 1 as N grows.  \nFor example, we use the approximation ~N 3/6 to de-\nscribe the number of times the if statement in \nThreeSum is executed, since N 3/6 /H11002 N 2/2 /H11001 N/3 di-\nvided by N 3/6 approaches 1 as N grows. Most of-\nten, we work with tilde approximations of the form \ng (N) ~af (N ) where f (N ) = N b (log N ) c with a, b, and c\nconstants and refer to ", "start": 191, "end": 191}, "251": {"text": "of-\nten, we work with tilde approximations of the form \ng (N) ~af (N ) where f (N ) = N b (log N ) c with a, b, and c\nconstants and refer to f (N ) as the    order of growth of g (N ). \nWhen using the logarithm in the order of growth, we gener-\nally do not specify the base, since the constant a can absorb \nthat detail. This usage covers the relatively few functions \nthat are commonly encountered in studying the order of \ngrowth of a program\u2019s  running time shown in the table at \nleft (with the exception of the exponential, which we defer \nto CONTEXT). We will describe these functions in more de-\ntail and brie\ufb02y discuss why they appear in the analysis of \nalgorithms after we complete our treatment of ThreeSum.\nfunction tilde\napproximation\norder\nof growth\nN 3/6 /H11002 N 2/2 /H11001 N/3 ~ N 3/6 N 3\nN 2/2 /H11002 N/2 ~ N 2/2 N 2\nlg N  + 1 ~ lg N lg N\n3 ~ 3 1\nTypical tilde approximations\norder of growth\ndescription function\n      c o n s t a n t  1\nlogarithmic log N\nlinear N\nlinearithmic N log N\nquadratic N 2\ncubic N 3\nexponential 2 N\nCommonly encountered\norder-of-growth functions\nLeading-term approximation\nN 3/6 \nN (N/H11002 1)(N/H11002 2)/6\n166,167,000\n1,000\n166,666,667\nN\n1791.4 \u25a0 Analysis of Algorithms\n  \n \nApproximate running time. To  f ", "start": 191, "end": 192}, "252": {"text": "1)(N/H11002 2)/6\n166,167,000\n1,000\n166,666,667\nN\n1791.4 \u25a0 Analysis of Algorithms\n  \n \nApproximate running time. To  f o l l ow  t h ro u g h  o n  Kn u t h\u2019s  a p p ro a c h  to  d e ve l o p  a  \nmathematical expression for the total running time of a Java program, we can (in prin-\nciple) study our Java compiler to \ufb01nd the number of machine instructions correspond-\ning to each Java instruction and study our machine speci\ufb01cations to \ufb01nd the time of \nexecution of each of the machine instructions, to produce a grand total. This process, \nfor ThreeSum, is brie\ufb02y summarized on the facing page. We classify blocks of Java state-\nments by their frequency of execution, develop leading-term approximations for the \nfrequencies, determine the cost of each statement, and then compute a total. Note that \nsome frequencies may depend on the input. In this case, the number of times cnt++\nis executed certainly depends on the input\u2014it is the number of triples that sum to 0, \nand could range from 0 to ~N 3/6. We stop short of exhibiting the details (values of the \nconstants) for any particular system, except to highlight that by using constant values t0, \nt1, t2, ... for the time taken by the blocks of statements, we are assuming that each block \nof Java statements corresponds to machine instructions that require a speci\ufb01ed \ufb01xed \namount of time. A key observation from this exercise is to note that only the instruc-\ntions that are executed the most frequently play a role in the \ufb01nal total\u2014we refer to \nthese instructions as the    inner loop of the ", "start": 192, "end": 192}, "253": {"text": "time. A key observation from this exercise is to note that only the instruc-\ntions that are executed the most frequently play a role in the \ufb01nal total\u2014we refer to \nthese instructions as the    inner loop of the program. For ThreeSum, the inner loop is the \nstatements that increment k and test that it is less than N and the statements that test \nwhether the sum of three given numbers is 0 (and possibly the statement that imple-\nments the count, depending on the input). This behavior is typical: the running times \nof a great many programs depend only on a small subset of their instructions.\n  O r d e r - o f - g r o w t h  h y p o t h e s i s .  In summary, the experiments on page 177 and the math-\nematical model on page 181 both support the following hypothesis:\nProperty A. The order of growth of the running time of  ThreeSum (to compute the \nnumber of triples that sum to 0 among N numbers) is N 3.\nEvidence: Let T(N ) be the running time of ThreeSum for N numbers. The math-\nematical model just described suggests that T(N ) ~ a N 3 for some machine-de -\npendent constant a; experiments on many computers (including yours and ours) \nvalidate that approximation.\nThroughout this book, we use the term  property to refer to a hypothesis that needs to \nbe validated through experimentation. The end result of our mathematical analysis is \nprecisely the same as the end result of our experimental analysis\u2014the running time \nof ThreeSum is ~ a N 3 for a machine-dependent constant a. This match validates both \nthe experiments and the mathematical model and also exhibits more insight about the \n180 CHAPTER 1 \u25a0 Fundamentals\n 1\nN\nx\ninner\nloop\n~N 2/ 2\n~N 3/ 6\nAnatomy ", "start": 192, "end": 193}, "254": {"text": "experiments and the mathematical model and also exhibits more insight about the \n180 CHAPTER 1 \u25a0 Fundamentals\n 1\nN\nx\ninner\nloop\n~N 2/ 2\n~N 3/ 6\nAnatomy of a program\u2019s statement execution frequencies\nA\nB\nC\nD\nE\nblocks of statements\nfrequencies of execution\npublic class ThreeSum\n{\n   public static int count(int[] a) \n   {\n      int N = a.length;\n      int cnt = 0;\n      for (int i = 0; i < N; i++ )\n         for (int j = i+1; j < N; j++ )\n            for (int k = j+1; k < N; k++ )\n               if (a[i] + a[j] + a[k] == 0)\n                  cnt++;\n      return cnt;\n   }\n   public static void main(String[] args)\n   {\n      int[] a = In.readInts(args[0]); \n      StdOut.println(count(a));\n   }\n}\nstatement \nblock\ntime in \nseconds frequency total time\nE t0 x  (depends on input) t0 x\nD t1 N 3/6 /H11002 N 2/2 /H11001 N/3 t1 (N 3/6 /H11002 N 2/2 /H11001 N/3)\nC t2 N 2/2 /H11002 N/2 t2 (N 2/2 /H11002 N/2)\nB t3 N t3 N\nA t4 1 t4\ngrand total\n     (t1/6) N 3\n         /H11001 (t2/2 /H11002 t1/2) N 2\n                  /H11001 (t1/3 /H11002 t2/2 /H11001 ", "start": 193, "end": 193}, "255": {"text": "N 3\n         /H11001 (t2/2 /H11002 t1/2) N 2\n                  /H11001 (t1/3 /H11002 t2/2 /H11001 t3) N\n                                                  /H11001 t4  /H11001 t0 x\ntilde approximation ~ (t1 / 6) N 3 (assuming x is small)\norder of growth N 3\n A n a l y z i n g  t h e  r u n n i n g  t i m e  o f  a  p r o g r a m  ( e x a m p l e )\n1811.4 \u25a0 Analysis of Algorithms\n program because it does not require experimentation to determine the exponent. With \nsome effort, we could validate the value of a on a particular system as well, though that \nactivity is generally reserved for experts in situations where performance is critical. \nAnalysis of algorithms. Hypotheses such as Property A are signi\ufb01cant because they \nrelate the abstract world of a Java program to the real world of a computer running it. \nWorking w ith the order of  g row th allows us to take one fur ther step: to separate a pro-\ngram from the algorithm it implements. The idea that the order of growth of the run -\nning time of ThreeSum is N 3 does not depend on the fact that it is implemented in Java \nor that it is running on your laptop or someone else\u2019s cellphone or a supercomputer; it \ndepends primarily on the fact that it examines all the different triples of numbers in the \ninput. The algorithm that you are using (and sometimes the input model) determines \nthe order of growth. Separating the algorithm from the implementation on a particular \ncomputer is a powerful concept because it allows us to develop knowledge about the \nperformance of algorithms and then apply that ", "start": 193, "end": 194}, "256": {"text": "(and sometimes the input model) determines \nthe order of growth. Separating the algorithm from the implementation on a particular \ncomputer is a powerful concept because it allows us to develop knowledge about the \nperformance of algorithms and then apply that knowledge to any computer. For ex-\nample, we might say that ThreeSum is an implementation of the brute-force algorithm \n\u201ccompute the sum of all different triples, counting those that sum to 0\u201d\u2014we expect that an \nimplementation of this algorithm in any programming language on any computer will \nlead to a running time that is proportional to N 3. In \nfact, much of the knowledge about the performance \nof classic algorithms was developed decades ago, but \nthat knowledge is still relevant to today\u2019s computers.\n   C o s t  m o d e l .  We focus attention on proper ties of  al-\ngorithms by articulating a cost model that de\ufb01nes the \nbasic operations used by the algorithms we are study-\ning to solve the problem at hand. For example, an ap-\npropriate cost model for the 3-sum problem, shown \nat right, is the number of times we access an array \nentry. With this cost model, we can make precise mathematical statements about prop-\nerties of an algorithm, not just a particular implementation, as follows:\nProposition B.  The brute-force 3-sum algorithm uses ~N 3/2 array accesses to \ncompute the number of triples that sum to 0 among N numbers.\nProof: The algorithm accesses each of the 3 numbers for each of the ~N 3/6 triples.\nWe use the term  proposition to refer to mathematical truths about algorithms in terms \nof a cost model. Throughout this book, we study the algorithms that we consider within \n3-sum cost model. When \nstudying algorithms to \nsolve the 3-sum problem, \nwe count  array accesses\n(the number of times an \narray entry is accessed, ", "start": 194, "end": 194}, "257": {"text": "we study the algorithms that we consider within \n3-sum cost model. When \nstudying algorithms to \nsolve the 3-sum problem, \nwe count  array accesses\n(the number of times an \narray entry is accessed, for \nread or write).\n182 CHAPTER 1 \u25a0 Fundamentals\n  the framework of a speci\ufb01c cost model. Our intent is to articulate cost models such that \nthe order of growth of the running time for a given implementation is the same as the \norder of growth of the cost of the underlying algorithm (in other words, the cost model \nshould include operations that fall within the inner loop). We seek precise mathemati-\ncal results about algorithms  (propositions) and also hypotheses about performance \nof implementations (properties) that you can check through experimentation. In this \ncase, Proposition B is a mathematical truth that supports the hypothesis stated in \nProperty A, which we have validated with experiments, in accordance with the scien-\nti\ufb01c method.\n1831.4 \u25a0 Analysis of Algorithms\n Summary. For many programs, developing a mathematical model of running time \nreduces to the following steps:\n\u25a0  Develop an input model, including a de\ufb01nition of the problem size.\n\u25a0 Identify the inner loop.\n\u25a0 De\ufb01ne a cost model that includes operations in the inner loop.\n\u25a0 \n \nDetermine the frequency of execution of those operations for the given input. \nDoing so might require mathematical analysis\u2014we will consider some examples \nin the context of speci\ufb01c fundamental algorithms later in the book. \nIf a program is de\ufb01ned in terms of multiple methods, we normally consider the \nmethods separately. As an example, consider our example program of  Section 1.1 , \nBinarySearch. \nBinary search. The input model is the array a[] of size N; the inner loop is the \nstatements in the single while loop; the  cost model  is ", "start": 194, "end": 196}, "258": {"text": "of  Section 1.1 , \nBinarySearch. \nBinary search. The input model is the array a[] of size N; the inner loop is the \nstatements in the single while loop; the  cost model  is the compare operation \n(compare the values of two array entries); and the analysis, discussed in Section \n1.1 and given in full detail in Proposition B in Section 3.1, shows that the num-\nber of compares is at most lg N /H11001 1.\nWhitelist. The input model is the N numbers in the whitelist and the M numbers \non standard input where we assume M >> N; the  inner loop is the statements in \nthe single while loop; the cost model is the compare operation (inherited from \nbinary search); and the analysis is immediate given the analysis of binary search\u2014\nthe number of compares is at most M (lg N /H11001 1).\nThus, we draw the conclusion that the order of growth of the running time of the \nwhitelist computation is at most M lg N , subject to the following considerations:\n\u25a0 If N is small, the input-output cost might dominate. \n\u25a0 \n \nThe number of compares depends on the input\u2014it lies between ~ M  and ~M\nlg N, depending on how many of the numbers on standard input are in the \nwhitelist and on how long the binary search takes to \ufb01nd the ones that are (typi-\ncally it is ~M lg N ). \n\u25a0 We are assuming that the cost of  Arrays.sort() is small compared to M lg N. \nArrays.sort() implements the mergesort algorithm, and in Section 2.2, we \nwill see that the order of growth of the running time of mergesort is N log N \n(see Proposition G in chapter 2), so this assumption is justi\ufb01ed. \nThus, the model supports our hypothesis from Section 1.1 ", "start": 196, "end": 196}, "259": {"text": "the order of growth of the running time of mergesort is N log N \n(see Proposition G in chapter 2), so this assumption is justi\ufb01ed. \nThus, the model supports our hypothesis from Section 1.1 that the binary search algo-\nrithm makes the computation feasible when M and N are large. If we double the length \nof the standard input stream, then we can expect the running time to double; if we \ndouble the size of the whitelist, then we can expect the running time to increase only \nslightly. \n184 CHAPTER 1 \u25a0 Fundamentals\n Developing MATHEMATICal models for the analysis of algorithms is a fruitful area \nof research that is somewhat beyond the scope of this book. Still, as you will see with \nbinary search, mergesort, and many other algorithms, understanding certain math-\nematical models is critical to understanding the ef\ufb01ciency of fundamental algorithms, \nso we often present details and/or quote the results of classic studies. When doing so, we \nencounter various functions and approximations that are widely used in mathemati-\ncal analysis. For reference, we summarize some of this information in the tables below.\ndescription approximation\n     h a r m o n i c  s u m  HN  =  1 /H11001 1/2 /H11001 1/3 /H11001 1/4 /H11001 . . . /H11001 1/N  ~  ln N\ntriangular sum  1 /H11001 2 /H11001 3 /H11001 4 /H11001 . . . /H11001 N  ~  N 2/2\n g e o m e t r i c  s u m  1 /H11001 2 /H11001 4 /H11001 8 /H11001 . ", "start": 196, "end": 197}, "260": {"text": "N 2/2\n g e o m e t r i c  s u m  1 /H11001 2 /H11001 4 /H11001 8 /H11001 . . . /H11001 N   =  2N  \u2013 1  ~  2N  when N = 2n\nStirling\u2019s\napproximation lg N !  =   lg 1 /H11001 lg 2 /H11001 lg 3 /H11001 lg 4 /H11001 . . . /H11001 lg N ~ N lg N\nbinomial\ncoef\ufb01cients (\nN\nk ) ~ N k/k!  when k is a small constant\nexponential (1 \u2013 1/x) x ~ 1/e\n U s e f u l  a p p r o x i m a t i o n s  f o r  t h e  a n a l y s i s  o f  a l g o r i t h m s\ndescription notation definition\n      \ufb02 o o r  \u23a3x\u23a6 largest integer not greater than x\nceiling \u23a1x\u23a4 smallest integer not smaller than x\nnatural    logarithm ln N log e N (x such that e x = N)\nbinary logarithm lg N log 2 N (x such that 2x = N)\ninteger\nbinary logarithm \u23a3lg N\u23a6 largest integer not greater than lg N \n(# bits in binary representation of N ) \u2013 1\nharmonic numbers HN 1 /H11001 1/2 /H11001 1/3 /H11001 1/4 /H11001 . . . /H11001 1/N\nfactorial N ! 1 /H11003 2 /H11003 3 ", "start": 197, "end": 197}, "261": {"text": "/H11001 1/3 /H11001 1/4 /H11001 . . . /H11001 1/N\nfactorial N ! 1 /H11003 2 /H11003 3 /H11003 4 /H11003 . . .  /H11003 N\nCommonly encountered functions in the analysis of algorithms\n1851.4 \u25a0 Analysis of Algorithms\n   O r d e r - o f - g r o w t h  c l a s s i \ufb01 c a t i o n s  We use just a few structural pr imitives (state-\nments, conditionals, loops, nesting, and method calls) to implement algorithms, so very \noften the order of growth of the cost is one of just a few functions of the problem size N. \nThese functions are summarized in the table on the facing page, along with the names \nthat we use to refer to them, typical code that leads to each function, and examples.\n            C o n s t a n t .  A program whose running time\u2019s order of growth is constant executes a \n\ufb01xed number of operations to \ufb01nish its job; consequently its running time does not \ndepend on N. Most Java operations take constant time.\nLogarithmic. A program whose running time\u2019s order of growth is logarithmic is barely \nslower than a constant-time program. The classic example of a program whose running \ntime is logarithmic in the problem size is binary search (see BinarySearch on page 47). \nThe base of the logarithm is not relevant with respect to the order of growth (since all \nlogarithms with a constant base are related by a constant factor), so we use log N when \nreferring to order of growth.\nLinear. Programs that spend a constant amount of time processing each piece of input \ndata, or that are based on a single for loop, ", "start": 197, "end": 198}, "262": {"text": "a constant factor), so we use log N when \nreferring to order of growth.\nLinear. Programs that spend a constant amount of time processing each piece of input \ndata, or that are based on a single for loop, are quite common. The order of growth of \nsuch a program is said to be linear \u2014its running time is proportional to N.\nLinearithmic. We use the term linearithmic to describe programs whose running time \nfor a problem of size N has order of growth N log N. Again, the base of the logarithm \nis not relevant with respect to the order of growth. The prototypical examples of lin-\nearithmic algorithms are Merge.sort() (see Algorithm 2.4) and Quick.sort() (see \nAlgorithm 2.5). \nQuadratic. A typical program whose running time has order of growth N 2 has \ntwo nested for loops, used for some calculation involving all pairs of N elements. \nThe elementary sorting algorithms Selection.sort() (see Algorithm 2.1) and \nInsertion.sort() (see Algorithm 2.2) are prototypes of the programs in this \nclassi\ufb01cation. \nCubic. A typical program whose running time has order of growth N 3 has three nested \nfor loops, used for some calculation involving all triples of N elements. Our example \nfor this section, ThreeSum, is a prototype. \nExponential. In ChAPter 6 (but not until then!) we will consider programs whose \nrunning times are proportional to 2N or higher. Generally, we use the term exponential \nto refer to algorithms whose order of growth is b N for any constant b > 1, even though \ndifferent values of b lead to vastly different running times. Exponential algorithms are \nextremely slow\u2014you will never run one of them to completion for a large problem. \nStill, exponential algorithms play a critical role in the theory of algorithms ", "start": 198, "end": 198}, "263": {"text": "values of b lead to vastly different running times. Exponential algorithms are \nextremely slow\u2014you will never run one of them to completion for a large problem. \nStill, exponential algorithms play a critical role in the theory of algorithms because \n186 CHAPTER 1 \u25a0 Fundamentals\n description order of \ngrowth typical code framework description example\nconstant 1 a = b + c; statement add two \nnumbers\nlogarithmic log N [ see page 47 ] divide in \nhalf\nbinary\nsearch\nlinear N\ndouble max = a[0]; \nfor (int i = 1; i < N; i++)\n   if (a[i] > max) max = a[i]; \nloop \ufb01nd the \nmaximum\nlinearithmic N log N [ see Algorithm 2.4 ] divide and \nconquer mergesort\nquadratic N 2\nfor (int i = 0; i < N; i++)\n   for (int j = i+1; j < N; j++)\n      if (a[i] + a[j] == 0)\n         cnt++;\ndouble \nloop\ncheck all \npairs\ncubic N 3\nfor (int i = 0; i < N; i++)\n   for (int j = i+1; j < N; j++)\n      for (int k = j+1; k < N; k++)\n         if (a[i] + a[j] + a[k] == 0)\n            cnt++;\ntriple loop check all \ntriples\nexponential 2 N [ see chapter 6 ] exhasutive \nsearch\ncheck all \nsubsets\n S u m m a r y  o f  c o m m o n  o r d e r - o f - g r o w t h  h y p o t h e s e s\n1871.4 \u25a0 Analysis of Algorithms\n there exists a large class of ", "start": 198, "end": 200}, "264": {"text": "m o n  o r d e r - o f - g r o w t h  h y p o t h e s e s\n1871.4 \u25a0 Analysis of Algorithms\n there exists a large class of problems for which it seems that an exponential algorithm \nis the best possible choice. \nThese classifications are the most common, but certainly not a complete set. The \norder of growth of an algorithm\u2019s cost might be N 2  log N or N 3/2 or some similar func-\ntion. Indeed, the detailed analysis of algorithms \ncan require the full gamut of mathematical tools \nthat have been developed over the centuries. \nA great many of the algorithms that we con-\nsider have straightforward performance charac-\nteristics that can be accurately described by one \nof the orders of growth that we have considered. \nAccordingly, we can usually work with speci\ufb01c \npropositions with a cost model, such as mergesort \nuses between \u00bd N lg N and N lg N compares that \nimmediately imply hypotheses (properties) such \nas the order of growth of mergesort\u2019s running time \nis linearithmic. For economy, we abbreviate such \na statement to just say mergesort is linearithmic.\nThe plots at left indicate the importance of \nthe order of growth in practice. The x-axis is \nthe problem size; the y-axis is the running time.   \nThese charts make plain that quadratic and cubic \nalgorithms are not feasible for use on large prob-\nlems. As it turns out, several important prob-\nlems have natural solutions that are quadratic \nbut clever algorithms that are linearithmic. Such \nalgorithms (including mergesort) are critically \nimportant in practice because they enable us to \naddress problem sizes far larger than could be \naddressed with quadratic solutions. Naturally, we \ntherefore focus in this book on developing loga-\nrithmic, linear, ", "start": 200, "end": 200}, "265": {"text": "\nimportant in practice because they enable us to \naddress problem sizes far larger than could be \naddressed with quadratic solutions. Naturally, we \ntherefore focus in this book on developing loga-\nrithmic, linear, and linearithmic algorithms for \nfundamental problems.\n1K\nT\n2T\n4T\n8T\n64T\n512T\nlogarithmic\nexponential\nconstant\nlinearithmiclinearquadratic\ncubic\n2K 4K 8K 512K\n100T\n200T\n500T\nlogarithmic\nexponential\nconstant\nproblem size\nproblem size\nlinearithmiclinear\n100K 200K 500K\ntimetime\nTypical orders of growth\nlog-log plot\nstandard plot\ncubic\nquadratic\n188 CHAPTER 1 \u25a0 Fundamentals\n Designing faster algorithms One of the primary reasons to study the order of \ngrowth of a program is to help design a faster algorithm to solve the same problem. T o \nillustrate this point, we consider next a faster algorithm for the 3-sum problem. How \ncan we devise a faster algorithm, before even embarking on the study of algorithms? \nThe answer to this question is that we have discussed and used two classic algorithms, \nmergesort and binary search, have introduced the facts that the mergesort is linearith-\nmic and binary search is logarithmic. How can we take advantage of these algorithms \nto solve the 3-sum problem? \nWar mup:  2-sum. Consider the easier problem of determining the number of pairs of \nintegers in an input \ufb01le that sum to 0. T o simplify the discussion, assume also that the \nintegers are distinct. This problem is easily solved in quadratic time by deleting the k\nloop and a[k] from ThreeSum.count(), leaving a double loop that examines all pairs, \nas shown in the ", "start": 200, "end": 201}, "266": {"text": "also that the \nintegers are distinct. This problem is easily solved in quadratic time by deleting the k\nloop and a[k] from ThreeSum.count(), leaving a double loop that examines all pairs, \nas shown in the quadratic entry in the table on page 187 (we refer to such an implementa-\ntion as TwoSum). The implementation below shows how mergesort and binary search \n(see page 47) can serve as a basis for a linearithmic solution to the 2-sum problem. The \nimproved algorithm is based on the fact that an entry a[i] is one of a pair that sums to \n0 if and only if the value -a[i] is in the array (and a[i] is not zero). T o solve the prob-\nlem, we sort the array (to enable binary search) and then, for every entry a[i] in the ar-\nray, do a binary search for -a[i] with rank() in BinarySearch. If the result is an index \nj with j > i, we increment the count. \nThis succinct test covers three cases:\n\u25a0 An unsuccessful binary search re-\nturns -1, so we do not increment \nthe count.\n\u25a0 If the binary search re-\nturns j > i, we have \na[i] + a[j] = 0, so we incre-\nment the count.\n\u25a0 If the binary search returns j\nbetween 0 and i, we also have \na[i] + a[j] = 0 but do not \nincrement the count, to avoid \ndouble counting.\nThe result of the computation is precise-\nly the same as the result of the quadratic \nalgorithm, but it takes much less time. \nThe running time of the mergesort is \nimport java.util.Arrays;\npublic class  TwoSumFast \n{\n   public static int count(int[] a) \n   {  // Count pairs that sum to 0.\n      Arrays.sort(a); ", "start": 201, "end": 201}, "267": {"text": "\nThe running time of the mergesort is \nimport java.util.Arrays;\npublic class  TwoSumFast \n{\n   public static int count(int[] a) \n   {  // Count pairs that sum to 0.\n      Arrays.sort(a); \n      int N = a.length;\n      int cnt = 0;\n      for (int i = 0; i < N; i++)\n         if (BinarySearch.rank(-a[i], a) > i)\n            cnt++; \n      return cnt;\n   }\n   public static void main(String[] args) \n   {\n      int[] a = In.readInts(args[0]);\n      StdOut.println(count(a));\n   } \n}\nLinearithmic  solution to the 2-sum problem\n1891.4 \u25a0 Analysis of Algorithms\n  \n \n \nproportional to N log N, and the N binary searches each take time proportional to log \nN, so the running time of the whole algorithm is proportional to N log N. Developing \na faster algorithm like this is not merely an academic exercise\u2014the faster algorithm \nenables us to address much larger problems. For example, you are likely to be able to \nsolve the 2-sum problem for 1 million integers ( 1Mints.txt) in a reasonable amount \nof time on your computer, but you would have to wait quite a long time to do it with \nthe quadratic algorithm (see Exercise 1.4.41).\nFast algorithm for   3-sum. The very same idea is effective for the 3-sum problem. \nAgain, assume also that the integers are distinct. A pair a[i] and a[j] is part of a triple  \nthat sums to 0 if and only if the value -(a[i] + a[j]) is in the array (and not a[i] or \na[j]). The code below sorts the array, then does N (N/H110021)/ 2 binary searches that each \ntake time ", "start": 201, "end": 202}, "268": {"text": "-(a[i] + a[j]) is in the array (and not a[i] or \na[j]). The code below sorts the array, then does N (N/H110021)/ 2 binary searches that each \ntake time proportional to log N, for a total running time proportional to N 2 log N. Note \nthat in this case the cost of the sort is insigni\ufb01cant. Again, this solution enables us to ad-\ndress much larger problems (see Exercise 1.4.42). The plots in the \ufb01gure at the bottom \nof the next page show the disparity in costs among these four algorithms for problem \nsizes in the range we have considered. Such differences certainly motivate the search for \nfaster algorithms.\n L o w e r  b o u n d s .  The table on page 191 summarizes the discussion of this section. An in-\nteresting question immediately arises: Can we \ufb01nd algorithms for the 2-sum and 3-sum  \nproblems that are substantially \nfaster than TwoSumFast and \nThreeSumFast? Is there a linear \nalgorithm for 2-sum or a linea-\nrithmic algorithm for 3-sum? \nThe answer to this question is no\nfor 2-sum (under a model that \ncounts and allows only compari-\nsons of linear or quadratic func-\ntions of the numbers) and no one \nknows for 3-sum, though experts \nbelieve that the best possible al-\ngorithm for 3-sum is quadratic. \nThe idea of a lower bound on the \norder of growth of the worst-case \nrunning time for all possible al -\ngorithms to solve a problem is a \nvery powerful one, which we will \nimport java.util.Arrays;\npublic class  ThreeSumFast \n{\n   public static int count(int[] a) \n   {  // Count triples that sum to 0.\n      Arrays.sort(a);\n      int ", "start": 202, "end": 202}, "269": {"text": "which we will \nimport java.util.Arrays;\npublic class  ThreeSumFast \n{\n   public static int count(int[] a) \n   {  // Count triples that sum to 0.\n      Arrays.sort(a);\n      int N = a.length;\n      int cnt = 0;\n      for (int i = 0; i < N; i++)\n         for (int j = i+1; j < N; j++)\n           if (BinarySearch.rank(-a[i]-a[j], a) > j)\n              cnt++;\n      return cnt;\n   }\n   public static void main(String[] args) \n   {\n      int[] a = In.readInts(args[0]);\n      StdOut.println(count(a));\n   } \n}\nN 2 lg N  solution to the 3-sum problem\n190 CHAPTER 1 \u25a0 Fundamentals\n revisit in detail in Section 2.2 in the context of sorting. Non-\ntrivial lower bounds are dif\ufb01cult to establish, but very helpful \nin guiding our search for ef\ufb01cient algorithms.\nThe examples in this section set the stage for our treat-\nment of algorithms in this book. Throughout the book, our \nstrategy for addressing new problems is the following:\n\u25a0 Implement and analyze a straighforward solution to \nthe problem. We usually refer to such solutions, like \nThreeSum and TwoSum, as the brute-force solution.\n\u25a0 Examine algorithmic improvements, usually designed \nto reduce the order of growth of the running time, such as TwoSumFast and \nThreeSumFast.\n\u25a0 Run experiments to validate the hypotheses that the new algorithms are faster.\nIn many cases, we examine several algorithms for the same problem, because running \ntime is only one consideration when choosing an algorithm for a practical problem. We \nwill develop this idea in detail in the context of fundamental problems throughout the \nbook.\nalgorithm order of growth\nof running time\nTwoSum N 2\nTwoSumFast ", "start": 202, "end": 203}, "270": {"text": "consideration when choosing an algorithm for a practical problem. We \nwill develop this idea in detail in the context of fundamental problems throughout the \nbook.\nalgorithm order of growth\nof running time\nTwoSum N 2\nTwoSumFast N log N\nThreeSum N 3\nThreeSumFast N 2 log N\n S u m m a r y  o f  r u n n i n g  t i m e s\nCosts of algorithms to solve the 2-sum and 3-sum problems\nproblem size N\nN \n3/2 N \n2 lgN\narray accesses (millions)\n1K\n200\n400\n600\n800\n1000\n2K 4K 8K\nThreeSum\nThreeSumFast\nproblem size N\nN \n2\narray accesses (thousands)\n1K\n20\n40\n60\n80\n100\n2K 4K 8K\nTwoSum\n4N \n lgN\nTwoSumFast\n1911.4 \u25a0 Analysis of Algorithms\n    D o u b l i n g  r a t i o  e x p e r i m e n t s  The following is a simple and effective shortcut for \npredicting performance and for determining the approximate order of growth of the \nrunning time of any program:\n\u25a0 Develop an input generator that produces inputs that model the inputs expected \nin practice (such as the random integers in timeTrial() in DoublingTest.\n\u25a0 Run the program DoublingRatio given below, a modi\ufb01cation of DoublingTest\nthat calculates the ratio of each running time with the previous.\n\u25a0 Run until the ratios approach a limit 2b.\nThis test is not effective if the ratios do not approach a limiting value, but they do for \nmany, many programs, implying the following conclusions:\n\u25a0 The order of growth of the running time is approximately N b.\n\u25a0 To  p ", "start": 203, "end": 204}, "271": {"text": "is not effective if the ratios do not approach a limiting value, but they do for \nmany, many programs, implying the following conclusions:\n\u25a0 The order of growth of the running time is approximately N b.\n\u25a0 To  p re d i c t  r u n n i n g  t i m e s , m u l t i p l y  t h e  l a s t  o b s e r ve d  r u n n i n g  t i m e  by  2b and \ndouble N, continuing as long as desired. If you want to predict for an input size \nthat is not a power of 2 times N, you can adjust ratios accordingly (see Exercise \n1.4.9). \nAs illustrated below, the ratio for ThreeSum is about 8 and we can predict the running \ntimes for N = 16,000, 32,000, 64,000 to be 408.8, 3270.4, 26163.2 seconds, respectively, \njust by successively multiplying the last time for 8,000 (51.1) by 8.\npublic class DoublingRatio \n{\n   public static double timeTrial(int N)\n   // same as for DoublingTest (page 177)\n   public static void main(String[] args)\n   {  \n      double prev = timeTrial(125);\n      for (int N = 250; true; N += N)\n      {  \n         double time = timeTrial(N);\n         StdOut.printf(\"%6d %7.1f \", N, time);\n         StdOut.printf(\"%5.1f\\n\", time/prev);\n         prev = time;\n      }\n   } \n}\nresults of experiments\nprogram to perform experiments\npredictions\n% java DoublingRatio\n   250     0.0   2.7\n   500     0.0 ", "start": 204, "end": 204}, "272": {"text": "time;\n      }\n   } \n}\nresults of experiments\nprogram to perform experiments\npredictions\n% java DoublingRatio\n   250     0.0   2.7\n   500     0.0   4.8\n  1000     0.1   6.9\n  2000     0.8   7.7\n  4000     6.4   8.0\n  8000    51.1   8.0\n 16000   408.8   8.0\n 32000  3270.4   8.0\n 64000 26163.2   8.0\n192 CHAPTER 1 \u25a0 Fundamentals\n  \nThis test is roughly equivalent to the process described on page 176 (run experiments, \nplot values on a log-log plot to develop the hypothesis that the running time is aN b, \ndetermine the value of b from the slope of the line, then solve for a), but it is sim-\npler to apply.  Indeed, you can accurately predict preformance by hand when you run \nDoublingRatio. As the ratio approaches a limit, just multiply by that ratio to \ufb01ll in later \nvalues in the table. Y our approximate model of the order of growth is a power law with \nthe binary logarithm of that ratio as the power.\nWhy does the ratio approach a constant? A simple mathematical calculation shows \nthat to be the case for all of the common orders of growth just discussed (except \nexponential):\n Proposition C. (Doubling ratio) If T(N)  ~ a N b lg N then T(2N)/T(N)  ~ 2b .\nProof: Immediate from the following calculation:\nT(2N)/T(N)  = a (2N )b lg (2N ) / a ", "start": 204, "end": 205}, "273": {"text": "then T(2N)/T(N)  ~ 2b .\nProof: Immediate from the following calculation:\nT(2N)/T(N)  = a (2N )b lg (2N ) / a N b lg N\n            = 2b (1 + lg 2  /  lg N )\n                                                           ~ 2b\n \nGenerally, the logarithmic factor cannot be ignored when developing a mathematical \nmodel, but it plays a less important role in predicting performance with a doubling \nhypothesis. \nYou should consider running doubling ratio experiments for every program that \nyou write where performance matters\u2014doing so is a very simple way to estimate the \norder of growth of the running time, perhaps revealing a performance bug where a \nprogram may turn out to be not as ef\ufb01cient as you might think. More generally, we can \nuse hypotheses about the order of growth of the running time of programs to predict \nperformance in one of the following ways:\nEstimating the feasibility of solving large problems. Yo u  n e e d  t o  b e  a b l e  t o  a n s w e r  \nthis basic question for every program that you write: Will the program be able to process \nthis given input data in a reasonable amount of time? T o address such questions for a \nlarge amount of data, we extrapolate by a much larger factor than for doubling, say 10, \nas shown in the fourth column in the table at the bottom of the next page. Whether it is \nan investment banker running daily \ufb01nancial models or a scientist running a program \nto analyze experimental data or an engineer running simulations to test a design, it is \nnot unusual for people to regularly run programs that take several hours to complete, \n1931.4 \u25a0 Analysis of Algorithms\n so the table focuses on that situation. Knowing the order of growth of the running time \nof an algorithm ", "start": 205, "end": 206}, "274": {"text": "\nnot unusual for people to regularly run programs that take several hours to complete, \n1931.4 \u25a0 Analysis of Algorithms\n so the table focuses on that situation. Knowing the order of growth of the running time \nof an algorithm provides precisely the information that you need to understand limita-\ntions on the size of the problems that you can solve. Developing such understanding is \nthe most important reason to study performance.  Without it, you are likely have no idea \nhow much time a program will consume; with it, you can make a back-of-the-envelope \ncalculation to estimate costs and proceed accordingly.\nEstimating the value of using a faster computer. Yo u  a l s o  m ay  b e  f a ce d  w i t h  t h i s  b a s i c  \nquestion, periodically: How much faster can I solve the problem if I get a faster computer? \nGenerally, if the new computer is x times faster than the old one, you can improve your \nrunning time by a factor of x. But it is usually the case that you can address larger prob-\nlems with your new computer. How will that change affect the running time? Again, the \norder of growth is precisely the information needed to answer that question. \nA famous rule of thumb known as  Moore\u2019s Law implies that you can expect to have a \ncomputer with about double the speed and double the memory 18 months from now, \nor a computer with about 10 times the speed and 10 times the memory in about 5 years. \nThe table below demonstrates that you cannot keep pace with Moore\u2019s Law if you are \nusing a quadratic or a cubic algorithm, and you can quickly determine whether that is \nthe case by doing a doubling ratio test and checking that the ratio of running times as \nthe input size doubles approaches 2, not 4 or 8.\norder of growth of time\n2x\nfactor\n10x\nfactor\nfor ", "start": 206, "end": 206}, "275": {"text": "determine whether that is \nthe case by doing a doubling ratio test and checking that the ratio of running times as \nthe input size doubles approaches 2, not 4 or 8.\norder of growth of time\n2x\nfactor\n10x\nfactor\nfor a program that takes a few hours for input of size N\ndescription function predicted time for 10N predicted time for10N\non a 10x faster computer\nlinear N 2 10 a day a few hours\nlinearithmic N log N 2 10 a day a few hours\nquadratic N 2 4 100 a few weeks a day\ncubic N 3 8 1,000 several months a few weeks\nexponential 2 N 2 N 2 9N never never\nPredictions on the basis of order-of-growth function\n194 CHAPTER 1 \u25a0 Fundamentals\n  \n  \n C a v e a t s  There are many reasons that you might get inconsistent or misleading re-\nsults when trying to analyze program performance in detail. All of them have to do with \nthe idea that one or more of the basic assumptions underlying our hypotheses might be \nnot quite correct. We can develop new hypotheses based on new assumptions, but the \nmore details that we need to take into account, the more care is required in the analysis.\nLarge constants. With leading-term approximations, we ignore constant coef\ufb01cients \nin lower-order terms, which may not be justifed. For example, when we approximate \nthe function 2 N 2 + c N by ~2 N 2, we are assuming that c is small. If that is not the case \n(suppose that c is 10 3 or 10 6) the approximation is misleading. Thus, we have to be \nsensitive to the possibility of large constants.\nNondominant   inner loop. The assumption that ", "start": 206, "end": 207}, "276": {"text": "\n(suppose that c is 10 3 or 10 6) the approximation is misleading. Thus, we have to be \nsensitive to the possibility of large constants.\nNondominant   inner loop. The assumption that the inner loop dominates may not \nalways be correct. The cost model might miss the true inner loop, or the problem size \nN might not be suf\ufb01ciently large to make the leading term in the mathematical descrip-\ntion of the frequency of execution of instructions in the inner loop so much larger \nthan lower-order terms that we can ignore them. Some programs have a signi\ufb01cant \namount of code outside the inner loop that needs to be taken into consideration. In \nother words, the cost model may need to be re\ufb01ned.\nInstruction time. The assumption that each instruction always takes the same amount \nof time is not always correct. For example, most modern computer systems use a tech-\nnique known as  caching to organize memory, in which case accessing elements in huge \narrays can take much longer if they are not close together in the array. Y ou might ob-\nserve the effect of caching for ThreeSum by letting DoublingTest run for a while. After \nseeming to converge to 8, the ratio of running times may jump to a larger value for large \narrays because of caching. \nSystem considerations. Typically, there are many, many things going on in your com-\nputer. Java is one application of many competing for resources, and Java itself has many \noptions and controls that signi\ufb01cantly affect performance. A  garbage collector or a  just-\nin-time compiler or a download from the internet might drastically affect the results \nof experiments. Such considerations can interfere with the bedrock principle of the \nscienti\ufb01c method that experiments should be reproducible, since what is happening at \nthis moment in your computer will never be reproduced again. ", "start": 207, "end": 207}, "277": {"text": "\nof experiments. Such considerations can interfere with the bedrock principle of the \nscienti\ufb01c method that experiments should be reproducible, since what is happening at \nthis moment in your computer will never be reproduced again. Whatever else is going \non in your system should in principle be negligible or possible to control.\nToo close to call. Often, when we compare two different programs for the same task, \none might be faster in some situations, and slower in others. One or more of the consid-\nerations just mentioned could make the difference.  There is a natural tendency among \n1951.4 \u25a0 Analysis of Algorithms\n  \n \n \nsome programmers (and some students) to devote an extreme amount of energy run-\nning races to \ufb01nd the \u201cbest\u201d implementation, but such work is best left for experts. \nStrong dependence on inputs. One of the \ufb01rst assumptions that we made in order to \ndetermine the order of growth of the program\u2019s running time of a program was that the \nrunning time should be relatively insensitive to the inputs. When that is not the case, we \nmay get inconsistent results or be unable to validate our hypotheses. For example, sup-\npose that we modify ThreeSum to answer the question  Does the input have a triple that \nsums to 0 ? by changing it to return a boolean value, replacing cnt++ by return true\nand adding return false as the last statement. The order of growth of the running \ntime of this program is constant if the \ufb01rst three integers sum to 0 and cubic if there are \nno such triples in the input.  \n M u l t i p l e  p r o b l e m  p a r a m e t e r s .  We have been focusing on measur ing performance as a \nfunction of a single parameter, generally the value of a command-line argument or the \nsize of the input. However, it is ", "start": 207, "end": 208}, "278": {"text": "m e t e r s .  We have been focusing on measur ing performance as a \nfunction of a single parameter, generally the value of a command-line argument or the \nsize of the input. However, it is not unusual to have several parameters. A typical ex-\nample arises when an algorithm involves building a data structure and then performing \na sequence of operations that use that data structure. Both the size of the data structure \nand the number of operations are parameters for such applications. We have already \nseen an example of this in our analysis of the problem of whitelisting using binary \nsearch, where we have N numbers in the whitelist and M numbers on standard input \nand a typical running time proportional to M log N.\nDespite all these caveats, understanding the order of growth of the running time of \neach program is valuable knowledge for any programmer, and the methods that we \nhave described are powerful and broadly applicable. Knuth\u2019s insight was that we can \ncarry these methods through to the last detail  in principle  to make detailed, accurate \npredictions. Typical computer systems are extremely complex and close analysis is best \nleft for experts, but the same methods are effective for developing approximate esti-\nmates of the running time of any program. A rocket scientist needs to have some idea \nof whether a test \ufb02ight will land in the ocean or in a city; a medical researcher needs to \nknow whether a drug trial will kill or cure all the subjects; and any scientist or engineer \nusing a computer program needs to have some idea of whether it will run for a second \nor for a year.\n196 CHAPTER 1 \u25a0 Fundamentals\n  \nCoping with dependence on inputs  For many problems, one of the most sig -\nni\ufb01cant of the caveats just mentioned is the dependence on inputs, because running \ntimes can vary widely. The running time of the modi\ufb01cation of ThreeSum mentioned \non the facing page ", "start": 208, "end": 209}, "279": {"text": "most sig -\nni\ufb01cant of the caveats just mentioned is the dependence on inputs, because running \ntimes can vary widely. The running time of the modi\ufb01cation of ThreeSum mentioned \non the facing page ranges from constant to cubic, depending on the input, so a closer \nanalysis is required if we want to predict performance. We brie\ufb02y consider here some of \nthe approaches that are effective and that we will consider for speci\ufb01c algorithms later \nin the book.\n  I n p u t  m o d e l s .  One approach is to more carefully model the kind of input to be pro-\ncessed in the problems that we need to solve. For example, we might assume that the \nnumbers in the input to ThreeSum are random int values. This approach is challenging \nfor two reasons:\n\u25a0 The model may be unrealistic.\n\u25a0  \n \nThe analysis may be extremely dif\ufb01cult, requiring mathematical skills quite be-\nyond those of the typical student or programmer.\nThe \ufb01rst of these is the more signi\ufb01cant, often because the goal of a computation is to \ndiscover characteristics of the input. For example, if we are writing a program to process \na genome, how can we estimate its performance on a different genome? A good model \ndescribing the genomes found in nature is precisely what scientists seek, so estimating \nthe running time of our programs on data found in nature actually amounts to con-\ntributing to that model! The second challenge leads to a focus on mathematical results \nonly for our most important algorithms. We will see several examples where a simple \nand tractable input model, in conjunction with classical mathematical analysis, helps \nus predict performance.\nWorst-case per for mance guarantees. Some applications demand that the running \ntime of a program be less than a certain bound, no matter what the input. T o provide \nsuch performance    guarantees, ", "start": 209, "end": 209}, "280": {"text": "\nus predict performance.\nWorst-case per for mance guarantees. Some applications demand that the running \ntime of a program be less than a certain bound, no matter what the input. T o provide \nsuch performance    guarantees, theoreticians take an extremely pessimistic view of the \nperformance of algorithms: what would the running time be in the worst case? For \nexample, such a conservative approach might be appropriate for the software that runs \na nuclear reactor or a pacemaker or the brakes in your car. We want to guarantee that \nsuch software completes its job within the bounds that we set because the result could \nbe catastrophic if it does not. Scientists normally do not contemplate the worst case \nwhen studying the natural world: in biology, the worst case might be the extinction \nof the human race; in physics, the worst case might be the end of the universe. But the \nworst case can be a very real concern in computer systems, where the input may be \ngenerated by another (potentially malicious) user, rather than by nature. For example, \nwebsites that do not use algorithms with performance guarantees are subject to  denial-\nof-service attacks, where hackers \ufb02ood them with pathological requests that make them \n1971.4 \u25a0 Analysis of Algorithms\n run much more slowly than planned. Accordingly, many of our algorithms are designed \nto provide performance guarantees, such as the following:\nProposition D. In the linked-list implementations of   Bag (Algorithm 1.4), Stack\n(Algorithm 1.2), and Queue (Algorithm 1.3), all operations take constant time \nin the worst case.\nProof: Immediate from the code. The number of instructions executed for each \noperation is bounded by a small constant. Caveat : This argument depends upon \nthe (reasonable) assumption that the Java system creates a new Node in constant \ntime.\n   R a n d o m i z e d  a l ", "start": 209, "end": 210}, "281": {"text": "bounded by a small constant. Caveat : This argument depends upon \nthe (reasonable) assumption that the Java system creates a new Node in constant \ntime.\n   R a n d o m i z e d  a l g o r i t h m s .  One important way to provide a performance guarantee is \nto introduce randomness. For example, the quicksort algorithm for sorting that we \nstudy in Section 2.3 (perhaps the most widely used sorting algorithm) is quadratic in \nthe worst case, but randomly ordering the input gives a probabilistic guarantee that its \nrunning time is linearithmic. Every time you run the algorithm, it will take a different \namount of time, but the chance that the time will not be linearithmic is so small as to be \nnegligible. Similarly, the hashing algorithms for symbol tables that we study in Section \n3.4 (again, perhaps the most widely used approach) are linear-time in the worst case, \nbut constant-time under a probabilistic guarantee. These guarantees are not absolute, \nbut the chance that they are invalid is less than the chance your computer will be struck \nby lightning. Thus, such guarantees are as useful in practice as worst-case guarantees.\nSequences of operations. For many applications, the algorithm \u201cinput\u201d might be \nnot just data, but the sequence of operations performed by the client. For example, a \npushdown stack where the client pushes N values, then pops them all, may have quite \ndifferent performance characteristics from one where the client issues an alternating \nsequence N of push and pop operations. Our analysis has to take both situations into \naccount (or to include a reasonable model of the sequence of operations).\n  A m o r t i z e d  a n a l y s i s .  Accordingly, another way to provide a performance guarantee is \nto amortize the cost, by keeping track of the total cost of all operations, ", "start": 210, "end": 210}, "282": {"text": "m o r t i z e d  a n a l y s i s .  Accordingly, another way to provide a performance guarantee is \nto amortize the cost, by keeping track of the total cost of all operations, divided by the \nnumber of operations. In this setting, we can allow some expensive operations, while \nkeeping the average cost of operations low. The prototypical example of this type of \nanalysis is the study of the resizing array data structure for Stack that we considered in \nSection 1.3 (Algorithm 1.1 on page 141). For simplicity, suppose that N is a power of 2. \nStarting with an empty structure, how many array entries are accessed for N consecu-\ntive calls to push()? This quantity is easy to calculate: the number of array accesses is\n198 CHAPTER 1 \u25a0 Fundamentals\n N + 4 + 8 + 16 + ... + 2N = 5N /H11002 4\nThe \ufb01rst term accounts for the array access \nwithin each of the N calls to push(); the sub -\nsequent terms account for the array accesses to \ninitialize the data structure each time it doubles \nin size. Thus the average number of array access-\nes per operation is constant, even though the last \noperation takes linear time. This is known as an \n\u201c amortized\u201d analysis because we spread the cost \nof the few expensive operations, by assigning a \nportion of it to each of a large number of inexpensive operations. VisualAccumulator\nprovides an easy way to illustrate the process, shown above. \nProposition E. In the    resizing array implementation of Stack (Algorithm 1.1), \nthe average number of array accesses for any sequence of operations starting from \nan empty data structure is constant in the worst case.\nProof sketch: For each push() that causes the array to grow ( say from size N to \nsize 2N), ", "start": 210, "end": 211}, "283": {"text": "number of array accesses for any sequence of operations starting from \nan empty data structure is constant in the worst case.\nProof sketch: For each push() that causes the array to grow ( say from size N to \nsize 2N), consider the N/2 /H11002 1 push() operations that most recently caused the \nstack size to grow to k, for k from N/2 + 2 to N. Averaging the 4N array accesses to \ngrow the array with N/2 array accesses (one for each push), we get an average cost \nof 9 array accesses per operation. Proving that the number of array accesses used by \nany sequence of M operations is proportional to M is more intricate (see Exercise \n1.4.32) \nThis kind of analysis is widely applicable. In particular, we use resizing arrays as the \nunderlying data structure for several algorithms that we consider later in this book.\nIt is the task of the algorithm analyst to discover as much relevant information \nabout an algorithm as possible, and it is the task of the applications programmer to \napply that knowledge to develop programs that effectively solve the problems at hand. \nIdeally, we want algorithms that lead to clear and compact code that provides both a \ngood guarantee and good performance on input values of interest. Many of the classic \nalgorithms that we consider in this chapter are important for a broad variety of ap-\nplications precisely because they have these properties. Using them as models, you can \ndevelop good solutions yourself for typical problems that you face while programming. \nAmortized cost of adding to a RandomBag\n0\n0 128\n256\ncost (array references)\nnumber of add() operations\none gray dot\nfor each operation\nred dots give cumulative average 5\n128\n64\n1991.4 \u25a0 Analysis of Algorithms\n  \n \n \n  M e m o r y  As with running time, a program\u2019s ", "start": 211, "end": 212}, "284": {"text": "operations\none gray dot\nfor each operation\nred dots give cumulative average 5\n128\n64\n1991.4 \u25a0 Analysis of Algorithms\n  \n \n \n  M e m o r y  As with running time, a program\u2019s memory usage connects directly to the \nphysical world: a substantial amount of your computer\u2019s circuitry enables your pro-\ngram to store values and later retrieve them. The more values you need to have stored \nat any given instant, the more circuitry you need. Y ou probably are aware of limits on \nmemory usage on your computer (even more so than for time) because you probably \nhave paid extra money to get more memory. \nMemory usage is well-de\ufb01ned for Java on your computer (every value requires pre -\ncisely the same amount of memory each time that you run your program), but Java is \nimplemented on a very wide range of computational devices, and memory consump-\ntion is implementation-dependent. For economy, we use the word typical to signal that \nvalues are subject to machine dependencies. \nOne of Java\u2019s most signi\ufb01cant features is its memory allocation system, \nwhich is supposed to relieve you from having to worry about memory. \nCertainly, you are well-advised to take advantage of this feature when ap-\npropriate. Still, it is your responsibility to know, at least approximately, \nwhen a program\u2019s memory requirements will prevent you from solving a \ngiven problem.\nAnalyzing memory usage is much easier than analyzing running time, \nprimarily because not as many program statements are involved (just dec-\nlarations) and because the analysis reduces complex objects to the primi-\ntive types, whose memory usage is well-de\ufb01ned and simple to understand: \nwe can count up the number of variables and weight them by the number \nof bytes according to their type. For example, since the Java int data type \nis the set of integer values between /H110022,147,483,648 ", "start": 212, "end": 212}, "285": {"text": "to understand: \nwe can count up the number of variables and weight them by the number \nof bytes according to their type. For example, since the Java int data type \nis the set of integer values between /H110022,147,483,648 and 2,147,483,647, a \ngrand total of 232 different values, typical Java implementations use 32 bits \nto represent int values. Similar considerations hold for other primitive types: typical \nJava implementations use 8-bit bytes, representing each char value with 2 bytes (16 \nbits), each int value with 4  bytes (32 bits), each double and each long value with 8 \nbytes (64 bits), and each boolean value with 1 byte (since computers typically access \nmemory one byte at a time). Combined with knowledge of the amount of memory \navailable, you can calculate limitations from these values. For example, if you have 1GB \nof memory on your computer (1 billion bytes), you cannot \ufb01t more than about 32 mil-\nlion int values or 16 million double values in memory at any one time.\nOn the other hand, analyzing memory usage is subject to various differences in ma-\nchine hardware and in Java implementations, so you should consider the speci\ufb01c ex-\namples that we give as indicative of how you might go about determining memory \nusage when warranted, not the \ufb01nal word for your computer. For example, many data \nstructures involve representation of machine addresses, and the amount of memory \ntype bytes\nboolean 1\nbyte 1\nchar 2\nint 4\nfloat 4\nlong 8\ndouble 8\nTypical memory \nrequirements for \n  p r i m i t i v e  t y p e s\n200 CHAPTER 1 \u25a0 Fundamentals\n needed for a machine address varies from machine to \nmachine. For consistency, we assume that 8 bytes ", "start": 212, "end": 213}, "286": {"text": "p r i m i t i v e  t y p e s\n200 CHAPTER 1 \u25a0 Fundamentals\n needed for a machine address varies from machine to \nmachine. For consistency, we assume that 8 bytes are \nneeded to represent addresses, as is typical for  64-bit \narchitectures that are now widely used, recognizing \nthat many older machines use a  32-bit architecture that \nwould involve just 4 bytes per machine address.\n  O b j e c t s .  To  d e te r m i n e  t h e  m e m o r y  u s a g e  o f  a n  o b j e c t , \nwe add the amount of memory used by each instance \nvariable to the overhead associated with each object, \ntypically 16 bytes. The overhead includes a reference to \nthe object\u2019s class, garbage collection information, and \nsynchronization information. Moreover, the memory \nusage is typically padded to be a multiple of 8 bytes \n(machine words, on a 64-bit machine). For example, \nan Integer object uses 24 bytes (16 bytes of overhead, \n4 bytes for its int instance variable, and 4 bytes of \npadding). Similarly, a Date (page 91) object also uses 32 \nbytes: 16 bytes of overhead, 4 bytes for each of its three \nint instance variables, and 4 bytes of padding. A ref-\nerence to an object typically is a memory address and \nthus uses 8 bytes of memory. For example, a Counter\n(page 89) object uses 32 bytes: 16 bytes of overhead, 8 \nbytes for its String instance variable (a reference), 4 \nbytes for its int instance variable, and 4 bytes of pad-\nding. When we account for the memory for a reference, \nwe account separately for the memory for the object \nitself, ", "start": 213, "end": 213}, "287": {"text": "variable (a reference), 4 \nbytes for its int instance variable, and 4 bytes of pad-\nding. When we account for the memory for a reference, \nwe account separately for the memory for the object \nitself, so this total does not count the memory for the \nString value. \n   L i n k e d  l i s t s .  A nested non-static (inner) class such \nas our Node class (page 142) requires an extra 8 bytes of \noverhead (for a reference to the enclosing instance). Thus, a Node object uses 40 bytes \n(16 bytes of object overhead, 8 bytes each for the references to the Item and Node ob-\njects, and 8 bytes for the extra overhead). Thus, since an Integer object uses 24 bytes, a \nstack with N integers built with a linked-list representation (Algorithm 1.2) uses 32 + \n64N bytes, the usual 16 for object overhead for Stack, 8 for its reference instance vari-\nable, 4 for its int instance variable, 4 for padding, and 64 for each entry, 40 for a Node\nand 24 for an Integer.\npublic class Integer\n{\n   private int x;\n...\n}\nTypical object memory requirements\nobject\noverhead\npublic class Node\n{\n   private Item item;\n   private Node next;\n...\n}\npublic class Counter\n{\n   private String name;\n   private int count;\n...\n}\n24 bytesinteger wrapper object \ncounter object\nnode object (inner class)\n32 bytes\nint\nvalue\nint\nvalue\nString\nreference\npublic class Date\n{\n   private int day;\n   private int month;\n   private int year;\n...\n}\ndate object\nx\nobject\noverhead\nname\ncount\n40 bytes\nreferences\nobject\noverhead\nextra\noverhead\nitem\nnext\n32 bytes\nint\nvalues\nobject\noverhead\nyear\nmonth\nday\npadding\npadding\npadding\n2011.4 ", "start": 213, "end": 213}, "288": {"text": "private int year;\n...\n}\ndate object\nx\nobject\noverhead\nname\ncount\n40 bytes\nreferences\nobject\noverhead\nextra\noverhead\nitem\nnext\n32 bytes\nint\nvalues\nobject\noverhead\nyear\nmonth\nday\npadding\npadding\npadding\n2011.4 \u25a0 Analysis of Algorithms\n  \n  A r r a y s .  Typical memor y requirements for various types of  arrays in Java are summa-\nrized in the diagrams on the facing page. Arrays in Java are implemented as objects, \ntypically with extra overhead for the length. An array of primitive-type values typically \nrequires 24 bytes of header information (16 bytes of object overhead, 4 bytes for the \nlength, and 4 bytes of padding) plus the memory needed to store the values. For ex-\nample, an array of N int values uses 24 /H11001 4N bytes (rounded up to be a multiple of \n8), and an array of N double values uses 24 /H11001 8N bytes. An array of objects is an array \nof references to the objects, so we need to add the space for the references to the space \nrequired for the objects. For example, an array of N Date objects (page 91) uses 24 bytes \n(array overhead) plus 8N bytes (references) plus 32 bytes for each object and 4 bytes of \npadding, for a grand total of 24 + 40N bytes. A two-dimensional array is an array of ar-\nrays (each array is an object). For example, a two-dimensional M-by-N array of double\nvalues uses 24 bytes (overhead for the array of arrays) plus 8 M bytes (references to the \nrow arrays) plus M times 16 bytes (overhead from the row arrays) plus M times N times \n8 bytes (for the N double values ", "start": 213, "end": 214}, "289": {"text": "the array of arrays) plus 8 M bytes (references to the \nrow arrays) plus M times 16 bytes (overhead from the row arrays) plus M times N times \n8 bytes (for the N double values in each of the M rows) for a grand total of 8 NM /H11001\n32M /H11001 24 ~ 8NM bytes. When array entries are objects, a similar accounting leads to a \ntotal of 8NM /H11001 32M /H11001 24 ~ 8NM bytes for the array of arrays \ufb01lled with references to \nobjects, plus the memory for the objects themselves.\n  S t r i n g  o b j e c t s .  We account for memor y in Java\u2019s String objects in the same way as \nfor any other object, except that  aliasing is common for strings. The standard String\nimplementation has four instance variables: a reference to a character array (8 bytes) \nand three int values (4 bytes each). The \ufb01rst int value is an offset into the character ar-\nray; the second is a count (the string length). In terms of the instance variable names in \nthe drawing on the facing page, the string that is represented consists of the characters \nvalue[offset] through value[offset + count - 1]. The third int value in String\nobjects is a hash code that saves recomputation in certain circumstances that need not \nconcern us now. Therefore, each String object uses a total of 40 bytes (16 bytes for \nobject overhead plus 4 bytes for each of the three int instance variables plus 8 bytes for \nthe array reference plus 4 bytes of padding). This space requirement is in addition to \nthe space needed for the characters themselves, which are in the array. The space needed \nfor the characters is accounted for separately because the char array is often shared \namong strings. ", "start": 214, "end": 214}, "290": {"text": "of padding). This space requirement is in addition to \nthe space needed for the characters themselves, which are in the array. The space needed \nfor the characters is accounted for separately because the char array is often shared \namong strings. Since String objects are  immutable, this arrangement allows the imple-\nmentation to save memory when String objects have the same underlying value[].\nString values and    substrings. A String of length N typically uses 40 bytes (for the \nString object) plus 24 /H11001 2N bytes (for the array that contains the characters) for a \ntotal of 64 + 2N bytes. But it is typical in string processing to work with substrings, and \nJava\u2019s representation is meant to allow us to do so without having to make copies of \n202 CHAPTER 1 \u25a0 Fundamentals\n int value\n(4 bytes)\n        length\nobject\noverhead\nd\nTypical memory requirements for arrays of int values, double values, objects, and arrays\nN references\n(8N bytes)\nTotal: 24 + 8N + N/H1100332 = 24 + 40N\nDate[] d;\nd = new Date[N];\nfor (int k = 0; k < N; k++)\n{\n   ...\n   a[k] = new Date (...);\n}\n32 bytes\n..\n.\nday\nmonth\nyear\npadding\npadding\nTotal: 24 + 8M + M/H11003(24 + 8N ) = 24 + 32M + 8MN\ndouble[][] t;\nt = new double[M][N];\n.\n.\n.\n24  + 8N bytes\narray of objects array of arrays (two-dimensional array)\nint[] a = new int[N];\n        N\nobject\noverhead\n16 bytes\narray of int values array of double values\nTotal: 24 + 4N (N even)\n16 bytes\nint ", "start": 214, "end": 215}, "291": {"text": "(two-dimensional array)\nint[] a = new int[N];\n        N\nobject\noverhead\n16 bytes\narray of int values array of double values\nTotal: 24 + 4N (N even)\n16 bytes\nint value\n(4 bytes)\nint value\n(4 bytes)\nN int values\n(4N bytes)\ndouble[] c = new double[N];\n        N\nobject\noverhead\nobject\noverhead\nTotal: 24 + 8N\na c\nN double values\n(8N bytes)\n16 bytes\n4 bytes        N\nobject\noverhead\nN double\nvalues\n(8N bytes)\n        N\nobject\noverhead\n        N\nobject\noverhead\n12 bytes\nint value\n(4 bytes)\n        M\nobject\noverhead\nt\nM references\n(8M bytes)\n16 bytes\npadding padding\npadding\npadding\npadding\npadding\nsummary\ntype bytes\nint[] ~4N\ndouble[] ~8N\nDate[] ~40N\ndouble[][] ~8NM \n2031.4 \u25a0 Analysis of Algorithms\n  \nthe string\u2019s characters. When you use the substring()\nmethod, you create a new String object (40 bytes) \nbut reuse the same value[] array, so a substring of an \nexisting string takes just 40 bytes. The character array \ncontaining the original string is aliased in the object for \nthe substring; the offset and length \ufb01elds identify the \nsubstring. In other words, a substring takes constant ex-\ntra memory and forming a substring takes constant time, \neven when the lengths of the string and the substring \nare huge. A naive representation that requires copying \ncharacters to make substrings would take linear time \nand space. The ability to create a substring using space \n(and time) independent of its length is the key to ef\ufb01-\nciency in many basic string-processing algorithms. ", "start": 215, "end": 216}, "292": {"text": "\ncharacters to make substrings would take linear time \nand space. The ability to create a substring using space \n(and time) independent of its length is the key to ef\ufb01-\nciency in many basic string-processing algorithms. \nThese basic mechanisms are effective for esti-\nmating the memory usage of a great many programs, \nbut there are numerous complicating factors that can \nmake the task signi\ufb01cantly more dif\ufb01cult. We have \nalready noted the potential effect of aliasing. More-\nover, memory consumption is a complicated dynamic \nprocess when function calls are involved because the \nsystem memory allocation mechanism plays a more \nimportant role, with more system dependencies. For \nexample, when your program calls a method, the sys -\ntem allocates the memory needed for the method (for \nits local variables) from a special area of memory called \nthe stack (a system pushdown stack), and when the \nmethod returns to the caller, the memory is returned \nto the stack. For this reason, creating arrays or other large objects in recursive programs \nis dangerous, since each recursive call implies signi\ufb01cant memory usage. When you \ncreate an object with new, the system allocates the memory needed for the object from \nanother special area of memory known as the heap (not the same as the binary heap \ndata structure we consider in Section 2.4), and you must remember that every object \nlives until no references to it remain, at which point a system process known as garbage \ncollection reclaims its memory for the heap. Such dynamics can make the task of pre-\ncisely estimating memory usage of a program challenging.\nA String and a substring\nString genome = \"CGCCTGGCGTCTGTAC\";\nString codon  = genome.substring(6, 3);\n 16 \nobject\noverhead\nchar\nvalues\nC  G\nC  C\nT  G\nG ", "start": 216, "end": 216}, "293": {"text": "\"CGCCTGGCGTCTGTAC\";\nString codon  = genome.substring(6, 3);\n 16 \nobject\noverhead\nchar\nvalues\nC  G\nC  C\nT  G\nG  C\nG  T\nC  T\nG  T\nA  C\n        0\n16\nobject\noverhead\ngenome\n        6\n3\nobject\noverhead\n codon\nhash\nhash\n..\n.\n        value\npublic class String\n{\n   private char[] value;\n   private int offset;\n   private int count;\n   private int hash;\n...\n}         offset\n        count\n        hash\nobject\noverhead\n40 bytes\n40 bytes\n40 bytes\n36 bytes\nString object (Java library)\nsubstring example\nreference\nint\nvalues\npadding\npadding\npadding\npadding\n        value\n        value\n204 CHAPTER 1 \u25a0 Fundamentals\n  \nPerspective Good performance is important. An impossibly slow program is al-\nmost as useless as an incorrect one, so it is certainly worthwhile to pay attention to the \ncost at the outset, to have some idea of which kinds of problems you might feasibly \naddress. In particular, it is always wise to have some idea of which code constitutes the \ninner loop of your programs. \nPerhaps the most common mistake made in programming is to pay too much at -\ntention to performance characteristics. Y our \ufb01rst priority is to make your code clear \nand correct. Modifying a program for the sole purpose of speeding it up is best left for \nexperts. Indeed, doing so is often counterproductive, as it tends to create code that is \ncomplicated and dif\ufb01cult to understand.  C. A. R. Hoare (the inventor of quicksort and \na leading proponent of writing clear and correct code) once summarized this idea by \nsaying that \u201cpremature ", "start": 216, "end": 217}, "294": {"text": "dif\ufb01cult to understand.  C. A. R. Hoare (the inventor of quicksort and \na leading proponent of writing clear and correct code) once summarized this idea by \nsaying that \u201cpremature optimization is the root of all evil , \u201d to which  Knuth added the \nquali\ufb01er \u201c(or at least most of it) in programming.\u201d Beyond that, improving the running \ntime is not worthwhile if the available cost bene\ufb01ts are insigni\ufb01cant. For example, im-\nproving the running time of a program by a factor of 10 is inconsequential if the run-\nning time is only an instant. Even when a program takes a few minutes to run, the total \ntime required to implement and debug an improved algorithm might be substantially \nmore than the time required simply to run a slightly slower one\u2014you may as well let \nthe computer do the work. Worse, you might spend a considerable amount of time and \neffort implementing ideas that should in theory improve a program but do not do so \nin practice.\nPerhaps the second most common mistake made in programming is to ignore per -\nformance characteristics. Faster algorithms are often more complicated than brute-\nforce ones, so you might be tempted to accept a slower algorithm to avoid having to \ndeal with more complicated code. However, you can sometimes reap huge savings with \njust a few lines of good code. Users of a surprising number of computer systems lose \nsubstantial time unknowingly waiting for brute-force quadratic algorithms to \ufb01nish \nsolving a problem, when linear or linearithmic algorithms are available that could solve \nthe problem in a fraction of the time. When we are dealing with huge problem sizes, we \noften have no choice but to seek better algorithms.\nWe generally take as implicit the methodolog y descr ibed in this section to estimate \nmemory usage and to develop an order-of-growth hypothesis of the running time ", "start": 217, "end": 217}, "295": {"text": "sizes, we \noften have no choice but to seek better algorithms.\nWe generally take as implicit the methodolog y descr ibed in this section to estimate \nmemory usage and to develop an order-of-growth hypothesis of the running time from \na tilde approximation resulting from a mathematical analysis within a cost model, and \nto check those hypotheses with experiments. Improving a program to make it more \nclear, ef\ufb01cient, and elegant should be your goal every time that you work on it. If you \npay attention to the cost all the way through the development of a program, you will \nreap the bene\ufb01ts every time you use it.\n2051.4 \u25a0 Analysis of Algorithms\n Q&A\n \n \n \nQ. Why not use StdRandom to generate random values instead of maintaining the \ufb01le \n1Mints.txt ?\nA. It is easier to debug code in development and to reproduce experiments. StdRandom\nproduces different values each time it is called, so running a program after \ufb01xing a bug \nmay not test the \ufb01x! Y ou could use the initialize() method in StdRandom to address \nthis problem, but a reference \ufb01le such as 1Mints.txt makes it easier to add test cases \nwhile debugging. Also, different programmers can compare performance on different \ncomputers, without worrying about the input model. Once you have debugged a pro -\ngram and have a good idea of how it performs, it is certainly worthwhile to test it on \nrandom data. For example, DoublingTest and DoublingRatio take this approach.\nQ. I ran DoublingRatio on my computer, but the results were not as consistent as in \nthe book. Some of the ratios were not close to 8. Why?\nA. That is why we discussed \u201ccaveats\u201d on page 195.  Most likely, your computer\u2019s operating \nsystem decided to do something else during the experiment. ", "start": 217, "end": 218}, "296": {"text": "ratios were not close to 8. Why?\nA. That is why we discussed \u201ccaveats\u201d on page 195.  Most likely, your computer\u2019s operating \nsystem decided to do something else during the experiment. One way to mitigate such \nproblems is to invest more time in more experiments. For example, you could change \nDoublingTest to run the experiments 1,000 times for each N, giving a much more ac-\ncurate estimate for the running time for each size (see Exercise 1.4.39).\nQ. What, exactly, does \u201cas N grows\u201d mean in the de\ufb01nition of the tilde notation?\nA. The formal de\ufb01nition of f(N)  ~ g(N) is limN\u2192\u221e f (N )/g (N ) = 1.\nQ. I\u2019ve seen other notations for describing order of growth. What\u2019s the story?\nbig-Oh\u201d notation is widely used: we say that f (N ) is O(g (N )) if there exist A. The \u201c   \nconstants c and N0 such that | f (N )| < c g (N ) for all N > N0. This notation is very use-\nful in providing asymptotic  upper bounds on the performance of algorithms, which is \nimportant in the theory of algorithms. But it is not useful for predicting performance \nor for comparing algorithms.\nQ. Why not?\nA. The primary reason is that it describes only an upper bound on the running time. \nActual performance might be much better. The running time of an algorithm might \nbe both O (N 2) and ~ a N log N. As a result, it cannot be used to justify tests like our \ndoubling ratio test (see Proposition C on page 193).\n206 CHAPTER 1 \u25a0 Fundamentals\n Q. So why is the big-Oh notation so widely used?\nA. ", "start": 218, "end": 219}, "297": {"text": "cannot be used to justify tests like our \ndoubling ratio test (see Proposition C on page 193).\n206 CHAPTER 1 \u25a0 Fundamentals\n Q. So why is the big-Oh notation so widely used?\nA. It facilitates development of bounds on the order of growth, even for complicated \nalgorithms for which more precise analysis might not be feasible. Moreover, it is com-\npatible with the \u201c big-Omega\u201d and  \u201c big-Theta\u201d notations that theoretical computer sci-\nentists use to classify algorithms by bounding their worst-case performance. We say \nthat f (N ) is /H9024(g (N )) if there exist constants c and N0 such that | f (N )| > c g (N ) for N \n> N0; and if  f (N ) is O(g (N )) and /H9024(g (N )), we say that  f (N ) is /H9008(g (N )). The \u201cbig-\nOmega\u201d notation is typically used to describe a lower bound on the worst case, and the \n\u201cbig-Theta\u201d notation is typically used to describe the performance of algorithms that \nare optimal in the sense that no algorithm can have better asymptotic worst-case order \nof growth. Optimal algorithms are certainly worth considering in practical applica-\ntions, but there are many other considerations, as you will see. \nQ. Aren\u2019t  upper bounds on asymptotic performance important?\nA. Ye s , b u t  we  p re f e r  to  d i s c u s s  p re c i s e  re s u l t s  i n  te r m s  o f  f re q u e n c y  o f  s t a te m e n t  ex -\nceution with respect to cost models, because they provide more information about \nalgorithm performance and because deriving such ", "start": 219, "end": 219}, "298": {"text": "f  f re q u e n c y  o f  s t a te m e n t  ex -\nceution with respect to cost models, because they provide more information about \nalgorithm performance and because deriving such results is feasible for the algorithms \nthat we discuss. For example, we say \u201c ThreeSum uses ~ N 3/2 array accesses\u201d and \u201cthe \nnumber of times cnt++ is executed in ThreeSum is ~N 3/6 in the worst case, \u201d which is a \nbit more verbose but much more informative than the statement \u201cthe running time of \nThreeSum is O (N 3). \u201d\nQ. When the order of growth of the running time of an algorithm is N log N, the dou-\nbling test will lead to the hypothesis that the running time is ~ a N for a constant a. Isn\u2019t \nthat a problem?\nA. We have to be careful not to tr y to infer that the exper imental data implies a par -\nticular mathematical model, but when we are just predicting performance, this is not \nreally a problem. For example, when N is between 16,000 and 32,000, the plots of 14 N\nand N lg N are very close to one another. The data \ufb01ts both curves. As N increases, the \ncurves become closer together. It actually requires some care to experimentally check \nthe hypothesis that an algorithm\u2019s running time is linearithmic but not linear.\nQ. Does int[] a = new int[N] count as N array accesses (to initialize entries to 0)?\nA. Most likely yes, so we make that assumption in this book, though a sophisticated \ncompiler implementation might try to avoid this cost for huge sparse arrays.\n2071.4 \u25a0 Analysis of Algorithms\n EXERCISES\n1.4.1  Show that the number of different triples that can be chosen from N items is \nprecisely ", "start": 219, "end": 220}, "299": {"text": "try to avoid this cost for huge sparse arrays.\n2071.4 \u25a0 Analysis of Algorithms\n EXERCISES\n1.4.1  Show that the number of different triples that can be chosen from N items is \nprecisely N (N/H110021)(N/H110022)/6. Hint : Use mathematical induction.\n1.4.2 Modify ThreeSum to work properly even when the int values are so large that \nadding two of them might cause over\ufb02ow.\n1.4.3 Modify DoublingTest to use StdDraw to produce plots like the standard and \nlog-log plots in the text, rescaling as necessary so that the plot always \ufb01lls a substantial \nportion of the window.\n1.4.4 Develop a table like the one on page 181 for TwoSum.\n1.4.5 Give tilde approximations for the following quantities:\na. N /H11001 1\nb. 1 /H11001 1/N \nc. (1 /H11001 1/N )(1 /H11001 2/N )\nd. 2N 3/H11002 15 N 2 /H11001 N\ne. lg(2N )/lg N\nf. lg(N 2 + 1) / lg N\ng.  N 100 / 2N\n1.4.6 Give the order of growth (as a function of N ) of the running times of each of the \nfollowing code fragments:\na.   int sum = 0;\n    for (int n = N; n > 0; n /= 2)\n      for(int i = 0; i < n; i++)\n         sum++;\nb.   int sum = 0;\n    for (int i = 1 i < N; i *= 2)\n        for (int j = 0; j < i; j++)\n ", "start": 220, "end": 220}, "300": {"text": "n; i++)\n         sum++;\nb.   int sum = 0;\n    for (int i = 1 i < N; i *= 2)\n        for (int j = 0; j < i; j++)\n            sum++;\n208 CHAPTER 1 \u25a0 Fundamentals\n c.    int sum = 0;\n    for (int i = 1 i < N; i *= 2)\n       for (int j = 0; j < N; j++)\n           sum++;\n1.4.7 Analyze ThreeSum under a cost model that counts arithmetic operations (and \ncomparisons) involving the input numbers.\n1.4.8 Write a program to determine the number pairs of values in an input \ufb01le that \nare equal. If your \ufb01rst try is quadratic, think again and use Arrays.sort() to develop \na linearithmic solution.\n1.4.9  Give a formula to predict the running time of a program for a problem of size N\nwhen doubling experiments have shown that the doubling factor is 2b and the running \ntime for problems of size N0 is T.\n1.4.10 Modify binary search so that it always returns the element with the smallest \nindex that matches the search element (and still guarantees logarithmic running time).\n1.4.11 Add an instance method howMany() to StaticSETofInts (page 99) that \ufb01nds the \nnumber of occurrences of a given key in time proportional to log N in the worst case.\n1.4.12 Write a program that, given two sorted arrays of N int values, prints all ele-\nments that appear in both arrays, in sorted order. The running time of your program \nshould be proportional to N in the worst case.\n1.4.13 Using the assumptions developed in the text, give the amount of memory need-\ned to represent an object of each of the following types:\na. ", "start": 220, "end": 221}, "301": {"text": "your program \nshould be proportional to N in the worst case.\n1.4.13 Using the assumptions developed in the text, give the amount of memory need-\ned to represent an object of each of the following types:\na. Accumulator\nb. Transaction\nc. FixedCapacityStackOfStrings with capacity C and N entries\nd. Point2D\ne. Interval1D\nf. Interval2D\ng. Double\n2091.4 \u25a0 Analysis of Algorithms\n CREATIVE PROBLEMS\n1.4.14  4-sum. Develop an algorithm for the 4-sum problem.\n1.4.15  Faster 3-sum. As a warmup, develop an implementation TwoSumFaster that \nuses a linear algorithm to count the pairs that sum to zero after the array is sorted (in-\nstead of the binary-search-based linearithmic algorithm). Then apply a similar idea to \ndevelop a quadratic algorithm for the 3-sum problem.\n1.4.16   Closest pair (in one dimension). Write a program that, given an array a[] of N \ndouble values, \ufb01nds a closest pair : two values whose difference is no greater than the \nthe difference of any other pair (in absolute value). The running time of your program \nshould be linearithmic in the worst case.\n1.4.17   Farthest pair (in one dimension). Write a program that, given an array a[] of N \ndouble values, \ufb01nds a farthest pair : two values whose difference is no smaller than the \nthe difference of any other pair (in absolute value). The running time of your program \nshould be linear in the worst case.\n1.4.18      Local minimum of an array. Write a program that, given an array a[] of N dis-\ntinct integers, \ufb01nds a local minimum: an index i such that a[i-1] < a[i] ", "start": 221, "end": 222}, "302": {"text": "Local minimum of an array. Write a program that, given an array a[] of N dis-\ntinct integers, \ufb01nds a local minimum: an index i such that a[i-1] < a[i] < a[i+1]. \nYo u r  p ro g r a m  s h o u l d  u s e  ~ 2 l g  N compares in the worst case..\nAnswer : Examine the middle value a[N/2] and its two neighbors a[N/2 - 1]  and \na[N/2 + 1]. If a[N/2] is a local minimum, stop; otherwise search in the half with the \nsmaller neighbor.\n1.4.19  Local minimum of a matrix. Given an N-by-N array a[] of N  2 distinct inte -\ngers, design an algorithm that runs in time proportional to N to \ufb01nd a local minimum: \na pair of indices i and j such that a[i][j] < a[i+1][j] , a[i][j] < a[i][j+1] , \na[i][j] < a[i-1][j], and a[i][j] < a[i][j-1]. The running time of your pro -\ngram should be proportional to N in the worst case.\n1.4.20     Bitonic search. An array is bitonic if it is comprised of an increasing sequence \nof integers followed immediately by a decreasing sequence of integers. Write a program \nthat, given a bitonic array of N distinct int values, determines whether a given integer \nis in the array. Y our program should use ~3lg N compares in the worst case.\n1.4.21  Binary search on distinct values. Develop an implementation of binary search \nfor StaticSETofInts (see page 98) where the running time of contains() is guaranteed \n210 ", "start": 222, "end": 222}, "303": {"text": "the worst case.\n1.4.21  Binary search on distinct values. Develop an implementation of binary search \nfor StaticSETofInts (see page 98) where the running time of contains() is guaranteed \n210 CHAPTER 1 \u25a0 Fundamentals\n  \nto be ~ lg R, where R is the number of different integers in the array given as argument \nto the constructor.\n1.4.22  Binary search with only addition and subtraction. [Mihai Patrascu] Write a \nprogram that, given an array of N distinct int values in ascending order, determines \nwhether a given integer is in the array.  Y ou may use only additions and subtractions \nand a constant amount of extra memory. The running time of your program should be \nproportional to log N in the worst case.\nAnswer : Instead of searching based on powers of two (binary search), use Fibonacci \nnumbers (which also grow exponentially). Maintain the current search range to be the \ninterval [i, i + F k] and keep F k and F k\u20131 in two variables. At each step compute Fk\u20132 via \nsubtraction, check element i + Fk\u20132 , and update the current range to either [ i, i + Fk\u20132] \nor [i + Fk\u20132, i + Fk\u20132 + Fk\u20131].\n1.4.23   Binary search for a fraction. Devise a method that uses a logarithmic number of \nqueries of the form Is the number less than x? to \ufb01nd a rational number p/q such that 0 \n< p < q < N. Hint : Two fractions with denominators less than N cannot differ by more \nthan 1/N 2.\n1.4.24  Throwing eggs from a building. Suppose that you have an N-story building and \nplenty of eggs. Suppose also that an egg is broken ", "start": 222, "end": 223}, "304": {"text": "differ by more \nthan 1/N 2.\n1.4.24  Throwing eggs from a building. Suppose that you have an N-story building and \nplenty of eggs. Suppose also that an egg is broken if it is thrown off \ufb02oor F or higher, \nand unhurt otherwise. First, devise a strategy to determine the value of F such that the \nnumber of broken eggs is ~lg N when using ~lg N throws, then \ufb01nd a way to reduce the \ncost to ~2lg F.\n1.4.25  Throwing two eggs from a building. Consider the previous question, but now \nsuppose you only have two eggs, and your cost model is the number of throws. Devise \na strategy to determine F such that the number of throws is at most 2 \u221aN, then \ufb01nd a \nway to reduce the cost to ~c \u221aF. This is analogous to a situation where search hits (egg \nintact) are much cheaper than misses (egg broken).\n1.4.26     3-collinearity. Suppose that you have an algorithm that takes as input N dis-\ntinct points in the plane and can return the number of triples that fall on the same line. \nShow that you can use this algorithm to solve the 3-sum problem. Strong hint : Use \nalgebra to show that (a, a3), (b, b3),  and (c, c3) are collinear if and only if a + b + c = 0.\n1.4.27  Queue with two stacks. Implement a queue with two stacks so that each queue \n2111.4 \u25a0 Analysis of Algorithms\n  \n \noperation takes a constant amortized number of stack operations. Hint : If you push \nelements onto a stack and then pop them all, they appear in reverse order. If you repeat \nthis process, they\u2019re ", "start": 223, "end": 224}, "305": {"text": "Analysis of Algorithms\n  \n \noperation takes a constant amortized number of stack operations. Hint : If you push \nelements onto a stack and then pop them all, they appear in reverse order. If you repeat \nthis process, they\u2019re now back in order.\n1.4.28  Stack with a queue. Implement a stack with a single queue so that each stack \noperations takes a linear number of queue operations. Hint : T o delete an item, get all \nof the elements on the queue one at a time, and put them at the end, except for the last \none which you should delete and return. (This solution is admittedly very inef\ufb01cient.)\n1.4.29   Steque with two stacks. Implement a steque with two stacks so that each steque \noperation (see Exercise 1.3.32) takes a constant amortized number of stack operations.\n1.4.30   Deque with a stack and a steque. Implement a deque with a stack and a steque \n(see Exercise 1.3.32) so that each deque operation takes a constant amortized number \nof stack and steque operations.\n1.4.31     Deque with three stacks. Implement a deque with three stacks so that each \ndeque operation takes a constant amortized number of stack operations.\n1.4.32    Amortized analysis. Prove that, starting from an empty stack, the number of ar-\nray accesses used by any sequence of M operations in the resizing array implementation \nof Stack is proportional to M. \n1.4.33  Memory requirements on a  32-bit machine. Give the memory requirements \nfor Integer, Date, Counter, int[], double[], double[][], String, Node, and Stack\n(linked-list representation) for a 32-bit machine. Assume that references are 4 bytes, \nobject overhead is 8 bytes, and padding is to a multiple ", "start": 224, "end": 224}, "306": {"text": "double[], double[][], String, Node, and Stack\n(linked-list representation) for a 32-bit machine. Assume that references are 4 bytes, \nobject overhead is 8 bytes, and padding is to a multiple of 4 bytes.\n1.4.34  Hot or cold. Yo u r  g o a l  i s  t o  g u e s s  a  s e c re t  i n t e g e r  b e t w e e n  1  a n d  N. You repeat-\nedly guess integers between 1 and N. After each guess you learn if your guess equals the \nsecret integer (and the game stops). Otherwise, you learn if the guess is hotter (closer to) \nor colder (farther from) the secret number than your previous guess. Design an algo-\nrithm that \ufb01nds the secret number in at most ~2 lg N guesses. Then design an algorithm \nthat \ufb01nds the secret number in at most ~ 1 lg N guesses.\nCREATIVE PROBLEMS  (continued)\n212 CHAPTER 1 \u25a0 Fundamentals\n 1.4.35   Time costs for pushdown stacks. Justify the entries in the table below, which \nshows typical time costs for various pushdown stack implementations, using a cost \nmodel that counts both data references (references to data pushed onto the stack, either \nan array reference or a reference to an object\u2019s instance variable) and objects created.\n1.4.36    Space usage for pushdown stacks. Justify the entries in the table below, which \nshows typical space usage for various pushdown stack implementations. Use a static \nnested class for linked-list nodes to avoid the non-static nested class overhead. \ndata structure item type\ncost to push N int values\ndata  references objects created\nlinked list\nint 2 N N\nInteger 3 N 2N\nresizing ", "start": 224, "end": 225}, "307": {"text": "for linked-list nodes to avoid the non-static nested class overhead. \ndata structure item type\ncost to push N int values\ndata  references objects created\nlinked list\nint 2 N N\nInteger 3 N 2N\nresizing array\nint ~5 N lg N\nInteger ~5 N ~N \nTime costs for pushdown stacks  (various implementations)\ndata structure item type space usage for N int values (bytes)\nlinked list\nint ~ 32 N\nInteger ~ 64 N\nresizing array\nint between\n~4 N and ~16 N\nInteger between\n~32 N and ~56 N\nSpace  usage in pushdown stacks (various implementations)\n2131.4 \u25a0 Analysis of Algorithms\n EXPERIMENTS\n \n \n1.4.37   Autoboxing performance penalty. Run experiments to determine the perfor-\nmance penalty on your machine for using autoboxing and auto-unboxing. Develop an \nimplementation FixedCapacityStackOfInts and use a client such as DoublingRatio\nto compare its performance with the generic FixedCapacityStack<Integer>, for a \nlarge number of push() and pop() operations. \n1.4.38    Naive 3-sum implementation. Run experiments to evaluate the following im -\nplementation of the inner loop of ThreeSum:\n      for (int i = 0; i < N; i++)\n         for (int j = 0; j < N; j++)\n            for (int k = 0; k < N; k++)\n               if (i < j && j < k)\n                  if (a[i] + a[j] + a[k] == 0)\n                     cnt++;\nDo so by developing a version of DoublingTest that computes the ratio of the running \ntimes of this program and ThreeSum.\n1.4.39    Improved accuracy for doubling test. Modify DoublingRatio to take a second \ncommand-line argument that speci\ufb01es ", "start": 225, "end": 226}, "308": {"text": "DoublingTest that computes the ratio of the running \ntimes of this program and ThreeSum.\n1.4.39    Improved accuracy for doubling test. Modify DoublingRatio to take a second \ncommand-line argument that speci\ufb01es the number of calls to make to timeTrial() for \neach value of N. Run your program for 10, 100, and 1,000 trials and comment on the \nprecision of the results.\n1.4.40    3-sum for random values. Formulate and validate a hypothesis describing the \nnumber of triples of N random int values that sum to 0. If you are skilled in math-\nematical analysis, develop an appropriate mathematical model for this problem, where \nthe values are uniformly distributed between \u2013M and M, where M is not small.\n1.4.41    Running times. Estimate the amount of time it would take to run TwoSumFast, \nTwoSum, ThreeSumFast and ThreeSum on your computer to solve the problems for a \ufb01le \nof 1 million numbers. Use DoublingRatio to do so.\n1.4.42    Problem sizes. Estimate the size of the largest value of P for which you can run \nTwoSumFast, TwoSum, ThreeSumFast, and ThreeSum on your computer to solve the \nproblems for a \ufb01le of 2P thousand numbers. Use DoublingRatio to do so.\n1.4.43  Resizing arrays versus linked lists. Run experiments to validate the hypothesis \nthat resizing arrays are faster than linked lists for stacks (see Exercise 1.4.35 and Exer-\ncise 1.4.36). Do so by developing a version of DoublingRatio that computes the ratio \n214 CHAPTER 1 \u25a0 Fundamentals\n  \nof the running times of the two programs.\n1.4.44     Birthday problem. Write a program that takes an integer N from the command \nline ", "start": 226, "end": 227}, "309": {"text": "DoublingRatio that computes the ratio \n214 CHAPTER 1 \u25a0 Fundamentals\n  \nof the running times of the two programs.\n1.4.44     Birthday problem. Write a program that takes an integer N from the command \nline and uses StdRandom.uniform() to generate a random sequence of integers be -\ntween 0 and N \u2013 1. Run experiments to validate the hypothesis that the number of \nintegers generated before the \ufb01rst repeated value is found is ~\u221a/H9266N/2.\n1.4.45     Coupon collector problem. Generating random integers as in the previous exer-\ncise, run experiments to validate the hypothesis that the number of integers generated \nbefore all possible values are generated is ~N HN.\n2151.4 \u25a0 Analysis of Algorithms\n 1.5     CASE STUDY: UNION-FIND\nTo illustrate our basic approach to developing and analyzing algorithms, we now \nconsider a detailed example. Our purpose is to emphasize the following themes.\n\u25a0 Good algorithms can make the difference between being able to solve a practical \nproblem and not being able to address it at all.\n\u25a0  An ef\ufb01cient algorithm can be as simple to code as an inef\ufb01cient one.\n\u25a0 Understanding the performance characteristics of an implementation can be an \ninteresting and satisfying intellectual challenge.\n\u25a0  The scienti\ufb01c method is an important tool in helping us choose among different \nmethods for solving the same problem.\n\u25a0  An iterative re\ufb01nement process can lead to increasingly ef\ufb01cient algorithms.\nThese themes are reinforced throughout the book. This prototypical example sets the \nstage for our use of the same general methodology for many other problems. \nThe problem that we consider is not a toy problem; it is a fundamental compu-\ntational task, and the solution that we develop is of use in a variety of applications, \nfrom percolation in physical chemistry to connectivity in communications networks. ", "start": 227, "end": 228}, "310": {"text": "is not a toy problem; it is a fundamental compu-\ntational task, and the solution that we develop is of use in a variety of applications, \nfrom percolation in physical chemistry to connectivity in communications networks. \nWe star t w ith a simple solution, then seek to understand that solution\u2019s performance \ncharacteristics, which help us to see how to improve the algorithm. \n   D y n a m i c  c o n n e c t i v i t y  We star t w ith the follow ing problem speci\ufb01cation: The \ninput is a sequence of pairs of integers, where each integer represents an object of some \ntype and we are to interpret the pair p q as meaning \u201cp is connected to q.\u201d We  a s s um e  \nthat \u201cis connected to\u201d is an      equivalence relation, which means that it is\n\u25a0 Re\ufb02exive : p is connected to p.  \n\u25a0 Symmetric : If p is connected to q, then q is connected to p.  \n\u25a0 \n \n   Transitive : If p is connected to q and q is connected to r, then p is connected to r.\nAn  equivalence relation partitions the objects into equivalence classes. In this case, two \nobjects are in the same equivalence class if and only if they are connected. Our goal is \nto write a program to \ufb01lter out extraneous pairs (pairs where both objects are in the \nsame equivalence class) from the sequence. In other words, when the program reads a \npair p q from the input, it should write the pair to the output only if the pairs it has \nseen to that point do not imply that p is connected to q. If the previous pairs do imply \nthat p is connected to q, then the program should ignore the pair p q and proceed to \nread in the next pair. The \ufb01gure on the facing page gives an example ", "start": 228, "end": 228}, "311": {"text": "q. If the previous pairs do imply \nthat p is connected to q, then the program should ignore the pair p q and proceed to \nread in the next pair. The \ufb01gure on the facing page gives an example of this process. To \nachieve the desired goal, we need to devise a data structure that can remember suf\ufb01cient \n216\n  \ninformation about the pairs it has seen to be able to decide whether or not a new pair of \nobjects is connected. Informally, we refer to the task of designing such a method as the \ndynamic connectivity problem. This problem arises applications such as the following:\nNetworks. The integers might represent computers in a large network, and the pairs \nmight represent connections in the network. Then, our program determines whether \nwe need to establish a new direct connection for p and q to be able \nto communicate or whether we can use existing connections to \nset up a communications path. Or, the integers might represent \ncontact sites in an electrical circuit, and the pairs might represent \nwires connecting the sites. Or, the integers might represent people \nin a social network, and the pairs might represent friendships. In \nsuch applications, we might need to process millions of objects \nand billions of connections.   \nVa r i a b l e - n a m e  e q u i va l e n c e . In certain programming environ -\nments, it is possible to declare two variable names as being equiv-\nalent (references to the same object). After a sequence of such dec-\nlarations, the system needs to be able to determine whether two \ngiven names are equivalent. This application is an early one (for \nthe  FORTRAN programming language) that motivated the devel-\nopment of the algorithms that we are about to consider.  \nMathematical sets. On a more abstract level, you can think of \nthe integers as belonging to mathematical sets. When we process a \npair ", "start": 228, "end": 229}, "312": {"text": "motivated the devel-\nopment of the algorithms that we are about to consider.  \nMathematical sets. On a more abstract level, you can think of \nthe integers as belonging to mathematical sets. When we process a \npair p q, we are asking whether they belong to the same set. If not, \nwe unite p\u2019s set and q\u2019s set, putting them in the same set.\nTo fix ideas, we will use networking terminology for the rest of \nthis section and refer to the objects as sites, the pairs as connec-\ntions, and the equivalence classes as  connected components, or just \ncomponents for short. For simplicity, we assume that we have N\nsites with integer names, from 0 to N-1. We do so without loss of \ngenerality because we shall be considering a host of algorithms in \nChapter 3 that can associate arbitrary names with such integer \nidenti\ufb01ers in an ef\ufb01cient manner.\nA larger example that gives some indication of the dif\ufb01culty of the connectivity \nproblem is depicted in the \ufb01gure at the top of the next page. Y ou can quickly identify \nthe component consisting of a single site in the left middle of the diagram and the \nDynamic connectivity example\n0 1 2 3 4\n5 6 7 8 9  \n4 3\n3 8\n6 5\n9 4\n2 1\n8 9\n5 0\n7 2\n6 1\n1 0\n6 7\n2 components\ndon\u2019t print\npairs that\nare already\nconnected\n2171.5 \u25a0 Case Study: Union-Find\n  \n \n \n \ncomponent consisting of \ufb01ve sites at the bottom left, but you might have dif\ufb01culty veri-\nfying that all of the other sites are connected to one another. For a program, the task ", "start": 229, "end": 230}, "313": {"text": "\n \n \ncomponent consisting of \ufb01ve sites at the bottom left, but you might have dif\ufb01culty veri-\nfying that all of the other sites are connected to one another. For a program, the task is \neven more dif\ufb01cult, because it has to work just with site names and connections and has \nno access to the geometric placement of sites in the diagram. How can we tell quickly \nwhether or not any given two sites in such a network are connected? \n T h e  \ufb01 r s t  t a s k  t h a t  w e  f a c e  i n  d e v e l o p i n g  a n  a l g o r i t h m  i s  t o  s p e c i f y  t h e  p r o b l e m  i n  a  \nprecise manner.  The more we require of an algorithm, the more time and space we may \nexpect it to need to \ufb01nish the job. It is impossible to quantify this relationship a priori, \nand we often modify a problem speci\ufb01cation on \ufb01nding that it is dif\ufb01cult or expensive \nto solve or, in happy circumstances, on \ufb01nding that an algorithm can provide informa-\ntion more useful than what was called for in the original speci\ufb01cation. For example, our \nMedium connectivity example (625 sites, 900 edges, 3 connected components) \nconnected\ncomponent\n218 CHAPTER 1 \u25a0 Fundamentals\n  \n \nconnectivity problem speci\ufb01cation requires only that our program be able to determine \nwhether or not any given pair p q is connected, and not that it be able to demonstrate a \nset of connections that connect that pair. Such a requirement makes the problem more \ndif\ufb01cult and leads us to a different family of algorithms, ", "start": 230, "end": 231}, "314": {"text": "given pair p q is connected, and not that it be able to demonstrate a \nset of connections that connect that pair. Such a requirement makes the problem more \ndif\ufb01cult and leads us to a different family of algorithms, which we consider in Section \n4.1.\nTo  s p e c i f y  t h e  p ro b l e m , w e  d e ve l o p  a n  A P I  t h a t  e n c a p s u l a t e s  t h e  b a s i c  o p e r a t i o n s  \nthat we need: initialize, add a connection between two sites, identify the component \ncontaining a site, determine whether two sites are in the same component, and count \nthe number of components. Thus, we articulate the following API:\npublic class    UF \nUF(int N) initialize N sites with integer names (0 to N-1) \nvoid union(int p, int q) add connection between p and q \nint find(int p) component identifier for p (0 to N-1) \nboolean connected(int p, int q) return true if p and q are in the same component\nint count() number of components\nUnion-find API\n \nThe union() operation merges two components if the two sites are in different com-\nponents, the find() operation returns an integer component identi\ufb01er for a given site, \nthe connected() operation determines whether two sites are in the same component, \nand the count() method returns the number of components. We start with N compo-\nnents, and each union() that merges two different components decrements the num-\nber of components by 1.\nAs we shall soon see, the development of an algorithmic solution for dynamic con -\nnectivity thus reduces to the task of developing an implementation of this API. Every \nimplementation has to\n\u25a0 ", "start": 231, "end": 231}, "315": {"text": "num-\nber of components by 1.\nAs we shall soon see, the development of an algorithmic solution for dynamic con -\nnectivity thus reduces to the task of developing an implementation of this API. Every \nimplementation has to\n\u25a0  De\ufb01ne a data structure to represent the known connections\n\u25a0 Develop ef\ufb01cient union(), find(),  connected(), and count() implementa-\ntions that are based on that data structure\nAs usual, the nature of the data structure has a direct impact on the ef\ufb01ciency of the \nalgorithms, so data structure and algorithm design go hand in hand. The API already \nspeci\ufb01es the convention that both sites and components will be identi\ufb01ed by int val-\nues between 0 and N-1, so it makes sense to use a site-indexed array id[] as our basic \n2191.5 \u25a0 Case Study: Union-Find\n data structure to represent the components. We always use the name of one of the sites \nin a component as the component identi\ufb01er, so you can think of each component as \nbeing represented by one of its sites. Initially, we start with N components, each site in \nits own component, so we initialize id[i] to i for all i from 0 to N-1. For each site \ni, we keep the information needed by find() to determine the component contain-\ning i in id[i], using various algorithm-dependent strategies. All of our implementa -\ntions use a one-line implementation of connected() that returns the boolean value \nfind(p) == find(q). \nIn summary, our starting point is Algorithm 1.5 on the facing \npage. We maintain two instance variables, the count of components \nand the array id[]. Implementations of find() and union() are \nthe topic of the remainder of this section. \nTo  te s t  t h ", "start": 231, "end": 232}, "316": {"text": "\npage. We maintain two instance variables, the count of components \nand the array id[]. Implementations of find() and union() are \nthe topic of the remainder of this section. \nTo  te s t  t h e  u t i l i t y  o f  t h e  A P I  a n d  to  p rov i d e  a  b a s i s  f o r  d e ve l o p-\nment, we include a client in main() that uses it to solve the dy-\nnamic connectivity problem. It reads the value of N followed by a \nsequence of pairs of integers (each in the range 0 to N-1), calling \nfind() for each pair: If the two sites in the pair are already con-\nnected, it moves on to the next pair; if they are not, it calls union()\nand prints the pair. Before considering implementations, we also \nprepare test data: the \ufb01le tinyUF.txt contains the 11 connections \namong 10 sites used in the small example illustrated on page 217, the \n\ufb01le mediumUF.txt contains the 900 connections among 625 sites \nillustrated on page 218, and the \ufb01le largeUF.txt is an example with \n2 million connections among 1 millions sites. Our goal is to be able \nto handle inputs such as largeUF.txt in a reasonable amount of \ntime.\nTo  a n a l y z e  t h e  a l g o r i t h m s , w e  f o c u s  o n  t h e  n u m b e r  o f  t i m e s  e a c h  \nalgorithm accesses an array entry. By doing so, we are implicitly for-\nmulating the hypothesis that the running times of the algorithms \non a particular machine are \nwithin a ", "start": 232, "end": 232}, "317": {"text": "t i m e s  e a c h  \nalgorithm accesses an array entry. By doing so, we are implicitly for-\nmulating the hypothesis that the running times of the algorithms \non a particular machine are \nwithin a constant factor of \nthis quantity. This hypothesis is immediate from \nthe code, is not dif\ufb01cult to validate through ex-\nperimentation, and provides a useful starting \npoint for comparing algorithms, as we will see.\n% more tinyUF.txt \n10 \n4 3 \n3 8 \n6 5 \n9 4 \n2 1 \n8 9 \n5 0 \n7 2 \n6 1 \n1 0 \n6 7\n% more mediumUF.txt \n625 \n528 503 \n548 523 \n... \n[900 connections]\n% more largeUF.txt \n1000000 \n786321 134521 \n696834 98245 \n... \n[2000000 connections]\nUnion-find   cost model. When \nstudying algorithms to imple-\nment the union-\ufb01nd API, we \ncount array accesses (the num-\nber of times an array entry is \naccessed, for read or write). \n220 CHAPTER 1 \u25a0 Fundamentals\n ALGORITHM 1.5   Union-find implementation\npublic class  UF \n{\n   private int[] id;     // access to component id (site indexed)\n   private int count;    // number of components\n   public UF(int N)\n   {  // Initialize component id array.\n      count = N;\n      id = new int[N];\n      for (int i = 0; i < N; i++)\n         id[i] = i;\n   }\n   public int count()\n   {  return count;  }\n   public boolean connected(int p, int q)\n   {  return find(p) == find(q);  }\n   public int  find(int p)\n   public void union(int p, ", "start": 232, "end": 233}, "318": {"text": "count()\n   {  return count;  }\n   public boolean connected(int p, int q)\n   {  return find(p) == find(q);  }\n   public int  find(int p)\n   public void union(int p, int q)\n   // See page 222 (quick-find),page 224 (quick-union) andpage 228 (weighted).\n   public static void main(String[] args)\n   {  // Solve dynamic connectivity problem on StdIn.\n      int N = StdIn.readInt();              // Read number of sites.\n      UF uf = new UF(N);                    // Initialize N components.\n      while (!StdIn.isEmpty())\n      {\n         int p = StdIn.readInt();  \n         int q = StdIn.readInt();           // Read pair to connect.\n         if (uf.connected(p, q)) continue;  // Ignore if connected.\n         uf.union(p, q);                    // Combine components\n         StdOut.println(p + \" \" + q);       //   and print connection.\n      }\n      StdOut.println(uf.count() + \" components\");\n   }\n}\nOur UF implementations are based on this code, which maintains an array of integers id[] such \nthat the find() method returns the same integer for every site in each connected component. The \nunion() method must maintain this invariant. \n% java UF < tinyUF.txt \n4 3 \n3 8 \n6 5 \n9 4 \n2 1 \n5 0 \n7 2 \n6 1 \n2 components\n2211.5 \u25a0 Case Study: Union-Find Implementations We shall consider three different implementations, all based on \nusing the site-indexed id[] array, to determine whether two sites are in the same con-\nnected component. \n  Q u i c k - \ufb01 n d .  One approach is to maintain the invariant that p and q are connected \nif and only if id[p] is equal to ", "start": 233, "end": 234}, "319": {"text": "the same con-\nnected component. \n  Q u i c k - \ufb01 n d .  One approach is to maintain the invariant that p and q are connected \nif and only if id[p] is equal to id[q]. In other words, all sites in a component must \nhave the same value in id[]. This method is called quick-\ufb01nd because find(p) just \nreturns id[p], which immediately implies that connected(p, q) reduces to just the \ntest id[p] == id[q]  and returns true if and only \nif p and q are in the same component. T o maintain \nthe invariant for the call union(p, q), we \ufb01rst check \nwhether they are already in the same component, in \nwhich case there is nothing to do. Otherwise, we are \nfaced with the situation that all of the id[] entries \ncorresponding to sites in the same component as \np have one value and all of the id[] entries corre-\nsponding to sites in the same component as q have \nanother value. T o combine the two components into \none, we have to make all of the id[] entries cor -\nresponding to both sets of sites the same value, as \nshown in the example at right. T o do so, we go through the array, changing all the entries \nwith values equal to id[p] to the value id[q]. We could have decided to change all the \nentries equal to id[q] to the value id[p]\u2014the choice between these two alternatives \nis arbitrary. The code for find() and \nunion() based on these descriptions, \ngiven at left, is straightforward. A full \ntrace for our development client with \nour sample test data tinyUF.txt is \nshown on the next page. \npublic int find(int p) \n{  return id[p];  }\npublic void union(int p, int q) \n{ ", "start": 234, "end": 234}, "320": {"text": "our development client with \nour sample test data tinyUF.txt is \nshown on the next page. \npublic int find(int p) \n{  return id[p];  }\npublic void union(int p, int q) \n{  // Put p and q into the same component.\n   int pID = find(p);\n   int qID = find(q);\n   // Nothing to do if p and q are already\n        in the same component.\n   if (pID == qID) return;\n   // Rename p\u2019s component to q\u2019s name.\n   for (int i = 0; i < id.length; i++)\n       if (id[i] == pID) id[i] = qID;\n   count--; \n}\n Q u i c k - f i n d\nQuick-find overview \nfind examines id[5] and id[9]\np q   0 1 2 3 4 5 6 7 8 9\n5 9   1 1 1 8 8 1 1 1 8 8\np q   0 1 2 3 4 5 6 7 8 9\n5 9   1 1 1 8 8 1 1 1 8 8\n      8 8 8 8 8 8 8 8 8 8\nunion has to change all 1s to 8s\n222 CHAPTER 1 \u25a0 Fundamentals\n  \n Q u i c k - \ufb01 n d  a n a l y s i s .  The find() operation is certainly quick, as it only accesses the \nid[] array once in order to complete the operation. But quick-\ufb01nd is typically not use-\nful for large problems because union() needs to scan through the whole id[] array for \neach input pair.\nProposition ", "start": 234, "end": 235}, "321": {"text": "\nid[] array once in order to complete the operation. But quick-\ufb01nd is typically not use-\nful for large problems because union() needs to scan through the whole id[] array for \neach input pair.\nProposition F. The quick-\ufb01nd algorithm uses one array access for each call to \nfind() and between N + 3 and 2N + 1 array accesses for each call to union() that \ncombines two components.\nProof: Immediate from the code. Each call to connected() tests two entries in the \nid[] array, one for each of the two calls to find(). Each call to union() that com-\nbines two components does so by making two calls to find(), testing each of the N \nentries in the id[] array, and changing between 1 and N /H11002 1 of them.\n \n \nIn particular, suppose that we use quick-\ufb01nd for the \ndynamic connectivity problem and wind up with a \nsingle component. This requires at least N/H110021 calls to \nunion(), and, consequently, at least (N/H110013)(N/H110021) ~ \nN 2 array accesses\u2014we are led immediately to the hy-\npothesis that dynamic connectivity with quick-\ufb01nd \ncan be a quadratic-time process. This analysis gener -\nalizes to say that quick-\ufb01nd is quadratic for typical \napplications where we end up with a small number of \ncomponents. You can easily validate this hypothesis \non your computer with a doubling test (see Exercise \n1.5.23 for an instructive example). Modern comput-\ners can execute hundreds of millions or billions of in-\nstructions per second, so this cost is not noticeable if \nN is small, but we also might \ufb01nd ourselves with mil-\nlions or billions of sites and connections to process in \na modern application, as represented by our test \ufb01le \nlargeUF.txt. ", "start": 235, "end": 235}, "322": {"text": "noticeable if \nN is small, but we also might \ufb01nd ourselves with mil-\nlions or billions of sites and connections to process in \na modern application, as represented by our test \ufb01le \nlargeUF.txt. If you are still not convinced and feel \nthat you have a particularly fast computer, try using \nquick-\ufb01nd to determine the number of components \nimplied by the pairs in largeUF.txt. The inescap -\nable conclusion is that we cannot feasibly solve such \na problem using the quick-\ufb01nd algorithm, so we seek \nbetter algorithms.Quick-find trace\n            id[]\np q  0 1 2 3 4 5 6 7 8 9\n4 3  0 1 2 3 4 5 6 7 8 9  \n     0 1 2 3 3 5 6 7 8 9  \n3 8  0 1 2 3 3 5 6 7 8 9  \n     0 1 2 8 8 5 6 7 8 9  \n6 5  0 1 2 8 8 5 6 7 8 9  \n     0 1 2 8 8 5 5 7 8 9  \n9 4  0 1 2 8 8 5 5 7 8 9  \n     0 1 2 8 8 5 5 7 8 8  \n2 1  0 1 2 8 8 5 5 7 8 8  \n     0 1 1 8 8 5 5 7 8 8  \n8 9  0 1 1 ", "start": 235, "end": 235}, "323": {"text": "8 8 5 5 7 8 8  \n     0 1 1 8 8 5 5 7 8 8  \n8 9  0 1 1 8 8 5 5 7 8 8  \n5 0  0 1 1 8 8 5 5 7 8 8  \n     0 1 1 8 8 0 0 7 8 8  \n7 2  0 1 1 8 8 0 0 7 8 8  \n     0 1 1 8 8 0 0 1 8 8  \n6 1  0 1 1 8 8 0 0 1 8 8  \n     1 1 1 8 8 1 1 1 8 8  \n1 0  1 1 1 8 8 1 1 1 8 8  \n6 7  1 1 1 8 8 1 1 1 8 8\nid[p] and id[q]\nmatch, so no change\n     id[p] and id[q] differ, so\nunion() changes entries equal\nto id[p] to id[q] (in red)\n2231.5 \u25a0 Case Study: Union-Find\n  \n    Q u i c k - u n i o n .  The next algorithm that we consider is a complementary method that \nconcentrates on speeding up the union() operation. It is based on the same data \nstructure\u2014the site-indexed id[] ar -\nray\u2014but we interpret the values dif -\nferently, to de\ufb01ne more complicated \nstructures. Speci\ufb01cally, the id[] entry \nfor each site is the ", "start": 235, "end": 236}, "324": {"text": "\nstructure\u2014the site-indexed id[] ar -\nray\u2014but we interpret the values dif -\nferently, to de\ufb01ne more complicated \nstructures. Speci\ufb01cally, the id[] entry \nfor each site is the name of another \nsite in the same component (possibly \nitself)\u2014we refer to this connection as \na link. To implement find(), we start \nat the given site, follow its link to an-\nother site, follow that site\u2019s link to yet \nanother site, and so forth, following \nlinks until reaching a root, a site that \nhas a link to itself (which is guaran-\nteed to happen, as you will see). Two \nsites are in the same component if and \nonly if this process leads them to the \nsame root. T o validate this process, we need union(p, q) to maintain this invariant, \nwhich is easily arranged: we follow links to \ufb01nd the roots associated with p and q, then \nrename one of the components by linking one of these roots to the other; hence the \nname quick-union. Again, we have an arbitrary choice of whether to rename the com -\nponent containing p or the component containing q; the implementation above re -\nnames the one containing p. The \n\ufb01gure on the next page shows a \ntrace of the quick-union algo -\nrithm for tinyUF.txt. This trace \nis best understood in terms of the \ngraphical representation depict -\ned at left, which we consider next.\nQuick-union overview \np q   0 1 2 3 4 5 6 7 8 9\n5 9   1 1 1 8 3 0 5 1 8 8\np q   0 1 2 3 4 5 6 7 8 9\n5 9   1 ", "start": 236, "end": 236}, "325": {"text": "1 1 8 3 0 5 1 8 8\np q   0 1 2 3 4 5 6 7 8 9\n5 9   1 1 1 8 3 0 5 1 8 8\n      1 8 1 8 3 0 5 1 8 8\n0\n5 4\n1 8\n6\n2 7 3 9\n0\n5\n4\n1\n8\n6\n2 7\n3 9\nfind(5) is\nid[id[id[5]]]\nfind(9) is\nid[id[9]]\nfind has to follow links to the root\nunion changes just one link\nid[] is parent-link representation\nof a forest of trees\nroot\n8 becomes parent of 1\nprivate int find(int p) \n{  // Find component name.\n   while (p != id[p]) p = id[p];\n   return p; \n}\npublic void union(int p, int q) \n{  // Give p and q the same root.\n   int pRoot = find(p);\n   int qRoot = find(q);\n   if (pRoot == qRoot) return;\n   id[pRoot] = qRoot;\n   count--; \n}\n Q u i c k - u n i o n\n224 CHAPTER 1 \u25a0 Fundamentals\n  \n  F o r e s t - o f - t r e e s  r e p r e s e n t a t i o n .  The code for quick-union is compact, but a bit opaque. \nRepresenting sites as nodes (labeled circles) and links as arrows from one node to an -\nother gives a graphical representation of the data structure that makes it relatively easy \nto understand the operation of the algorithm. The resulting structures ", "start": 236, "end": 237}, "326": {"text": "\nRepresenting sites as nodes (labeled circles) and links as arrows from one node to an -\nother gives a graphical representation of the data structure that makes it relatively easy \nto understand the operation of the algorithm. The resulting structures are trees\u2014in \ntechnical terms, our id[] array \nis a    parent-link representation \nof a forest (set) of trees. T o sim-\nplify the diagrams, we often omit \nboth the arrowheads in the links \n(because they all point upwards) \nand the self-links in the roots \nof the trees. The forests corre-\nsponding to the id[] array for \ntinyUF.txt are shown at right. \nWhen we start at the node cor-\nresponding to any site and follow \nlinks, we eventually end up at the \nroot of the tree containing that \nnode. We can prove this prop-\nerty to be true by induction: It is \ntrue after the array is initialized \nto have every node link to itself, \nand if it is true before a given \nunion() operation, it is certainly \ntrue afterward. Thus, the find() \nmethod on page 224 returns the \nname of the site at the root (so \nthat connected() checks wheth-\ner two sites are in the same tree). \nThis representation is useful for \nthis problem because the nodes \ncorresponding to two sites are in \nthe same tree if and only if the \nsites are in the same component. \nMoreover, the trees are not dif\ufb01cult to build: the union() implementation on page 224 \ncombines two trees into one in a single statement, by making the root of one the parent \nof the other. \nQuick-union trace (with corresponding forests of trees)\n             id[]\np q   0 1 2 3 4 5 6 7 8 9\n4 3   0 1 2 ", "start": 237, "end": 237}, "327": {"text": "\nQuick-union trace (with corresponding forests of trees)\n             id[]\np q   0 1 2 3 4 5 6 7 8 9\n4 3   0 1 2 3 4 5 6 7 8 9  \n     0 1 2 3 3 5 6 7 8 9  \n3 8   0 1 2 3 3 5 6 7 8 9  \n     0 1 2 8 3 5 6 7 8 9\n6 5   0 1 2 8 3 5 6 7 8 9  \n     0 1 2 8 3 5 5 7 8 9\n9 4   0 1 2 8 3 5 5 7 8 9  \n     0 1 2 8 3 5 5 7 8 8\n2 1   0 1 2 8 3 5 5 7 8 8  \n     0 1 1 8 3 5 5 7 8 8\n8 9   0 1 1 8 3 5 5 7 8 8  \n5 0   0 1 1 8 3 5 5 7 8 8  \n     0 1 1 8 3 0 5 7 8 8\n7 2   0 1 1 8 3 0 5 7 8 8  \n     0 1 1 8 3 0 5 1 8 8\n6 1   0 ", "start": 237, "end": 237}, "328": {"text": "1 1 8 3 0 5 7 8 8  \n     0 1 1 8 3 0 5 1 8 8\n6 1   0 1 1 8 3 0 5 1 8 8  \n     1 1 1 8 3 0 5 1 8 8\n1 0   1 1 1 8 3 0 5 1 8 8\n6 7   1 1 1 8 3 0 5 1 8 8\n2251.5 \u25a0 Case Study: Union-Find\n  \n \n \n \n \n Q u i c k - u n i o n  a n a l y s i s .  The quick-union algorithm would seem to be faster than the \nquick-\ufb01nd algorithm, because it does not have to go through the entire array for each \ninput pair; but how much faster is it? Analyzing the \ncost of quick-union is more dif\ufb01cult than it was for \nquick-\ufb01nd, because the cost is more dependent on \nthe nature of the input. In the best case, find() just \nneeds one array access to \ufb01nd the identi\ufb01er associ-\nated with a site, as in quick-\ufb01nd; in the worst case, it \nneeds 2N + 1 array accesses, as for 0 in the example \nat left (this count is conservative since compiled \ncode will typically not do an array access for the \nsecond reference to id[p] in the while loop). Ac-\ncordingly, it is not dif\ufb01cult to construct a best-case \ninput for which the running time of our dynamic \nconnectivity client is linear; on the other hand it is \nalso not dif\ufb01cult ", "start": 237, "end": 238}, "329": {"text": "Ac-\ncordingly, it is not dif\ufb01cult to construct a best-case \ninput for which the running time of our dynamic \nconnectivity client is linear; on the other hand it is \nalso not dif\ufb01cult to construct a worst-case input for \nwhich the running time is quadratic (see the dia-\ngram at left and Proposition G below). Fortunate-\nly, we do not need to face the problem of analyzing \nquick union and we will not dwell on comparative \nperformance of quick-\ufb01nd and quick-union be -\ncause we will next examine another variant that is far more ef\ufb01cient than either. For the \nmoment, you can regard quick-union as an improvement over quick-\ufb01nd because it \nremoves quick-\ufb01nd\u2019s main liability (that union() always takes linear time). This differ-\nence certainly represents an improvement for typical data, but quick-union still has the \nliability that we cannot guarantee it to be substantially faster than quick-\ufb01nd in every \ncase (for certain input data, quick-union is no faster than quick-\ufb01nd). \n D e f i n i t i o n .  The size of a    tree is its number of nodes. The    depth of a node in a tree \nis the number of links on the path from it to the root. The  height of a tree is the \nmaximum depth among its nodes. \nProposition G.  The number of array accesses used by find() in quick-union is 1 \nplus the twice the depth of the node corresponding to the given site. The number \nof array accesses used by union() and connected() is the cost of the two find()\noperations (plus 1 for union() if the given sites are in different trees).\nProof: Immediate from the code.\nQuick-union worst case\n         id[]\np q   0 1 2 3 ", "start": 238, "end": 238}, "330": {"text": "two find()\noperations (plus 1 for union() if the given sites are in different trees).\nProof: Immediate from the code.\nQuick-union worst case\n         id[]\np q   0 1 2 3 4 ...\n0 1   0 1 2 3 4 ...  \n      1 1 2 3 4 ...  \n0 2   0 1 2 3 4 ...  \n      1 2 2 3 4 ...  \n0 3   0 1 2 3 4 ...  \n      1 2 3 3 4 ...\n0 4   0 1 2 3 4 ...  \n      1 2 3 4 4 ...\n .\n .\n . \n...\n...\n...\n...\n...\n0 1 2 3 4\n0\n1 2 3 4\n0\n1\n2 3 4\n0\n1\n2\n3 4\n0\n1\n2\n3\n4\ndepth 4\n226 CHAPTER 1 \u25a0 Fundamentals\n Again, suppose that we use quick-union for the dynamic connectivity problem and \nwind up with a single component. An immediate implication of Proposition G is that \nthe running time is quadratic, in the worst case. Suppose that the input pairs come \nin the order 0-1, then 0-2, then 0-3, and so forth. After N /H11002 1 such pairs, we have N\nsites all in the same set, and the tree that is formed by the quick-union algorithm has \nheight N /H11002 1, with 0 linking to 1, which links to 2, which links to 3, and so forth (see \nthe diagram on the facing page). By Proposition G, the number of array accesses for \nthe ", "start": 238, "end": 239}, "331": {"text": "1, with 0 linking to 1, which links to 2, which links to 3, and so forth (see \nthe diagram on the facing page). By Proposition G, the number of array accesses for \nthe union() operation for the pair 0 i is exactly 2i + 2 (site 0 is at depth i and site i at \ndepth 0). Thus, the total number of array accesses for the find() operations for these \nN pairs is 2 (1 + 2 + . . . + N ) ~N 2.\n   W e i g h t e d  q u i c k - u n i o n .  Fortunately, there is an \neasy modi\ufb01cation to quick-union that allows us \nto guarantee that bad cases such as this one do \nnot occur. Rather than arbitrarily connecting the \nsecond tree to the \ufb01rst for union(), we keep track \nof the size of each tree and always connect the \nsmaller tree to the larger. This change requires \nslightly more code and another array to hold the \nnode counts, as shown on page 228, but it leads to \nsubstantial improvements in ef\ufb01ciency. We refer \nto this algorithm as the weighted quick-union al-\ngorithm. The forest of trees constructed by this \nalgorithm for tinyUF.txt is shown in the \ufb01gure \nat left on the top of page 229. Even for this small example, the tree height is substantially \nsmaller than the height for the unweighted version.\nWe ig hted quick-union analy sis. The \ufb01gure at right on the top of page 229 illustrates \nthe worst case for weighted quick union, when the sizes of the trees to be merged by \nunion() are always equal (and a power \nof 2). These tree structures look complex, \nbut they have the simple property that \nthe ", "start": 239, "end": 239}, "332": {"text": "case for weighted quick union, when the sizes of the trees to be merged by \nunion() are always equal (and a power \nof 2). These tree structures look complex, \nbut they have the simple property that \nthe height of a tree of 2 n nodes is n. Fur-\nthermore, when we merge two trees of 2 n\nnodes, we get a tree of 2 n/H110011 nodes, and we \nincrease the height of the tree to n/H110011. This \nobservation generalizes to provide a proof \nthat the weighted algorithm can guarantee \nlogarithmic performance.\nsmaller\ntree\nlarger\ntree\nq\np\nsmaller\ntreelarger\ntree\nq\np\nsmaller\ntree larger\ntree\nq\np\nsmaller\ntree larger\ntree\nq\np\nWeighted quick-union \nweighted\nquick-union\nalways chooses the\nbetter alternative\nmight put the\nlarger tree lower\n% java WeightedQuickUnionUF < mediumUF.txt \n528 503 \n548 523 \n... \n3 components\n% java WeightedQuickUnionUF < largeUF.txt \n786321 134521 \n696834 98245 \n... \n6 components\n2271.5 \u25a0 Case Study: Union-Find\n  ALGORITHM 1.5  (continued) Union-find implementation (weighted quick-union)\npublic class    WeightedQuickUnionUF \n{ \n   private int[] id;     // parent link (site indexed)\n   private int[] sz;     // size of component for roots (site indexed)\n   private int count;    // number of components\n   public WeightedQuickUnionUF(int N)\n   { \n      count = N;\n      id = new int[N];\n      for (int i = 0; i < N; i++) id[i] = i;\n      sz = new int[N];\n      for ", "start": 239, "end": 240}, "333": {"text": "N)\n   { \n      count = N;\n      id = new int[N];\n      for (int i = 0; i < N; i++) id[i] = i;\n      sz = new int[N];\n      for (int i = 0; i < N; i++) sz[i] = 1;\n   }\n   public int count()\n   {  return count;  }\n   public boolean connected(int p, int q)\n   {  return find(p) == find(q);  }\n   private int find(int p)\n   {  // Follow links to find a root.\n      while (p != id[p]) p = id[p];\n      return p;\n   }\n   public void union(int p, int q)\n   {  \n      int i = find(p);\n      int j = find(q);\n      if (i == j) return;\n     // Make smaller root point to larger one.\n      if   (sz[i] < sz[j]) { id[i] = j; sz[j] += sz[i]; }\n      else                 { id[j] = i; sz[i] += sz[j]; }\n      count--;\n   } \n}\nThis code is best understood in terms of the forest-of-trees representation described in the text. We \nadd a site-indexed array sz[] as an instance variable so that union() can link the root of the smaller \ntree to the root of the larger tree. This addition makes it feasible to address large problems.\n228 CHAPTER 2 \u25a0 Fundamentals Weighted quick-union traces (forests of trees)\nreference input\np q\n4 3\n3 8\n6 5\n9 4\n2 1\n8 9\n5 0\n7 2\n6 1\n1 0\n6 7\nworst-case input\np q\n0 1\n2 3\n4 5\n6 7\n0 2\n4 ", "start": 240, "end": 241}, "334": {"text": "0\n7 2\n6 1\n1 0\n6 7\nworst-case input\np q\n0 1\n2 3\n4 5\n6 7\n0 2\n4 6\n0 4\nProposition H.  The  depth of any node in a forest built by weighted quick-union for \nN sites is at most lg N.\nProof: We prove a stronger fact by (strong) induction: The heig ht of  ever y tree of  \nsize k in the forest is at most lg k. The base case follows from the fact that the tree \nheight is 0 when k is 1. By the inductive hypothesis, assume that the tree height of a \ntree of size i is at most lg i for all i < k. When we combine a tree of size i with a tree \nof size j with i /H11349 j and i /H11001 j = k, we increase the depth of each node in the smaller set \nby 1, but they are now in a tree of size i /H11001 j = k, so the property is preserved because \n1+ lg i = lg(i /H11001 i ) /H11349 lg(i /H11001 j ) = lg k.\n2291.5 \u25a0 Case Study: Union-Find\n Corollary. For weighted quick-union with N sites, the worst-case order of growth \nof the cost of find(), connected(), and union() is log N.\nProof. Each operation does at most a constant number of array accesses for each \nnode on the path from a node to a root in the forest.\n \nFor dynamic connectivity, the practical implication of Proposition H and its corollary \nis that weighted quick-union is the only one of the three algorithms that can feasibly \nbe used for huge practical problems. The weighted quick-union algorithm uses at ", "start": 241, "end": 242}, "335": {"text": "the practical implication of Proposition H and its corollary \nis that weighted quick-union is the only one of the three algorithms that can feasibly \nbe used for huge practical problems. The weighted quick-union algorithm uses at most\nc M lg N array accesses to process M connections among N sites for a small constant c. \nThis result is in stark contrast to our \ufb01nding that quick-\ufb01nd always (and quick-union \nsometimes) uses at least MN array accesses. Thus, with weighted quick-union, we can \nguarantee that we can solve huge practical dynamic connectivity problems in a reason-\nable amount of time. For the price of a few extra lines of code, we get a program that \ncan be millions of times faster than the simpler algorithms for the huge dynamic con-\nnectivity problems that we might encounter in practical applications.\nA 100-site example is shown on the top of this page. It is evident from this diagram \nthat relatively few nodes fall far from the root with weighted quick-union. Indeed it is \nfrequently the case that a 1-node tree is merged with a larger tree, which puts the node \njust one link from the root. Empirical studies on huge problems tell us that weighted \nquick-union typically solves practical problems in constant time per operation. We \ncould hardly expect to \ufb01nd a more ef\ufb01cient algorithm.\nQuick-union and weighted quick-union (100 sites, 88 union() operations)\nweighted\nquick-union\naverage depth: 1.52\naverage depth: 5.11\n230 CHAPTER 1 \u25a0 Fundamentals\n  \n \n \n \n \nOptimal algorithms. Can we \ufb01nd an algorithm that has guaranteed constant-time-\nper-operation performance? This question is an extremely dif\ufb01cult one that plagued \nresearchers for many years. In pursuit of an answer, a number of variations of quick-\nunion and weighted quick-union have been ", "start": 242, "end": 243}, "336": {"text": "constant-time-\nper-operation performance? This question is an extremely dif\ufb01cult one that plagued \nresearchers for many years. In pursuit of an answer, a number of variations of quick-\nunion and weighted quick-union have been studied. For example, the following meth-\nod, known as  path compression, is easy to implement. Ideally, we would like every node \nto link directly to the root of its tree, but we do not want to pay the price of changing a \nlarge number of links, as we did in the quick-\ufb01nd algorithm. We can approach the ideal \nsimply by making all the nodes that we do examine directly link to the root. This step \nseems drastic at \ufb01rst blush, but it is easy to implement, and there is nothing sacrosanct \nabout the structure of these trees: if we can modify them to make the algorithm more \nef\ufb01cient, we should do so. T o implement path compression, we just add another loop to \nfind() that sets the id[] entry corresponding to each node encountered along the way \nto link directly to the root. The net result is to \ufb02atten the trees almost completely, ap -\nproximating the ideal achieved by the quick-\ufb01nd algorithm. The method is simple and \neffective, but you are not likely to be able to discern any improvement over weighted \nquick-union in a practical situation (see Exercise 1.5.24). Theoretical results about \nthe situation are extremely complicated and quite remarkable. Weighted quick union \nwith path compression is optimal but not  quite constant-time per operation.  That is, not \nonly is weighted quick-\ufb01nd with path compression not constant-time per operation \nin the worst case (   amortized), but also there exists no algorithm that can guarantee to \nperform each union-\ufb01nd operation in amortized constant time (under the very general \n\u201ccell ", "start": 243, "end": 243}, "337": {"text": "not constant-time per operation \nin the worst case (   amortized), but also there exists no algorithm that can guarantee to \nperform each union-\ufb01nd operation in amortized constant time (under the very general \n\u201ccell probe\u201d model of computation). Weighted quick-union with path compression is \nvery close to the best that we can do for this problem.\nalgorithm\norder of growth for N sites (worst case)\nconstructor union find\nquick-find N N 1\nquick-union N tree height tree height\nweighted quick-union N lg N lg N\nweighted quick-union with \n  p a t h  c o m p r e s s o n  N very, very nearly, but not quite 1 (amortized )\n(see Exercise 1.5.13)\nimpossible N 1 1\n P e r f o r m a n c e  c h a r a c t e r i s t i c s  o f  u n i o n - f i n d  a l g o r i t h m s\n2311.5 \u25a0 Case Study: Union-Find\n  \nAmortized cost plots. As with any data type implementation, it is worthwhile to run \nexperiments to test the validity of our performance hypotheses for typical clients, as dis-\ncussion in Section 1.4. The \ufb01gure at left shows \ndetails of the performance of the algorithms for \nour dynamic connectivity development client \nwhen solving our 625-site connectivity example \n(mediumUF.txt). Such diagrams are easy to pro-\nduce (see Exercise 1.5.16): For the i th connec-\ntion processed, we maintain a variable cost that \ncounts the number of array accesses (to id[] or \nsz[]) and a variable total that is the sum of \nthe total number of array accesses so far. Then \nwe plot a gray dot ", "start": 243, "end": 244}, "338": {"text": "maintain a variable cost that \ncounts the number of array accesses (to id[] or \nsz[]) and a variable total that is the sum of \nthe total number of array accesses so far. Then \nwe plot a gray dot at (i, cost) and a red dot \nat (i, total/i). The red dots are the average \ncost per operation, or amortized cost. These \nplots provide good insights into algorithm be-\nhavior. For quick-\ufb01nd, every union() opera -\ntion uses at least 625 accesses (plus 1 for each \ncomponent merged, up to another 625) and \nevery connected() operation uses 2 accesses. \nInitially, most of the connections lead to a call \non union(), so the cumulative average hovers \naround 625; later, most connections are calls to \nconnected() that cause the call to union() to \nbe skipped, so the cumulative average decreas-\nes, but still remains relatively high. (Inputs that \nlead to a large number of connected() calls that \ncause union() to be skipped will exhibit signi\ufb01-\ncantly better performance\u2014see Exercise 1.5.23 \nfor an example). For quick-union, all operations \ninitially require only a few array accesses; eventu-\nally, the height of the trees becomes a signi\ufb01cant \nfactor and the amortized cost grows noticably. \nFor weighted quick-union, the tree height stays \nsmall, none of the operations are expensive, and \nthe amortized cost is low. These experiments \nvalidate our conclusion that weighted quick-union is certainly worth implementing \nand that there is not much further room for improvement for practical problems.\nCost of all operations (625 sites)\nquick-find\nquick-union\nweighted quick-union\n0\n0 900\n1300\n458\nnumber of array references\nnumber of connections\n0\n100\n0\n20\none ", "start": 244, "end": 244}, "339": {"text": "problems.\nCost of all operations (625 sites)\nquick-find\nquick-union\nweighted quick-union\n0\n0 900\n1300\n458\nnumber of array references\nnumber of connections\n0\n100\n0\n20\none gray dot\nfor each connection\nprocessed by client\nred dots give\ncumulative average\nunion() operations\nuse at least 625 references \nconnected() operations\nuse exactly 2  array accesses\nfind() operations\nbecome expensive \nno expensive operations\n20\n8\n232 CHAPTER 1 \u25a0 Fundamentals\n  \n \n \nPerspective Each of the UF implementations that we considered is an improvement \nover the previous in some intuitive sense, but the process is arti\ufb01cially smooth because \nwe have the bene\ufb01t of hindsight in looking over the development of the algorithms as \nthey were studied by researchers over the years. The implementations are simple and \nthe problem is well speci\ufb01ed, so we can evaluate the various algorithms directly by run-\nning empirical studies. Furthermore, we can use these studies to validate mathematical \nresults that quantify the performance of these algorithms. When possible, we follow the \nsame basic steps for fundamental problems throughout the book that we have taken for \nunion\u2013\ufb01nd algorithms in this section, some of which are highlighted in this list: \n\u25a0  Decide on a complete and speci\ufb01c problem statement, including identifying \nfundamental abstract operations that are intrinsic to the problem and an API.\n\u25a0 Carefully develop a succinct implementation for a straightforward algorithm, \nusing a well-thought-out development client and realistic input data.\n\u25a0 Know when an implementation could not possibly be used to solve problems on \nthe scale contemplated and must be improved or abandoned.\n\u25a0\n \nDevelop improved implementations through a process of stepwise re\ufb01nement, \nvalidating the ef\ufb01cacy of ideas for improvement through empirical analysis, \nmathematical analysis, or both.\n\u25a0 ", "start": 244, "end": 245}, "340": {"text": "improved or abandoned.\n\u25a0\n \nDevelop improved implementations through a process of stepwise re\ufb01nement, \nvalidating the ef\ufb01cacy of ideas for improvement through empirical analysis, \nmathematical analysis, or both.\n\u25a0 Find high-level abstract representations of data structures or algorithms in op-\neration that enable effective high-level design of improved versions. \n\u25a0 Strive for worst-case performance guarantees when possible, but accept good \nperformance on typical data when available.\n\u25a0\n  \n \n \nKnow when to leave further improvements for detailed in-depth study to skilled \nresearchers and move on to the next problem.\nThe potential for spectacular performance improvements for practical problems such \nas those that we saw for union\u2013\ufb01nd makes algorithm design a compelling \ufb01eld of study.\nWhat other design activities hold the potential to reap savings factors of millions or \nbillions, or more?\nDeveloping an ef\ufb01cient algorithm is an intellectually satisfying activity that can have \ndirect practical payoff. As the dynamic connectivity problem indicates, a simply stated \nproblem can lead us to study numerous algorithms that are not only both useful and \ninteresting, but also intricate and challenging to understand. We shall encounter many \ningenious algorithms that have been developed over the years for a host of practical \nproblems. As the scope of applicability of computational solutions to scienti\ufb01c and \ncommercial problems widens, so also grows the importance of being able to use ef -\n\ufb01cient algorithms to solve known problems and of being able to develop ef\ufb01cient solu-\ntions to new problems.\n2331.5 \u25a0 Case Study: Union-Find\n Q&A\n \nQ. I\u2019d like to add a delete() method to the API that allows clients to delete connec-\ntions. Any advice on how to proceed?\nA. No one has devised an algorithm as simple and ef\ufb01cient as the ones in this section \nthat can handle deletions. This theme recurs throughout ", "start": 245, "end": 246}, "341": {"text": "delete connec-\ntions. Any advice on how to proceed?\nA. No one has devised an algorithm as simple and ef\ufb01cient as the ones in this section \nthat can handle deletions. This theme recurs throughout this book. Several of the data \nstructures that we consider have the property that deleting something is much more \ndif\ufb01cult than adding something.\nQ. What is the  cell-probe model?\nA. A model of computation where we only count accesses to a random-access memory \nlarge enough to hold the input and consider all other operations to be free.\n234 CHAPTER 1 \u25a0 Fundamentals\n EXERCISES\n \n \n1.5.1  Show the contents of the id[] array and the number of times the ar -\nray is accessed for each input pair when you use quick-\ufb01nd for the sequence \n9-0 3-4 5-8 7-2 2-1 5-7 0-3 4-2.\n1.5.2 Do Exercise 1.5.1, but use quick-union (page 224). In addition, draw the forest of \ntrees represented by the id[] array after each input pair is processed.\n1.5.3 Do Exercise 1.5.1, but use weighted quick-union (page 228). \n1.5.4 Show the contents of the sz[] and id[] arrays and the number of array accesses \nfor each input pair corresponding to the weighted quick-union examples in the text   \n(both the reference input and the worst-case input).\n1.5.5  Estimate the minimum amount of time (in days) that would be required for \nquick-\ufb01nd to solve a dynamic connectivity problem with 10 9 sites and 106 input pairs, \non a computer capable of executing 109 instructions per second. Assume that each itera-\ntion of the inner for loop requires ", "start": 246, "end": 247}, "342": {"text": "\nquick-\ufb01nd to solve a dynamic connectivity problem with 10 9 sites and 106 input pairs, \non a computer capable of executing 109 instructions per second. Assume that each itera-\ntion of the inner for loop requires 10 machine instructions.\n1.5.6 Repeat Exercise 1.5.5 for weighted quick-union.\n1.5.7  Develop classes QuickUnionUF and QuickFindUF that implement quick-union \nand quick-\ufb01nd, respectively. \n1.5.8 Give a counterexample that shows why this intuitive implementation of union()\nfor quick-\ufb01nd is not correct:\npublic void union(int p, int q) \n{ \n   if (connected(p, q)) return;\n   // Rename p\u2019s component to q\u2019s name.\n   for (int i = 0; i < id.length; i++)\n       if (id[i] == id[p]) id[i] = id[q];\n   count--; \n}\n1.5.9 Draw the tree corresponding to the id[] array depicted at \nright. Can this be the result of running weighted quick-union? \nExplain why this is impossible or give a sequence of operations \nthat results in this array.\ni    0 1 2 3 4 5 6 7 8 9\nid[i]  1 1 3 1 5 6 1 3 4 5\n2351.5 \u25a0 Case Study: Union-Find\n  \n1.5.10 In the weighted quick-union algorithm, suppose that we set id[find(p)] to q\ninstead of to id[find(q)]. Would the resulting algorithm be correct?\nAnswer : Y es, but it would increase the tree height, so the performance guarantee would \nbe invalid.\n1.5.11 Implement  weighted quick-\ufb01nd , where you always change the id[] entries ", "start": 247, "end": 248}, "343": {"text": "correct?\nAnswer : Y es, but it would increase the tree height, so the performance guarantee would \nbe invalid.\n1.5.11 Implement  weighted quick-\ufb01nd , where you always change the id[] entries of \nthe smaller component to the identi\ufb01er of the larger component. How does this change \naffect performance?\nEXERCISES  (continued)\n236 CHAPTER 1 \u25a0 Fundamentals\n CREATIVE PROBLEMS\n1.5.12     Quick-union with path compression. Modify quick-union (page 224) to include \npath compression, by adding a loop to union() that links every site on the paths from \np and q to the roots of their trees to the root of the new tree. Give a sequence of input \npairs that causes this method to produce a path of length 4. Note : The  amortized cost \nper operation for this algorithm is known to be logarithmic.\n1.5.13       Weighted quick-union with path compression. Modify weighted quick-union \n(Algorithm 1.5) to implement path compression, as described in Exercise 1.5.12. \nGive a sequence of input pairs that causes this method to produce a tree of height 4.\nNote : The amortized cost per operation for this algorithm is known to be bounded by a \nfunction known as the inverse Ackermann function and is less than 5 for any conceivable \npractical value of N.\n1.5.14   Weighted quick-union by height. Develop a UF implementation that uses the \nsame basic strategy as weighted quick-union but keeps track of tree height and always \nlinks the shorter tree to the taller one. Prove a logarithmic upper bound on the height \nof the trees for N sites with your algorithm.\n1.5.15     Binomial trees. Show that the number of nodes at each level in the worst-case \ntrees for weighted quick-union ", "start": 248, "end": 249}, "344": {"text": "logarithmic upper bound on the height \nof the trees for N sites with your algorithm.\n1.5.15     Binomial trees. Show that the number of nodes at each level in the worst-case \ntrees for weighted quick-union are binomial coef\ufb01cients. Compute the average depth of \na node in a worst-case tree with N = 2n nodes. \n1.5.16    Amortized costs plots. Instrument your implementations from Exercise 1.5.7\nto make amortized costs plots like those in the text. \n1.5.17    Random connections. Develop a UF client ErdosRenyi that takes an integer \nvalue N from the command line, generates random pairs of integers between 0 and N-1, \ncalling connected() to determine if they are connected and then union() if not (as in \nour development client), looping until all sites are connected, and printing the number \nof connections generated. Package your program as a static method count() that takes \nN as argument and returns the number of connections and a main() that takes N from \nthe command line, calls count(), and prints the returned value.\n1.5.18    Random grid generator. Write a program RandomGrid that takes an int value \nN from the command line, generates all the connections in an N-by-N grid, puts them \nin random order, randomly orients them (so that p q and q p are equally likely to oc-\ncur), and prints the result to standard output. T o randomly order the connections, use \na RandomBag (see Exercise 1.3.34 on page 167). T o encapsulate p and q in a single object, \n2371.5 \u25a0 Case Study: Union-Find\n use the Connection nested class shown below. Package your program as two static \nmethods:  generate(), which takes N as argument and returns an array of connec-\ntions, ", "start": 249, "end": 250}, "345": {"text": "object, \n2371.5 \u25a0 Case Study: Union-Find\n use the Connection nested class shown below. Package your program as two static \nmethods:  generate(), which takes N as argument and returns an array of connec-\ntions, and main(), which takes N from the command line, calls generate(), and iterates \nthrough the returned array to print the connections. \n1.5.19  Animation. Write a RandomGrid client (see Exercise 1.5.18) that uses \nUnionFind as in our development client to check connectivity and uses StdDraw to \ndraw the connections as they are processed.\n1.5.20  Dynamic growth. Using linked lists or a resizing array, develop a weighted \nquick-union implementation that removes the restriction on needing the number of \nobjects ahead of time. Add a method newSite() to the API, which returns an int\nidenti\ufb01er.\nprivate class Connection \n{\n   int p;\n   int q;\n   public Connection(int p, int q)\n   {  this.p = p; this.q = q;  } \n}\nRecord to encapsulate connections\nCREATIVE PROBLEMS  (continued)\n238 CHAPTER 1 \u25a0 Fundamentals\n EXPERIMENTS\n \n \n1.5.21   Erd\u00f6s-Renyi model. Use your client from Exercise 1.5.17 to test the hypothesis \nthat the number of pairs generated to get one component is ~ \u00bdN ln N.\n1.5.22    Doubling test for Erd\u00f6s-Renyi model. Develop a performance-testing client that \ntakes an int value T from the command line and performs T trials of the following ex-\nperiment: Use your client from Exercise 1.5.17 to generate random connections, using \nUnionFind to determine connectivity as in our development client, looping until all \nsites are connected. For each N, print the value of N, the average number of ", "start": 250, "end": 251}, "346": {"text": "1.5.17 to generate random connections, using \nUnionFind to determine connectivity as in our development client, looping until all \nsites are connected. For each N, print the value of N, the average number of connections \nprocessed, and the ratio of the running time to the previous. Use your program to vali-\ndate the hypotheses in the text that the running times for quick-\ufb01nd and quick-union \nare quadratic and weighted quick-union is near-linear. \n1.5.23    Compare quick-\ufb01nd with quick-union for Erd\u00f6s-Renyi model. Develop a perfor-\nmance-testing client that takes an int value T from the command line and performs \nT trials of the following experiment: Use your client from Exercise 1.5.17 to generate \nrandom connections. Save the connections, so that you can use both quick-union and \nquick-\ufb01nd to determine connectivity as in our development client, looping until all \nsites are connected. For each N, print the value of N and the ratio of the two running \ntimes. \n1.5.24    Fast algorithms for Erd\u00f6s-Renyi model. Add weighted quick-union and weight-\ned quick-union with path compression to your tests from Exercise 1.5.23 . Can you \ndiscern a difference between these two algorithms? \n1.5.25  Doubling test for random grids. Develop a performance-testing client that takes \nan int value T from the command line and performs T trials of the following experie-\nment: Use your client from Exercise 1.5.18 to generate the connections in an N-by-N \nsquare grid, randomly oriented and in random order, then use UnionFind to determine \nconnectivity as in our development client, looping until all sites are connected. For each \nN, print the value of N, the average number of connections processed, and the ratio of \nthe ", "start": 251, "end": 251}, "347": {"text": "order, then use UnionFind to determine \nconnectivity as in our development client, looping until all sites are connected. For each \nN, print the value of N, the average number of connections processed, and the ratio of \nthe running time to the previous. Use your program to validate the hypotheses in the \ntext that the running times for quick-\ufb01nd and quick-union are quadratic and weighted \nquick-union is near-linear. Note : As N doubles, the number of sites in the grid increases \nby a factor of 4, so expect a doubling factor of 16 for quadratic and 4 for linear.\n2391.5 \u25a0 Case Study: Union-Find\n 1.5.26    Amortized plot for Erd\u00f6s-Renyi. Develop a client that takes an int value N from \nthe command line and does an amortized plot of the cost of all operations in the style \nof the plots in the text for the process of generating random pairs of integers between 0\nand N-1, calling connected() to determine if they are connected and then union() if \nnot (as in our development client), looping until all sites are connected.\nEXPERIMENTS  (continued)\n240 CHAPTER 1 \u25a0 Fundamentals\n This page intentionally left blank \n 2.1 Elementary Sorts.  .  .  .  .  .  .  .  .  .  .  .  .  244\n2.2 Mergesort  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  270\n2.3 Quicksort   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  288\n2.4 Priority Queues .  . ", "start": 251, "end": 254}, "348": {"text": ".  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  288\n2.4 Priority Queues .  .  .  .  .  .  .  .  .  .  .  .  .  .  308\n2.5 Applications.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  336\nTWO\n S o r t i n g S\norting is the process of  rearranging a sequence  of objects so as to put them in \nsome logical order. For example, your credit card bill presents transactions in \norder by date\u2014they were likely put into that order by a sorting algorithm. In the \nearly days of computing, the common wisdom was that up to 30 percent of all com-\nputing cycles was spent sorting. If that fraction is lower today, one likely reason is that \nsorting algorithms are relatively ef\ufb01cient, not that sorting has diminished in relative \nimportance. Indeed, the ubiquity of computer usage has put us awash in data, and the \n\ufb01rst step to organizing data is often to sort it. All computer systems have implementa-\ntions of sorting algorithms, for use by the system and by users. \nThere are three practical reasons for you to study sorting algorithms, even though \nyou might just use a system sort:\n\u25a0 Analyzing sorting algorithms is a thorough introduction to the approach that we \nuse to compare algorithm performance throughout the book.\n\u25a0 Similar techniques are effective in addressing other problems.\n\u25a0 We often use sor ting algor ithms as a star ting point to solve other problems. \nMore important than these practical reasons is that the algorithms are elegant, classic, \nand effective. \nSorting plays a major role in commercial data processing ", "start": 254, "end": 255}, "349": {"text": "sor ting algor ithms as a star ting point to solve other problems. \nMore important than these practical reasons is that the algorithms are elegant, classic, \nand effective. \nSorting plays a major role in commercial data processing and in modern scienti\ufb01c \ncomputing. Applications abound in transaction processing, combinatorial optimiza -\ntion, astrophysics, molecular dynamics, linguistics, genomics, weather prediction, and \nmany other \ufb01elds. Indeed, a sorting algorithm (quicksort, in Section 2.3) was named \nas one of the top ten algorithms for science and engineering of the 20th century.\nIn this chapter, we consider several classical sorting methods and an ef\ufb01cient imple-\nmentation of a fundamental data type known as the priority queue.  We discuss the \ntheoretical basis for comparing sorting algorithms and conclude the chapter with a \nsurvey of applications of sorting and priority queues.\n243\n 2.1  ELEMENTARY SORTS\nFor our first excursion into the area of sorting algorithms, we shall study two ele-\nmentary sorting methods and a variation of one of them. Among the reasons for study-\ning these relatively simple algorithms in detail are the following: First, they provide \ncontext in which we can learn terminology and basic mechanisms. Second, these simple \nalgorithms are more effective in some applications than the sophisticated algorithms \nthat we shall discuss later. Third, they are useful in improving the ef\ufb01ciency of more \nsophisticated algorithms, as we will see.\nRules of the game Our primary concern is algorithms for rearranging arrays of \nitems where each    item contains a key. The objective of the sorting algorithm is to rear-\nrange the items such that their keys are ordered according to some well-de\ufb01ned order-\ning rule (usually numerical or alphabetical order). We want to rearrange the array so \nthat each entry\u2019s key is no smaller ", "start": 255, "end": 256}, "350": {"text": "rear-\nrange the items such that their keys are ordered according to some well-de\ufb01ned order-\ning rule (usually numerical or alphabetical order). We want to rearrange the array so \nthat each entry\u2019s key is no smaller than the key in each entry with a lower index and \nno larger than the key in each entry with a larger index. Speci\ufb01c characteristics of the \nkeys and the items can vary widely across applications. In Java, items are just objects, \nand the abstract notion of a key is captured in a built-in mechanism\u2014the  Comparable\ninterface\u2014that is described on page 247.\nThe class Example on the facing page illustrates the conventions that we shall use: \nwe put our sort code in a sort() method within a single class along with private helper \nfunctions less() and exch() (and perhaps some others) and a sample client main(). \nExample also illustrates code that might be useful for initial debugging: its test client \nmain() sorts strings from standard input using the private method show() to print the \ncontents of the array. Later in this chapter, we will examine various test clients for com-\nparing algorithms and for studying their performance. T o differentiate sorting meth-\nods, we give our various sort classes different names. Clients can call different imple-\nmentations by name: Insertion.sort(), Merge.sort(), Quick.sort(), and so forth. \nWith but a few exceptions, our sort code refers to the data only through two opera-\ntions: the method less() that compares items and the method exch() that exchanges \nthem. The exch() method is easy to implement, and the Comparable interface makes \nit easy to implement less(). Restricting data access to these two operations makes our \ncode readable and portable, and makes it easier for us certify that algorithms are cor -\nrect, to study performance and to compare algorithms. Before proceeding to consider \nsort implementations, we discuss a number of ", "start": 256, "end": 256}, "351": {"text": "these two operations makes our \ncode readable and portable, and makes it easier for us certify that algorithms are cor -\nrect, to study performance and to compare algorithms. Before proceeding to consider \nsort implementations, we discuss a number of important issues that need to be care-\nfully considered for every sort.\n244\n % more tiny.txt \nS O R T E X A M P L E\n% java Example < tiny.txt \nA E E L M O P R S T X\n T e m p l a t e  f o r  s o r t  c l a s s e s\npublic class  Example \n{\n   public static void sort(Comparable[] a)\n   {  /* See Algorithms 2.1, 2.2, 2.3, 2.4, 2.5, or 2.7. */  }\n   private static boolean    less(Comparable v, Comparable w)\n   {  return v.compareTo(w) < 0;  }\n   private static void exch(Comparable[] a, int i, int j)\n   {  Comparable t = a[i]; a[i] = a[j]; a[j] = t;  }\n   private static void show(Comparable[] a)\n   {  // Print the array, on a single line.\n      for (int i = 0; i < a.length; i++)\n         StdOut.print(a[i] + \" \");\n      StdOut.println();\n   }\n   public static boolean isSorted(Comparable[] a)\n   {  // Test whether the array entries are in order.\n      for (int i = 1; i < a.length; i++)\n         if (less(a[i], a[i-1]))  return false;\n      return true;\n   }\n   public static void main(String[] args)\n   {  // Read strings from standard input, sort them, and print.\n      String[] a = In.readStrings(); ", "start": 256, "end": 257}, "352": {"text": "a[i-1]))  return false;\n      return true;\n   }\n   public static void main(String[] args)\n   {  // Read strings from standard input, sort them, and print.\n      String[] a = In.readStrings(); \n      sort(a);\n      assert isSorted(a);\n      show(a);\n   } \n}\nThis class illustrates our conventions for imple-\nmenting array sorts. For each sorting algorithm \nthat we consider, we present a sort() method for \na class like this with Example changed to a name \nthat corresponds to the algorithm. The test client \nsorts strings taken from standard input, but, with \nthis code, our sort methods are effective for any \ntype of data that implements Comparable.\n% more words3.txt \nbed bug dad yes zoo ... all bad yet\n% java Example < words.txt \nall bad bed bug dad ... yes yet zoo\n2452.1 \u25a0 Elementary Sorts  \n  C e r t i \ufb01 c a t i o n .  Does the sort implementation always put the array in order, no mat-\nter what the initial order? As a conservative practice, we include the statement \nassert isSorted(a); in our test client to certify that array entries are in order after \nthe sort. It is reasonable to include this statement in every sort implementation, even \nthough we normally test our code and develop mathematical arguments that our al-\ngorithms are correct. Note that this test is suf\ufb01cient only if we use exch() exclusively \nto change array entries. When we use code that stores values into the array directly, we \ndo not have full assurance (for example, code that destroys the original input array by \nsetting all values to be the same would pass this test).  \nRunning time. We also test algor ithm performance. We start by \nproving facts about the number of basic operations (compares \nand exchanges, or perhaps the number of times the ", "start": 257, "end": 258}, "353": {"text": "same would pass this test).  \nRunning time. We also test algor ithm performance. We start by \nproving facts about the number of basic operations (compares \nand exchanges, or perhaps the number of times the array is ac-\ncessed, for read or write) that the various sorting algorithms per -\nform for various natural input models. Then we use these facts \nto develop hypotheses about the comparative performance of the \nalgorithms and present tools that you can use to experimentally \ncheck the validity of such hypotheses. We use a consistent coding \nstyle to facilitate the development of valid hypotheses about per -\nformance that will hold true for typical implementations.\n E x t r a  m e m o r y .  The amount of extra memory used by a sorting algorithm is often as \nimportant a factor as running time. The sorting algorithms divide into two basic types: \nthose that sort  in place and use no extra memory except perhaps for a small  function-\ncall stack or a constant number of instance variables, and those that need enough extra \nmemory to hold another copy of the array to be sorted.\n  T y p e s  o f  d a t a .  Our sort code is effective for any item type that implements the \nComparable interface. Adhering to Java\u2019s convention in this way is convenient be-\ncause many of the types of data that you might want to sort implement    Comparable. \nFor example, Java\u2019s numeric wrapper types such as Integer and Double implement \nComparable, as do String and various advanced types such as File or URL. Thus, \nyou can just call one of our sort methods with an array of any of these types as argu -\nment. For example, the code at right uses quicksort (see Section 2.3) to sort N random \nDouble values. When we create types of our \nown, we can enable client code to sort that type \nof data by implementing the Comparable in-\nterface. ", "start": 258, "end": 258}, "354": {"text": "right uses quicksort (see Section 2.3) to sort N random \nDouble values. When we create types of our \nown, we can enable client code to sort that type \nof data by implementing the Comparable in-\nterface. To do so, we just need to implement a \ncompareTo() method that de\ufb01nes an ordering \non objects of that type known as the natural \nDouble a[] = new Double[N]; \nfor (int i = 0; i < N; i++)\n   a[i] = StdRandom.uniform(); \nQuick.sort(a);\nSorting an array of random values\n  S o r t i n g  c o s t  m o d e l .  \nWhen studying sorting\nalgorithms, we count\ncompares and exchanges.\nFor algorithms that do \nnot use exchanges, we \ncount array accesses. \n246 CHAPTER 2 \u25a0 Sorting\n order for that type, as shown here for our Date data type (see page 91). Java\u2019s convention \nis that the call v.compareTo(w) returns an integer that is negative, zero, or positive \n(usually -1, 0, or +1) when v < w,  v = w, \nor v > w, respectively. For economy, we \nuse standard notation like v>w as short-\nhand for code like v.compareTo(w)>0 \nfor the remainder of this paragraph. By \nconvention, v.compareTo(w) throws \nan exception if v and w are incompatible \ntypes or either is null. Furthermore, \ncompareTo() must implement a      total \norder: it must be\n\u25a0    Re\ufb02exive (for all v, v = v)\n\u25a0 Antisymmetric (for all v and w, if \nv < w then w > v and if v = w then \nw = v) \n\u25a0 \n \nTransitive (for all v, w, and x, if \nv <= ", "start": 258, "end": 259}, "355": {"text": "(for all v and w, if \nv < w then w > v and if v = w then \nw = v) \n\u25a0 \n \nTransitive (for all v, w, and x, if \nv <= w and w <= x then v <=x )\nThese rules are intuitive and standard \nin mathematics\u2014you will have little \ndif\ufb01culty adhering to them. In short, \ncompareTo() implements our key ab-\nstraction\u2014it de\ufb01nes the ordering of \nthe items (objects) to be sorted, which \ncan be any type of data that implements \nComparable. Note that compareTo() need not use all of the instance variables. Indeed, \nthe key might be a small part of each item.\nFor the remainder of this chapter, we shall address numerous algorithms for sort-\ning arrays of objects having a natural order. T o compare and contrast the algorithms, \nwe shall examine a number of their properties, including the number of compares and \nexchanges that they use for various types of inputs and the amount of extra memory \nthat they use. These properties lead to the development of hypotheses about perfor -\nmance properties, many of which have been validated on countless computers over the \npast several decades. Speci\ufb01c implementations always need to be checked, so we also \nconsider tools for doing so. After considering the classic selection sort, insertion sort, \nshellsort, mergesort, quicksort, and heapsort algorithms, we will consider practical is -\nsues and applications, in Section 2.5.\npublic class    Date implements Comparable<Date> \n{\n   private final int day;\n   private final int month;\n   private final int year;\n   public Date(int d, int m, int y)\n   {  day = d; month = m; year = y; }\n   public int day()   {  return day;    }\n   public int month() {  return month;  }\n   public ", "start": 259, "end": 259}, "356": {"text": "int y)\n   {  day = d; month = m; year = y; }\n   public int day()   {  return day;    }\n   public int month() {  return month;  }\n   public int year()  {  return year;    }\n   public int compareTo(Date that)\n   {\n      if (this.year  > that.year ) return +1;\n      if (this.year  < that.year ) return -1;\n      if (this.month > that.month) return +1;\n      if (this.month < that.month) return -1;\n      if (this.day   > that.day  ) return +1;\n      if (this.day   < that.day  ) return -1;\n      return 0;\n   }\n   public String toString()\n   { return month + \"/\" + day + \"/\" + year; } \n}\n D e f i n i n g  a  c o m p a r a b l e  t y p e\n2472.1 \u25a0 Elementary Sorts\n  \n \n   S e l e c t i o n  s o r t  One of the simplest sorting algorithms works as follows: First, \ufb01nd \nthe smallest item in the array and exchange it with the \ufb01rst entry (itself if the \ufb01rst entry \nis already the smallest). Then, \ufb01nd the next smallest item and exchange it with the sec-\nond entry. Continue in this way until the entire array is sorted. This method is called \nselection sort because it works by repeatedly selecting the smallest remaining item.\nAs you can see from the implementation in Algorithm 2.1, the inner loop of selec-\ntion sort is just a compare to test a current item against the smallest item found so far \n(plus the code necessary to increment the current index and to check that it does not \nexceed the array bounds); it could hardly be simpler. The work of moving ", "start": 259, "end": 260}, "357": {"text": "test a current item against the smallest item found so far \n(plus the code necessary to increment the current index and to check that it does not \nexceed the array bounds); it could hardly be simpler. The work of moving the items \naround falls outside the inner loop: each exchange puts an item into its \ufb01nal position, \nso the number of exchanges is N. Thus, the running time is dominated by the number \nof compares. \nProposition A.  Selection sort uses /H11011N 2/2 compares and N exchanges to sort an \narray of length N.\nProof: Yo u  c a n  p rove  t h i s  f a c t  by  e x a m i n i n g  t h e  t r a ce , w h i c h  i s  a n  N-by-N table \nin which unshaded letters correspond to compares. About one-half of the entries \nin the table are unshaded\u2014those on and above the diagonal. The entries on the \ndiagonal each correspond to an exchange. More precisely, examination of the code \nreveals that, for each i from 0 to N /H11002 1, there is one exchange and N /H11002 1 /H11002 i  com-\npares, so the totals are N exchanges and (N /H11002 1) + (N /H11002 2) + . . . + 2 + 1+ 0 = N(N \n/H11002 1) / 2 /H11011 N 2 / 2 compares.\n \nIn summary, selection sort is a simple sorting method that is easy to understand and to \nimplement and is characterized by the following two signature properties:\nRunning time is insensitive to input. The process of \ufb01nding the smallest item on one \npass through the array does not give ", "start": 260, "end": 260}, "358": {"text": "that is easy to understand and to \nimplement and is characterized by the following two signature properties:\nRunning time is insensitive to input. The process of \ufb01nding the smallest item on one \npass through the array does not give much information about where the smallest item \nmight be on the next pass. This property can be disadvantageous in some situations. \nFor example, the person using the sort client might be surprised to realize that it takes \nabout as long to run selection sort for an array that is already in order or for an array \nwith all keys equal as it does for a randomly-ordered array! As we shall see, other algo-\nrithms are better able to take advantage of initial order in the input.\n D a t a  m o v e m e n t  i s  m i n i m a l .   Each of the N exchanges changes the value of two array \nentries, so selection sort uses N exchanges\u2014the number of array accesses is a linear\nfunction of the array size. None of the other sorting algorithms that we consider have \nthis property (most involve linearithmic or quadratic growth).\n248 CHAPTER 2 \u25a0 Sorting\n ALGORITHM 2.1   Selection sort\npublic class    Selection \n{\n   public static void sort(Comparable[] a)\n   {  // Sort a[] into increasing order.\n      int N = a.length;               // array length\n      for (int i = 0; i < N; i++)\n      {  // Exchange a[i] with smallest entry in a[i+1...N).\n         int min = i;                 // index of minimal entr.\n         for (int j = i+1; j < N; j++)\n            if (less(a[j], a[min])) min = j;\n         exch(a, i, min);\n      }\n   }\n   // See page 245 for less(), exch(), isSorted(), and main(). \n}\nFor ", "start": 260, "end": 261}, "359": {"text": "(less(a[j], a[min])) min = j;\n         exch(a, i, min);\n      }\n   }\n   // See page 245 for less(), exch(), isSorted(), and main(). \n}\nFor each i, this implementation puts the ith smallest item in a[i]. The entries to the left of position \ni are the i smallest items in the array and are not examined again.\nTrace of selection sort (array contents just after each exchange)\n                       a[]\n i min   0  1  2  3  4  5  6  7  8  9 10\n         S  O  R  T  E  X  A  M  P  L  E \n 0   6   S  O  R  T  E  X  A  M  P  L  E \n 1   4   A  O  R  T  E  X  S  M  P  L  E \n 2  10   A  E  R  T  O  X  S  M  P  L  E \n 3   9   A  E  E  T  O  X  S  M  P  L  R \n 4   7   A  E  E  L  O  X  S  M  P  T  R \n 5   7   A  E  E  L  M  X  S  O  P  T  R \n 6   8   A  E  E  L  M  O  S  X  P  T  R \n 7  10   A  E  E  L  M  O  P  X  S  T  R \n 8   8   A  E  E  L  M  O  P ", "start": 261, "end": 261}, "360": {"text": "7  10   A  E  E  L  M  O  P  X  S  T  R \n 8   8   A  E  E  L  M  O  P  R  S  T  X \n 9   9   A  E  E  L  M  O  P  R  S  T  X \n10  10   A  E  E  L  M  O  P  R  S  T  X \n         A  E  E  L  M  O  P  R  S  T  X  \nentries in gray are\nin final position\nentries in black\nare examined to find\nthe minimum\nentries in red\nare a[min]\n2492.1 \u25a0 Elementary Sorts   I n s e r t i o n  s o r t   The algorithm that people often use to sort bridge hands is to con-\nsider the cards one at a time, inserting each into its proper place among those already \nconsidered (keeping them sorted). In a computer implementation, we need to make \nspace to insert the current item by moving larger items one position to the right, before \ninserting the current item into the vacated position. Algorithm 2.2 is an implementa-\ntion of this method, which is called insertion sort.\nAs in selection sort, the items to the left of the current index are in sorted order dur-\ning the sort, but they are not in their \ufb01nal position, as they may have to be moved to \nmake room for smaller items encountered later. The array is, however, fully sorted when \nthe index reaches the right end.\nUnlike that of selection sort, the running time of insertion sort depends on the ini -\ntial order of the items in the input. For example, if the array is large and its entries are \nalready in order (or ", "start": 261, "end": 262}, "361": {"text": "end.\nUnlike that of selection sort, the running time of insertion sort depends on the ini -\ntial order of the items in the input. For example, if the array is large and its entries are \nalready in order (or nearly in order), then insertion sort is much, much faster than if \nthe entries are randomly ordered or in reverse order.\nProposition B.  Insertion sort uses /H11011N 2/4 compares and /H11011N 2/4 exchanges to sort \na randomly ordered array of length N with distinct keys, on the average. The worst \ncase is /H11011N 2/2 compares and /H11011N 2/2 exchanges and the best case is N /H11002 1 compares \nand 0 exchanges.\nProof: Just as for Proposition A, the number of compares and exchanges is easy to \nvisualize in the N-by-N diagram that we use to illustrate the sort. We count entries \nbelow the diagonal\u2014all of them, in the worst case, and none of them, in the best \ncase. For randomly ordered arrays, we expect each item to go about halfway back, \non the average, so we count one-half of the entries below the diagonal.\nThe number of compares is the number of exchanges plus an additional term \nequal to N minus the number of times the item inserted is the smallest so far. In the \nworst case (array in reverse order), this term is negligible in relation to the total; in \nthe best case (array in order) it is equal to N /H11002 1.\nInsertion sort works well for certain types of nonrandom arrays that often arise in \npractice, even if they are huge. For example, as just mentioned, consider what happens \nwhen you use insertion sort on an array that is already sorted. Each item is immediately \ndetermined to be in its proper place in the array, ", "start": 262, "end": 262}, "362": {"text": "if they are huge. For example, as just mentioned, consider what happens \nwhen you use insertion sort on an array that is already sorted. Each item is immediately \ndetermined to be in its proper place in the array, and the total running time is linear. \n(The running time of selection sort is quadratic for such an array.) The same is true \nfor arrays whose keys are all equal (hence the condition in Proposition B that the keys \nmust be distinct).\n250 CHAPTER 2 \u25a0 Sorting\n ALGORITHM 2.2   Insertion sort\npublic class  Insertion \n{ \n   public static void sort(Comparable[] a)\n   {  // Sort a[] into increasing order.\n      int N = a.length;             \n      for (int i = 1; i < N; i++)\n      {  // Insert a[i] among a[i-1], a[i-2], a[i-3]... ..\n         for (int j = i; j > 0 && less(a[j], a[j-1]); j--)\n            exch(a, j, j-1);\n      }\n   }\n   // See page 245 for less(), exch(), isSorted(), and main(). \n}\nFor each i from 0 to N-1, exchange a[i] with the entries that are smaller in a[0] through a[i-1]. As \nthe index i travels from left to right, the entries to its left are in sorted order in the array, so the array \nis fully sorted when i reaches the right end.\nTrace of insertion sort (array contents just after each insertion)\n                       a[]\n i   j   0  1  2  3  4  5  6  7  8  9 10\n         S  O  R  T  E  X  A  M  P  L  E \n 1   0 ", "start": 262, "end": 263}, "363": {"text": "4  5  6  7  8  9 10\n         S  O  R  T  E  X  A  M  P  L  E \n 1   0   O  S  R  T  E  X  A  M  P  L  E \n 2   1   O  R  S  T  E  X  A  M  P  L  E \n 3   3   O  R  S  T  E  X  A  M  P  L  E \n 4   0   E  O  R  S  T  X  A  M  P  L  E \n 5   5   E  O  R  S  T  X  A  M  P  L  E \n 6   0   A  E  O  R  S  T  X  M  P  L  E \n 7   2   A  E  M  O  R  S  T  X  P  L  E \n 8   4   A  E  M  O  P  R  S  T  X  L  E \n 9   2   A  E  L  M  O  P  R  S  T  X  E \n10   2   A  E  E  L  M  O  P  R  S  T  X  \n         A  E  E  L  M  O  P  R  S  T  X  \nentries in black\nmoved one position\nright for insertion\nentries in gray\ndo not move \nentry in red \nis a[j]\n2512.1 \u25a0 Elementary Sorts  More generally, we consider the concept of a  partially sorted array, as follows: An ", "start": 263, "end": 264}, "364": {"text": "for insertion\nentries in gray\ndo not move \nentry in red \nis a[j]\n2512.1 \u25a0 Elementary Sorts  More generally, we consider the concept of a  partially sorted array, as follows: An  in-\nversion is a pair of entries that are out of order in the array. For instance, E X A M P L E\nhas 11 inversions: E-A,  X-A,  X-M,  X-P,  X-L,  X-E,  M-L,  M-E,  P-L,  P-E, and  L-E. If the \nnumber of inversions in an array is less than a constant multiple of the array size, we \nsay that the array is partially sorted. Typical examples of partially sorted arrays are the \nfollowing:\n\u25a0 An array where each entry is not far from its \ufb01nal position\n\u25a0 A small array appended to a large sorted array\n\u25a0 \n \nAn array with only a few entries that are not in place\nInsertion sort is an ef\ufb01cient method for such arrays; selection sort is not. Indeed, when \nthe number of inversions is low, insertion sort is likely to be faster than any sorting \nmethod that we consider in this chapter.\n Proposition C. The number of exchanges used by insertion sort is equal to the \nnumber of inversions in the array, and the number of compares is at least equal to \nthe number of inversions and at most equal to the number of inversions plus the \narray size minus 1.\nProof: Every exchange involves two inverted adjacent entries and thus reduces the \nnumber of inversions by one, and the array is sorted when the number of inver -\nsions reaches zero. Every exchange corresponds to a compare, and an additional \ncompare might happen for each value of i from 1 to N-1 (when a[i] does not \nreach the left end of the array).\n It ", "start": 264, "end": 264}, "365": {"text": "reaches zero. Every exchange corresponds to a compare, and an additional \ncompare might happen for each value of i from 1 to N-1 (when a[i] does not \nreach the left end of the array).\n It is not dif\ufb01cult to speed up insertion sort substantially, by shortening its inner loop to \nmove the larger entries to the right one position rather than doing full exchanges (thus \ncutting the number of array accesses in half). We leave this improvement for an exercise \n(see Exercise 2.1.25). \nIn summary, insertion sort is an excellent method for partially sorted arrays and is also \na \ufb01ne method for tiny arrays. These facts are important not just because such arrays \nfrequently arise in practice, but also because both types of arrays arise in intermediate \nstages of advanced sorting algorithms, so we will be considering insertion sort again in \nrelation to such algorithms.\n252 CHAPTER 2 \u25a0 Sorting\n Visualizing sorting algorithms Throughout this chapter, we will be using \na simple visual representation to help describe the properties of sorting algorithms. \nRather than tracing the progress of a sort with key values such as letters, numbers, \nor words, we use vertical bars, to be sorted by their \nheights. The advantage of such a representation is \nthat it can give insights into the behavior of a sort-\ning method.\nFor example, you can see at a glance on the visual \ntraces at right that insertion sort  does not touch \nentries to the right of the scan pointer and selec-\ntion sort does not touch entries to the left of the \nscan pointer. Moreover, it is clear from the visual \ntraces that, since insertion sort also does not touch \nentries smaller than the inserted item, it uses about \nhalf the number of compares as selection sort, on \nthe average.\nWith our StdDraw library, developing a visual \ntrace is not much more dif\ufb01cult than ", "start": 264, "end": 265}, "366": {"text": "\nentries smaller than the inserted item, it uses about \nhalf the number of compares as selection sort, on \nthe average.\nWith our StdDraw library, developing a visual \ntrace is not much more dif\ufb01cult than doing a stan -\ndard trace. We sort Double values, instrument the \nalgorithm to call show() as appropriate (just as we \ndo for a standard trace), and develop a version of \nshow() that uses StdDraw to draw the bars instead \nof printing the results. The most complicated task \nis setting the scale for the y-axis so that the lines of \nthe trace appear in the expected order. Y ou are en-\ncouraged to work Exercise 2.1.18 in order to gain a \nbetter appreciation of the value of visual traces and \nthe ease of creating them. \nAn even simpler task is to animate the trace so \nthat you can see the array dynamically evolve to \nthe sorted result. Developing an animated trace in-\nvolves essentially the same process described in the previous paragraph, but without \nhaving to worry about the y-axis (just clear the window and redraw the bars each time). \nThough we cannot make the case on the printed page, such animated representations \nare also effective in gaining insight into how an algorithm works. Y ou are also encour-\naged to work Exercise 2.1.17 to see for yourself.\nblack entries\nare involved \nin compares\ngray entries\nare untouched\nVisual traces of elementary sorting algorithms\ninsertion sort selection sort\n2532.1 \u25a0 Elementary Sorts\n Comparing two sorting algorithms Now that we have two implementations, \nwe are naturally interested in knowing which one is faster: selection sort ( Algorithm \n2.1) or insertion sort ( Algorithm 2.2). Questions like this arise again and again and \nagain in the study of algorithms and are a major focus throughout this book. We have \ndiscussed some fundamental ideas in Chapter 1, ", "start": 265, "end": 266}, "367": {"text": "or insertion sort ( Algorithm 2.2). Questions like this arise again and again and \nagain in the study of algorithms and are a major focus throughout this book. We have \ndiscussed some fundamental ideas in Chapter 1, but we use this \ufb01rst case in point to \nillustrate our basic approach to answering such questions. Generally, following the ap-\nproach introduced in Section 1.4, we compare algorithms by\n\u25a0 Implementing and debugging them\n\u25a0 Analyzing their basic properties\n\u25a0 Formulating a hypothesis about comparative performance\n\u25a0 \n \nRunning experiments to validate the hypothesis\nThese steps are nothing more than the time-honored scienti\ufb01c method , applied to the \nstudy of algorithms. \nIn the present context, Algorithm 2.1 and Algorithm 2.2 are evidence of the \n\ufb01rst step; Propositions A, B, and C constitute the second step; Property D on page 255 \nconstitutes the third step; and the class SortCompare on page 256 enables the fourth step. \nThese activities are all interrelated. \nOur brief descriptions mask a substantial amount of effort that is required to prop-\nerly implement, analyze, and test algorithms. Every programmer knows that such code \nis the product of a long round of debugging and re\ufb01nement, every mathematician \nknows that proper analysis can be very dif\ufb01cult, and every scientist knows that formu-\nlating hypotheses and designing and executing experiments to validate them require \ngreat care. Full development of such results is reserved for experts studying our most \nimportant algorithms, but every programmer using an algorithm should be aware of \nthe scienti\ufb01c context underlying its performance properties. \nHaving developed implementations, our next choice is to settle on an appropriate \nmodel for the input. For sorting, a natural model, which we have used for Proposi-\ntions A, B, and C, is to assume ", "start": 266, "end": 266}, "368": {"text": "\nHaving developed implementations, our next choice is to settle on an appropriate \nmodel for the input. For sorting, a natural model, which we have used for Proposi-\ntions A, B, and C, is to assume that the arrays are randomly ordered and that the key \nvalues are distinct. In applications where signi\ufb01cant numbers of equal key values are \npresent we will need a more complicated model.\nHow do we formulate a hypothesis about the running times of insertion sort and \nselection sort for randomly ordered arrays? Examining Algorithms 2.1 and 2.2 and \nPropositions A and B, it follows immediately that the running time of both algorithms \nshould be quadratic for randomly ordered arrays. That is, the running time of insertion \nsort for such an input is proportional to some small constant times N 2 and the running \ntime of selection sort is proportional to some other small constant times N 2. The values \nof the two constants depend on the cost of compares and exchanges on the particular \ncomputer being used. For many types of data and for typical computers, it is reasonable \n254 CHAPTER 2 \u25a0 Sorting\n to assume that these costs are similar (though we will see a few signi\ufb01cant exceptions). \nThe following hypothesis follows directly:\n \nProperty D.    The running times of insertion sort and selection sort are quadratic \nand within a small constant factor of one another for randomly ordered arrays of \ndistinct values.\nEvidence: This statement has been validated on many different computers over \nthe past half-century. Insertion sort was about twice as fast as selection sort when \nthe \ufb01rst edition of this book was written in 1980 and it still is today, even though it \ntook several hours to sort 100,000 items with these algorithms then and just several \nseconds today. Is insertion sort a bit faster than selection sort on your computer? \nTo  \ufb01 n d  o u t ", "start": 266, "end": 267}, "369": {"text": "\ntook several hours to sort 100,000 items with these algorithms then and just several \nseconds today. Is insertion sort a bit faster than selection sort on your computer? \nTo  \ufb01 n d  o u t , yo u  c a n  u s e  t h e  c l a s s  SortCompare on the next page, which uses the \nsort() methods in the classes named as command-line arguments to perform the \ngiven number of experiments (sorting arrays of the given size) and prints the ratio \nof the observed running times of the algorithms.\nTo  v a l i d a te  t h i s  hy p o t h e s i s , w e  u s e  SortCompare (see page 256) to perform the experi-\nments. As usual, we use Stopwatch to compute the running time. The implementation \nof time() shown here does the job for the basic sorts in this chapter. The \u201crandomly or-\ndered\u201d input model is embedded in the timeRandomInput() method in SortCompare, \nwhich generates random Double values, sorts them, and returns the total measured \ntime of the sort for the given \nnumber of trials. Using ran -\ndom Double values between \n0.0 and 1.0 is much simpler \nthan the alternative of us-\ning a library function such \nas StdRandom.shuffle() \nand is effective because equal \nkey values are very unlikely \n(see Exercise 2.5.31). As \ndiscussed in Chapter 1 , the \nnumber of trials is taken as an \nargument both to take advan-\ntage of the law of large numbers (the more trials, the total running time divided by the \nnumber of trials is a more accurate estimate of the true average running time) and to \nhelp damp out system effects.  Y ou are encouraged to experiment with SortCompare \npublic static double time(String alg, ", "start": 267, "end": 267}, "370": {"text": "divided by the \nnumber of trials is a more accurate estimate of the true average running time) and to \nhelp damp out system effects.  Y ou are encouraged to experiment with SortCompare \npublic static double time(String alg, Comparable[] a) \n{\n   Stopwatch timer = new Stopwatch();\n   if (alg.equals(\"Insertion\")) Insertion.sort(a);\n   if (alg.equals(\"Selection\")) Selection.sort(a);\n   if (alg.equals(\"Shell\"))     Shell.sort(a);\n   if (alg.equals(\"Merge\"))     Merge.sort(a);\n   if (alg.equals(\"Quick\"))     Quick.sort(a);\n   if (alg.equals(\"Heap\"))      Heap.sort(a);\n   return timer.elapsedTime(); \n}\nTiming one of the sort algorithms in this chapter on a given input\n2552.1 \u25a0 Elementary Sorts\n  C o m p a r i n g  t w o  s o r t i n g  a l g o r i t h m s\npublic class  SortCompare \n{\n   public static double time(String alg, Double[] a)\n   {  /* See text. */  }\n   public static double timeRandomInput(String alg, int N, int T)\n   {  // Use alg to sort T random arrays of length N. \n      double total = 0.0;\n      Double[] a = new Double[N];\n      for (int t = 0; t < T; t++)\n      {  // Perform one experiment (generate and sort an array).\n         for (int i = 0; i < N; i++)\n            a[i] = StdRandom.uniform();\n         total += time(alg, a);\n      }\n      return total;\n   }\n   public static void main(String[] args)\n   {\n      String alg1 = args[0];\n      String alg2 = args[1];\n      int N = Integer.parseInt(args[2]);\n      int T = Integer.parseInt(args[3]);\n ", "start": 267, "end": 268}, "371": {"text": "public static void main(String[] args)\n   {\n      String alg1 = args[0];\n      String alg2 = args[1];\n      int N = Integer.parseInt(args[2]);\n      int T = Integer.parseInt(args[3]);\n      double t1 = timeRandomInput(alg1, N, T); // total for alg1\n      double t2 = timeRandomInput(alg2, N, T); // total for alg2\n      StdOut.printf(\"For %d random Doubles\\n    %s is\", N, alg1);\n      StdOut.printf(\" %.1f times faster than %s\\n\", t2/t1, alg2);\n   } \n}\nThis client runs the two sorts named in the \ufb01rst two command-line arguments on arrays of N (the \nthird command-line argument) random Double values between 0.0 and 1.0, repeating the experi-\nment T (the fourth command-line argument) times, then prints the ratio of the total running times.\n% java SortCompare Insertion Selection 1000 100 \nFor 1000 random Doubles\n  Insertion is 1.7 times faster than Selection\n256 CHAPTER 2 \u25a0 Sorting on your computer to learn the extent to which its conclusion about insertion sort and \nselection sort is robust.\nProperty D is intentionally a bit vague\u2014the value of the small constant factor is left \nunstated and the assumption that the costs of compares and exchanges are similar is left \nunstated\u2014so that it can apply in a broad variety of situations. When possible, we try to \ncapture essential aspects of the performance of each of the algorithms that we study in \nstatements like this. As discussed in Chapter 1, each Property that we consider needs to \nbe tested scienti\ufb01cally in a given situation, perhaps supplemented with a more re\ufb01ned \nhypothesis based upon a related Proposition (mathematical truth).\nFor ", "start": 268, "end": 269}, "372": {"text": "1, each Property that we consider needs to \nbe tested scienti\ufb01cally in a given situation, perhaps supplemented with a more re\ufb01ned \nhypothesis based upon a related Proposition (mathematical truth).\nFor practical applications, there is one further step, which is crucial: run experiments \nto validate the hypothesis on the data at hand . We defer consideration of this step to \nSection 2.5 and the exercises. In this case, if your sort keys are not distinct and/or \nnot randomly ordered, Property D might not hold. Y ou can randomly order an array \nwith StdRandom.shuffle(), but applications with signi\ufb01cant numbers of equal keys \ninvolve more careful analysis.\nOur discussions of the analyses of algorithms are intended to be starting points, not \n\ufb01nal conclusions. If some other question about performance of the algorithms comes \nto mind, you can study it with a tool like SortCompare. Many opportunities to do so \nare presented in the exercises.\nWe do not dwell further on the comparative performance of insertion sort and selec-\ntion sort because we are much more interested in algorithms that can run a hundred or \na thousand or a million times faster than either. Still, understanding these elementary \nalgorithms is worthwhile for several reasons:\n\u25a0 They help us work out the ground rules.\n\u25a0 They provide performance benchmarks.\n\u25a0 They often are the method of choice in some specialized situations.\n\u25a0 They can serve as the basis for developing better algorithms.\nFor these reasons, we will use the same basic approach and consider elementary algo -\nrithms for every problem that we study throughout this book, not just sorting. Pro -\ngrams like SortCompare play a critical role in this incremental approach to algorithm \ndevelopment. At every step along the way, we can use such a program to help evaluate \nwhether a new algorithm or an improved version of a known algorithm provides the \nperformance gains that we expect.\n2572.1 \u25a0 Elementary ", "start": 269, "end": 269}, "373": {"text": "\ndevelopment. At every step along the way, we can use such a program to help evaluate \nwhether a new algorithm or an improved version of a known algorithm provides the \nperformance gains that we expect.\n2572.1 \u25a0 Elementary Sorts\n  \n  S h e l l s o r t  To  e x h i b i t  t h e  v a l u e  o f  k n ow i n g  p ro p e r t i e s  o f  e l e m e n t a r y  s o r t s , w e  n e x t  \nconsider a fast algorithm based on insertion sort. Insertion sort is slow for large un -\nordered arrays because the only exchanges it does involve adjacent entries, so items \ncan move through the array only one place at a time. For example, if the item with the \nsmallest key happens to be at the end of the array, N/H110021 exchanges are needed to get that \none item where it belongs. Shellsort is a simple extension of insertion sort that gains \nspeed by allowing exchanges of array entries that are far apart, to produce partially \nsorted arrays that can be ef\ufb01ciently sorted, eventually by insertion sort.\nThe idea is to rearrange the array to give it the property that taking every  hth entry \n(starting anywhere) yields a sorted subsequence. Such an array is said to be  h-sorted. Put \nanother way, an h-sorted array is  h inde-\npendent sorted subsequences, interleaved \ntogether. By h-sorting for some large val -\nues of h, we can move items in the array \nlong distances and thus make it easier to \nh-sort for smaller values of h. Using such \na procedure for any sequence of values of\nh that ends in 1 will produce a sorted ar -\nray: that is shellsort. The ", "start": 269, "end": 270}, "374": {"text": "thus make it easier to \nh-sort for smaller values of h. Using such \na procedure for any sequence of values of\nh that ends in 1 will produce a sorted ar -\nray: that is shellsort. The implementation \nin Algorithm 2.3 on the facing page uses the sequence of decreasing values \u00bd(3k/H110021), \nstarting at the largest increment less than N/3 and decreasing to 1. We refer to such a \nsequence as an  increment sequence. Algorithm 2.3 computes its increment sequence; \nanother alternative is to store an increment sequence in an array.\nOne way to implement shellsort would be, for each h, to use insertion sort indepen-\ndently on each of the h subsequences. Because the subsequences are independent, we \ncan use an even simpler approach: when h-sorting the array, we insert each item among \nthe previous items in its h-subsequence by exchanging it with those that have larger \nkeys (moving them each one position to the right in the subsequence). We accomplish \nthis task by using the insertion-sort code, but modi\ufb01ed to decrement by h instead of 1 \nwhen moving through the array. This observation reduces the shellsort implementa-\ntion to an insertion-sort-like pass through the array for each increment.\nShellsort gains ef\ufb01ciency by making a tradeoff between size and partial order in the \nsubsequences. At the beginning, the subsequences are short; later in the sort, the subse-\nquences are partially sorted. In both cases, insertion sort is the method of choice. The \nextent to which the subsequences are partially sorted is a variable factor that depends \nstrongly on the increment sequence. Understanding shellsort\u2019s performance is a chal-\nlenge. Indeed, Algorithm 2.3 is the only sorting method we consider whose perfor -\nmance on randomly ordered arrays has not been precisely ", "start": 270, "end": 270}, "375": {"text": "\nstrongly on the increment sequence. Understanding shellsort\u2019s performance is a chal-\nlenge. Indeed, Algorithm 2.3 is the only sorting method we consider whose perfor -\nmance on randomly ordered arrays has not been precisely characterized. \nL  E  E  A  M  H  L  E  P  S  O  L  T  S  X  R\nL           M           P           T  \n   E           H           S           S  \n      E           L           O           X \n         A           E           L           R\nh = 4\nAn h-sorted sequence is h interleaved sorted subsequences\n258 CHAPTER 2 \u25a0 Sorting\n ALGORITHM 2.3   Shellsort\npublic class    Shell \n{  \n   public static void sort(Comparable[] a)\n   {  // Sort a[] into increasing order.\n      int N = a.length;\n      int h = 1;\n      while (h < N/3) h = 3*h + 1; // 1, 4, 13, 40, 121, 364, 1093, ...\n      while (h >= 1)\n      {  // h-sort the array.\n         for (int i = h; i < N; i++)\n         {  // Insert a[i] among a[i-h], a[i-2*h], a[i-3*h]... .\n            for (int j = i; j >= h && less(a[j], a[j-h]); j -= h)\n               exch(a, j, j-h);\n         }\n         h = h/3;\n      }\n   }\n   // See page 245 for less(), exch(), isSorted(), and main().\n}\nIf we modify insertion sort ( Algorithm 2.2) to h-sort the array and add an outer loop to decrease \nh through a sequence of increments starting at an increment as large ", "start": 270, "end": 271}, "376": {"text": "exch(), isSorted(), and main().\n}\nIf we modify insertion sort ( Algorithm 2.2) to h-sort the array and add an outer loop to decrease \nh through a sequence of increments starting at an increment as large as a constant fraction of the ar-\nray length and ending at 1, we are led to this compact shellsort implementation.\n% java SortCompare Shell Insertion 100000 100 \nFor 100000 random Doubles\n  Shell is 600 times faster than Insertion\nShellsort trace (array contents after each pass)\nP  H  E  L  L  S  O  R  T  E  X  A  M  S  L  E  \nA  E  E  E  H  L  L  L  M  O  P  R  S  S  T  X  \nL  E  E  A  M  H  L  E  P  S  O  L  T  S  X  R  \nS  H  E  L  L  S  O  R  T  E  X  A  M  P  L  Einput \n13-sort\n4-sort\n1-sort\n2592.1 \u25a0 Elementary Sorts How do we decide what increment sequence to use? In general, this question is a dif -\n\ufb01cult one to answer. The performance of the algorithm depends not just on the num-\nber of increments, but also on arithmetical interactions among the increments such as \nthe size of their common divi-\nsors and other properties. Many \ndifferent increment sequences \nhave been studied in the lit-\nerature, but no provably best \nsequence has been found. The \nincrement sequence that is used \nin Algorithm 2.3 is easy to \ncompute and use, and performs \nnearly as well as more sophisti-\ncated increment sequences that \nhave ", "start": 271, "end": 272}, "377": {"text": "\nsequence has been found. The \nincrement sequence that is used \nin Algorithm 2.3 is easy to \ncompute and use, and performs \nnearly as well as more sophisti-\ncated increment sequences that \nhave been discovered that have \nprovably better worst-case per -\nformance. Increment sequences \nthat are substantially better still \nmay be waiting to be discovered.\nShellsort is useful even for \nlarge arrays, particularly by \ncontrast with selection sort and \ninsertion sort. It also performs \nwell on arrays that are in arbi -\ntrary order (not necessarily ran-\ndom). Indeed, constructing an \narray for which shellsort runs \nslowly for a particular incre-\nment sequence is usually a chal-\nlenging exercise. \nAs you can learn with \nSortCompare, shellsort is much \nfaster than insertion sort and \nselection sort, and its speed ad-\nvantage increases with the array \nsize. Before reading further, try using SortCompare to compare shellsort with insertion \nsort and selection sort for array sizes that are increasing powers of 2 on your computer \n(see Exercise 2.1.27). Y ou will see that shellsort makes it possible to address sorting \nDetailed trace of shellsort (insertions)\n13-sort\ninput\nresult\n4-sort\n1-sort\nS  H  E  L  L  S  O  R  T  E  X  A  M  P  L  E\nE  L  E  A  M  H  L  E  P  S  O  L  T  S  X  R \nE  E  L  A  M  H  L  E  P  S  O  L  T  S  X  R \nA  E  E  L  M  H  L  E  P  S  O  L  T  S  X ", "start": 272, "end": 272}, "378": {"text": "P  S  O  L  T  S  X  R \nA  E  E  L  M  H  L  E  P  S  O  L  T  S  X  R \nA  E  E  L  M  H  L  E  P  S  O  L  T  S  X  R \nA  E  E  H  L  M  L  E  P  S  O  L  T  S  X  R \nA  E  E  H  L  L  M  E  P  S  O  L  T  S  X  R \nA  E  E  E  H  L  L  M  P  S  O  L  T  S  X  R \nA  E  E  E  H  L  L  M  P  S  O  L  T  S  X  R \nA  E  E  E  H  L  L  M  P  S  O  L  T  S  X  R \nA  E  E  E  H  L  L  M  O  P  S  L  T  S  X  R \nA  E  E  E  H  L  L  L  M  O  P  S  T  S  X  R \nA  E  E  E  H  L  L  L  M  O  P  S  T  S  X  R \nA  E  E  E  H  L  L  L  M  O  P  S  S  T  X  R \nA  E  E  E  H  L  L  L  M  O  P  S  S  T  X  R ", "start": 272, "end": 272}, "379": {"text": "O  P  S  S  T  X  R \nA  E  E  E  H  L  L  L  M  O  P  S  S  T  X  R \nA  E  E  E  H  L  L  L  M  O  P  R  S  S  T  X \nL  H  E  L  P  S  O  R  T  E  X  A  M  S  L  E \nL  H  E  L  P  S  O  R  T  E  X  A  M  S  L  E \nL  H  E  L  P  S  O  R  T  E  X  A  M  S  L  E \nL  H  E  L  P  S  O  R  T  E  X  A  M  S  L  E \nL  H  E  L  P  S  O  R  T  E  X  A  M  S  L  E \nL  E  E  L  P  H  O  R  T  S  X  A  M  S  L  E \nL  E  E  L  P  H  O  R  T  S  X  A  M  S  L  E \nL  E  E  A  P  H  O  L  T  S  X  R  M  S  L  E \nL  E  E  A  M  H  O  L  P  S  X  R  T  S  L  E \nL  E  E  A  M  H  O  L  P  S  X  R  T  S  L  E \nL ", "start": 272, "end": 272}, "380": {"text": "X  R  T  S  L  E \nL  E  E  A  M  H  O  L  P  S  X  R  T  S  L  E \nL  E  E  A  M  H  L  L  P  S  O  R  T  S  X  E \nL  E  E  A  M  H  L  E  P  S  O  L  T  S  X  R \nP  H  E  L  L  S  O  R  T  E  X  A  M  S  L  E \nP  H  E  L  L  S  O  R  T  E  X  A  M  S  L  E \nP  H  E  L  L  S  O  R  T  E  X  A  M  S  L  E \nA  E  E  E  H  L  L  L  M  O  P  R  S  S  T  X \n260 CHAPTER 2 \u25a0 Sorting\n Visual trace of shellsort\ninput\n40-sorted\n13-sorted\n4-sorted\nresult\nproblems that could not be addressed with the more elementary algorithms. This ex-\nample is our \ufb01rst practical illustration of an important principle that pervades this \nbook: achieving speedups that enable the solution of problems that could not otherwise be \nsolved is one of the prime reasons to study algorithm performance and design.\nThe study of the performance characteristics of shellsort requires mathematical ar -\nguments that are beyond the scope of this book. If you want to be convinced, start \nby thinking about how you would prove the following fact: when an h-sorted array is \nk-sorted, it remains h-sorted. As for the performance of Algorithm 2.3, ", "start": 272, "end": 273}, "381": {"text": "If you want to be convinced, start \nby thinking about how you would prove the following fact: when an h-sorted array is \nk-sorted, it remains h-sorted. As for the performance of Algorithm 2.3, the most im -\nportant result in the present context is the knowledge that the running time of shellsort \nis not necessarily quadratic\u2014for example, it is known that the worst-case number of \ncompares for Algorithm 2.3 is proportional to N 3/2. That such a simple modi\ufb01cation \n2612.1 \u25a0 Elementary Sorts\n  \ncan break the quadratic-running-time barrier is quite interesting, as doing so is a prime \ngoal for many algorithm design problems.\nNo mathematical results are available about the average-case number of compares \nfor shellsort for randomly ordered input. Increment sequences have been devised that \ndrive the asymptotic growth of the worst-case number of compares down to N 4/3, N 5/4, \nN 6/5, . . . , but many of these results are primarily of academic interest because these \nfunctions are hard to distinguish from one another (and from a constant factor of N ) \nfor practical values of N. \nIn practice, you can safely take advantage of the past scienti\ufb01c study of shellsort just \nby using the increment sequence in Algorithm 2.3 (or one of the increment sequences \nin the exercises at the end of this section, which may improve performance by 20 to 40 \npercent). Moreover, you can easily validate the following hypothesis:\nProperty E.  The number of compares used by shellsort with the increments 1, 4, \n13, 40, 121, 364, . . . is bounded by a small multiple of N times the number of incre-\nments used.\nEvidence: Instrumenting Algorithm 2.3 to count compares and divide by the \nnumber of increments ", "start": 273, "end": 274}, "382": {"text": "121, 364, . . . is bounded by a small multiple of N times the number of incre-\nments used.\nEvidence: Instrumenting Algorithm 2.3 to count compares and divide by the \nnumber of increments used is a straightforward exercise (see Exercise 2.1.12). Ex-\ntensive experiments suggest that the average number of compares per increment \nmight be N 1/5, but it is quite dif\ufb01cult to discern the growth in that function unless \nN is huge. This property also seems to be rather insensitive to the input model.\n \nExperienced programmers sometimes choose shellsort because it has acceptable \nrunning time even for moderately large arrays; it requires a small amount of code; and \nit uses no extra space. In the next few sections, we shall see methods that are more ef-\n\ufb01cient, but they are perhaps only twice as fast (if that much) except for very large N, and \nthey are more complicated. If you need a solution to a sorting problem, and are work-\ning in a situation where a system sort may not be available (for example, code destined \nfor hardware or an embedded system), you can safely use shellsort, then determine \nsometime later whether it will be worthwhile to replace it with a more sophisticated \nmethod.\n262 CHAPTER 2 \u25a0 Sorting\n Q&A\nQ. Sorting seems like a toy problem.  Aren\u2019t many of the other things that we do with \ncomputers much more interesting?\nA. Perhaps, but many of those interesting things are made possible by fast sorting al-\ngorithms. Y ou will \ufb01nd many examples in Section 2.5 and throughout the rest of the \nbook. Sorting is worth studying now because the problem is easy to understand, and \nyou can appreciate the ingenuity behind the faster algorithms.\nQ. Why so many sorting algorithms? \nA. One reason is that the performance of many algorithms depends on the input val-\nues, ", "start": 274, "end": 275}, "383": {"text": "because the problem is easy to understand, and \nyou can appreciate the ingenuity behind the faster algorithms.\nQ. Why so many sorting algorithms? \nA. One reason is that the performance of many algorithms depends on the input val-\nues, so different algorithms might be appropriate for different applications having dif-\nferent kinds of input. For example, insertion sort is the method of choice for partially \nsorted or tiny arrays. Other constraints, such as space and treatment of equal keys, also \ncome into play. We will revisit this question in Section 2.5.\nQ. Why bother using the tiny helper methods less() and exch()?\nA. They are basic abstract operations needed by any sort algorithm, and the code is \neasier to understand in terms of these abstractions. Moreover, they make the code di-\nrectly portable to other settings. For example, much of the code in Algorithms 2.1 \nand 2.2 is legal code in several other programming languages. Even in Java, we can use \nthis code as the basis for sorting primitive types (which are not Comparable): simply \nimplement less() with the code v < w.\nQ. When I run SortCompare, I get different values each time that I run it (and those \nare different from the values in the book). Why?\nA. For starters, you have a different computer from the one we used, not to mention \na different operating system, Java runtime, and so forth. All of these differences might \nlead to slight differences in the machine code for the algorithms. Differences each time \nthat you run it on your computer might be due to other applications that you are run-\nning or various other conditions. Running a very large number of trials should dampen \nthe effect. The lesson is that small differences in algorithm performance are dif\ufb01cult to \nnotice nowadays. That is a primary reason that we focus on large ones!\n2632.1 \u25a0 ", "start": 275, "end": 275}, "384": {"text": "number of trials should dampen \nthe effect. The lesson is that small differences in algorithm performance are dif\ufb01cult to \nnotice nowadays. That is a primary reason that we focus on large ones!\n2632.1 \u25a0 Elementary Sorts\n EXERCISES\n2.1.1 Show, in the style of the example trace with Algorithm 2.1, how selection sort \nsorts the array E A S Y Q U E S T I O N.\n2.1.2 What is the maximum number of exchanges involving any particular element \nduring selection sort? What is the average number of exchanges involving an element?\n2.1.3 Give an example of an array of N items that maximizes the number of times the \ntest a[j] < a[min]  fails (and, therefore, min gets updated) during the operation of \nselection sort (Algorithm 2.1). \n2.1.4 Show, in the style of the example trace with Algorithm 2.2, how insertion sort \nsorts the array E A S Y Q U E S T I O N.\n2.1.5 For each of the two conditions in the inner for loop in insertion sort ( Algo-\nrithm 2.2), describe an array of N items where that condition is always false when the \nloop terminates.\n2.1.6 Which method runs faster for an array with all keys identical, selection sort or \ninsertion sort?\n2.1.7 Which method runs faster for an array in reverse order, selection sort or inser -\ntion sort?\n2.1.8 Suppose that we use insertion sort on a randomly ordered array where elements \nhave only one of three values. Is the running time linear, quadratic, or something in \nbetween?\n2.1.9 Show, in the style of the example trace with Algorithm 2.3, how shellsort sorts \nthe array E A S Y S ", "start": 275, "end": 276}, "385": {"text": "the running time linear, quadratic, or something in \nbetween?\n2.1.9 Show, in the style of the example trace with Algorithm 2.3, how shellsort sorts \nthe array E A S Y S H E L L S O R T Q U E S T I O N.\n2.1.10 Why not use selection sort for h-sorting in shellsort?\n2.1.11  Implement a version of shellsort that keeps the increment sequence in an array, \nrather than computing it.\n2.1.12  Instrument shellsort to print the number of compares divided by the array size \nfor each increment. Write a test client that tests the hypothesis that this number is a \nsmall constant, by sorting arrays of random Double values, using array sizes that are \nincreasing powers of 10, starting at 100.\n264 CHAPTER 2 \u25a0 Sorting\n CREATIVE PROBLEMS\n2.1.13    Deck sort. Explain how you would put a deck of cards in order by suit (in the \norder spades, hearts, clubs, diamonds) and by rank within each suit, with the restriction \nthat the cards must be laid out face down in a row, and the only allowed operations are \nto check the values of two cards and to exchange two cards (keeping them face down).\n2.1.14  Dequeue sort. Explain how you would sort a deck of cards, with the restric-\ntion that the only allowed operations are to look at the values of the top two cards, to \nexchange the top two cards, and to move the top card to the bottom of the deck.\n2.1.15  Expensive exchange. A clerk at a shipping company is charged with the task of \nrearranging a number of large crates in order of the time they are to be shipped out. \nThus, the cost of compares is very low (just look at the labels) relative ", "start": 276, "end": 277}, "386": {"text": "company is charged with the task of \nrearranging a number of large crates in order of the time they are to be shipped out. \nThus, the cost of compares is very low (just look at the labels) relative to the cost of ex-\nchanges (move the crates).  The warehouse is nearly full\u2014there is extra space suf\ufb01cient \nto hold any one of the crates, but not two. What sorting method should the clerk use?\n2.1.16      Certi\ufb01cation. Write a check() method that calls sort() for a given array and \nreturns true if sort() puts the array in order and leaves the same set of objects in the \narray as were there initially, false otherwise. Do not assume that sort() is restricted to \nmove data only with exch(). You may use Arrays.sort() and assume that it is correct.\n2.1.17    Animation. Add code to Insertion and Selection to make them draw the \narray contents as vertical bars like the visual traces in this section, redrawing the bars \nafter each pass, to produce an animated effect, ending in a \u201csorted\u201d picture where the \nbars appear in order of their height. Hint : Use a client like the one in the text that gener-\nates random Double values, insert calls to show() as appropriate in the sort code, and \nimplement a show() method that clears the canvas and draws the bars.\n2.1.18    Visual trace. Modify your solution to the previous exercise to make Insertion\nand Selection produce visual traces such as those depicted in this section. Hint : Judi-\ncious use of setYscale() makes this problem easy. Extra credit : Add the code necessary \nto produce red and gray color accents such as those in our \ufb01gures.\n2.1.19  Shellsort worst case. Construct an array of 100 elements containing the num -\nbers 1 through 100 ", "start": 277, "end": 277}, "387": {"text": "\nto produce red and gray color accents such as those in our \ufb01gures.\n2.1.19  Shellsort worst case. Construct an array of 100 elements containing the num -\nbers 1 through 100 for which shellsort, with the increments 1 4 13 40, uses as large a \nnumber of compares as you can \ufb01nd.\n2.1.20  Shellsort best case. What is the best case for shellsort? Justify your answer.\n2652.1 \u25a0 Elementary Sorts\n 2.1.21    Comparable transactions. Using our code for Date (page 247) as a model, ex-\npand your implementation of Transaction (Exercise 1.2.13 ) so that it implements \nComparable, such that transactions are kept in order by amount.\nSolution :\npublic class    Transaction implements Comparable<Transaction> \n{\n   ...\n   private final double amount;\n   ...\n   public int compareTo(Transaction that)\n   {\n      if (this.amount > that.amount) return +1;\n      if (this.amount < that.amount) return -1;\n      return 0;\n   }\n   ... \n}\n2.1.22    Transaction sort test client. Write a class SortTransactions that consists of a \nstatic method main() that reads a sequence of transactions from standard input, sorts \nthem, and prints the result on standard output (see Exercise 1.3.17).\nSolution :\npublic class SortTransactions \n{\n   public static Transaction[] readTransactions()\n   {  // See Exercise 1.3.17  }\n   public static void main(String[] args)\n   {\n      Transaction[] transactions = readTransactions();\n      Shell.sort(transactions);\n      for (Transaction t : transactions)\n         StdOut.println(t);\n   } \n}\nCREATIVE PROBLEMS  (continued)\n266 CHAPTER 2 \u25a0 Sorting\n EXPERIMENTS\n \n2.1.23  Deck sort. Ask ", "start": 277, "end": 279}, "388": {"text": "(Transaction t : transactions)\n         StdOut.println(t);\n   } \n}\nCREATIVE PROBLEMS  (continued)\n266 CHAPTER 2 \u25a0 Sorting\n EXPERIMENTS\n \n2.1.23  Deck sort. Ask a few friends to sort a deck of cards (see Exercise 2.1.13). Ob-\nserve them carefully and write down the method(s) that they use.\n2.1.24  Insertion sort with sentinel. Develop an implementation of insertion sort that \neliminates the j>0 test in the inner loop by \ufb01rst putting the smallest item into position. \nUse SortCompare to evaluate the effectiveness of doing so. Note : It is often possible to \navoid an index-out-of-bounds test in this way\u2014the element that enables the test to be \neliminated is known as a sentinel.\n2.1.25    Insertion sort without exchanges. Develop an implementation of insertion sort \nthat moves larger elements to the right one position with one array access per entry, \nrather than using exch(). Use SortCompare to evaluate the effectiveness of doing so.\n2.1.26    Primitive types. Develop a version of insertion sort that sorts arrays of int\nvalues and compare its performance with the implementation given in the text (which \nsorts Integer values and implicitly uses autoboxing and auto-unboxing to convert).\n2.1.27    Shellsort is subquadratic. Use SortCompare to compare shellsort with insertion \nsort and selection sort on your computer. Use array sizes that are increasing powers of \n2, starting at 128.\n2.1.28    Equal keys. Formulate and validate hypotheses about the running time of in -\nsertion sort and selection sort for arrays that contain just two key values, assuming that \nthe values are equally likely to occur.\n2.1.29  Shellsort increments. Run experiments to compare the increment sequence in \nAlgorithm 2.3 with the ", "start": 279, "end": 279}, "389": {"text": "arrays that contain just two key values, assuming that \nthe values are equally likely to occur.\n2.1.29  Shellsort increments. Run experiments to compare the increment sequence in \nAlgorithm 2.3 with the sequence 1, 5, 19, 41, 109, 209, 505, 929, 2161, 3905, 8929, \n16001, 36289, 64769, 146305, 260609 (which is formed by merging together the se-\nquences 9\u00b74k /H110029\u00b72k  /H110011 and 4k /H110023\u00b72k  /H110011). See Exercise 2.1.11.\n2.1.30  Geometric increments. Run experiments to determine a value of t that leads to \nthe lowest running time of shellsort for random arrays for the increment sequence 1, \n\u23a3t\u23a6, \u23a3t 2\u23a6, \u23a3t 3\u23a6, \u23a3t 4\u23a6, . . .  for N = 10 6. Give the values of t and the increment sequences for \nthe best three values that you \ufb01nd.\n2672.1 \u25a0 Elementary Sorts\n EXPERIMENTS  (continued)\n \nThe following exercises describe various clients for helping to evaluate sorting methods. They \nare intended as starting points for helping to understand performance properties, using ran-\ndom data. In all of them, use time(), as in SortCompare, so that you can get more accurate \nresults by specifying more trials in the second command-line argument. We refer back to these \nexercises in later sections when evaluating more sophisticated methods. \n2.1.31  Doubling test.  Write a client that performs a doubling test for sort algorithms. \nStart at N equal to 1000, ", "start": 279, "end": 280}, "390": {"text": "to these \nexercises in later sections when evaluating more sophisticated methods. \n2.1.31  Doubling test.  Write a client that performs a doubling test for sort algorithms. \nStart at N equal to 1000, and print N, the predicted number of seconds, the actual num-\nber of seconds, and the ratio as N doubles. Use your program to validate that insertion \nsort and selection sort are quadratic for random inputs, and formulate and test a hy-\npothesis for shellsort. \n2.1.32  Plot running times. Write a client that uses StdDraw to plot the average running \ntimes of the algorithm for random inputs and various values of the array size. Y ou may \nadd one or two more command-line arguments. Strive to design a useful tool.\n2.1.33  Distribution. Write a client that enters into an in\ufb01nite loop running sort() on \narrays of the size given as the third command-line argument, measures the time taken \nfor each run, and uses StdDraw to plot the average running times. A picture of the dis-\ntribution of the running times should emerge.\n2.1.34   Corner cases. Write a client that runs sort() on dif\ufb01cult or pathological cases \nthat might turn up in practical applications. Examples include arrays that are already \nin order, arrays in reverse order, arrays where all keys are the same, arrays consisting of \nonly two distinct values, and arrays of size 0 or 1.\n2.1.35    Nonuniform distributions. Write a client that generates test data by randomly \nordering objects using other distributions than uniform, including the following:\n\u25a0 Gaussian\n\u25a0 Poisson\n\u25a0 Geometric\n\u25a0 Discrete (see Exercise 2.1.28 for a special case)\nDevelop and test hypotheses about the effect of such input on the performance of the \nalgorithms in this section.\n268 CHAPTER 2 \u25a0 Sorting\n ", "start": 280, "end": 280}, "391": {"text": "Geometric\n\u25a0 Discrete (see Exercise 2.1.28 for a special case)\nDevelop and test hypotheses about the effect of such input on the performance of the \nalgorithms in this section.\n268 CHAPTER 2 \u25a0 Sorting\n 2.1.36    Nonuniform data. Write a client that generates test data that is not uniform, \nincluding the following:\n\u25a0 Half the data is 0s, half 1s.\n\u25a0 Half the data is 0s, half the remainder is 1s, half the remainder is 2s, and so forth.\n\u25a0 Half the data is 0s, half random int values.\nDevelop and test hypotheses about the effect of such input on the performance of the \nalgorithms in this section.\n2.1.37  Partially sorted. Write a client that generates partially sorted arrays, including \nthe following:\n\u25a0 95 percent sorted, last percent random values\n\u25a0  All entries within 10 positions of their \ufb01nal place in the array\n\u25a0 Sorted except for 5 percent of the entries randomly dispersed throughout the \narray\nDevelop and test hypotheses about the effect of such input on the performance of the \nalgorithms in this section.\n2.1.38  Var ious types of items. Write a client that generates arrays of items of various \ntypes with random key values, including the following:\n\u25a0 String key (at least ten characters), one double value\n\u25a0 double key, ten String values (all at least ten characters)\n\u25a0 int key, one int[20] value\nDevelop and test hypotheses about the effect of such input on the performance of the \nalgorithms in this section.\n2692.1 \u25a0 Elementary Sorts\n 2.2    MERGESORT\nThe algorithms that we consider in this section are based on a simple operation \nknown as merging : combining two ordered arrays to make one larger ordered array. \nThis operation immediately leads to a simple recursive ", "start": 280, "end": 282}, "392": {"text": "2.2    MERGESORT\nThe algorithms that we consider in this section are based on a simple operation \nknown as merging : combining two ordered arrays to make one larger ordered array. \nThis operation immediately leads to a simple recursive sort method known as merge-\nsort : to sort an array,  divide it into two halves, sort the two halves (recursively), and \nthen merge the results. As you will see, one of mergesort\u2019s most attractive properties is \nthat it guarantees to sort any array of N items in time proportional to N log N. Its prime \ndisadvantage is that it uses extra space proportional to N.\n  A b s t r a c t   i n - p l a c e   m e r g e  The straightforward approach to implementing merg-\ning is to design a method that merges two disjoint ordered arrays of Comparable ob-\njects into a third array. This strategy is easy to implement: create an output array of the \nrequisite size and then choose successively the smallest remaining item from the two \ninput arrays to be the next item added to the output array. \nHowever, when we mergesort a large array, we are doing a huge number of merges, \nso the cost of creating a new array to hold the output every time that we do a merge is \nproblematic. It would be much more desirable to have an in-place method so that we \ncould sort the \ufb01rst half of the array in place, then sort the second half of the array in \nplace, then do the merge of the two halves by moving the items around within the ar -\nray, without using a signi\ufb01cant amount of other extra space. It is worthwhile to pause \nmomentarily to consider how you might do that. At \ufb01rst blush, this problem seems to \nbe one that must be simple to solve, but solutions that are known are quite complicated, \nespecially by comparison to alternatives ", "start": 282, "end": 282}, "393": {"text": "\nmomentarily to consider how you might do that. At \ufb01rst blush, this problem seems to \nbe one that must be simple to solve, but solutions that are known are quite complicated, \nespecially by comparison to alternatives that use extra space.\nStill, the abstraction of an in-place merge is useful. Accordingly, we use the method \nsignature merge(a, lo, mid, hi) to specify a merge method that puts the result of \nmerging the subarrays a[lo..mid] with a[mid+1..hi] into a single ordered array, \nleaving the result in a[lo..hi]. The code on the next page implements this merge \nmethod in just a few lines by copying everything to an auxiliary array and then merging \nback to the original. Another approach is described in Exercise 2.2.10.\nM  E  R  G  E  S  O  R  T  E  X  A  M  P  L  E\nE  E  G  M  O  R  R  S  T  E  X  A  M  P  L  E\nE  E  G  M  O  R  R  S  A  E  E  L  M  P  T  X\nA  E  E  E  E  G  L  M  M  O  P  R  R  S  T  X\ninput\nsort left half\nsort right half\nmerge results\nMergesort overview\n270\n    A b s t r a c t  i n - p l a c e  m e r g e\npublic static void merge(Comparable[] a, int lo, int mid, int hi) \n{  // Merge a[lo..mid] with a[mid+1..hi].\n   int i = lo, j = mid+1;\n   for (int ", "start": 282, "end": 283}, "394": {"text": "merge(Comparable[] a, int lo, int mid, int hi) \n{  // Merge a[lo..mid] with a[mid+1..hi].\n   int i = lo, j = mid+1;\n   for (int k = lo; k <= hi; k++)  // Copy a[lo..hi] to aux[lo..hi].\n      aux[k] = a[k];\n   for (int k = lo; k <= hi; k++)  // Merge back to a[lo..hi].\n      if      (i > mid)              a[k] = aux[j++];\n      else if (j > hi )              a[k] = aux[i++];\n      else if (less(aux[j], aux[i])) a[k] = aux[j++];\n      else                           a[k] = aux[i++]; \n}\n This method merges by \ufb01rst copying into the auxiliary array aux[] then merging back to a[]. In the \nmerge (the second for loop), there are four conditions: left half exhausted (take from the right), right \nhalf exhausted (take from the left), current key on right less than current key on left (take from the \nright), and current key on right greater than or equal to current key on left (take from the left).\n                 a[]                                 aux[]\nk   0  1  2  3  4  5  6  7  8  9   i  j   0  1  2  3  4  5  6  7  8  9\n    E  E  G  M  R  A  C  E  R  T          -  -  -  -  -  -  -  -  -  -\n    E  E  G  M  R  A  C  E  R  T          E  E ", "start": 283, "end": 283}, "395": {"text": "T          -  -  -  -  -  -  -  -  -  -\n    E  E  G  M  R  A  C  E  R  T          E  E  G  M  R  A  C  E  R  T\n                                   0  5\n0   A                              0  6   E  E  G  M  R  A  C  E  R  T\n1   A  C                           0  7   E  E  G  M  R     C  E  R  T  \n2   A  C  E                        1  7   E  E  G  M  R        E  R  T  \n3   A  C  E  E                     2  7      E  G  M  R        E  R  T  \n4   A  C  E  E  E                  2  8         G  M  R        E  R  T  \n5   A  C  E  E  E  G               3  8         G  M  R           R  T  \n6   A  C  E  E  E  G  M            4  8            M  R           R  T  \n7   A  C  E  E  E  G  M  R         5  8               R           R  T  \n8   A  C  E  E  E  G  M  R  R      5  9                           R  T  \n9   A  C  E  E  E  G  M  R  R  T   6 10                              T \n    A  C  E  E  E  G  M  R  R  T                         \ninput\ncopy\nAbstract in-place merge trace\nmerged ", "start": 283, "end": 283}, "396": {"text": "M  R  R  T   6 10                              T \n    A  C  E  E  E  G  M  R  R  T                         \ninput\ncopy\nAbstract in-place merge trace\nmerged result\n2712.2 \u25a0 Mergesort    T o p - d o w n  m e r g e s o r t  Algorithm 2.4 is a recur -\nsive mergesort implementation based on this abstract in-\nplace merge. It is one of the best-known examples of the \nutility of the divide-and-conquer paradigm for ef\ufb01cient \nalgorithm design. This recursive code is the basis for an \ninductive proof that the algorithm sorts the array: if it \nsorts the two subarrays, it sorts the whole array, by merg-\ning together the subarrays. \nTo  u n d e r s t a n d  m e r g e s o r t , i t  i s  wo r t hw h i l e  to  co n s i d e r  \ncarefully the dynamics of the method calls, shown in the \ntrace at right. T o sort a[0..15], the sort() method calls \nitself to sort a[0..7] then calls itself to sort a[0..3]\nand a[0..1] before \ufb01nally doing the \ufb01rst merge of a[0]\nwith a[1] after calling itself to sort a[0] and then a[1]\n(for brevity, we omit the calls for the base-case 1-entry \nsorts in the trace). Then the next merge is a[2] with a[3]\nand then a[0..1] with a[2..3] and so forth. From this \ntrace, we see that the sort code simply provides an orga -\nnized way to sequence the calls to ", "start": 283, "end": 284}, "397": {"text": "a[3]\nand then a[0..1] with a[2..3] and so forth. From this \ntrace, we see that the sort code simply provides an orga -\nnized way to sequence the calls to the merge() method. \nThis insight will be useful later in this section.\nThe recursive code also provides us with the basis for \nanalyzing mergesort\u2019s running time. Because mergesort \nis a prototype of the divide-and-conquer algorithm de-\nsign paradigm, we will consider this analysis in detail. \n \nProposition F.    To p - d ow n  m e r g e s o r t  u s e s  b e t w e e n  \u00bd  N lg N and N lg N compares to \nsort any array of length N.\nProof: Let C(N) be the number of compares needed to sort an array of length N. \nWe have C(0) = C(1) = 0 and for N > 0 we can write a    recurrence relationship that \ndirectly mirrors the recursive sort() method to establish an upper bound:\nC(N ) /H11349 C(\u23a3N/2\u23a6)  /H11001 C(\u23a1N/2\u23a4) /H11001 N\nThe \ufb01rst term on the right is the number of compares to sort the left half of the ar-\nray, the second term is the number of compares to sort the right half, and the third\nsort(a, 0, 15)\n  sort(a, 0, 7)\n    sort(a, 0, 3)\n      sort(a, 0, 1)\n        merge(a, 0, 0, 1)\n      sort(a, 2, 3)\n        merge(a, 2, 2, 3)\n      merge(a, 0, 1, ", "start": 284, "end": 284}, "398": {"text": "merge(a, 0, 0, 1)\n      sort(a, 2, 3)\n        merge(a, 2, 2, 3)\n      merge(a, 0, 1, 3)\n    sort(a, 4, 7)\n      sort(a, 4, 5)\n        merge(a, 4, 4, 5)\n      sort(a, 6, 7)\n        merge(a, 6, 6, 7)\n      merge(a, 4, 5, 7)\n    merge(a, 0, 3, 7)\n  sort(a, 8, 15)\n    sort(a, 8, 11)\n      sort(a, 8, 9)\n        merge(a, 8, 8, 9)\n      sort(a, 10, 11)\n        merge(a, 10, 10, 11)\n      merge(a, 8, 9, 11)\n    sort(a, 12, 15)\n      sort(a, 12, 13)\n        merge(a, 12, 12, 13)\n      sort(a, 14, 15)\n        merge(a, 14, 14,15)\n      merge(a, 12, 13, 15)\n    merge(a, 8, 11, 15)\n  merge(a, 0, 7, 15)\nTop-down mergesor t call trace\nsort\nleft  half\nsort\nright half\nmerge\nresults\n272 CHAPTER 2 \u25a0 Sorting\n ALGORITHM 2.4    Top-down mergesort\npublic class Merge \n{  \n   private static Comparable[] aux;      // auxiliary array for merges\n   public static void sort(Comparable[] a)\n   {\n      aux = new Comparable[a.length];    // Allocate space just once.\n      sort(a, 0, ", "start": 284, "end": 285}, "399": {"text": "static Comparable[] aux;      // auxiliary array for merges\n   public static void sort(Comparable[] a)\n   {\n      aux = new Comparable[a.length];    // Allocate space just once.\n      sort(a, 0, a.length - 1);\n   }\n   private static void sort(Comparable[] a, int lo, int hi)\n   {  // Sort a[lo..hi].\n      if (hi <= lo) return;\n      int mid = lo + (hi - lo)/2;\n      sort(a, lo, mid);       // Sort left half.\n      sort(a, mid+1, hi);     // Sort right half.\n      merge(a, lo, mid, hi);  // Merge results (code on page 271).\n   } \n}\nTo  s o r t  a  s u b a r r ay  a[lo..hi] we divide it into two parts: a[lo..mid] and a[mid+1..hi], sort them \nindependently (via recursive calls), and merge the resulting ordered subarrays to produce the result.\nTrace of merge results for top-down mergesort\n                                                  a[]\n                             0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 \n                             M  E  R  G  E  S  O  R  T  E  X  A  M  P  L  E\n      merge(a,  0,  0,  1)   E  M  R  G  E  S  O  R  T  E  X  A  M  P  L  E \n      merge(a,  2,  2,  3)   E  M  G  R  E  S  O  R  T  E ", "start": 285, "end": 285}, "400": {"text": "A  M  P  L  E \n      merge(a,  2,  2,  3)   E  M  G  R  E  S  O  R  T  E  X  A  M  P  L  E \n    merge(a,  0,  1,  3)     E  G  M  R  E  S  O  R  T  E  X  A  M  P  L  E \n      merge(a,  4,  4,  5)   E  G  M  R  E  S  O  R  T  E  X  A  M  P  L  E \n      merge(a,  6,  6,  7)   E  G  M  R  E  S  O  R  T  E  X  A  M  P  L  E \n    merge(a,  4,  5,  7)     E  G  M  R  E  O  R  S  T  E  X  A  M  P  L  E \n  merge(a,  0,  3,  7)       E  E  G  M  O  R  R  S  T  E  X  A  M  P  L  E \n      merge(a,  8,  8,  9)   E  E  G  M  O  R  R  S  E  T  X  A  M  P  L  E \n      merge(a, 10, 10, 11)   E  E  G  M  O  R  R  S  E  T  A  X  M  P  L  E \n ", "start": 285, "end": 285}, "401": {"text": "merge(a, 10, 10, 11)   E  E  G  M  O  R  R  S  E  T  A  X  M  P  L  E \n    merge(a,  8,  9, 11)     E  E  G  M  O  R  R  S  A  E  T  X  M  P  L  E \n      merge(a, 12, 12, 13)   E  E  G  M  O  R  R  S  A  E  T  X  M  P  L  E \n      merge(a, 14, 14, 15)   E  E  G  M  O  R  R  S  A  E  T  X  M  P  E  L \n    merge(a, 12, 13, 15)     E  E  G  M  O  R  R  S  A  E  T  X  E  L  M  P \n  merge(a,  8, 11, 15)       E  E  G  M  O  R  R  S  A  E  E  L  M  P  T  X \nmerge(a,  0,  7, 15)         A  E  E  E  E  G  L  M  M  O  P  R  R  S  T  X \nlo hi\n2732.2 \u25a0 Mergesort  \nterm is the number of compares for the merge. The lower bound\nC(N ) /H11350 C(\u23a3N/2\u23a6)  /H11001 C(\u23a1N/2\u23a4) /H11001  \u23a3N/2\u23a6 ", "start": 285, "end": 286}, "402": {"text": "bound\nC(N ) /H11350 C(\u23a3N/2\u23a6)  /H11001 C(\u23a1N/2\u23a4) /H11001  \u23a3N/2\u23a6 \nfollows because the number of compares for the merge is at least \u23a3N/2\u23a6 .\nWe der ive an exact solution to the recur rence when equalit y holds and N is a \npower of 2 (say N = 2n). First, since  \u23a3N/2\u23a6 = \u23a1N/2\u23a4 = 2n/H110021, we have\nC(2n ) = 2C(2n/H110021) /H11001 2n\nDividing both sides by 2n gives\nC(2n )/2n = C(2n/H110021)/2n/H110021 /H11001 1\nApplying the same equation to the \ufb01rst term on the right, we have\nC(2n )/2n = C(2n/H110022)/2n/H110022 /H11001 1 /H11001 1\nRepeating the previous step n /H11002 1 additional times gives\nC(2n )/2n =C(20)/20 /H11001 n\nwhich, after multiplying both sides by 2n, leaves us with the solution\nC(N ) = C(2n ) = n 2n\n  = N lg N\nExact solutions for general N are more complicated, but it is not dif\ufb01cult to apply \nthe same argument to the inequalities describing the bounds on the number of \ncompares to prove the stated result for all values of N. This proof is valid no matter \nwhat the input values are and no matter in what order they ", "start": 286, "end": 286}, "403": {"text": "\nthe same argument to the inequalities describing the bounds on the number of \ncompares to prove the stated result for all values of N. This proof is valid no matter \nwhat the input values are and no matter in what order they appear. \nAnother way to understand Proposition F is to examine the tree drawn below, where \neach node depicts a subarray for which sort() does a merge(). The tree has precisely \nn levels. For k from 0 to n /H11002 1, the kth level from the top depicts 2 k subarrays, each of \nlength 2n/H11002k, each of which thus requires at most 2n/H11002k compares for the merge. Thus we \nhave 2k \u00b7 2n/H11002k   = 2n\n  total cost for each of the n levels, for a total of n 2n\n  = N lgN.\nMergesort subarray dependence tree for N = 16\na[0..1] a[2..3] a[4..5] a[6..7] a[8..9] a[10..11] a[12..13] a[14..15]\na[0..3]\na[0..7]\na[4..7]\na[0..15]\na[8..11]\na[8..15]\na[12..15]\nlg N\n274 CHAPTER 2 \u25a0 Sorting\n Proposition G. To p - d ow n  m e r g e s o r t  u s e s  a t  m o s t  6N lg N array accesses to sort an \narray of length N.\nProof: Each merge uses at most 6 N array accesses (2N for the copy, 2 N for the \nmove back, and at most 2N for compares). The result follows from the same ", "start": 286, "end": 287}, "404": {"text": "length N.\nProof: Each merge uses at most 6 N array accesses (2N for the copy, 2 N for the \nmove back, and at most 2N for compares). The result follows from the same argu-\nment as for Proposition F. \n \nPropositionS F and G tell us that we can expect the time required by mergesort to be \nproportional to N log N. That fact brings us to a different level from the elementary \nmethods in Section 2.1 because it tells us that we can sort huge arrays using just a \nlogarithmic factor more time than it takes to examine every entry. Y ou can sort millions \nof items (or more) with mergesort, but not with insertion sort or selection sort. The \nprimary drawback of mergesort is that it requires extra space proportional to N, for \nthe auxiliary array for merging. If space is at a premium, we need to consider another \nmethod.  On the other hand, we can cut the running time of mergesort substantially \nwith some carefully considered modi\ufb01cations to the implementation.\n U s e  i n s e r t i o n  s o r t  f o r  s m a l l  s u b a r r a y s .  We can improve most recursive algor ithms by \nhandling small cases differently, because the recursion guarantees that the method will \nbe used often for small cases, so improvements in handling them lead to improvements \nin the whole algorithm. In the case of sorting, we know that insertion sort (or selection \nsort) is simple and therefore likely to be faster than mergesort for tiny subarrays. As \nusual, a visual trace provides insight into the operation of mergesort. The visual trace \non the facing page shows the operation of a mergesort implementation with a cutoff \nfor small subarrays. Switching to insertion sort for small subarrays (length ", "start": 287, "end": 287}, "405": {"text": "trace provides insight into the operation of mergesort. The visual trace \non the facing page shows the operation of a mergesort implementation with a cutoff \nfor small subarrays. Switching to insertion sort for small subarrays (length 15 or less, \nsay) will improve the running time of a typical mergesort implementation by 10 to 15 \npercent (see Exercise 2.2.23).\nTe s t  w h e t h e r  t h e  a r ray  i s  a l re a dy  i n  o rd e r. We can reduce the running time to be \nlinear for arrays that are already in order by adding a test to skip the call to merge() if \na[mid] is less than or equal to a[mid+1]. With this change, we still do all the recursive \ncalls, but the running time for any sorted subarray is linear (see Exercise 2.2.8).\nEliminate the copy to the auxiliary array. It is possible to eliminate the time (but not \nthe space) taken to copy to the auxiliary array used for merging. T o do so, we use two \ninvocations of the sort method: one takes its input from the given array and puts the \nsorted output in the auxiliary array; the other takes its input from the auxiliary array \nand puts the sorted output in the given array. With this approach, in a bit of recursive \ntrickery, we can arrange the recursive calls such that the computation switches the roles \nof the input array and the auxiliary array at each level (see Exercise 2.2.11).\n2752.2 \u25a0 Mergesort\n Visual trace of top-down mergesort with cutoff for small subarrays\nfirst subarray\nsecond subarray\nfirst merge\nfirst half sorted\nsecond half sorted\nresult\n276 CHAPTER 2 \u25a0 Sorting\n It is appropriate to repeat here a point raised in Chapter ", "start": 287, "end": 289}, "406": {"text": "with cutoff for small subarrays\nfirst subarray\nsecond subarray\nfirst merge\nfirst half sorted\nsecond half sorted\nresult\n276 CHAPTER 2 \u25a0 Sorting\n It is appropriate to repeat here a point raised in Chapter 1 that is easily forgotten and \nneeds reemphasis. Locally, we treat each algorithm in this book as if it were critical in \nsome application. Globally, we try to reach general conclusions about which approach \nto recommend. Our discussion of such improvements is not necessarily a recommen -\ndation to always implement them, rather a warning not to draw absolute conclusions \nabout performance from initial implementations. When addressing a new problem, \nyour best bet is to use the simplest implementation with which you are comfortable \nand then re\ufb01ne it if it becomes a bottleneck. Addressing improvements that decrease \nrunning time just by a constant factor may not otherwise be worthwhile. Y ou need to \ntest the effectiveness of speci\ufb01c improvements by running experiments, as we indicate \nin exercises throughout.\nIn the case of mergesort, the three improvements just listed are simple to implement \nand are of interest when mergesort is the method of choice\u2014for example, in situations \ndiscussed at the end of this chapter.\nBottom-up mergesort The recursive implementation of mergesort is prototypi-\ncal of the divide-and-conquer algorithm design paradigm, where we solve a large prob-\nlem by dividing it into pieces, solving the subproblems, then using the solutions for the \npieces to solve the whole problem. Even though we are thinking in terms of merging \ntogether two large subarrays, the fact is that most merges are merging together tiny \nsubarrays. Another way to implement mergesort is to organize the merges so that we do \nall the merges of tiny subarrays on one pass, then do a second pass to merge those sub-\narrays in pairs, and so forth, continuing until we \ndo a merge that encompasses the whole ", "start": 289, "end": 289}, "407": {"text": "merges so that we do \nall the merges of tiny subarrays on one pass, then do a second pass to merge those sub-\narrays in pairs, and so forth, continuing until we \ndo a merge that encompasses the whole array. This \nmethod requires even less code than the standard \nrecursive implementation. We start by doing a pass \nof 1-by-1 merges (considering individual items as \nsubarrays of size 1), then a pass of 2-by-2 merges \n(merge subarrays of size 2 to make subarrays of \nsize 4), then 4-by-4 merges, and so forth. The sec-\nond subarray may be smaller than the \ufb01rst in the \nlast merge on each pass (which is no problem for \nmerge()), but otherwise all merges involve subar-\nrays of equal size, doubling the sorted subarray size \nfor the next pass.\nsz = 1\n2\n4\n  8\n16\nVisual trace of bottom-up mergesort\n2772.2 \u25a0 Mergesort\n Bottom-up mergesort\npublic class    MergeBU \n{  \n   private static Comparable[] aux;      // auxiliary array for merges\n   // See page 271 for merge() code.\n   public static void sort(Comparable[] a)\n   {  // Do lg N passes of pairwise merges.  \n      int N = a.length;\n      aux = new Comparable[N];\n      for (int sz = 1; sz < N; sz = sz+sz)        // sz: subarray size\n         for (int lo = 0; lo < N-sz; lo += sz+sz) // lo: subarray index\n            merge(a, lo, lo+sz-1, Math.min(lo+sz+sz-1, N-1));\n   } \n}\nBottom-up mergesort consists of a sequence of passes over the whole array, doing sz-by-sz ", "start": 289, "end": 290}, "408": {"text": "merge(a, lo, lo+sz-1, Math.min(lo+sz+sz-1, N-1));\n   } \n}\nBottom-up mergesort consists of a sequence of passes over the whole array, doing sz-by-sz merges, \nstarting with sz equal to 1 and doubling sz on each pass. The \ufb01nal subarray is of size sz only when \nthe array size is an even multiple of sz (otherwise it is less than sz). \nTrace of merge results for bottom-up mergesort\n                                                   a[i]\n                               0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15  \n                               M  E  R  G  E  S  O  R  T  E  X  A  M  P  L  E\n        merge(a,  0,  0,  1)   E  M  R  G  E  S  O  R  T  E  X  A  M  P  L  E  \n        merge(a,  2,  2,  3)   E  M  G  R  E  S  O  R  T  E  X  A  M  P  L  E  \n        merge(a,  4,  4,  5)   E  M  G  R  E  S  O  R  T  E  X  A  M  P  L  E  \n        merge(a,  6,  6,  7)   E  M  G  R  E  S  O  R  T  E  X  A  M  P  L  E  \n        merge(a,  8,  8,  9) ", "start": 290, "end": 290}, "409": {"text": "M  G  R  E  S  O  R  T  E  X  A  M  P  L  E  \n        merge(a,  8,  8,  9)   E  M  G  R  E  S  O  R  E  T  X  A  M  P  L  E  \n        merge(a, 10, 10, 11)   E  M  G  R  E  S  O  R  E  T  A  X  M  P  L  E  \n        merge(a, 12, 12, 13)   E  M  G  R  E  S  O  R  E  T  A  X  M  P  L  E  \n        merge(a, 14, 14, 15)   E  M  G  R  E  S  O  R  E  T  A  X  M  P  E  L\n  \n      merge(a,  0,  1,  3)     E  G  M  R  E  S  O  R  E  T  A  X  M  P  E  L \n      merge(a,  4,  5,  7)     E  G  M  R  E  O  R  S  E  T  A  X  M  P  E  L  \n      merge(a,  8,  9, 11)     E  G  M  R  E  O  R  S  A  E  T  X  M  P  E  L  \n      merge(a, 12, 13, 15)     E  G  M  R  E  O  R  S  A  E  T  X ", "start": 290, "end": 290}, "410": {"text": "X  M  P  E  L  \n      merge(a, 12, 13, 15)     E  G  M  R  E  O  R  S  A  E  T  X  E  L  M  P\n    merge(a,  0,  3,  7)       E  E  G  M  O  R  R  S  A  E  T  X  E  L  M  P  \n    merge(a,  8, 11, 15)       E  E  G  M  O  R  R  S  A  E  E  L  M  P  T  X\n merge(a,  0,  7, 15)         A  E  E  E  E  G  L  M  M  O  P  R  R  S  T  X \nsz = 1\nsz = 2\nsz = 4\nsz = 8\n278 CHAPTER 2 \u25a0 Sorting Proposition H.  Bottom-up mergesort uses between \u00bd N lg N and N lg N compares  \nand at most 6N lg N array accesses to sort an array of length N.  \nProof: The number of passes through the array is precisely \u23a3lg N\u23a6 (that is precisely \nthe value of n such that 2n  /H11349 N < 2n/H110011). For each pass, the number of array accesses \nis exactly 6N and the number of compares is at most N and no less than N/ 2. \n \nWhen the array length is a power of 2, top-down and bottom-up mergesort per-\nform precisely the same compares and array accesses, just in a different order. When the \narray length is not a power of 2, the sequence of compares and ", "start": 290, "end": 291}, "411": {"text": "of 2, top-down and bottom-up mergesort per-\nform precisely the same compares and array accesses, just in a different order. When the \narray length is not a power of 2, the sequence of compares and array accesses for the two \nalgorithms will be different (see Exercise 2.2.5).\nA version of bottom-up mergesort is the method of choice for sorting data orga-\nnized in a  linked list. Consider the list to be sorted sublists of size 1, then pass through to \nmake sorted subarrays of size 2 linked together, then size 4, and so forth. This method \nrearranges the links to sort the list in place (without creating any new list nodes).\nBoth the top-down and bottom-up approaches to implementing a divide-and-\nconquer algorithm are intuitive. The lesson that you can take from mergesort is this: \nWhenever you encounter an algorithm based on one of these approaches, it is worth \nconsidering the other. Do you want to solve the problem by breaking it up into smaller \nproblems (and solving them recursively) as in Merge.sort() or by building small solu-\ntions into larger ones as in MergeBU.sort()? \n    T h e   c o m p l e x i t y  o f  s o r t i n g  One important reason to know about mergesort is \nthat we use it as the basis for proving a fundamental result in the \ufb01eld of computational \ncomplexity that helps us understand the intrinsic dif\ufb01culty of sorting.  In general, com-\nputational complexity plays an important role in the design of algorithms, and this \nresult in particular is directly relevant to the design of sorting algorithms, so we next \nconsider it in detail.\nThe \ufb01rst step in a study of complexity is to establish a model of computation. Gen -\nerally, researchers strive to understand the simplest model ", "start": 291, "end": 291}, "412": {"text": "to the design of sorting algorithms, so we next \nconsider it in detail.\nThe \ufb01rst step in a study of complexity is to establish a model of computation. Gen -\nerally, researchers strive to understand the simplest model relevant to a problem. For \nsorting, we study the class of  compare-based algorithms that make their decisions about \nitems only on the basis of comparing keys. A compare-based algorithm can do an ar -\nbitrary amount of computation between compares, but cannot get any information \nabout a key except by comparing it with another one. Because of our restriction to the \nComparable API, all of the algorithms in this chapter are in this class (note that we are \nignoring the cost of array accesses), as are many algorithms that we might imagine. In \nChapter 5, we consider algorithms that are not restricted to Comparable items.\n2792.2 \u25a0 Mergesort\n Proposition I.  No  compare-based sorting algorithm can guarantee to sort N items \nwith fewer than lg(N !) ~ N lg N compares. \nProof: First, we assume that the keys are all distinct, since any algorithm must be \nable to sort such inputs. Now, we use a   binary tree to describe the sequence of com-\npares. Each node in the tree is either a leaf i0 i1 i2 ... iN-1  that indicates that the \nsort is complete and has discovered that the original inputs were in the order \na[i0], a[i1], ...a[iN-1], or an internal node i:j  that corresponds to a com-\npare operation between a[i] and a[j], with a left subtree corresponding to the \nsequence of compares in the case that a[i] is less than a[j], and a right subtree \ncorresponding to what happens if a[i] is greater than a[j]. Each path from the \nroot to a leaf corresponds to the sequence of compares that the algorithm ", "start": 291, "end": 292}, "413": {"text": "a[i] is less than a[j], and a right subtree \ncorresponding to what happens if a[i] is greater than a[j]. Each path from the \nroot to a leaf corresponds to the sequence of compares that the algorithm uses to \nestablish the ordering given in the leaf. For example, here is a compare tree for \nN = 3:\n0 1 2 1 0 2\n0 2 1 2 0 1 1 2 0 2 1 0\n0:1\n0:2\n1:2\n1:2\n0:2\n \nWe never explicitly construct such a tree\u2014it is a mathematical dev ice for descr ib -\ning the compares used by any algorithm. \nThe \ufb01rst key observation in the proof is that the tree must have at least N ! leaves \nbecause there are N ! different permutations of N distinct keys. If there are fewer \nthan N ! leaves, then some permutation is missing from the leaves, and the algo-\nrithm would fail for that permutation. \nThe number of internal nodes on a path from the root to a leaf in the tree is the \nnumber of compares used by the algorithm for some input. We are interested in the \nlength of the longest such path in the tree (known as the tree height) since it mea-\nsures the worst-case number of compares used by the algorithm. Now, it is a basic \ncombinatorial property of binary trees that a tree of height h has no more than 2h\nleaves\u2014the tree of height h with the maximum number of leaves is perfectly bal-\nanced, or complete. An example for h = 4 is diagrammed on the next page. \n280 CHAPTER 2 \u25a0 Sorting\n complete tree of\nheight 4 (gray) has\n24 = 16 leaves\nany other tree of\nheight 4 (black) has\nfewer ", "start": 292, "end": 293}, "414": {"text": "on the next page. \n280 CHAPTER 2 \u25a0 Sorting\n complete tree of\nheight 4 (gray) has\n24 = 16 leaves\nany other tree of\nheight 4 (black) has\nfewer than 16 leaves\nCombining the previous two paragraphs, we have shown that any compare-based \nsorting algorithm corresponds to a compare tree of height h with\nN ! /H11349 number of leaves /H11349 2h\nat least N! leaves no more than 2h leaves\nh\nThe value of h is precisely the worst-case number of compares, so we can take the \nlogarithm (base 2) of both sides of this equation and conclude that the number of \ncompares used by any algorithm must be at least lg N ! . The approximation lg N ! ~\nN lg N  follows immediately from Stirling\u2019s approximation to the factorial function \n(see page 185). \nThis result serves as a guide for us to know, when designing a sorting algorithm, how \nwell we can expect to do. For example, without such a result, one might set out to try \nto design a compare-based sorting algorithm that uses half as many compares as does \nmergesort, in the worst case. The lower bound in Proposition I says that such an effort \nis futile\u2014no such algorithm exists. It is an extremely strong statement that applies to any \nconceivable compare-based algorithm.\nProposition H asserts that the number of compares used by mergesort in the worst \ncase is ~ N lg N. This result is an  upper bound on the dif\ufb01culty of the sorting problem \nin the sense that a better algorithm would have to guarantee to use a smaller number of \ncompares. Proposition I asserts that no sorting algorithm can guarantee to use fewer \n2812.2 \u25a0 Mergesort\n than ~ N lg N compares. It is a lower bound on the ", "start": 293, "end": 294}, "415": {"text": "use a smaller number of \ncompares. Proposition I asserts that no sorting algorithm can guarantee to use fewer \n2812.2 \u25a0 Mergesort\n than ~ N lg N compares. It is a lower bound on the dif\ufb01culty of the sorting problem in \nthe sense that even the best possible algorithm must use at least that many compares in \nthe worst case. T ogether, they imply:\nProposition J.   Mergesort is an asymptotically optimal compare-based sorting \nalgorithm.\nProof: Precisely, we mean by this statement that both the number of compares used \nby mergesort in the worst case and the minimum number of compares that any com-\npare-based sorting algorithm can guarantee are  ~N lg N. Propositions H and\u00a0I es-\ntablish these facts. \nIt is important to note that, like the model of computation, we need to precisely de\ufb01ne \nwhat we mean by an optimal algorithm. For example, we might tighten the de\ufb01nition \nof optimality and insist that an optimal algorithm for sorting is one that uses precisely\nlg N! compares. We do not do so because we could not notice the difference between \nsuch an algorithm and (for example) mergesort for large N. Or, we might broaden the \nde\ufb01nition of optimality to include any sorting algorithm whose worst-case number of \ncompares is within a constant factor of N lg N. We do not do so because we mig ht ver y \nwell notice the difference between such an algorithm and mergesort for large N.\nComputational complexity may seem rather abstract , but fundamental re -\nsearch on the intrinsic dif\ufb01culty of solving computational problems hardly needs jus-\nti\ufb01cation. Moreover, when it does apply, it is emphatically the case that computational \ncomplexity affects the development of good software. First, good upper bounds allow \nsoftware engineers to provide performance guarantees; ", "start": 294, "end": 294}, "416": {"text": "needs jus-\nti\ufb01cation. Moreover, when it does apply, it is emphatically the case that computational \ncomplexity affects the development of good software. First, good upper bounds allow \nsoftware engineers to provide performance guarantees; there are many documented \ninstances where poor performance has been traced to someone using a quadratic sort \ninstead of a linearithmic one. Second, good lower bounds spare us the effort of search-\ning for performance improvements that are not attainable. \nBut the optimality of mergesort is not the end of the story and should not be mis -\nused to indicate that we need not consider other methods for practical applications. \nThat is not the case because the theory in this section has a number of limitations. For \nexample:\n\u25a0 Mergesort is not optimal with respect to space usage.\n\u25a0 The worst case may not be likely in practice.\n\u25a0 Operations other than compares (such as array accesses) may be important.\n\u25a0 One can sort certain data without using any compares.\nThus, we shall be considering several other sorting methods in this book.\n282 CHAPTER 2 \u25a0 Sorting\n Q&A\nQ. Is mergesort faster than shellsort?\nA. In practice, their running times are within a small constant factor of one another \n(when shellsort is using a well-tested increment sequence like the one in Algorithm \n2.3), so comparative performance depends on the implementations. \n% java SortCompare Merge Shell 100000\nFor 100000 random Double values\n    Merge is 1.2 times faster than Shell\nIn theory, no one has been able to prove that shellsort is linearithmic for random data, \nso there remains the possibility that the asymptotic growth of the average-case perfor-\nmance of shellsort is higher. Such a gap has been proven for worst-case performance, \nbut it is not relevant in practice.\nQ. Why not make the aux[] array local to merge()?\nA. To ", "start": 294, "end": 295}, "417": {"text": "perfor-\nmance of shellsort is higher. Such a gap has been proven for worst-case performance, \nbut it is not relevant in practice.\nQ. Why not make the aux[] array local to merge()?\nA. To  avo i d  t h e  ove r h e a d  o f  c re a t i n g  a n  a r r ay  f o r  e ve r y  m e r g e , e ve n  t h e  t i ny  o n e s . T h i s  \ncost would dominate the running time of mergesort (see Exercise 2.2.26). A more \nproper solution (which we avoid in the text to reduce clutter in the code) is to make \naux[] local to sort() and pass it as an argument to merge() (see Exercise 2.2.9).\nQ. How does mergesort fare when there are duplicate values in the array?\nA. If all the items have the same value, the running time is linear (with the extra test to \nskip the merge when the array is sorted), but if there is more than one duplicate value, \nthis performance gain is not necessarily realized. For example, suppose that the input \narray consists of N items with one value in odd positions and N items with another \nvalue in even positions. The running time is linearithmic for such an array (it satis\ufb01es \nthe same recurrence as for items with distinct values), not linear.\n2832.2 \u25a0 Mergesort\n EXERCISES\n2.2.1 Give a trace, in the style of the trace given at the beginning of this section, show-\ning how the keys A E Q S U Y E I N O S T  are merged with the abstract in-place \nmerge() method.\n2.2.2  Give traces, in the style ", "start": 295, "end": 296}, "418": {"text": "this section, show-\ning how the keys A E Q S U Y E I N O S T  are merged with the abstract in-place \nmerge() method.\n2.2.2  Give traces, in the style of the trace given with Algorithm 2.4, showing how the \nkeys E A S Y Q U E S T I O N are sorted with top-down mergesort.\n2.2.3 Answer Exercise 2.2.2 for bottom-up mergesort.\n2.2.4 Does the abstract in-place merge produce proper output if and only if the two \ninput subarrays are in sorted order? Prove your answer, or provide a counterexample.\n2.2.5  Give the sequence of subarray sizes in the merges performed by both the top-\ndown and the bottom-up mergesort algorithms, for N = 39.\n2.2.6 Write a program to compute the exact value of the number of array accesses used \nby top-down mergesort and by bottom-up mergesort. Use your program to plot the val-\nues for N from 1 to 512, and to compare the exact values with the upper bound 6N lg N.\n2.2.7 Show that the number of compares used by mergesort is monotonically increas-\ning (C(N+1) > C(N) for all N > 0).\n2.2.8  Suppose that Algorithm 2.4 is modi\ufb01ed to skip the call on merge() whenever \na[mid] <= a[mid+1]. Prove that the number of compares used to mergesort a sorted \narray is linear.\n2.2.9  Use of a static array like aux[] is inadvisable in library software because multiple \nclients might use the class concurrently. Give an implementation of Merge that does not \nuse a static array. Do not make aux[] local to merge() (see the Q&A ", "start": 296, "end": 296}, "419": {"text": "aux[] is inadvisable in library software because multiple \nclients might use the class concurrently. Give an implementation of Merge that does not \nuse a static array. Do not make aux[] local to merge() (see the Q&A for this section). \nHint : Pass the auxiliary array as an argument to the recursive sort().\n284 CHAPTER 2 \u25a0 Sorting\n CREATIVE PROBLEMS\n \n \n  \n \n2.2.10    Faster merge. Implement a version of merge() that copies the second half of \na[] to aux[] in decreasing order and then does the merge back to a[]. This change al-\nlows you to remove the code to test that each of the halves has been exhausted from the \ninner loop. Note: The resulting sort is not stable (see page 341).\n2.2.11    Improvements. Implement the three improvements to mergesort that are de -\nscribed in the text on page 275: Add a cutoff for small subarrays, test whether the array is \nalready in order, and avoid the copy by switching arguments in the recursive code.\n2.2.12  Sublinear extra space. Develop a merge implementation that reduces the extra \nspace requirement to max(M, N/M), based on the following idea: Divide the array into \nN/M blocks of size M (for simplicity in this description, assume that N is a multiple \nof M). Then, (i) considering the blocks as items with their \ufb01rst key as the sort key, sort \nthem using selection sort; and (ii) run through the array merging the \ufb01rst block with \nthe second, then the second block with the third, and so forth.\n2.2.13    Lower bound for average case. Prove that the expected number of compares used \nby any compare-based sorting algorithm must be at least ~ N lg N (assuming that all \npossible orderings of the input are equally likely). Hint: The expected number ", "start": 296, "end": 297}, "420": {"text": "case. Prove that the expected number of compares used \nby any compare-based sorting algorithm must be at least ~ N lg N (assuming that all \npossible orderings of the input are equally likely). Hint: The expected number of com-\npares is at least the external path length of the compare tree (the sum of the lengths of \nthe paths from the root to all leaves), which is minimized when it is balanced.\n2.2.14    Merging sorted queues. Develop a static method that takes two queues of sorted \nitems as arguments and returns a queue that results from merging the queues into \nsorted order.\n2.2.15    Bottom-up queue mergesort. Develop a bottom-up mergesort implementation \nbased on the following approach: Given N items, create N queues, each containing one \nof the items. Create a queue of the N queues. Then repeatedly apply the merging opera-\ntion of Exercise 2.2.14 to the \ufb01rst two queues and reinsert the merged queue at the end. \nRepeat until the queue of queues contains only one queue.\n2.2.16      Natural mergesort. Write a version of bottom-up mergesort that takes advan -\ntage of order in the array by proceeding as follows each time it needs to \ufb01nd two arrays \nto merge: \ufb01nd a sorted subarray (by incrementing a pointer until \ufb01nding an entry that \nis smaller than its predecessor in the array), then \ufb01nd the next, then merge them. Ana-\nlyze the running time of this algorithm in terms of the array size and the number of \n2852.2 \u25a0 Mergesort\n maximal increasing sequences in the array.\n2.2.17    Linked-list sort. Implement a natural mergesort for linked lists. (This is the \nmethod of choice for sorting linked lists because it uses no extra space and is guaranteed \nto be linearithmic.)\n2.2.18 ", "start": 297, "end": 298}, "421": {"text": "array.\n2.2.17    Linked-list sort. Implement a natural mergesort for linked lists. (This is the \nmethod of choice for sorting linked lists because it uses no extra space and is guaranteed \nto be linearithmic.)\n2.2.18   Shuf\ufb02ing a linked list. Develop and implement a divide-and-conquer algo-\nrithm that randomly  shuf\ufb02es a linked list in linearithmic time and logarithmic extra \nspace.\n2.2.19   Inversions. Develop and implement a linearithmic algorithm for computing \nthe number of inversions in a given array (the number of exchanges that would be \nperformed by insertion sort for that array\u2014see Section 2.1). This quantity is related \nto the  Kendall tau distance; see Section 2.5.\n2.2.20    Indirect sort. Develop and implement a version of mergesort that does not re-\narrange the array, but returns an int[] array perm such that perm[i] is the index of the \ni th smallest entry in the array.\n2.2.21  Triplicates. Given three lists of N names each, devise a linearithmic algorithm \nto determine if there is any name common to all three lists, and if so, return the \ufb01rst \nsuch name.\n2.2.22  3-way mergesort. Suppose instead of dividing in half at each step, you divide \ninto thirds, sort each third, and combine using a 3-way merge. What is the order of \ngrowth of the overall running time of this algorithm?\nCREATIVE PROBLEMS  (continued)\n286 CHAPTER 2 \u25a0 Sorting\n EXPERIMENTS\n2.2.23    Improvements. Run empirical studies to evaluate the effectiveness of each of the \nthree improvements to mergesort that are described in the text (see Exercise 2.2.11). ", "start": 298, "end": 299}, "422": {"text": "EXPERIMENTS\n2.2.23    Improvements. Run empirical studies to evaluate the effectiveness of each of the \nthree improvements to mergesort that are described in the text (see Exercise 2.2.11). \nAlso, compare the performance of the merge implementation given in the text with the \nmerge described in Exercise 2.2.10. In particular, empirically determine the best value \nof the parameter that decides when to switch to insertion sort for small subarrays.\n2.2.24  Sort-test improvement. Run empirical studies for large randomly ordered ar -\nrays to study the effectiveness of the modi\ufb01cation described in Exercise 2.2.8 for ran-\ndom data. In particular, develop a hypothesis about the average number of times the \ntest (whether an array is sorted) succeeds, as a function of N (the original array size for \nthe sort).\n2.2.25    Multiway mergesort. Develop a mergesort implementation based on the idea of \ndoing k-way merges (rather than 2-way merges). Analyze your algorithm, develop a hy-\npothesis regarding the best value of k, and run experiments to validate your hypothesis.\n2.2.26    Array creation. Use SortCompare to get a rough idea of the effect on perfor -\nmance on your machine of creating aux[] in merge() rather than in sort().\n2.2.27  Subarray lengths. Run mergesort for large random arrays, and make an empiri-\ncal determination of the average length of the other subarray when the \ufb01rst subarray \nexhausts, as a function of N (the sum of the two subarray sizes for a given merge).\n2.2.28  Top-dow n versus bottom-up. Use SortCompare to compare top-down and bot-\ntom-up mergesort for N=103, 104, 105, and 106.\n2.2.29 ", "start": 299, "end": 299}, "423": {"text": "merge).\n2.2.28  Top-dow n versus bottom-up. Use SortCompare to compare top-down and bot-\ntom-up mergesort for N=103, 104, 105, and 106.\n2.2.29  Natural mergesort. Determine empirically the number of passes needed in a \nnatural mergesort (see Exercise 2.2.16) for random Long keys with N=103, 106, and \n109. Hint: Y ou do not need to implement a sort (or even generate full 64-bit keys) to \ncomplete this exercise.\n2872.2 \u25a0 Mergesort\n 2.3    QUICKSORT\nThe subject of this section  is the sorting algorithm that is probably used more \nwidely than any other, quicksort. Quicksort is popular because it is not dif\ufb01cult to \nimplement, works well for a variety of different kinds of input data, and is substantially \nfaster than any other sorting method in typical applications. The quicksort algorithm\u2019s \ndesirable features are that it is in-place (uses only a small auxiliary stack) and that \nit requires time proportional to N log N on the average to sort an array of length N. \nNone of the algorithms that we have so far considered combine these two properties. \nFurthermore, quicksort has a shorter inner loop than most other sorting algorithms, \nwhich means that it is fast in practice as well as in theory. Its primary drawback is that \nit is fragile in the sense that some care is involved in the implementation to be sure to \navoid bad performance. Numerous examples of mistakes leading to quadratic perfor -\nmance in practice are documented in the literature. Fortunately, the lessons learned \nfrom these mistakes have led to various improvements to the algorithm that make it of \neven broader utility, as we shall see.\nThe basic algorithm  Quicksort is a  divide-and-conquer method for sorting. It ", "start": 299, "end": 300}, "424": {"text": "learned \nfrom these mistakes have led to various improvements to the algorithm that make it of \neven broader utility, as we shall see.\nThe basic algorithm  Quicksort is a  divide-and-conquer method for sorting. It \nworks by  partitioning an array into two subarrays, then sorting the subarrays indepen-\ndently. Quicksort is complementary to mergesort: for mergesort, we break the array \ninto two subarrays to be sorted and then combine the ordered subarrays to make the \nwhole ordered array; for quicksort, we rearrange the array such that, when the two \nsubarrays are sorted, the whole array is ordered. In the \ufb01rst instance, we do the two \nrecursive calls before working on the whole array; in the second instance, we do the two \nrecursive calls after working on the whole array. For mergesort, the array is divided in \nhalf; for quicksort, the position of the partition depends on the contents of the array. \nQ  U  I  C  K  S  O  R  T  E  X  A  M  P  L  E\nK  R  A  T  E  L  E  P  U  I  M  Q  C  X  O  S\nE  C  A  I  E  K  L  P  U  T  M  Q  R  X  O  S\nA  C  E  E  I  K  L  P  U  T  M  Q  R  X  O  S\nA  C  E  E  I  K  L  M  O  P  Q  R  S  T  U  X\nA  C  E  E  I  K  L  M  O  P  Q  R  S  T  U  X\nnot ", "start": 300, "end": 300}, "425": {"text": "P  Q  R  S  T  U  X\nA  C  E  E  I  K  L  M  O  P  Q  R  S  T  U  X\nnot greater not less\npartitioning element\ninput\nshuffle\npartition\nsort left\nsort right\nresult\nQuicksort overview\n288\n ALGORITHM 2.5    Quicksort\npublic class Quick \n{  \n   public static void sort(Comparable[] a)\n   {  \n      StdRandom.shuffle(a);          // Eliminate dependence on input.\n      sort(a, 0, a.length - 1);\n   }\n   private static void sort(Comparable[] a, int lo, int hi)\n   {\n      if (hi <= lo) return;\n      int j = partition(a, lo, hi);  // Partition (see page 291).\n      sort(a, lo, j-1);              // Sort left part a[lo .. j-1].\n      sort(a, j+1, hi);              // Sort right part a[j+1 .. hi].\n   } \n}\nQuicksort is a  recursive program that sorts a subarray a[lo...hi] by using a partition() method \nthat puts a[i] into position and arranges the rest of the entries such that the recursive calls \ufb01nish \nthe sort.\n lo   j  hi   0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n              Q  U  I  C  K  S  O  R  T  E  X  A  M  P  L  E\n              K  R  A  T  E  L  E  P  U  I  M  Q  C  X  O  S \n  0 ", "start": 300, "end": 301}, "426": {"text": "X  A  M  P  L  E\n              K  R  A  T  E  L  E  P  U  I  M  Q  C  X  O  S \n  0   5  15   E  C  A  I  E  K  L  P  U  T  M  Q  R  X  O  S  \n  0   3   4   E  C  A  E  I  K  L  P  U  T  M  Q  R  X  O  S  \n  0   2   2   A  C  E  E  I  K  L  P  U  T  M  Q  R  X  O  S  \n  0   0   1   A  C  E  E  I  K  L  P  U  T  M  Q  R  X  O  S  \n  1       1   A  C  E  E  I  K  L  P  U  T  M  Q  R  X  O  S  \n  4       4   A  C  E  E  I  K  L  P  U  T  M  Q  R  X  O  S  \n  6   6  15   A  C  E  E  I  K  L  P  U  T  M  Q  R  X  O  S  \n  7   9  15   A  C  E  E  I  K  L  M  O  P  T  Q  R  X  U  S  \n  7   7   8   A  C  E  E  I  K  L  M ", "start": 301, "end": 301}, "427": {"text": "K  L  M  O  P  T  Q  R  X  U  S  \n  7   7   8   A  C  E  E  I  K  L  M  O  P  T  Q  R  X  U  S  \n  8       8   A  C  E  E  I  K  L  M  O  P  T  Q  R  X  U  S  \n10  13  15   A  C  E  E  I  K  L  M  O  P  S  Q  R  T  U  X  \n10  12  12   A  C  E  E  I  K  L  M  O  P  R  Q  S  T  U  X  \n10  11  11   A  C  E  E  I  K  L  M  O  P  Q  R  S  T  U  X  \n 10      10   A  C  E  E  I  K  L  M  O  P  Q  R  S  T  U  X  \n14  14  15   A  C  E  E  I  K  L  M  O  P  Q  R  S  T  U  X  \n 15      15   A  C  E  E  I  K  L  M  O  P  Q  R  S  T  U  X  \n  \n              A  C  E  E  I  K  L  M  O  P  Q  R  S  T  U  X \nno partition\nfor subarrays\n of size 1\ninitial values\nrandom shuffle\nresult\n2892.3 \u25a0 Mergesort ", "start": 301, "end": 301}, "428": {"text": "M  O  P  Q  R  S  T  U  X \nno partition\nfor subarrays\n of size 1\ninitial values\nrandom shuffle\nresult\n2892.3 \u25a0 Mergesort The crux of the method is the  partitioning process, which rearranges the array to \nmake the following three conditions hold: \n\u25a0  The entry a[j] is in its \ufb01nal place in the array, for some j. \n\u25a0 No entry in a[lo] through a[j-1] is greater than a[j]. \n\u25a0 No entry in a[j+1] through a[hi] is less than a[j].\nWe achieve a complete sor t by par titioning , then recursively apply ing the method.\nBecause the partitioning process always \ufb01xes one item into its position, a formal \nproof by induction that the recursive method constitutes a proper sort is not dif\ufb01cult \nto develop: if the left subarray and the right subarray are both properly sorted, then the \nresult array, made up of the left subarray (in order, with no entry larger than the par -\ntitioning item), the partitioning item, and the right subarray (in order, with no entry \nsmaller that the partitioning item), is in order. Algorithm 2.5 is a recursive program \nthat implements this idea. It is a  randomized algorithm, be-\ncause it randomly shuf\ufb02es the array before sorting it. Our \nreason for doing so is to be able to predict (and depend \nupon) its performance characteristics, as discussed below.\nTo  co m p l e te  t h e  i m p l e m e n t a t i o n , w e  n e e d  to  i m p l e m e n t  \nthe partitioning method. We use the following general strat-\negy: ", "start": 301, "end": 302}, "429": {"text": "l e m e n t a t i o n , w e  n e e d  to  i m p l e m e n t  \nthe partitioning method. We use the following general strat-\negy: First, we arbitrarily choose a[lo] to be the   partitioning \nitem\u2014the one that will go into its \ufb01nal position. Next, we \nscan from the left end of the array until we \ufb01nd an entry \ngreater than (or equal to) the partitioning item, and we scan \nfrom the right end of the array until we \ufb01nd an entry less \nthan (or equal to) the partitioning item. The two items that \nstopped the scans are out of place in the \ufb01nal partitioned array, so we exchange them. \nContinuing in this way, we ensure that no array entries to the left of the left index i are \ngreater than the partitioning item, and no array entries to the right of the right index j\nare less than the partitioning item. When the scan indices cross, all that we need to do to \ncomplete the partitioning process is to exchange the partitioning item a[lo] with the \nrightmost entry of the left subarray (a[j]) and return its index j.\nThere are several subtle issues with respect to implementing quicksort that are re-\n\ufb02ected in this code and worthy of mention, because each either can lead to incorrect \ncode or can signi\ufb01cantly impact performance. Next, we discuss several of these is -\nsues. Later in this section, we will consider three important higher-level algorithmic \nimprovements. \ni\nvv\nj\nv\nv\nlo hi\nlo hi\nv\nvv\nj\nbefore\nduring\nafter\nQuicksort partitioning overview\n290 CHAPTER 2 \u25a0 Sorting\n   Q u i c k s o r t  p ", "start": 302, "end": 303}, "430": {"text": "\ni\nvv\nj\nv\nv\nlo hi\nlo hi\nv\nvv\nj\nbefore\nduring\nafter\nQuicksort partitioning overview\n290 CHAPTER 2 \u25a0 Sorting\n   Q u i c k s o r t  p a r t i t i o n i n g\nprivate static int partition(Comparable[] a, int lo, int hi) \n{  // Partition into a[lo..i-1], a[i], a[i+1..hi]. \n   int i = lo, j = hi+1;            // left and right scan indices\n   Comparable v = a[lo];            // partitioning item\n   while (true)\n   {  // Scan right, scan left, check for scan complete, and exchange. \n      while (less(a[++i], v)) if (i == hi) break;\n      while (less(v, a[--j])) if (j == lo) break;\n      if (i >= j) break;\n      exch(a, i, j);\n   }\n   exch(a, lo, j);       // Put v = a[j] into position \n   return j;             // with a[lo..j-1] <= a[j] <= a[j+1..hi]. \n}\nThis code partitions on the item v in a[lo]. The main loop exits when the scan indices i and j cross. \nWithin the loop, we increment i while a[i] is less than v and decrement j while a[j] is greater than \nv, then do an exchange to maintain the invariant property that no entries to the left of i are greater \nthan v and no entries to the right of j are smaller than v. Once the indices meet, we complete the \npartitioning by exchanging a[lo] with a[j] (thus leaving the partitioning value in a[j]). \n                              a[]\n i   j    0 ", "start": 303, "end": 303}, "431": {"text": "j are smaller than v. Once the indices meet, we complete the \npartitioning by exchanging a[lo] with a[j] (thus leaving the partitioning value in a[j]). \n                              a[]\n i   j    0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n 0  16    K  R  A  T  E  L  E  P  U  I  M  Q  C  X  O  S\n 1  12    K  R  A  T  E  L  E  P  U  I  M  Q  C  X  O  S\n1  12    K  C  A  T  E  L  E  P  U  I  M  Q  R  X  O  S\n 3   9    K  C  A  T  E  L  E  P  U  I  M  Q  R  X  O  S\n3   9    K  C  A  I  E  L  E  P  U  T  M  Q  R  X  O  S\n 5   6    K  C  A  I  E  L  E  P  U  T  M  Q  R  X  O  S\n5   6    K  C  A  I  E  E  L  P  U  T  M  Q  R  X  O  S\n 6   5    K  C  A  I  E  E  L  P  U  T  M  Q  R  X  O  S\n6   5    E  C  A ", "start": 303, "end": 303}, "432": {"text": "5    K  C  A  I  E  E  L  P  U  T  M  Q  R  X  O  S\n6   5    E  C  A  I  E  K  L  P  U  T  M  Q  R  X  O  S\n     5    E  C  A  I  E  K  L  P  U  T  M  Q  R  X  O  S\nPartitioning trace (array contents before and after each exchange)\ninitial values\nscan left, scan right\nexchange\nscan left, scan right\nexchange\nscan left, scan right\nexchange\nscan left, scan right\nfinal exchange\nresult\nv\n2912.3 \u25a0 Quicksort  \n \nPartitioning in place. If we use an extra array, partitioning is easy to implement, but \nnot so much easier that it is worth the extra cost of copying the partitioned version back \ninto the original. A novice Java programmer might even create a new spare array within \nthe recursive method, for each partition, which would drastically slow down the sort.\nStaying in bounds. If the smallest item or the largest item in the array is the partition-\ning item, we have to take care that the pointers do not run off the left or right ends of \nthe array, respectively. Our partition() implementation has explicit tests to guard \nagainst this circumstance. The test (j == lo) is redundant, since the partitioning item \nis at a[lo] and not less than itself. With a similar technique on the right it is not dif-\n\ufb01cult to eliminate both tests (see Exercise 2.3.17). \nPreserving   randomness. The random shuf\ufb02e puts the array in random order. Since it \ntreats all items in the subarrays uniformly, Algorithm 2.5 ", "start": 303, "end": 304}, "433": {"text": "2.3.17). \nPreserving   randomness. The random shuf\ufb02e puts the array in random order. Since it \ntreats all items in the subarrays uniformly, Algorithm 2.5 has the property that its two \nsubarrays are also in random order. This fact is crucial to the predictability of the algo-\nrithm\u2019s running time. An alternate way to preserve randomness is to choose a random \nitem for partitioning within partition().\nTe r m i n a t i n g  t h e  l o o p. Experienced programmers know to take special care to ensure \nthat any loop must always terminate, and the partitioning loop for quicksort is no ex-\nception. Properly testing whether the pointers have crossed is a bit trickier than it might \nseem at \ufb01rst glance. A common error is to fail to take into account that the array might \ncontain other items with the same key value as the partitioning item.\nHandling items with  keys equal to the partitioning item\u2019s key. It is best to stop the \nleft scan for items with keys greater than or equal to  the partitioning item\u2019s key and \nthe right scan for items with key less than or equal to the partitioning item\u2019s key, as in \nAlgorithm 2.5. Even though this policy might seem to create unnecessary exchanges \ninvolving items with keys equal to the partitioning item\u2019s key, it is crucial to avoiding \nquadratic running time in certain typical applications (see Exercise 2.3.11). Later, we \ndiscuss a better strategy for the case when the array contains a large number of items \nwith  equal keys.\nTe r m i n a t i n g  t h e  re c u r s i o n . Experienced programmers also know to take special care \nto ensure that any recursive method must always terminate, and quicksort is again no ", "start": 304, "end": 304}, "434": {"text": "i n a t i n g  t h e  re c u r s i o n . Experienced programmers also know to take special care \nto ensure that any recursive method must always terminate, and quicksort is again no \nexception. For instance, a common mistake in implementing quicksort involves not \nensuring that one item is always put into position, then falling into an in\ufb01nite recursive \nloop when the partitioning item happens to be the largest or smallest item in the array.\n292 CHAPTER 2 \u25a0 Sorting\n  \n \nPerformance characteristics Quicksort has been subjected to very thorough \nmathematical analysis, so that we can make precise statements about its performance. \nThe analysis has been validated through extensive empirical experience, and is a useful \ntool in tuning the algorithm for optimum performance.\nThe inner loop of quicksort (in the partitioning method) increments an index and \ncompares an array entry against a \ufb01xed value. This simplicity is one factor that makes \nquicksort quick: it is hard to envision a shorter inner loop in a sorting algorithm. For \nexample, mergesort and shellshort are typically slower than quicksort because they also \ndo data movement within their inner loops.\nThe second factor that makes quicksort quick is that it uses few compares. Ulti-\nmately, the ef\ufb01ciency of the sort depends on how well the partitioning divides the array, \nwhich in turn depends on the value of the partitioning item\u2019s key. Partitioning divides \na large randomly ordered array into two smaller randomly ordered subarrays, but the \nactual split is equally likely (for distinct keys) to be anywhere in the array. Next, we \nconsider the analysis of the algorithm, which allows us to see how this choice compares \nto the ideal choice.\nThe best case for quicksort is when each partitioning stage divides the array exactly \nin half. This circumstance would make the number of compares used by quicksort \nsatisfy ", "start": 304, "end": 305}, "435": {"text": "us to see how this choice compares \nto the ideal choice.\nThe best case for quicksort is when each partitioning stage divides the array exactly \nin half. This circumstance would make the number of compares used by quicksort \nsatisfy the    divide-and-conquer recurrence CN = 2CN/2 + N. The 2 CN/2 term covers the \ncost of sorting the two subarrays; the N is the cost of examining each entry, using one \npartitioning index or the other. As in the proof of  Proposition F  for mergesort, we \nknow that this recurrence has the solution CN ~ N lg N. Although things do not always \ngo this well, it is true that the partition falls in the middle on the average. Taking into \naccount the precise probability of each partition position makes the recurrence more \ncomplicated and more dif\ufb01cult to solve, but the \ufb01nal result is similar. The proof of \nthis result is the basis for our con\ufb01dence in quicksort. If you are not mathematically \ninclined, you may wish to skip (and trust) it; if you are mathematically inclined, you \nmay \ufb01nd it intriguing. \nProposition K.    Quicksort uses ~ 2N ln N compares (and one-sixth that many ex-\nchanges) on the average to sort an array of length N with distinct keys.\nProof: Let CN be the average number of compares needed to sort N items with \ndistinct values. We have C0 = C1 = 0 and for N > 1 we can write a recurrence relation-\nship that directly mirrors the recursive program:\n2932.3 \u25a0 Quicksort\n CN = N /H11001 1 /H11001 (C0 /H11001 C1 /H11001. . ./H11001 CN/H110022 /H11001 CN/H110021) / ", "start": 305, "end": 306}, "436": {"text": "CN = N /H11001 1 /H11001 (C0 /H11001 C1 /H11001. . ./H11001 CN/H110022 /H11001 CN/H110021) / N + (CN/H110021 /H11001 CN/H110022 /H11001. . ./H11001 C0 )/N\nThe \ufb01rst term is the cost of partitioning (always N /H11001 1), the second term is the \naverage cost of sorting the left subarray (which is equally likely to be any size from \n0 to N /H11002 1), and the third term is the average cost for the right subarray (which is \nthe same as for the left subarray). Multiplying by N and collecting terms transforms \nthis equation to\nNCN = N(N /H11001 1) + 2(C0 + C1+ . . .+CN/H110022+CN/H110021)\nSubtracting this from the same equation for N /H11002 1 gives\nNCN /H11002 (N /H11002 1)CN/H110021= 2N + 2CN/H110021\nRearranging terms and dividing by N(N /H11001 1) leaves\nCN  /(N /H11001 1) = CN/H110021  /N /H11001 2 /(N /H11001 1)\nwhich telescopes to give the result\nCN   ~ 2 (N /H11001 1)(1/3 /H11001 1/4 /H11001 . . . /H11001 1/(N /H11001 1)  ) \nThe parenthesized quantity is the discrete estimate of the area under the curve 2 /x\nfrom 3 to N, /H11001 1 and ", "start": 306, "end": 306}, "437": {"text": "1/(N /H11001 1)  ) \nThe parenthesized quantity is the discrete estimate of the area under the curve 2 /x\nfrom 3 to N, /H11001 1 and CN ~ 2N lnN by integration. Note that 2 N ln N /H33360 1.39N lg \nN, so the average number of compares is only about 39 percent higher than in the \nbest case.\nA similar (but much more complicated) analysis is needed to establish the stated \nresult for exchanges . \n \nWhen keys may not be distinct, as is typical in practical applications, precise anaysis is \nconsiderably more complicated, but it is not dif\ufb01cult to show that the average number \nof compares is no greater than than CN  , even when duplicate keys may be present (on  \npage 296, we will look at a way to improve quicksort in this case).\nDespite its many assets, the basic quicksort program has one potential liability: it \ncan be extremely inef\ufb01cient if the partitions are unbalanced. For example, it could be \nthe case that the \ufb01rst partition is on the smallest item, the second partition on the next \nsmallest item, and so forth, so that the program will remove just one item for each call, \nleading to an excessive number of partitions of large subarrays. Avoiding this situation \nis the primary reason that we randomly shuf\ufb02e the array before using quicksort. This \naction makes it so unlikely that bad partitions will happen consistently that we need not \nworry about the possibility.\n294 CHAPTER 2 \u25a0 Sorting\n  \nProposition L. Quicksort uses ~ N 2/2 compares in the worst case, but random \nshuf\ufb02ing protects against this case.\nProof: By the argument just given, the number of compares used when one of the \nsubarrays is ", "start": 306, "end": 307}, "438": {"text": "N 2/2 compares in the worst case, but random \nshuf\ufb02ing protects against this case.\nProof: By the argument just given, the number of compares used when one of the \nsubarrays is empty for every partition is \nN /H11001 (N /H11002 1) + (N /H11002 2) /H11001  . . . /H11001 2 /H11001 1 = (N /H11001 1) N / 2\nThis behavior means not only that the time required will be quadratic but also that \nthe space required to handle the recursion will be linear, which is unacceptable for \nlarge arrays. But (with quite a bit more work) it is possible to extend the analysis \nthat we did for the average to \ufb01nd that the standard deviation of the number of \ncompares is about .65 N, so the running time tends to the average as N grows and is \nunlikely to be far from the average. For example, even the rough estimate provided \nby Chebyshev\u2019s inequality says that the probability that the running time is more \nthan ten times the average for an array with a million elements is less than .00001 \n(and the true probability is far smaller). The probability that the running time for \na large array is close to quadratic is so remote that we can safely ignore the pos-\nsibility (see Exercise 2.3.10). For example, the probability that quicksort will use \nas many compares as insertion sort or selection sort when sorting a large array on \nyour computer is much less than the probability that your computer will be struck \nby lightning during the sort!\n \nIn summary, you can be sure that the running time of Algorithm 2.5 will be within \na constant factor of 1.39 N lg N whenever it is used to sort N items. The same is true \nof ", "start": 307, "end": 307}, "439": {"text": "\nIn summary, you can be sure that the running time of Algorithm 2.5 will be within \na constant factor of 1.39 N lg N whenever it is used to sort N items. The same is true \nof mergesort, but quicksort is typically faster because (even though it does 39 per -\ncent more compares) it does much less data movement. This mathematical assurance is \nprobabilistic, but you can certainly rely upon it. \nAlgorithmic improvements Quicksort was invented in 1960 by C. A. R. Hoare, \nand many people have studied and re\ufb01ned it since that time. It is tempting to try to \ndevelop ways to improve quicksort: a faster sorting algorithm is computer science\u2019s \n\u201cbetter mousetrap, \u201d and quicksort is a venerable method that seems to invite tinkering. \nAlmost from the moment Hoare \ufb01rst published the algorithm, people began proposing \nways to improve the algorithm. Not all of these ideas are fully successful, because the al-\ngorithm is so well-balanced that the effects of improvements can be more than offset by \nunexpected side effects, but a few of them, which we now consider, are quite effective. \n2952.3 \u25a0 Quicksort\n If your sort code is to be used a great many times or to sort a huge array (or, in par-\nticular, if it is to be used as a library sort that will be used to sort arrays of unknown \ncharacteristics), then it is worthwhile to consider the improvements that are discussed \nin the next few paragraphs. As noted, you need to run experiments to determine the \neffectiveness of these improvements and to determine the best choice of parameters for \nyour implementation. Typically, improvements of 20 to 30 percent are available.\nCutoff to insertion sort. As with most recursive algorithms, an easy way to improve \nthe performance of quicksort is based ", "start": 307, "end": 308}, "440": {"text": "choice of parameters for \nyour implementation. Typically, improvements of 20 to 30 percent are available.\nCutoff to insertion sort. As with most recursive algorithms, an easy way to improve \nthe performance of quicksort is based on the following two observations:\n\u25a0 Quicksort is slower than insertion sort for tiny subarrays.\n\u25a0 \n \nBeing recursive, quicksort\u2019s sort() is certain to call itself for tiny subarrays.\nAccordingly, it pays to switch to insertion sort for tiny subarrays. A simple change to \nAlgorithm 2.5 accomplishes this improvement: replace the statement\nif (hi <= lo) return;\n in sort() with a statement that invokes insertion sort for small subarrays:\nif (hi <= lo + M) {  Insertion.sort(a, lo, hi); return;  }\nThe optimum value of the cutoff M is system-dependent, but any value between 5 and \n15 is likely to work well in most situations  (see Exercise 2.3.25).\n   M e d i a n - o f - t h r e e  p a r t i t i o n i n g .  A second easy way to improve the performance of \nquicksort is to use the median of a small sample of items taken from the subarray as the \npartitioning item. Doing so will give a slightly better partition, but at the cost of com-\nputing the median. It turns out that most of the available improvement comes from \nchoosing a sample of size 3 and then partitioning on the middle item (see  Exercises \n2.3.18 and 2.3.19). As a bonus, we can use the sample items as sentinels at the ends of \nthe array and remove both array bounds tests in partition(). \n  E n t r o p y - o p t i m a l  s o r t i n g .  Arrays with large ", "start": 308, "end": 308}, "441": {"text": "at the ends of \nthe array and remove both array bounds tests in partition(). \n  E n t r o p y - o p t i m a l  s o r t i n g .  Arrays with large numbers of duplicate keys arise fre-\nquently in applications. For example, we might wish to sort a large personnel \ufb01le \nby year of birth, or perhaps to separate females from males. In such situations, the \nquicksort implementation that we have considered has acceptable performance, \nbut it can be substantially improved. For example, a subarray that consists solely of \nitems that are equal (just one key value) does not need to be processed further, but \nour implementation keeps partitioning down to small subarrays. In a situation where \nthere are large numbers of duplicate keys in the input array, the recursive nature of \nquicksort ensures that subarrays consisting solely of items with keys that are equal will \noccur often. There is potential for signi\ufb01cant improvement, from the linearithmic-time \nperformance of the implementations seen so far to linear-time performance.\n296 CHAPTER 2 \u25a0 Sorting\n partitioning element\nQuicksort with median-of-3 partitioning and cutoff for small subarrays\ninput\nresult\nresult of\nfirst partition\nleft subarray\npartially sorted\nboth subarrays \npartially sorted\n2972.3 \u25a0 Quicksort\n One straightforward idea is to partition the array into    three parts, one each for \nitems with keys smaller than, equal to, and larger than the partitioning item\u2019s key. \nAccomplishing this partitioning is more complicated than the 2-way partitioning that \nwe have been using, and various different methods have been suggested for the task. \nIt was a classical programming exercise popularized by  E. W. Dijkstra as the  Dutch \nNational Flag problem, because it is like sorting an array with three possible ", "start": 308, "end": 310}, "442": {"text": "various different methods have been suggested for the task. \nIt was a classical programming exercise popularized by  E. W. Dijkstra as the  Dutch \nNational Flag problem, because it is like sorting an array with three possible key values, \nwhich might correspond to the three colors on the \ufb02ag.\nDijkstra\u2019s solution to this problem leads to the remarkably simple partition code \nshown on the next page. It is based on a single left-to-right pass through the array that \nmaintains a pointer lt such that a[lo..lt-1] is less than v, a pointer gt such that \na[gt+1, hi] is greater than v, and a pointer i such that a[lt..i-1] are equal to v and \na[i..gt] are not yet examined. Starting with i equal to lo, we process a[i] using the \n3-way comparison given us by the Comparable interface (instead of using less()) to \ndirectly handle the three possible cases:\n\u25a0 a[i] less than v: exchange a[lt] with a[i] and increment both lt and i\n\u25a0 a[i] greater than v: exchange a[i] with a[gt] and decrement gt\n\u25a0 a[i] equal to v: increment i\nEach of these operations both maintains the invariant and decreases the value of \ngt-i (so that the loop terminates). Furthermore, every item encountered leads to an \nexchange except for those items with keys equal to the partitioning item\u2019s key.\nThough this code was developed not long after quicksort in the 1970s, it fell out of \nfavor because it uses many more exchanges \nthan the standard 2-way partitioning method \nfor the common case when the number of \nduplicate keys in the array is not high. In the \n1990s  J. Bentley and  D. McIlroy developed a \nclever implementation that overcomes this ", "start": 310, "end": 310}, "443": {"text": "\nfor the common case when the number of \nduplicate keys in the array is not high. In the \n1990s  J. Bentley and  D. McIlroy developed a \nclever implementation that overcomes this \nproblem (see Exercise 2.3.22), and observed \nthat 3-way partitioning makes quicksort \nasymptotically faster than mergesort and \nother methods in practical situations \ninvolving large numbers of equal keys. Later, \n J .  B e n t l e y  a n d   R .  S e d g e w i c k  d e v e l o p e d  a  p r o o f  o f  t h i s  f a c t ,  w h i c h  w e  d i s c u s s  n e x t .\nBut we proved that mergesort is optimal. How have we defeated that lower bound? \nThe answer to this question is that Proposition I in Section 2.2  addresses worst-\ncase performance over all possible inputs, while now we are looking at worst-case \nperformance with some information about the key values at hand. Mergesort does not \nguarantee optimal performance for any given distribution of duplicates in the input: \nlt\n<v =v >v\ngti\nv\n>v<v =v\nlo hi\nlt gtlo hi\nbefore\nduring\nafter\n3-way partitioning overview\n298 CHAPTER 2 \u25a0 Sorting\n Quicksort with 3-way partitioning\npublic class  Quick3way \n{  \n   private static void sort(Comparable[] a, int lo, int hi)\n   {  // See page 289 for public sort() that calls this method.\n      if (hi <= lo) return;\n      int lt = lo, i = lo+1, gt = hi;\n      Comparable v = a[lo];\n ", "start": 310, "end": 311}, "444": {"text": "See page 289 for public sort() that calls this method.\n      if (hi <= lo) return;\n      int lt = lo, i = lo+1, gt = hi;\n      Comparable v = a[lo];\n      while (i <= gt)\n      {\n         int cmp = a[i].compareTo(v);\n         if      (cmp < 0) exch(a, lt++, i++);\n         else if (cmp > 0) exch(a, i, gt--);\n         else              i++;\n      }  // Now a[lo..lt-1] < v = a[lt..gt] < a[gt+1..hi].\n      sort(a, lo, lt - 1);\n      sort(a, gt + 1, hi);\n   } \n}\n \nThis sort code partitions to put keys equal to the partitioning element in place and thus does not have \nto include those keys in the subarrays for the recursive calls. It is far more ef\ufb01cient than the standard \nquicksort implementation for arrays with large numbers of duplicate keys (see text).\n                             a[]\nlt   i  gt    0  1  2  3  4  5  6  7  8  9 10 11 \n0   0  11    R  B  W  W  R  W  B  R  R  W  B  R\n0   1  11    R  B  W  W  R  W  B  R  R  W  B  R\n1   2  11    B  R  W  W  R  W  B  R  R  W  B  R\n1   2  10    B  R  R  W  R  W  B  R  R  W  B  W\n1   3 ", "start": 311, "end": 311}, "445": {"text": "R  R  W  B  R\n1   2  10    B  R  R  W  R  W  B  R  R  W  B  W\n1   3  10    B  R  R  W  R  W  B  R  R  W  B  W\n1   3   9    B  R  R  B  R  W  B  R  R  W  W  W\n2   4   9    B  B  R  R  R  W  B  R  R  W  W  W\n2   5   9    B  B  R  R  R  W  B  R  R  W  W  W\n2   5   8    B  B  R  R  R  W  B  R  R  W  W  W\n2   5   7    B  B  R  R  R  R  B  R  W  W  W  W\n2   6   7    B  B  R  R  R  R  B  R  W  W  W  W\n3   7   7    B  B  B  R  R  R  R  R  W  W  W  W\n3   8   7    B  B  B  R  R  R  R  R  W  W  W  W\n3   8   7    B  B  B  R  R  R  R  R  W  W  W  W\nv\n3-way partitioning trace (array contents after each loop iteration)\n2992.3 \u25a0 Quicksort for example, mergesort is linearithmic for a randomly ordered array ", "start": 311, "end": 312}, "446": {"text": "W  W  W  W\nv\n3-way partitioning trace (array contents after each loop iteration)\n2992.3 \u25a0 Quicksort for example, mergesort is linearithmic for a randomly ordered array that has only a \nconstant number of distinct key values, but quicksort with 3-way partitioning is linear \nfor such an array. Indeed, by examining the visual trace above, you can see that N times \nthe number of key values is a conservative bound on the running time.\nThe analysis that makes these notions precise takes the distribution of key values \ninto account. Given N keys with k distinct key values, for each i  from 1 to k de\ufb01ne fi to \nbe frequency of occurrence of the i th key value and pi to be fi / N, the probability that \nthe i th key value is found when a random entry of the array is sampled. The Shannon \n e n t r o p y of the keys (a classic measure of their information content) is de\ufb01ned as\nH = /H11002 ( p1 lg p1 /H11001 p2 lg p2 /H11001 . . . /H11001 pk lg pk )\nGiven any array of items to be sorted, we can calculate its entropy by counting the fre-\nquency of each key value. Remarkably, we can also derive from the entropy both a lower \nbound on the number of compares and an upper bound on the number of compares \nused by quicksort with 3-way partitioning.\nProposition M.  No compare-based  sorting algorithm can guarantee to sort N items  \nwith fewer than NH /H11002 N compares, where H is the  Shannon entropy, de\ufb01ned from \nthe frequencies of key values.\nProof sketch: This result follows from a (relatively easy) generalization of the low-\ner bound proof of Proposition I in Section 2.2.\nequal ", "start": 312, "end": 312}, "447": {"text": "entropy, de\ufb01ned from \nthe frequencies of key values.\nProof sketch: This result follows from a (relatively easy) generalization of the low-\ner bound proof of Proposition I in Section 2.2.\nequal to partitioning element\nVisual trace of quicksort with 3-way partitioning\n300 CHAPTER 2 \u25a0 Sorting\n  \n \nProposition N. Quicksort with  3-way partitioning uses ~ (2ln 2) N H compares to \nsort N items, where H is the Shannon entropy, de\ufb01ned from the frequencies of key \nvalues.\nProof sketch: This result follows from a (relatively dif\ufb01cult) generalization of the \naverage-case analysis of quicksort in Proposition K. As with distinct keys, this \ncosts about 39 percent more than the optimum (but within a constant factor) .\n \nNote that H = lg N when the keys are all distinct (all the probabilities are 1/ N), which \nis consistent with Proposition I in Section 2.2 and Proposition K. The worst case \nfor 3-way partitioning happens when the keys are distinct; when duplicate keys are \npresent, it can do much better than mergesort. More important, these two properties \ntogether imply that quicksort with 3-way partitioning is entropy-optimal, in the sense \nthat the average number of compares used by the best possible compare-based sorting \nalgorithm and the average number of compares used by 3-way quicksort are within a \nconstant factor of one another, for any given distribution of input key values.\nAs with standard quicksort, the running time tends to the average as the array size \ngrows, and large deviations from the average are extremely unlikely, so that you can \ndepend on 3-way quicksort\u2019s running time to be proportional to N times the entropy \nof the distribution of input key values. This property of the algorithm is important in \npractice ", "start": 312, "end": 313}, "448": {"text": "average are extremely unlikely, so that you can \ndepend on 3-way quicksort\u2019s running time to be proportional to N times the entropy \nof the distribution of input key values. This property of the algorithm is important in \npractice because it reduces the time of the sort from linearithmic to linear for arrays with \nlarge numbers of  duplicate keys. The order of the keys is immaterial, because the algo -\nrithm shuf\ufb02es them to protect against the worst case. The distribution of keys de\ufb01nes \nthe entropy and no compare-based algorithm can use fewer compares than de\ufb01ned by \nthe entropy. This ability to adapt to duplicates in the input makes 3-way quicksort the \nalgorithm of choice for a library sort\u2014clients that sort arrays containing large numbers \nof duplicate keys are not unusual. \nA carefully tuned version of quicksort is likely to run signi\ufb01cantly faster on most \ncomputers for most applications than will any other compare-based sorting method. \nQuicksort is widely used throughout today\u2019s computational infrastructure because \nthe mathematical models that we have discussed suggest that it will outperform other \nmethods in practical applications, and extensive experiments and experience over the \npast several decades have validated that conclusion.\nWe w ill see in Chapter 5 that this is not quite the end of the story in the development \nof sorting algorithms, because is it possible to develop algorithms that do not use \ncompares at all! But a version of quicksort turns out to be best in that situation, as well.\n3012.3 \u25a0 Quicksort\n Q & A\n \n \nQ. Is there some way to just divide the array into two halves, rather than letting the \npartitioning element fall where it may?\nA. That is a question that stumped experts for over a decade. It is tantamount to \ufb01nd-\ning the median key value in the array and then partitioning on that ", "start": 313, "end": 314}, "449": {"text": "\npartitioning element fall where it may?\nA. That is a question that stumped experts for over a decade. It is tantamount to \ufb01nd-\ning the median key value in the array and then partitioning on that value. We discuss \nthe problem of \ufb01nding the median on page 346. It is possible to do so in linear time, but \nthe cost of doing so with known algorithms (which are based on quicksort partition-\ning!) far exceeds the 39 percent savings available from splitting the array into equal \nparts.\nQ. Randomly shuf\ufb02ing the array seems to take a signi\ufb01cant fraction of the total time \nfor the sort. Is doing so really worthwhile?\nA. Ye s . It  p ro te c t s  a g a i n s t  t h e  wo r s t  c a s e  a n d  m a ke s  t h e  r u n n i n g  t i m e  p re d i c t a b l e . Ho a re  \nproposed this approach when he presented the algorithm in 1960\u2014it is a prototypical \n(and among the \ufb01rst) randomized algorithm. \nQ. Why all the focus on items with equal keys?\nA. The issue directly impacts performance in practical situations. It was overlooked by \nmany for decades, with the result that some older implementations of quicksort take \nquadratic time for arrays with large numbers of  items with equal keys, which certainly \ndo arise in applications. Better implementations such as Algorithm 2.5 take linearith-\nmic time for such arrays, but improving that to linear-time as in the entropy-optimal \nsort at the end of this section is worthwhile in many situations.\n302 CHAPTER 2 \u25a0 Sorting\n EXERCISES\n \n2.3.1 Show, in the style of the trace given with partition(), ", "start": 314, "end": 315}, "450": {"text": "entropy-optimal \nsort at the end of this section is worthwhile in many situations.\n302 CHAPTER 2 \u25a0 Sorting\n EXERCISES\n \n2.3.1 Show, in the style of the trace given with partition(), how that method pati -\ntions the array E A S Y Q U E S T I O N.\n2.3.2 Show, in the style of the quicksort trace given in this section, how quicksort sorts \nthe array E A S Y Q U E S T I O N  (for the purposes of this exercise, ignore the \ninitial shuf\ufb02e).\n2.3.3 What is the maximum number of times during the execution of Quick.sort()\nthat the largest item can be exchanged, for an array of length N ?\n2.3.4 Suppose that the initial random shuf\ufb02e is omitted. Give six arrays of ten elements \nfor which Quick.sort() uses the worst-case number of compares.\n2.3.5 Give a code fragment that sorts an array that is known to consist of items having  \njust two distinct keys.\n2.3.6 Write a program to compute the exact value of CN, and compare the exact value \nwith the approximation 2N ln N, for N = 100, 1,000, and 10,000.\n2.3.7 Find the expected number of subarrays of size 0, 1, and 2 when quicksort is used \nto sort an array of N items with distinct keys. If you are mathematically inclined, do the \nmath; if not, run some experiments to develop hypotheses.\n2.3.8 About how many compares will Quick.sort() make when sorting an array of \nN items that are all equal?\n2.3.9 Explain what happens when Quick.sort() is run on an array having items with \njust two distinct keys, and then explain what happens when it ", "start": 315, "end": 315}, "451": {"text": "when sorting an array of \nN items that are all equal?\n2.3.9 Explain what happens when Quick.sort() is run on an array having items with \njust two distinct keys, and then explain what happens when it is run on an array having \njust three distinct keys.\n2.3.10   Chebyshev\u2019s inequality says that the probability that a random variable is more \nthan k standard deviations away from the mean is less than 1/k 2. For N = 1 million, use \nChebyshev\u2019s inequality to bound the probability that the number of compares used by \nquicksort is more than 100 billion (.1 N 2).\n2.3.11  Suppose that we scan over items with keys equal to the partitioning item\u2019s key \ninstead of stopping the scans when we encounter them. Show that the running time \nof this version of quicksort is quadratic for all arrays with just a constant number of \ndistinct keys.\n3032.3 \u25a0 Quicksort\n  \n2.3.12 Show, in the style of the trace given with the code, how the entropy-optimal sort \n\ufb01rst partitions the array B A B A B A B A C A D A B R A.\n2.3.13 What is the  recursive depth of quicksort, in the best, worst, and average cases? \nThis is the size of the stack that the system needs to keep track of the recursive calls. See \nExercise 2.3.20 for a way to guarantee that the recursive depth is logarithmic in the \nworst case.\n2.3.14 Prove that when running quicksort on an array with N distinct items, the prob-\nability of comparing the i th and j th largest items is 2/( j /H11002 i). Then use this result to \nprove Proposition K. \nEXERCISES  (continued)\n304 CHAPTER 2 \u25a0 Sorting\n ", "start": 315, "end": 316}, "452": {"text": "of comparing the i th and j th largest items is 2/( j /H11002 i). Then use this result to \nprove Proposition K. \nEXERCISES  (continued)\n304 CHAPTER 2 \u25a0 Sorting\n CREATIVE PROBLEMS\n \n \n \n2.3.15  Nuts and bolts. (G. J. E. Rawlins) Y ou have a mixed pile of N nuts and N bolts \nand need to quickly \ufb01nd the corresponding pairs of nuts and bolts. Each nut matches \nexactly one bolt, and each bolt matches exactly one nut. By \ufb01tting a nut and bolt to -\ngether, you can see which is bigger, but it is not possible to directly compare two nuts or \ntwo bolts. Give an ef\ufb01cient method for solving the problem.\n2.3.16  Best case. Write a program that produces a best-case array (with no duplicates) \nfor sort() in Algorithm 2.5: an array of N items with distinct keys having the prop-\nerty that every partition will produce subarrays that differ in size by at most 1 (the same \nsubarray sizes that would happen for an array of N equal keys). (For the purposes of this \nexercise, ignore the initial shuf\ufb02e.)\nThe following exercises describe variants of quicksort. Each of them calls for an implementa-\ntion, but naturally you will also want to use SortCompare for experiments to evaluate the \neffectiveness of each suggested modi\ufb01cation.\n2.3.17    Sentinels. Modify the code in Algorithm 2.5 to remove both bounds checks \nin the inner while loops. The test against the left end of the subarray is redundant since \nthe partitioning item acts as a sentinel (v is never less than a[lo]). T o enable removal of \nthe other test, put an item whose key is the largest in the whole ", "start": 316, "end": 317}, "453": {"text": "the subarray is redundant since \nthe partitioning item acts as a sentinel (v is never less than a[lo]). T o enable removal of \nthe other test, put an item whose key is the largest in the whole array into a[length-1]\njust after the shuf\ufb02e. This item will never move (except possibly to be swapped with an \nitem having the same key) and will serve as a sentinel in all subarrays involving the end \nof the array. Note : When sorting interior subarrays, the leftmost entry in the subarray \nto the right serves as a sentinel for the right end of the subarray. \n2.3.18      Median-of-3 partitioning. Add median-of-3 partitioning to quicksort, as de -\nscribed in the text (see page 296). Run doubling tests to determine the effectiveness of\nthe change.\n2.3.19        Median-of-5 partitioning. Implement a quicksort based on partitioning on the \nmedian of a random sample of \ufb01ve items from the subarray. Put the items of the sample \nat the appropriate ends of the array so that only the median participates in partitioning. \nRun doubling tests to determine the effectiveness of the change, in comparison both \nto the standard algorithm and to median-of-3 partitioning (see the previous exercise). \nExtra credit : Devise a median-of-5 algorithm that uses fewer than seven compares on \nany input. \n3052.3 \u25a0 Quicksort\n  \n2.3.20     Nonrecursive quicksort. Implement a nonrecursive version of quicksort based \non a main loop where a subarray is popped from a stack to be partitioned, and the re-\nsulting subarrays are pushed onto the stack. Note :  Push the larger of the subarrays onto \nthe stack \ufb01rst, which guarantees that the stack will have at most lg N entries.\n2.3.21 ", "start": 317, "end": 318}, "454": {"text": "and the re-\nsulting subarrays are pushed onto the stack. Note :  Push the larger of the subarrays onto \nthe stack \ufb01rst, which guarantees that the stack will have at most lg N entries.\n2.3.21   Lower bound for sorting with equal keys.  Complete the \ufb01rst part of the proof \nof Proposition M by following the logic in the proof of Proposition I and using the \nobservation that there are N! / f1!f2! . . . fk! different ways to arrange keys with k different \nvalues, where the i th value appears with frequency fi (= Npi , in the notation of Proposi-\ntion M), with f1+. . . +fk = N.\n2.3.22     Fast 3-way partitioning. ( J. Bentley and  D. McIlroy) Implement an entropy-\noptimal sort based on keeping item's with equal keys at both the left and right ends \nof the subarray. Maintain indices p and q such that \na[lo..p-1] and a[q+1..hi] are all equal to a[lo], \nan index i such that a[p..i-1] are all less than a[lo], \nand an index j such that a[j+1..q] are all greater than \na[lo]. Add to the inner partitioning loop code to swap \na[i] with a[p] (and increment p) if it is equal to v and \nto swap a[j] with a[q] (and decrement q) if it is equal \nto v before the usual comparisons of a[i] and a[j]\nwith v. After the partitioning loop has terminated, add \ncode to swap the items with equal keys into position. \nNote : This code complements the code given in the \ntext, in the sense that it does extra swaps for keys equal to the partitioning item\u2019s ", "start": 318, "end": 318}, "455": {"text": "terminated, add \ncode to swap the items with equal keys into position. \nNote : This code complements the code given in the \ntext, in the sense that it does extra swaps for keys equal to the partitioning item\u2019s key, \nwhile the code in the text does extra swaps for keys that are not equal to the partitioning \nitem\u2019s key.\n2.3.23    Java system sort. Add to your implementation from Exercise 2.3.22 code to \nuse the  Tuke y ninther to compute the partitioning item\u2014choose three sets of three \nitems, take the median of each, then use the median of the three medians as the parti-\ntioning item. Also, add a cutoff to insertion sort for small subarrays. \n2.3.24   Samplesort. ( W . Frazer and  A. McKellar) Implement a quicksort based on us-\ning a sample of size 2 k /H11002 1. First, sort the sample, then arrange to have the recursive \nroutine partition on the median of the sample and to move the two halves of the rest of \nthe sample to each subarray, such that they can be used in the subarrays, without having \nto be sorted again. This algorithm is called samplesort. \ni\n=v=v <v >v\njp q\nv\nj\n>v<v =v\ni\nlo\nlo\nlo\nhi\nhi\nhi\nbefore\nduring\nafter\nBentley-McIlroy 3-way partitioning\nCREATIVE PROBLEMS  (continued)\n306 CHAPTER 2 \u25a0 Sorting\n EXPERIMENTS\n \n \n2.3.25    Cutoff to insertion sort. Implement quicksort with a cutoff to insertion sort \nfor subarrays with less than M elements, and empirically determine the value of M for \nwhich quicksort runs fastest in your computing environment to sort random arrays \nof ", "start": 318, "end": 319}, "456": {"text": "insertion sort. Implement quicksort with a cutoff to insertion sort \nfor subarrays with less than M elements, and empirically determine the value of M for \nwhich quicksort runs fastest in your computing environment to sort random arrays \nof N doubles, for N = 103, 104, 105, and 106. Plot average running times for M from 0 \nto 30 for each value of M. Note : Y ou need to add a three-argument sort() method to \nAlgorithm 2.2 for sorting subarrays such that the call Insertion.sort(a, lo, hi)\nsorts the subarray a[lo..hi].\n2.3.26  Subarray sizes. Write a program that plots a histogram of the subarray sizes left \nfor insertion sort when you run quicksort for an array of size N with a cutoff for subar-\nrays of size less than M. Run your program for M=10, 20, and 50 and N = 105.\n2.3.27  Ignore small subarrays. Run experiments to compare the following strategy for \ndealing with small subarrays with the approach described in Exercise 2.3.25: Simply \nignore the small subarrays in quicksort, then run a single insertion sort after the quick-\nsort completes. Note : Y ou may be able to estimate the size of your computer\u2019s  cache \nmemory with these experiments, as the performance of this method is likely to degrade \nwhen the array does not \ufb01t in the  cache.\n2.3.28  Recursion depth. Run empirical studies to determine the average recursive \ndepth used by quicksort with cutoff for arrays of size M, when sorting arrays of N\ndistinct elements, for M=10, 20, and 50 and N = 103, 104, 105, and 106.\n2.3.29   Randomization. Run empirical studies to compare ", "start": 319, "end": 319}, "457": {"text": "N\ndistinct elements, for M=10, 20, and 50 and N = 103, 104, 105, and 106.\n2.3.29   Randomization. Run empirical studies to compare the effectiveness of \nthe strategy of choosing a random partitioning item with the strategy of initially \nrandomizing the array (as in the text). Use a cutoff for arrays of size M, and sort arrays \nof N distinct elements, for M=10, 20, and 50 and N = 103, 104, 105, and 106.\n2.3.30  Corner cases. Te s t  q u i c k s o r t  o n  l a r g e  n o n r a n d o m  a r r ay s  o f  t h e  k i n d  d e s c r i b e d  \nin Exercises 2.1.35 and 2.1.36 both with and without the initial random shuf\ufb02e. How \ndoes shuf\ufb02ing affect its performance for these arrays?\n2.3.31  Histogram of running times. Write a program that takes command-line argu -\nments N and T, does T trials of the experiment of running quicksort on an array of \nN random Double values, and plots a histogram of the observed running times. Run \nyour program for N = 103, 104, 105, and 106, with T as large as you can afford to make \nthe curves smooth. Y our main challenge for this exercise is to appropriately scale the \nexperimental results.\n3072.4 \u25a0 Quicksort\n 2.4    PRIORITY QUEUES\n \n \nMany applications require that we process items having keys in order, but not nec-\nessarily in full sorted order and not necessarily all at once. Often, we collect a set of \nitems, ", "start": 319, "end": 320}, "458": {"text": "2.4    PRIORITY QUEUES\n \n \nMany applications require that we process items having keys in order, but not nec-\nessarily in full sorted order and not necessarily all at once. Often, we collect a set of \nitems, then process the one with the largest key, then perhaps collect more items, then \nprocess the one with the current largest key, and so forth. For example, you are likely to \nhave a computer (or a cellphone) that is capable of running several applications at the \nsame time. This effect is typically achieved by assigning a priority to events associated \nwith applications, then always choosing to process next the highest-priority event. For \nexample, most cellphones are likely to process an incoming call with higher priority \nthan a game application.\nAn appropriate data type in such an environment supports two operations: remove \nthe maximum  and insert. Such a data type is called a priority queue . Using priority \nqueues is similar to using queues (remove the oldest) and stacks (remove the newest), \nbut implementing them ef\ufb01ciently is more challenging. \nIn this section, after a short discussion of elementary representations where one or \nboth of the operations take linear time, we consider a classic priority-queue implemen-\ntation based on the binary heap data structure, where items are kept in an array, subject \nto certain ordering constraints that allow for ef\ufb01cient (logarithmic-time) implementa-\ntions of remove the maximum and insert. \nSome important applications of priority queues include simulation systems, where \nthe keys correspond to event times, to be processed in chronological order; job schedul-\ning, where the keys correspond to priorities indicating which tasks are to be performed \n\ufb01rst; and numerical computations, where the keys represent computational errors, in-\ndicating in which order we should deal with them. We consider in Chapter 6 a detailed \ncase study showing the use of priority queues ", "start": 320, "end": 320}, "459": {"text": "\n\ufb01rst; and numerical computations, where the keys represent computational errors, in-\ndicating in which order we should deal with them. We consider in Chapter 6 a detailed \ncase study showing the use of priority queues in a particle-collision simulation.\nWe can use any pr ior it y queue as the basis for a sor ting algor ithm by inser ting a se-\nquence of items, then successively removing the smallest to get them out, in order. An \nimportant sorting algorithm known as heapsort also follows naturally from our heap-\nbased priority-queue implementations. Later on in this book, we shall see how to use \npriority queues as building blocks for other algorithms. In Chapter 4, we shall see how \npriority queues are an appropriate abstraction for implementing several fundamental \ngraph-searching algorithms; in Chapter 5, we shall develop a data-compression algo-\nrithm using methods from this section. These are but a few examples of the important \nrole played by the priority queue as a tool in algorithm design.\n308\n  \nAPI The priority queue is a prototypical abstract data type (see Section 1.2): it rep-\nresents a set of values and operations on those values, and it provides a convenient ab-\nstraction that allows us to separate application programs (clients) from various imple-\nmentations that we will consider in this section. As in Section 1.2, we precisely de\ufb01ne \nthe operations by specifying an applications programming interface (API) that provides \nthe information needed by clients. Priority queues are characterized by the remove the \nmaximum and insert operations, so we shall focus on them. We use the method names \ndelMax() for remove the maximum  and insert() for insert. By convention, we will \ncompare keys only with a helper less() method, as we have been doing for sorting. \nThus, if items can have  duplicate keys, maximum means any item with the ", "start": 320, "end": 321}, "460": {"text": "insert() for insert. By convention, we will \ncompare keys only with a helper less() method, as we have been doing for sorting. \nThus, if items can have  duplicate keys, maximum means any item with the largest key \nvalue. T o complete the API, we also need to add constructors (like the ones we used for \nstacks and queues) and a test if empty operation. For \ufb02exibility, we use a  generic imple-\nmentation with a parameterized type Key that implements the Comparable interface. \nThis choice eliminates our distinction between items and keys and enables clearer and \nmore compact descriptions of data structures and algorithms. For example, we refer to \nthe \u201clargest key\u201d instead of the \u201clargest item\u201d or the \u201citem with the largest key. \u201d\nFor convenience in client code, the API includes three constructors, which enable \nclients to build priority queues of an initial \ufb01xed size (perhaps initialized with a given \narray of keys). T o clarify client code, we will use a separate class MinPQ whenever ap-\npropriate, which is the same as MaxPQ except that it has a delMin() method that deletes \nand returns an item with the smallest key in the queue. Any MaxPQ implementation is \neasily converted into a MinPQ implementation and vice versa, simply by reversing the \nsense of the comparison in less(). \npublic class MaxPQ< Key extends Comparable<Key>> \nMaxPQ() create a priority queue\nMaxPQ(int max) create a priority queue of initial capacity max \nMaxPQ(Key[] a) create a priority queue from the keys in a[]\nvoid insert(Key v) insert a key into the priority queue\nKey max() return the largest key\nKey delMax() return and remove the largest key\nboolean isEmpty() is the priority queue empty?\nint size() number of keys in the priority queue\nAPI for a ", "start": 321, "end": 321}, "461": {"text": "the priority queue\nKey max() return the largest key\nKey delMax() return and remove the largest key\nboolean isEmpty() is the priority queue empty?\nint size() number of keys in the priority queue\nAPI for a generic priority queue\n3092.4 \u25a0 Priority Queues\n A priority-queue client. To  a p p re c i a te  t h e  \nvalue of the priority-queue abstraction, con-\nsider the following problem: Y ou have a huge \ninput stream of N strings and associated inte-\nger values, and your task is to \ufb01nd the largest \nor smallest M integers (and associated strings) \nin the input stream. Y ou might imagine the \nstream to be \ufb01nancial transactions, where \nyour interest is to \ufb01nd the big ones, or pesti -\ncide levels in an agricultural product, where \nyour interest is to \ufb01nd the small ones, or re -\nquests for service, or results from a scienti\ufb01c \nexperiment, or whatever. In some applications, the size of the input stream is so huge \nthat it is best to consider it to be unbounded. One way to address this problem would \nbe to sort the input stream and take the M largest keys from the result, but we have \njust stipulated that the input stream is too large for that. Another approach would be \nto compare each new key against the M largest seen so far, but that is also likely to be \nprohibitively expensive unless M is small. With priority queues, we can solve the prob-\nlem with the MinPQ client TopM on the next page provided that we can develop ef\ufb01cient \nimplementations of both insert() and delMin(). That is precisely our aim in this sec-\ntion. For the huge values of N that are likely to be encountered in our modern compu-\ntational infrastructure, these implementations can make the difference ", "start": 321, "end": 322}, "462": {"text": "both insert() and delMin(). That is precisely our aim in this sec-\ntion. For the huge values of N that are likely to be encountered in our modern compu-\ntational infrastructure, these implementations can make the difference between being \nable to address such a problem and not having the resources to do it at all. \nElementary implementations The basic data structures that we discussed in \nChapter 1 provide us with four immediate starting points for implementing priority \nqueues. We can use an array or a linked list, kept in order or unordered. These imple-\nmentations are useful for small priority queues, situations where one of the two opera-\ntions are predominant, or situations where some assumptions can be made about the \norder of the keys involved in the operations. Since these implementations are elemen-\ntary, we will be content with brief descriptions here in the text and leave the code for \nexercises (see Exercise 2.4.3).\n  A r r a y  r e p r e s e n t a t i o n  ( u n o r d e r e d ) .  Perhaps the simplest priority-queue implementa-\ntion is based on our code for pushdown stacks in Section 2.1. The code for insert in the \npriority queue is the same as for push in the stack. T o implement remove the maximum, \nwe can add code like the inner loop of selection sort to exchange the maximum item \nwith the item at the end and then delete that one, as we did with pop() for stacks. As \nwith stacks, we can add resizing-array code to ensure that the data structure is always at \nleast one-quarter full and never over\ufb02ows.\nclient order of growth\ntime space\nsort client N log N N\nPQ client using\nelementary implementation NM M\nPQ client using\nheap-based implementation N log M M\nCosts of finding the ", "start": 322, "end": 322}, "463": {"text": "over\ufb02ows.\nclient order of growth\ntime space\nsort client N log N N\nPQ client using\nelementary implementation NM M\nPQ client using\nheap-based implementation N log M M\nCosts of finding the largest M in a stream of N items\n310 CHAPTER 2 \u25a0 Sorting\n  A  p r i o r i t y - q u e u e  c l i e n t\npublic class  TopM \n{  \n   public static void main(String[] args)\n   {  // Print the top M lines in the input stream.\n      int M = Integer.parseInt(args[0]);\n      MinPQ<Transaction> pq = new MinPQ<Transaction>(M+1);\n      while (StdIn.hasNextLine())\n      {  // Create an entry from the next line and put on the PQ.\n         pq.insert(new Transaction(StdIn.readLine()));\n         if (pq.size() > M)\n            pq.delMin();     // Remove minimum if M+1 entries on the PQ.\n      }  // Top M entries are on the PQ.\n      Stack<Transaction> stack = new Stack<Transaction>();\n      while (!pq.isEmpty()) stack.push(pq.delMin());\n      for (Transaction t : stack) StdOut.println(t);\n   } \n}\nGiven an integer M from the command line and an input stream where each line contains a trans-\naction, this MinPQ client prints the M lines whose numbers are the highest. It does so by using our \nTransaction class (see page 79, Exercise 1.2.19, and Exercise 2.1.21) to build a priority queue using \nthe numbers as keys, deleting the minimum after each \ninsertion once the size of the priority queue reaches M. \nOnce all the transactions have been processed, the top M\ncome off the priority queue in increasing order, so this \ncode puts them on a stack, then iterates through the ", "start": 322, "end": 323}, "464": {"text": "the size of the priority queue reaches M. \nOnce all the transactions have been processed, the top M\ncome off the priority queue in increasing order, so this \ncode puts them on a stack, then iterates through the \nstack to reverse the order and print them in increasing \norder.\n% more tinyBatch.txt \nTuring      6/17/1990   644.08 \nvonNeumann  3/26/2002  4121.85 \nDijkstra    8/22/2007  2678.40 \nvonNeumann  1/11/1999  4409.74 \nDijkstra   11/18/1995   837.42 \nHoare       5/10/1993  3229.27 \nvonNeumann  2/12/1994  4732.35 \nHoare       8/18/1992  4381.21 \nTuring      1/11/2002    66.10 \nThompson    2/27/2000  4747.08 \nTuring      2/11/1991  2156.86 \nHoare       8/12/2003  1025.70 \nvonNeumann 10/13/1993  2520.97 \nDijkstra    9/10/2000   708.95 \nTuring     10/12/1993  3532.36 \nHoare       2/10/2005  4050.20\n% java TopM 5 < tinyBatch.txt \nThompson    2/27/2000  4747.08 \nvonNeumann  2/12/1994  4732.35 \nvonNeumann  1/11/1999  4409.74 ", "start": 323, "end": 323}, "465": {"text": "2/27/2000  4747.08 \nvonNeumann  2/12/1994  4732.35 \nvonNeumann  1/11/1999  4409.74 \nHoare       8/18/1992  4381.21 \nvonNeumann  3/26/2002  4121.85\n3112.4 \u25a0 Priority Queues  \n  A r r a y  r e p r e s e n t a t i o n  ( o r d e r e d ) .  Another approach is to add code for insert to move \nlarger entries one position to the right, thus keeping the keys in the array in order (as in \ninsertion sort). Thus, the largest element is always at the end, and the code for remove \nthe maximum in the priority queue is the same as for pop in the stack.\n    L i n k e d - l i s t  r e p r e s e n t a t i o n s .  Similarly, we can start with our linked-list code for push-\ndown stacks, modifying either the code for pop() to \ufb01nd and return the maximum or \nthe code for push() to keep keys in reverse order and the code for pop() to unlink and \nreturn the \ufb01rst (maximum) item on the list.\nUsing unordered sequences is the prototypical lazy\napproach to this problem, where we defer doing \nwork until necessary (to \ufb01nd the maximum); us -\ning ordered sequences is the prototypical eager ap-\nproach to the problem, where we do as much work \nas we can up front (keep the list sorted on insertion) \nto make later operations ef\ufb01cient.\nThe signi\ufb01cant difference between implementing \nstacks or queues and implementing priority queues \nhas to do with performance. For stacks ", "start": 323, "end": 324}, "466": {"text": "front (keep the list sorted on insertion) \nto make later operations ef\ufb01cient.\nThe signi\ufb01cant difference between implementing \nstacks or queues and implementing priority queues \nhas to do with performance. For stacks and queues, \nwe were able to develop implementations of all the \noperations that take constant time; for priority queues, all of the elementary imple-\nmentations just discussed have the property that either the insert or the remove the \nmaximum operation takes linear time in the worst case. The heap data structure that we \nconsider next enables implementations where both operations are guaranteed to be fast.\nP               1     P                        P\nQ               2     P  Q                     P  Q\nE               3     P  Q  E                  E  P  Q  \n        Q       2     P  E                     E  P\nX               3     P  E  X                  E  P  X\nA               4     P  E  X  A               A  E  P  X\nM               5     P  E  X  A  M            A  E  M  P  X\n        X       4     P  E  M  A               A  E  M  P\nP               5     P  E  M  A  P            A  E  M  P  P\nL               6     P  E  M  A  P  L         A  E  L  M  P  \nE               7     P  E  M  A  P  L  E      A  E  E  L  M  \n        P       6     E  E  M  A  P  L         A  E  E  L  M  \ninsert\ninsert\ninsert\nremove max\ninsert\ninsert\ninsert\nremove max\ninsert\ninsert\ninsert\nremove max\noperation  argument ", "start": 324, "end": 324}, "467": {"text": "A  P  L         A  E  E  L  M  \ninsert\ninsert\ninsert\nremove max\ninsert\ninsert\ninsert\nremove max\ninsert\ninsert\ninsert\nremove max\noperation  argument return value contents (unordered) contents (ordered)size\nA sequence of operations on a priority queue\ndata structure insert remove\nmaximum\nordered array N 1\nunordered array 1 N\nheap log N log N\nimpossible 1 1\nOrder of growth of worst-case running time\nfor priority-queue implementations\n312 CHAPTER 2 \u25a0 Sorting\n  \n  H e a p  d e \ufb01 n i t i o n s  The  binary heap is a data structure that can ef\ufb01ciently support \nthe basic priority-queue operations. In a binary heap, the keys are stored in an array \nsuch that each key is guaranteed to be larger than (or equal to) the keys at two other \nspeci\ufb01c positions. In turn, each of those keys must be larger than (or equal to) two ad-\nditional keys, and so forth. This ordering is easy to see if we view the keys as being in \na binary tree structure with edges from each key to the two keys known to be smaller.\nDefinition. A    binary tree is    heap-ordered if the key in each node is larger than or \nequal to the keys in that node\u2019s two children (if any). \nEquivalently, the key in each node of a heap-ordered binary tree is smaller than or \nequal to the key in that node\u2019s parent (if any). Moving up from any node, we get a \nnondecreasing sequence of keys; moving down from any node, we get a nonincreasing \nsequence of keys. In particular: \nProposition O. The  largest key in a heap-ordered binary tree is found at the root.\nProof: By induction on the ", "start": 324, "end": 325}, "468": {"text": "from any node, we get a nonincreasing \nsequence of keys. In particular: \nProposition O. The  largest key in a heap-ordered binary tree is found at the root.\nProof: By induction on the size of the tree. \n B i n a r y  h e a p  r e p r e s e n t a t i o n .  If we use a linked representation for heap-ordered binary \ntrees, we would need to have three links associated with each key to allow travel up and \ndown the tree (each node would have one pointer to its parent and one to each child). \nIt is particularly convenient, instead, to use a  complete binary tree like the one drawn at \nright. W e draw such a structure by placing the root node \nand then proceeding down the page and from left to right, \ndrawing and connecting two nodes beneath each node on \nthe previous level until we have drawn N nodes. Complete \ntrees provide the opportunity to use a compact array rep -\nresentation that does not involve explicit links. Speci\ufb01cally, \nwe represent complete binary trees sequentially within an \narray by putting the nodes in  level order, with the root at \nposition 1, its children at positions 2 and 3, their children \nin positions 4, 5, 6, and 7, and so on.\nE\nP\nI\nS\nH\nN\nG\nT\nO\nR\nA\nA heap-ordered complete binary tree\n3132.4 \u25a0 Priority Queues\n Definition. A  binary heap is a collection of keys arranged in a complete heap-or -\ndered binary tree, represented in level order in an array (not using the \ufb01rst entry).\n \n(For brevity, from now on we drop the \u201cbinary\u201d \nmodi\ufb01er and use the term heap when referring \nto a binary heap.) ", "start": 325, "end": 326}, "469": {"text": "in an array (not using the \ufb01rst entry).\n \n(For brevity, from now on we drop the \u201cbinary\u201d \nmodi\ufb01er and use the term heap when referring \nto a binary heap.) In a heap, the parent of the \nnode in position k is in position \u23a3k /2\u23a6 and, con-\nversely, the two children of the node in position \nk are in positions 2k and 2k + 1. Instead of using \nexplicit links (as in the binary tree structures that \nwe will consider in Chapter 3), we can travel up \nand down by doing simple arithmetic on array \nindices: to move up the tree from a[k] we set k\nto k/2; to move down the tree we set k to 2*k or \n2*k+1. \nComplete binary trees represented as arrays \n(heaps) are rigid structures, but they have just \nenough \ufb02exibility to allow us to implement ef\ufb01-\ncient priority-queue operations. Speci\ufb01cally, we \nwill use them to develop logarithmic-time insert and remove the maximum implemen-\ntations. These algorithms take advantage of the capability to move up and down paths \nin the tree without pointers and have guaranteed logarithmic performance because of \nthe following property of complete binary trees:\nProposition P .  The     height of a complete  binary tree of size N is \u23a3 lg N \u23a6 .\nProof: The stated result is easy to prove by induction or by noting that the height \nincreases by 1 when N is a power of 2.\n  i   0  1  2  3  4  5  6  7  8  9 10 11\na[i]  -  T  S  R  P  N  O ", "start": 326, "end": 326}, "470": {"text": "1  2  3  4  5  6  7  8  9 10 11\na[i]  -  T  S  R  P  N  O  A  E  I  H  G\n E  I  H  G\nP  N  O  A\nS  R\nT\n1\n2\n4 5 6 7\n10 118 9\n3\nE\nP\nI\nS\nH\nN\nG\nT\nO\nR\nA\nHeap representations\n314 CHAPTER 2 \u25a0 Sorting\n  \nAlgorithms on heaps We represent a heap of  size N in private array pq[] of \nlength N + 1, with pq[0] unused and the heap in pq[1] through pq[N]. As for sorting \nalgorithms, we access keys only through private helper functions less() and exch(), \nbut since all items are in the instance \nvariable pq[], we use the more com -\npact implementations on the next \npage that do not involve passing the \narray name as a parameter. The heap \noperations that we consider work by \n\ufb01rst making a simple modi\ufb01cation \nthat could violate the heap condition, \nthen traveling through the heap, modifying the heap as required to ensure that the heap \ncondition is satis\ufb01ed everywhere. We refer to this process as reheapifying, or restoring \nheap order.\nThere are two cases. When the priority of some node is increased (or a new node is \nadded at the bottom of a heap), we have to travel up the heap to restore the heap order. \nWhen the priority of some node is decreased (for example, if we replace the node at \nthe root with a new node that has a smaller key), we have to travel down the heap to \nrestore the heap order. First, we will ", "start": 326, "end": 327}, "471": {"text": "of some node is decreased (for example, if we replace the node at \nthe root with a new node that has a smaller key), we have to travel down the heap to \nrestore the heap order. First, we will consider how to implement these two basic auxil-\niary operations; then, we shall see how to use them to implement insert and remove the \nmaximum.\nBottom-up reheapify ( swim). If the heap order is violated because a node\u2019s key be -\ncomes larger than that node\u2019s parent\u2019s key, then we can make progress toward \ufb01xing \nthe violation by exchanging the node with \nits parent. After the exchange, the node is \nlarger than both its children (one is the old \nparent, and the other is smaller than the old \nparent because it was a child of that node) \nbut the node may still be larger than its par-\nent. We can \ufb01x that violation in the same \nway, and so forth, moving up the heap until \nwe reach a node with a larger key, or the \nroot. Coding this process is straightforward \nwhen you keep in mind that the parent of \nthe node at position k in a heap is at po-\nsition k/2.  The loop in swim() preserves \nthe invariant that the only place the heap \n5\nE\nG\nI\nP\nH\nT\nG\nS\nO\nR\nA\nviolates heap order\n(larger key than parent)\nE\nG\nI\nS\nH\nP\nG\nT\nO\nR\nA5\n2\n1\nBottom-up reheapify (swim)\nprivate boolean  less(int i, int j) \n{  return pq[i].compareTo(pq[j]) < 0;  }\nprivate void  exch(int i, int j) \n{  Key t = pq[i]; pq[i] = pq[j]; ", "start": 327, "end": 327}, "472": {"text": "j) \n{  return pq[i].compareTo(pq[j]) < 0;  }\nprivate void  exch(int i, int j) \n{  Key t = pq[i]; pq[i] = pq[j]; pq[j] = t;  }\nCompare and exchange methods for heap implementations\n3152.4 \u25a0 Priority Queues\n order could be violated is when the node at \nposition k might be larger than its parent. \nTherefore, when we get to a place where that \nnode is not larger than its parent, we know \nthat the heap order is satis\ufb01ed throughout \nthe heap. T o jus-\ntify the method\u2019s \nname, we think \nof the new node, \nhaving too large a \nkey, as having to swim to a higher level in the heap.\nTop-dow n reheapify (sink). If the heap order is violated be-\ncause a node\u2019s key becomes smaller than one or both of that \nnode\u2019s children\u2019s keys, then we can make progress toward \ufb01x-\ning the violation by exchanging the node with the larger of its \ntwo children. This switch may cause a violation at the child; \nwe \ufb01x that violation in the same way, and so forth, moving \ndown the heap until we reach a node with both children \nsmaller (or equal), or the bottom. The code again follows di-\nrectly from \nthe fact that \nthe children of the node at position \nk in a heap are at positions 2k and \n2k+1. To justify the method\u2019s name, \nwe think about the node, having too \nsmall a key, as having to sink to a low-\ner level in the heap. \nIf we imagine the heap to represent \na cutthroat corporate hierarchy, with \neach of the children of a node repre-\nsenting subordinates (and the parent \nrepresenting the immediate superior), ", "start": 327, "end": 328}, "473": {"text": "in the heap. \nIf we imagine the heap to represent \na cutthroat corporate hierarchy, with \neach of the children of a node repre-\nsenting subordinates (and the parent \nrepresenting the immediate superior), then these operations have amusing interpreta-\ntions. The swim() operation corresponds to a promising new manager arriving on the \nscene, being promoted up the chain of command (by exchanging jobs with any lower-\nquali\ufb01ed boss) until the new person encounters a higher-quali\ufb01ed boss. The sink()\noperation is analogous to the situation when the president of the company resigns and \nis replaced by someone from the outside. If the president\u2019s most powerful subordinate \n5\nE\nP\nI\nH\nN\nS\nG\nT\nO\nR\nA\nviolates heap order\n(smaller than a child)\nE\nP\nI\nS\nH\nN\nG\nT\nO\nR\nA5\n10\n2\n2\nTop-down reheapify (sink)\nprivate void sink(int k) \n{\n   while (2*k <= N)\n   {\n      int j = 2*k;\n      if (j < N && less(j, j+1)) j++;\n      if (!less(k, j)) break;\n      exch(k, j);\n      k = j;\n   } \n}\nTop-down reheapify (sink) implementation\nprivate void swim(int k) \n{\n   while (k > 1 && less(k/2, k))\n   {\n      exch(k/2, k);\n      k = k/2;\n   } \n}\nBottom-up reheapify (swim) implementation\n316 CHAPTER 2 \u25a0 Sorting\n is stronger than the new person, they exchange jobs, and we move down the chain of \ncommand, demoting the new person and promoting others until the level of compe -\ntence of the new ", "start": 328, "end": 329}, "474": {"text": "2 \u25a0 Sorting\n is stronger than the new person, they exchange jobs, and we move down the chain of \ncommand, demoting the new person and promoting others until the level of compe -\ntence of the new person is reached, where there is no higher-quali\ufb01ed subordinate. \nThese idealized scenarios may rarely be seen in the real world, but they may help you \nbetter understand basic operation on heaps.\nThese sink() and swim() operations provide the basis for ef\ufb01cient implementation \nof the priority-queue API, as diagrammed below and implemented in Algorithm 2.6 \non the next page.\n I n s e r t .  We add the new key at the end of  the ar ray, increment the size of  the heap, \nand then swim up through the heap with that key to restore the heap condition.\nRemove the maximum . We take the largest key off  the top, put the item from \nthe end of the heap at the top, decrement the size of the heap, and then sink down \nthrough the heap with that key to restore the heap condition. \nAlgorithm 2.6 solves the basic problem that we posed at the beginning of this section: \nit is a priority-queue API implementation for which both insert and delete the maxi-\nmum are guaranteed to take time logarithmic in the size of the queue. \nHeap operations\nE\nP\nI\nN\nG\nH\nS\nT\nO\nR\nA\nkey to insert\nE\nP\nI\nN\nG\nH\nS\nT\nO\nR\nA\nadd key to heap\nviolates heap order\nE\nP\nI\nS\nG\nN\nH\nT\nO\nR\nA\nswim up\nE\nP\nI\nS\nG\nN\nH\nT\nO\nR\nA\nkey to remove\nviolates\nheap ", "start": 329, "end": 329}, "475": {"text": "order\nE\nP\nI\nS\nG\nN\nH\nT\nO\nR\nA\nswim up\nE\nP\nI\nS\nG\nN\nH\nT\nO\nR\nA\nkey to remove\nviolates\nheap order\nexchange keys\nwith root\nE\nP\nI\nS\nG\nN\nT\nH\nO\nR\nA\nremove node\nfrom heap\nE\nH\nI\nP\nG\nN\nS\nO\nR\nA\nsink down\ninsert remove the maximum\n3172.4 \u25a0 Priority Queues\n ALGORITHM 2.6   Heap priority queue\npublic class    MaxPQ<Key extends Comparable<Key>> \n{\n   private Key[] pq;             // heap-ordered complete binary tree\n   private int N = 0;            //    in pq[1..N] with pq[0] unused\n   public MaxPQ(int maxN)\n   {  pq = (Key[]) new Comparable[maxN+1];  }\n   public boolean isEmpty()\n   {  return N == 0;  }\n   public int size()\n   {  return N;  }\n   public void insert(Key v)\n   {  \n      pq[++N] = v;\n      swim(N);\n   }\n   public Key delMax()\n   {  \n      Key max = pq[1];           // Retrieve max key from top.\n      exch(1, N--);              // Exchange with last item.\n      pq[N+1] = null;            // Avoid loitering.\n      sink(1);                   // Restore heap property.\n      return max;\n   }\n   // See pages 145-147 for implementations of these helper methods.\n   private boolean less(int i, int j)\n   private void exch(int i, int j)\n   private void swim(int k)\n   private void sink(int k) \n}\nThe ", "start": 329, "end": 330}, "476": {"text": "145-147 for implementations of these helper methods.\n   private boolean less(int i, int j)\n   private void exch(int i, int j)\n   private void swim(int k)\n   private void sink(int k) \n}\nThe priority queue is maintained in a heap-ordered complete binary tree in the array pq[] with \npq[0] unused and the N keys in the priority queue in pq[1] through pq[N]. To implement insert(), \nwe increment N, add the new element at the end, then use swim() to restore the heap order. For \ndelMax(), we take the value to be returned from pq[1], then move pq[N] to pq[1], decrement the \nsize of the heap, and use sink() to restore the heap condition. We also set the now-unused position \npq[N+1] to null to allow the system to reclaim the memory associated with it. Code for dynamic \narray resizing is omitted, as usual (see Section 1.3). See Exercise 2.4.19 for the other constructors.\n318 CHAPTER 2 \u25a0 Sorting Proposition Q.    In an N-key priority queue, the heap al-\ngorithms require no more than 1 + lg N compares for in-\nsert and no more than 2  lg N  compares for remove the \nmaximum.\nProof: By Proposition P , both operations involve mov-\ning along a path between the root and the bottom of the \nheap whose number of links is no more than  lg N. The \nremove the maximum  operation requires two compares \nfor each node on the path (except at the bottom): one \nto \ufb01nd the child with the larger key, the other to decide \nwhether that child needs to be promoted.\nFor typical applications that require a large number of in -\ntermixed insert and remove the maximum operations in a \nlarge priority queue, Proposition Q represents an important \nperformance breakthrough, summarized in the table shown ", "start": 330, "end": 331}, "477": {"text": "child needs to be promoted.\nFor typical applications that require a large number of in -\ntermixed insert and remove the maximum operations in a \nlarge priority queue, Proposition Q represents an important \nperformance breakthrough, summarized in the table shown \non page 312. Where elementary implementations using an or-\ndered array or an unordered array require linear time for one \nof the operations, a heap-based implementation provides a \nguarantee that both operations complete in logarithmic time. \nThis improvement can make the difference between solving a \nproblem and not being able to address it at all.\n  M u l t i w a y  h e a p s .  It is not dif\ufb01cult to modify our code to \nbuild heaps based on an array representation of complete \nheap-ordered ternary trees, with an entry at position k larger \nthan or equal to entries at positions 3k/H110021, 3k, and 3k/H110011 and \nsmaller than or equal to entries at position \u23a3(k+1) /H11408 3\u23a6, for all \nindices between 1 and N in an array of N items, and not much \nmore dif\ufb01cult to use d-ary heaps for any given d. There is a \ntradeoff between the lower cost from the reduced tree height \n(log d N) and the higher cost of \ufb01nding the largest of the d \nchildren at each node. This tradeoff is dependent on details \nof the implementation and the expected relative frequency of \noperations.\nP\nQ\nP\nQ\nE\nP\nE\nX\nP\nA\nM\nE\nX\nP\nA\nM\nP\nE\nA\nP\nM\nP\nE\nA\nP\nM\nP\nE\nL\nA\nP\nM\nP\nE\nL\nE\nA\nM\nE\nP\nE\nL\nP\nE\nE\nX\nP\nA\nPriority ", "start": 331, "end": 331}, "478": {"text": "\noperations.\nP\nQ\nP\nQ\nE\nP\nE\nX\nP\nA\nM\nE\nX\nP\nA\nM\nP\nE\nA\nP\nM\nP\nE\nA\nP\nM\nP\nE\nL\nA\nP\nM\nP\nE\nL\nE\nA\nM\nE\nP\nE\nL\nP\nE\nE\nX\nP\nA\nPriority queue operations in a heap\ninsert   P\ninsert   Q\ninsert   E\nremove max   (Q)\ninsert   X\ninsert   A\ninsert   M\nremove max   (X)\ninsert   P\ninsert   L\ninsert   E\nremove max   (P)\n3192.4 \u25a0 Priority Queues\n  \n A r r a y  r e s i z i n g .  We can add a no-argument constructor, code for ar ray doubling in \ninsert(), and code for array halving in delMax(), just as we did for stacks in Section \n1.3. Thus, clients need not be concerned about arbitrary size restrictions. The logarith-\nmic time bounds implied by PROPOSITION Q are  amortized when the size of the priority \nqueue is arbitrary and the arrays are resized (see Exercise 2.4.22). \nImmutability of keys. The priority queue contains objects that are created by clients \nbut assumes that client code does not change the keys (which might invalidate the \nheap-order invariant). It is possible to develop mechanisms to enforce this assumption, \nbut programmers typically do not do so because they complicate the code and are likely \nto degrade performance. \n  I n d e x  p r i o r i t y  q u e u e .  In many applications, it makes sense to allow clients to refer \nto items that are already on the priority queue. One easy way ", "start": 331, "end": 332}, "479": {"text": "d e x  p r i o r i t y  q u e u e .  In many applications, it makes sense to allow clients to refer \nto items that are already on the priority queue. One easy way to do so is to associate \na unique integer index with each item. Moreover, it is often the case that clients have \na universe of items of a known size N and perhaps are using (parallel) arrays to store \ninformation about the items, so other unrelated client code might already be using an \ninteger index to refer to items. These considerations lead us to the following API:\nIndexMinPQ<Item extends Comparable<Item>>public class    \nIndexMinPQ(int maxN) create a priority queue of capacity maxN\nwith possible indices between 0 and maxN-1 \nvoid insert(int k, Item item) insert item ; associate it with k \nvoid change(int k, Item item) change the item associated with k to item\nboolean contains(int k) is k associated with some item?\nvoid delete(int k) remove k and its associated item\nItem min() return a minimal item\nint minIndex() return a minimal item\u2019s index\nint delMin() remove a minimal item and return its index\nboolean isEmpty() is the priority queue empty?\nint size() number of items in the priority queue\n A P I  f o r  a  g e n e r i c  p r i o r i t y  q u e u e  w i t h  a s s o c i a t e d  i n d i c e s\n320 CHAPTER 2 \u25a0 Sorting\n A useful way of thinking of this data type is as implementing an array, but with fast ac-\ncess to the smallest entry in the array. Actually it does even better\u2014it gives fast access \nto the minimum entry in a speci\ufb01ed subset of an array\u2019s entries (the ones that have ", "start": 332, "end": 333}, "480": {"text": "but with fast ac-\ncess to the smallest entry in the array. Actually it does even better\u2014it gives fast access \nto the minimum entry in a speci\ufb01ed subset of an array\u2019s entries (the ones that have been \ninserted. In other words, you can think of an IndexMinPQ named pq as representing \na subset of an array pq[0..N-1] of items. Think of the call pq.insert(k, item) as \nadding k to the subset and setting pq[k] = item and the call pq.change(k, item)\nas setting pq[k] = item, both also maintaining data structures needed to support the \nother operations, most importantly delMin() (remove and return the index of the \nminimum key) and change() (change the item associated with an index that is already \nin the data structure\u2014just as in pq[i] = item). These operations are \nimportant in many applications and are enabled by our ability to refer \nto the key (with the index). Exercise 2.4.33 describes how to extend \nAlgorithm 2.6 to implement index priority queues with remarkable \nef\ufb01ciency and with remarkably little code. Intuitively, when an item in \nthe heap changes, we can restore the heap invariant with a sink opera-\ntion (if the key increases) and a swim operation (if the key decreases). \nTo  p e r f o r m  t h e  o p e r a t i o n s , w e  u s e  t h e  i n d e x  to  \ufb01 n d  t h e  i te m  i n  t h e  \nheap. The ability to locate an item in the heap also allows us to add the \ndelete() operation to the API. \nProposition Q (continued).   In an  index priority queue of size N, \nthe number of ", "start": 333, "end": 333}, "481": {"text": "\nheap. The ability to locate an item in the heap also allows us to add the \ndelete() operation to the API. \nProposition Q (continued).   In an  index priority queue of size N, \nthe number of compares required is proportional to at most log N\nfor insert,       change priority, delete, and remove the minimum.\nProof: Immediate from inspection of the code and the fact that all \npaths in a heap are of length at most ~lg N.\nThis discussion is for a minimum-oriented queue; as usual, we also \nimplement on the booksite a maximum-oriented version IndexMaxPQ. \nIndex priority-queue client. The IndexMinPQ client Multiway on page 322 solves the \nmultiway merge  problem: it merges together several sorted input streams into one \nsorted output stream. This problem arises in many applications: the streams might \nbe the output of scienti\ufb01c instruments (sorted by time), lists of information from the \nweb such as music or movies (sorted by title or artist name), commercial transactions \n(sorted by account number or time), or whatever. If you have the space, you might just \nread them all into an array and sort them, but with a priority queue, you can read input \nstreams and put them in sorted order on the output no matter how long they are. \noperation\norder of \ngrowth of \nnumber of \ncompares\ninsert() log N\nchange() log N\ncontains() 1\ndelete() log N\nmin() 1\nminIndex() 1\ndelMin() log N\nWorst-case costs for \nan N-item  heap-based \nindexed priority queue\n3212.4 \u25a0 Priority Queues\n  M u l t i w a y  m e r g e  p r i o r i t y - q u e u e  c l i e n t\npublic class  Multiway \n{\n ", "start": 333, "end": 334}, "482": {"text": "Queues\n  M u l t i w a y  m e r g e  p r i o r i t y - q u e u e  c l i e n t\npublic class  Multiway \n{\n   public static void merge(In[] streams)\n   {\n      int N = streams.length;\n      IndexMinPQ<String> pq = new IndexMinPQ<String>(N);\n      for (int i = 0; i < N; i++)\n         if (!streams[i].isEmpty())\n             pq.insert(i, streams[i].readString());\n      while (!pq.isEmpty())\n      {\n         StdOut.println(pq.min());\n         int i = pq.delMin();\n         if (!streams[i].isEmpty())\n             pq.insert(i, streams[i].readString());\n      }\n   }\n   public static void main(String[] args)\n   {\n      int N = args.length;\n      In[] streams = new In[N];\n      for (int i = 0; i < N; i++)\n          streams[i] = new In(args[i]);\n      merge(streams);\n   } \n}\nThis IndexMinPQ client merges together the sorted input stream given as command-line arguments \ninto a single sorted output stream on standard output (see text). Each stream index is associated with \na key (the next string in the stream). After initialization, it enters a loop that prints the smallest string \nin the queue and removes the corresponding entry, then adds a new entry for the next string in that \nstream. For economy, the output is shown on one line below\u2014the actual output is one string per line.\n% more m1.txt \nA B C F G I I Z \n% more m2.txt \nB D H P Q Q \n% more m3.txt \nA B E F J N\n% java Multiway m1.txt m2.txt m3.txt \nA A B B B C D E F F G H I I ", "start": 334, "end": 334}, "483": {"text": "H P Q Q \n% more m3.txt \nA B E F J N\n% java Multiway m1.txt m2.txt m3.txt \nA A B B B C D E F F G H I I J N P Q Q Z\n322 CHAPTER 2 \u25a0 Sorting  \n \n \n \n    H e a p s o r t  We can use any pr ior it y queue to develop a sor ting method. We inser t \nall the items to be sorted into a minimum-oriented priority queue, then repeatedly use \nremove the minimum to remove them all in order. Using a priority queue represented as \nan unordered array in this way corresponds to doing a selection sort; using an ordered \narray corresponds to doing an insertion sort. What sorting method do we get if we use \na heap? An entirely different one! Next, we use the heap to develop a classic elegant sort-\ning algorithm known as heapsort.\nHeapsort breaks into two phases: heap construction, where we reorganize the original \narray into a heap, and the sortdown, where we pull the items out of the heap in decreas-\ning order to build the sorted result. For consistency with the code we have studied, we \nuse a maximum-oriented priority queue and repeatedly remove the maximum. Focus-\ning on the task of sorting, we abandon the notion of hiding the representation of the \npriority queue and use swim() and sink() directly. Doing so allows us to sort an array \nwithout needing any extra space, by maintaining the heap within the array to be sorted.\nHeap construction. How dif\ufb01cult is the process of building a heap from N given items? \nCertainly we can accomplish this task in time proportional to N log N, by proceeding \nfrom left to right through the array, using swim() to ensure that the items to the left of \nthe scanning pointer make up a heap-ordered complete tree, like successive priority-\nqueue insertions. ", "start": 334, "end": 335}, "484": {"text": "N, by proceeding \nfrom left to right through the array, using swim() to ensure that the items to the left of \nthe scanning pointer make up a heap-ordered complete tree, like successive priority-\nqueue insertions. A clever method that is much more ef\ufb01cient is to proceed from right \nto left, using sink() to make subheaps as we go. Every position in the array is the root \nof a small subheap; sink() works for such subheaps, as well. If the two children of a \nnode are heaps, then calling sink() on that node makes the subtree rooted at the par-\nent a heap. This process establishes the heap order inductively. The scan starts halfway \nback through the array because we can skip the subheaps of size 1. The scan ends at \nposition 1, when we \ufb01nish building the heap with one call to sink(). As the \ufb01rst phase \nof a sort, heap construction is a bit counterintuitive, because its goal is to produce a \nheap-ordered result, which has the largest item \ufb01rst in the array (and other larger items \nnear the beginning), not at the end, where it is destined to \ufb01nish.\nProposition R.   Sink-based heap construction uses fewer than 2 N compares and \nfewer than N exchanges to construct a heap from N items.\nProof: This fact follows from the observation that most of the heaps processed are \nsmall. For example, to build a heap of 127 elements, we process 32 heaps of size 3, \n16 heaps of size 7, 8 heaps of size 15, 4 heaps of size 31, 2 heaps of size 63, and 1 \nheap of size 127, so 32\u00b71 + 16\u00b72 + 8\u00b73 + 4\u00b74 + 2\u00b75 + 1\u00b76 ", "start": 335, "end": 335}, "485": {"text": "2 heaps of size 63, and 1 \nheap of size 127, so 32\u00b71 + 16\u00b72 + 8\u00b73 + 4\u00b74 + 2\u00b75 + 1\u00b76 = 120 exchanges (twice as \nmany compares) are required (at worst). See Exercise 2.4.20 for a complete proof.\n3232.4 \u25a0 Priority Queues\n ALGORITHM 2.7    Heapsort\npublic static void sort(Comparable[] a) \n{\n   int N = a.length;\n   for (int k = N/2; k >= 1; k--)\n      sink(a, k, N);\n   while (N > 1)\n   {\n      exch(a, 1, N--);\n      sink(a, 1, N);\n   } \n}\nThis code sorts a[1] through a[N] using the sink() method (modi\ufb01ed to take a[] and N as argu-\nments). The for loop constructs the heap; then the while loop exchanges the largest element a[1]\nwith a[N] and then repairs the heap, continuing until the heap is empty. Decrementing the array in-\ndices in the implementations of exch() and less() gives an implementation that sorts  a[0] through \na[N-1], consistent with our other sorts. \n                       a[i]\n  N   k   0  1  2  3  4  5  6  7  8  9 10 11\n             S  O  R  T  E  X  A  M  P  L  E\n 11   5      S  O  R  T  L  X  A  M  P  E  E  \n 11   4      S  O  R  T  L  X ", "start": 335, "end": 336}, "486": {"text": "E\n 11   5      S  O  R  T  L  X  A  M  P  E  E  \n 11   4      S  O  R  T  L  X  A  M  P  E  E \n 11   3      S  O  X  T  L  R  A  M  P  E  E  \n 11   2      S  T  X  P  L  R  A  M  O  E  E  \n 11   1      X  T  S  P  L  R  A  M  O  E  E\n             X  T  S  P  L  R  A  M  O  E  E\n 10   1      T  P  S  O  L  R  A  M  E  E  X\n 9   1      S  P  R  O  L  E  A  M  E  T  X  \n  8   1      R  P  E  O  L  E  A  M S  T  X  \n  7   1      P  O  E  M  L  E  A  R  S  T  X \n  6   1      O  M  E  A  L  E  P  R  S  T  X  \n  5   1      M  L  E  A  E  O  P  R  S  T  X  \n  4   1      L  E  E  A  M  O  P  R  S  T  X \n  3   1      E  A  E  L  M  O  P  R  S  T  X  \n  2   1 ", "start": 336, "end": 336}, "487": {"text": "O  P  R  S  T  X \n  3   1      E  A  E  L  M  O  P  R  S  T  X  \n  2   1      E  A  E  L  M  O  P  R  S  T  X  \n 1   1      A  E  E  L  M  O  P  R  S  T  X\n             A  E  E  L  M  O  P  R  S  T  X \ninitial values\nheap-ordered\nsorted result\nHeapsort trace (array contents just after each sink)\n324 CHAPTER 2 \u25a0 Sorting sink(5, 11)\nsink(4, 11)\nsink(3, 11)\nsink(2, 11)\nsink(1, 11)\nexch(1, 6)\nsink(1, 5)\nexch(1, 5)\nsink(1, 4)\nexch(1, 4)\nsink(1, 3)\nexch(1, 3)\nsink(1, 2)\nexch(1, 2)\nsink(1, 1)\nsortdown \nexch(1, 11)\nsink(1, 10)\nexch(1, 10)\nsink(1, 9)\nexch(1, 9)\nsink(1, 8)\nexch(1, 8)\nsink(1, 7)\nexch(1, 7)\nsink(1, 6)\nHeapsort: constructing (left) and sorting down (right) a heap\nM\nT\nP\nO\nL\nE\nE\nS\nX\nR\nA\nM\nT\nP\nO\nE\nL\nE\nS\nX\nR\nA\nM\nT\nP\nO\nE\nL\nE\nS\nX\nR\nA\nM\nT\nP\nO\nE\nL\nE\nS\nR\nX\nA\nM\nP\nO\nT\nE\nL\nE\nS\nR\nX\nA\nM\nP\nO\nT\nE\nL\nE\nX\nR\nS\nA\nR\nA\nS\nL\nT\nE\nX\nM\nO\nE\nP\nR\nA\nS\nE\nT\nM\nX\nL\nO\nE\nP\nR\nL\nS\nA\nT\nM\nX\nE\nO\nE\nP\nR\nL\nS\nA\nT\nM\nX\nE\nO\nE\nP\nR\nL\nS\nE\nT\nM\nX\nA\nO\nE\nP\nR\nL\nS\nE\nT\nM\nX\nA\nO\nE\nP\nM\nP\nO\nT\nE\nL\nE\nX\nR\nS\nA\nM\nO\nE\nP\nE\nL\nX\nT\nR\nS\nA\nM\nO\nE\nP\nT\nL\nX\nS\nE\nR\nA\nM\nO\nS\nP\nT\nL\nX\nR\nE\nE\nA\nR\nM\nS\nO\nT\nL\nX\nP\nE\nE\nA\nR\nA\nS\nM\nT\nL\nX\nO\nE\nE\nP\n1\n2\n4 ", "start": 336, "end": 337}, "488": {"text": "heap\nM\nT\nP\nO\nL\nE\nE\nS\nX\nR\nA\nM\nT\nP\nO\nE\nL\nE\nS\nX\nR\nA\nM\nT\nP\nO\nE\nL\nE\nS\nX\nR\nA\nM\nT\nP\nO\nE\nL\nE\nS\nR\nX\nA\nM\nP\nO\nT\nE\nL\nE\nS\nR\nX\nA\nM\nP\nO\nT\nE\nL\nE\nX\nR\nS\nA\nR\nA\nS\nL\nT\nE\nX\nM\nO\nE\nP\nR\nA\nS\nE\nT\nM\nX\nL\nO\nE\nP\nR\nL\nS\nA\nT\nM\nX\nE\nO\nE\nP\nR\nL\nS\nA\nT\nM\nX\nE\nO\nE\nP\nR\nL\nS\nE\nT\nM\nX\nA\nO\nE\nP\nR\nL\nS\nE\nT\nM\nX\nA\nO\nE\nP\nM\nP\nO\nT\nE\nL\nE\nX\nR\nS\nA\nM\nO\nE\nP\nE\nL\nX\nT\nR\nS\nA\nM\nO\nE\nP\nT\nL\nX\nS\nE\nR\nA\nM\nO\nS\nP\nT\nL\nX\nR\nE\nE\nA\nR\nM\nS\nO\nT\nL\nX\nP\nE\nE\nA\nR\nA\nS\nM\nT\nL\nX\nO\nE\nE\nP\n1\n2\n4 5 ", "start": 337, "end": 337}, "489": {"text": "5 6 7\n8 9 10 11\n3\n1\n2\n4 5 6 7\n8 9 10 11\n3\nheap construction\nresult (heap-ordered)\nresult (sorted)\nstarting point (heap-ordered)starting point (arbitrary order)\n3252.4 \u25a0 Priority Queues\n Sortdown. Most of the work during heapsort is done during \nthe second phase, where we remove the largest remaining item \nfrom the heap and put it into the array position vacated as the \nheap shrinks. This process is a bit like selection sort (taking the \nitems in decreasing order instead of in increasing order), but it \nuses many fewer compares because the heap provides a much \nmore ef\ufb01cient way to \ufb01nd the largest item in the unsorted part \nof the array.\nProposition S.  Heapsort uses fewer than 2 N lg N + 2 N\ncompares (and half that many exchanges) to sort N items.\nProof: The 2 N term covers the cost of heap construc-\ntion (see Proposition R). The 2 N lg N term follows from \nbounding the cost of each sink operation during the sort-\ndown by 2lg N (see Proposition PQ). \n \nAlgorithm 2.7 is a full implementation based on these ideas, \nthe classical heapsort algorithm, which was invented by J. W. \nJ. Williams and re\ufb01ned by  R. W. Floyd in 1964. Although the \nloops in this program seem to do different tasks (the \ufb01rst \nconstructs the heap, and the second destroys the heap for the \nsortdown), they are both built around the sink() method. We \nprovide an implementation outside of our priority-queue API \nto highlight the simplicity of the sorting algorithm (eight lines \nof code for sort() and another eight lines of code for ", "start": 337, "end": 338}, "490": {"text": "are both built around the sink() method. We \nprovide an implementation outside of our priority-queue API \nto highlight the simplicity of the sorting algorithm (eight lines \nof code for sort() and another eight lines of code for sink()) \nand to make it an in-place sort. \nAs usual, you can gain some insight into the operation of the \nalgorithm by studying a visual trace. At \ufb01rst, the process seems \nto do anything but sort, because large items are moving to the \nbeginning of the array as the heap is being constructed. But \nthen the method looks more like a mirror image of selection \nsort (except that it uses far fewer compares).\nAs for all of the other methods that we have studied, various \npeople have investigated ways to improve heap-based priority-\nqueue implementations and heapsort. We now brie\ufb02y consider \none of them.\ninput\nsorted\nresult\nheap-\nordered\nred entries are \nitems that sank\ngray entries\ndo not move\nblack entries\nare involved\nin exchanges\nVisual trace of heapsort\n326 CHAPTER 2 \u25a0 Sorting\n Sink to the bottom, then swim. Most items reinserted into the heap during sortdown \ngo all the way to the bottom.  Floyd observed in 1964 that we can thus save time by \navoiding the check for whether the item has reached its position, simply promoting \nthe larger of the two children until the bottom is reached, then moving back up the \nheap to the proper position. This idea cuts the number of compares by a factor of 2 as-\nymptotically\u2014close to the number used by mergesort (for a randomly-ordered array). \nThe method requires extra bookkeeping, and it is useful in practice only when the cost \nof compares is relatively high (for example, when we are sorting items with strings or \nother types of long keys).\nHeapsort is significant in the ", "start": 338, "end": 339}, "491": {"text": "extra bookkeeping, and it is useful in practice only when the cost \nof compares is relatively high (for example, when we are sorting items with strings or \nother types of long keys).\nHeapsort is significant in the study of the complexity of sorting (see page 279) because \nit is the only method that we have seen that is optimal (within a constant factor) in its \nuse of both time and space\u2014it is guaranteed to use ~2 N lg N compares and constant \nextra space in the worst case. When space is very tight (for example, in an embedded \nsystem or on a low-cost mobile device) it is popular because it can be implemented \nwith just a few dozen lines (even in machine code) while still providing optimal per -\nformance. However, it is rarely used in typical applications on modern systems because \nit has poor  cache performance: array entries are rarely compared with nearby array \nentries, so the number of cache misses is far higher than for quicksort, mergesort, and \neven shellsort, where most compares are with nearby entries.\nOn the other hand, the use of heaps to implement priority queues plays an increas-\ningly important role in modern applications, because it provides an easy way to guar -\nantee logarithmic running time for dynamic situations where large numbers of insert\nand remove the maximum  operations are intermixed. We will encounter several ex -\namples later in this book.\n3272.4 \u25a0 Priority Queues\n Q&A\n \nQ. I\u2019m still not clear on the purpose of priority queues. Why exactly don\u2019t we just sort \nand then consider the items in increasing order in the sorted array?\nA. In some data-processing examples such as TopM and Multiway, the total amount of \ndata is far too large to consider sorting (or even storing in memory). If you are looking \nfor the top ten entries among a billion items, do you really want to sort a billion-entry \narray? ", "start": 339, "end": 340}, "492": {"text": "Multiway, the total amount of \ndata is far too large to consider sorting (or even storing in memory). If you are looking \nfor the top ten entries among a billion items, do you really want to sort a billion-entry \narray? With a priority queue, you can do it with a ten-entry priority queue. In other ex-\namples, all the data does not even exist together at any point in time: we take something \nfrom the priority queue, process it, and as a result of processing it perhaps add some \nmore things to the priority queue.\nQ. Why not use Comparable, as we do for sorts, instead of the generic Item in MaxPQ?\nA. Doing so would require the client to  cast the return value of delMax() to an  actual \ntype, such as String. Generally, casts in client code are to be avoided.\nQ. Why not use a[0] in the heap representation?\nA. Doing so simpli\ufb01es the arithmetic a bit. It is not dif\ufb01cult to implement the heap \nmethods based on a 0-based heap where the children of a[0] are a[1] and a[2], the \nchildren of a[1] are a[3] and a[4], the children of a[2] are a[5] and a[6], and \nso forth, but most programmers prefer the simpler arithmetic that we use. Also, us -\ning a[0] as a sentinel value (in the parent of a[1]) is useful in some heap applications. \nQ. Building a heap in heapsort by inserting items one by one seems simpler to me than \nthe tricky bottom-up method described on page 323 in the text. Why bother?\nA. For a sort implementation, it is 20 percent faster and requires half as much tricky \ncode (no swim() needed). The dif\ufb01culty of understanding an ", "start": 340, "end": 340}, "493": {"text": "page 323 in the text. Why bother?\nA. For a sort implementation, it is 20 percent faster and requires half as much tricky \ncode (no swim() needed). The dif\ufb01culty of understanding an algorithm has not neces-\nsarily much to do with its simplicity, or its ef\ufb01ciency.\nQ.  What happens if I leave off the extends Comparable<Key> phrase in an implemen-\ntation like MaxPQ ?\nA. As usual, the easiest way for you to answer a question of this sort for yourself is to \nsimply try it.  If you do so for MaxPQ you will get a compile-time error:\nMaxPQ.java:21: cannot find symbol \nsymbol  : method compareTo(Item)\nwhich is Java\u2019s way of telling you that it does not know about compareTo() in Item\nbecause you neglected to declare that Item extends Comparable<Item>.\n328 CHAPTER 2 \u25a0 Sorting\n EXERCISES\n2.4.1  Suppose that the sequence P R I O * R * * I * T * Y * * * Q U E * * * \nU * E (where a letter means insert and an asterisk means remove the maximum) is ap-\nplied to an initially empty priority queue. Give the sequence of letters returned by the \nremove the maximum operations.\n2.4.2 Criticize the following idea: To implement \ufb01nd the maximum in constant time, \nwhy not use a stack or a queue, but keep track of the maximum value inserted so far, \nthen return that value for \ufb01nd the maximum?\n2.4.3  Provide priority-queue implementations that support insert and remove the \nmaximum, one for each of the following underlying data structures: unordered ar -\nray, ordered array, unordered linked list, and linked list. Give a table of the worst-case \nbounds for each operation for each of your four implementations.\n2.4.4 ", "start": 340, "end": 341}, "494": {"text": "for each of the following underlying data structures: unordered ar -\nray, ordered array, unordered linked list, and linked list. Give a table of the worst-case \nbounds for each operation for each of your four implementations.\n2.4.4 Is an array that is sorted in decreasing order a max-oriented heap?\n2.4.5 Give the heap that results when the keys E A S Y Q U E S T I O N are inserted \nin that order into an initially empty max-oriented heap.\n2.4.6 Using the conventions of Exercise 2.4.1, give the sequence of heaps produced \nwhen the operations P R I O * R * * I * T * Y * * * Q U E * * * U * E are \nperformed on an initially empty max-oriented heap.\n2.4.7 The largest item in a heap must appear in position 1, and the second largest must \nbe in position 2 or position 3. Give the list of positions in a heap of size 31 where the \nkth largest (i) can appear, and (ii) cannot appear, for k=2, 3, 4 (assuming the values to \nbe distinct).\n2.4.8 Answer the previous exercise for the kth smallest item.\n2.4.9 Draw all of the different heaps that can be made from the \ufb01ve keys A B C D E, \nthen draw all of the different heaps that can be made from the \ufb01ve keys A A A B B.\n2.4.10 Suppose that we wish to avoid wasting one position in a heap-ordered array \npq[], putting the largest value in pq[0], its children in pq[1] and pq[2], and so forth, \nproceeding in level order. Where are the parents and children of pq[k]?\n2.4.11 Suppose that your application will have a huge number of ", "start": 341, "end": 341}, "495": {"text": "pq[1] and pq[2], and so forth, \nproceeding in level order. Where are the parents and children of pq[k]?\n2.4.11 Suppose that your application will have a huge number of insert operations, but \nonly a few remove the maximum operations.  Which priority-queue implementation do \nyou think would be most effective: heap, unordered array, or ordered array?\n3292.4 \u25a0 Priority Queues\n 2.4.12 Suppose that your application will have a huge number of \ufb01nd the maximum\noperations, but a relatively small number of insert and remove the maximum operations. \nWhich priority-queue implementation do you think would be most effective: heap, \nunordered array, or ordered array?\n2.4.13 Describe a way to avoid the j < N test in sink().\n2.4.14 What is the minimum number of items that must be exchanged during a re-\nmove the maximum operation in a heap of size N with no duplicate keys? Give a heap \nof size 15 for which the minimum is achieved. Answer the same questions for two and \nthree successive remove the maximum operations.\n2.4.15 Design a linear-time  certi\ufb01cation algorithm to check whether an array pq[] is \na min-oriented heap.\n2.4.16 For N=32, give arrays of items that make heapsort use as many and as few com-\npares as possible.\n2.4.17 Prove that building a minimum-oriented priority queue of size k then doing N \n/H11002 k replace the minimum (insert followed by remove the minimum) operations leaves the \nk largest of the N items in the priority queue.\n2.4.18 In MaxPQ, suppose that a client calls insert() with an item that is larger than \nall items in the queue, and then immediately calls delMax(). Assume that there are \nno duplicate keys. Is the resulting ", "start": 341, "end": 342}, "496": {"text": "queue.\n2.4.18 In MaxPQ, suppose that a client calls insert() with an item that is larger than \nall items in the queue, and then immediately calls delMax(). Assume that there are \nno duplicate keys. Is the resulting heap identical to the heap as it was before these op-\nerations? Answer the same question for two insert() operations (the \ufb01rst with a key \nlarger than all keys in the queue and the second for a key larger than that one) followed \nby two delMax() operations.\n2.4.19  Implement the constructor for MaxPQ that takes an array of items as argument, \nusing the bottom-up heap construction method described on page 323 in the text.\n2.4.20  Prove that sink-based heap construction uses fewer than 2 N compares and \nfewer than N exchanges.\nEXERCISES  (continued)\n330 CHAPTER 2 \u25a0 Sorting\n CREATIVE PROBLEMS\n  \n2.4.21  Elementary data structures. Explain how to use a priority queue to implement \nthe stack, queue, and randomized queue data types from Chapter 1.\n2.4.22    Array resizing. Add array resizing to MaxPQ, and prove bounds like those of \nProposition Q for array accesses, in an amortized sense.\n2.4.23  Multiway heaps. Considering the cost of compares only, and assuming that \nit takes t compares to \ufb01nd the largest of t items, \ufb01nd the value of t that minimizes the \ncoef\ufb01cient of N lg N in the compare count when a t-ary heap is used in heapsort. First, \nassume a straightforward generalization of sink(); then, assume that Floyd\u2019s method \ncan save one compare in the inner loop.\n2.4.24  Priority queue with explicit links. Implement a priority queue using a heap-\nordered binary tree, but use a ", "start": 342, "end": 343}, "497": {"text": "sink(); then, assume that Floyd\u2019s method \ncan save one compare in the inner loop.\n2.4.24  Priority queue with explicit links. Implement a priority queue using a heap-\nordered binary tree, but use a triply linked structure instead of an array. Y ou will need \nthree links per node: two to traverse down the tree and one to traverse up the tree. Y our \nimplementation should guarantee logarithmic running time per operation, even if no \nmaximum priority-queue size is known ahead of time.\n2.4.25  Computational number theory. Write a program CubeSum.java that prints out \nall integers of the form a3 + b 3 where a and b are integers between 0 and N in sorted \norder, without using excessive space. That is, instead of computing an array of the N2\nsums and sorting them, build a minimum-oriented priority queue, initially containing \n(03, 0, 0), (13, 1, 0), (23, 2, 0),  . . .  , (N3, N, 0). Then, while the priority queue is nonempty, \nremove the smallest item(i3 + j3, i, j), print it, and then, if j < N, insert the item (i3 + (j+1)3, \ni, j+1). Use this program to \ufb01nd all distinct mintegers a, b, c, and d between 0 and 10 6\nsuch that a3 + b3  = c3 + d3.\n2.4.26  Heap without exchanges. Because the exch() primitive is used in the sink()\nand swim() operations, the items are loaded and stored twice as often as necessary. \nGive more ef\ufb01cient implementations that avoid this inef\ufb01ciency, a la insertion sort (see \nExercise 2.1.25).\n2.4.27 ", "start": 343, "end": 343}, "498": {"text": "swim() operations, the items are loaded and stored twice as often as necessary. \nGive more ef\ufb01cient implementations that avoid this inef\ufb01ciency, a la insertion sort (see \nExercise 2.1.25).\n2.4.27  Find the minimum. Add a min() method to MaxPQ. Your implementation \nshould use constant time and constant extra space.\n2.4.28  Selection \ufb01lter. Write a TopM client that reads points (x, y, z) from standard in-\nput, takes a value M from the command line, and prints the M points that are closest to \nthe origin in Euclidean distance. Estimate the running time of your client for N = 108 \n3312.4 \u25a0 Priority Queues\n  \nand M = 104. \n2.4.29  Min/max priority queue. Design a data type that supports the following opera-\ntions: insert, delete the maximum, and delete the minimum (all in logarithmic time); and \n\ufb01nd the maximum and \ufb01nd the minimum (both in constant time). Hint: Use two heaps.\n2.4.30   Dynamic median-\ufb01nding. Design a data type that supports insert in logarith-\nmic time, \ufb01nd the median in constant time, and delete the median in logarithmic time. \nHint: Use a min-heap and a max-heap.\n2.4.31  Fast insert. Develop a compare-based implementation of the MinPQ API such \nthat insert uses ~ log log N compares and delete the minimum uses ~2 log N compares. \nHint : Use binary search on parent pointers to \ufb01nd the ancestor in swim().\n2.4.32   Lower bound. Prove that it is impossible to develop a compare-based imple -\nmentation of the MinPQ API such that both insert and delete the minimum guarantee to \nuse ~N log ", "start": 343, "end": 344}, "499": {"text": "swim().\n2.4.32   Lower bound. Prove that it is impossible to develop a compare-based imple -\nmentation of the MinPQ API such that both insert and delete the minimum guarantee to \nuse ~N log log N compares.\n2.4.33    Index priority-queue implementation. Implement the basic operations in the \nindex priority-queue API on page 320 by modifying Algorithm 2.6 as follows: Change \npq[] to hold indices, add an array keys[] to hold the key values, and add an array qp[]\nthat is the inverse of pq[] \u2014 qp[i] gives the position of i in pq[] (the index j such that \npq[j] is i). Then modify the code in Algorithm 2.6 to maintain these data structures. \nUse the convention that qp[i] = -1  if i is not on the queue, and include a method \ncontains() that tests this condition. Y ou need to modify the helper methods exch()\nand less() but not sink() or swim().\nCREATIVE PROBLEMS  (continued)\n332 CHAPTER 2 \u25a0 Sorting\n Partial solution :\npublic class IndexMinPQ<Key extends Comparable<Key>> \n{\n   private int N;           // number of elements on PQ\n   private int[] pq;        // binary heap using 1-based indexing\n   private int[] qp;        // inverse: qp[pq[i]] = pq[qp[i]] = i\n   private Key[] keys;      // items with priorities\n   public IndexMinPQ(int maxN)\n   {\n      keys = (Key[]) new Comparable[maxN + 1];\n      pq   = new int[maxN + 1];\n      qp   = new int[maxN + 1];\n      for (int i = 0; i <= maxN; i++) qp[i] = -1;\n   }\n   public boolean isEmpty()\n   {  return N == ", "start": 344, "end": 345}, "500": {"text": "= new int[maxN + 1];\n      for (int i = 0; i <= maxN; i++) qp[i] = -1;\n   }\n   public boolean isEmpty()\n   {  return N == 0;  }\n   public boolean contains(int k)\n   {  return qp[k] != -1;  }\n   public void insert(int k, Key key)\n   {\n      N++;\n      qp[k] = N;\n      pq[N] = k;\n      keys[k] = key;\n      swim(N);\n   }\n   public Item min()\n   {  return keys[pq[1]];  }\n   public int delMin()\n   {\n      int indexOfMin = pq[1];\n      exch(1, N--);\n      sink(1);\n      keys[pq[N+1]] = null;\n      qp[pq[N+1]] = -1;\n      return indexOfMin;\n}\n3332.4 \u25a0 Priority Queues\n 2.4.34  Index priority-queue implementation (additional operations). Add minIndex(), \nchange(),  and delete() to your implementation of Exercise 2.4.33. \nSolution :\n   public int minIndex()\n   {  return pq[1];  }\n   public void change(int k, Item item)\n   {\n      keys[k] = key;\n      swim(qp[k]);\n      sink(qp[k]);\n   }\n   public void delete(int k)\n   {\n      exch(k, N--);\n      swim(qp[k]);\n      sink(qp[k]);\n      keys[pq[N+1]] = null;\n      qp[pq[N+1]] = -1;\n   }\n2.4.35  Sampling from a discrete probability distribution. Write a class Sample with a \nconstructor that takes an array p[] of double values as argument and supports the fol-\nlowing two operations: random()\u2014return an index i with probability p[i]/T (where ", "start": 345, "end": 346}, "501": {"text": "probability distribution. Write a class Sample with a \nconstructor that takes an array p[] of double values as argument and supports the fol-\nlowing two operations: random()\u2014return an index i with probability p[i]/T (where \nT is the sum of the numbers in p[])\u2014and change(i, v)\u2014change the value of p[i]\nto v. Hint: Use a complete binary tree where each node has implied weight p[i]. Store \nin each node the cumulative weight of all the nodes in its subtree. T o generate a ran-\ndom index, pick a random number between 0 and T and use the cumulative weights to \ndetermine which branch of the subtree to explore. When updating p[i], change all of \nthe weights of the nodes on the path from the root to i. Avoid explicit pointers, as we \ndo for heaps.\nCREATIVE PROBLEMS  (continued)\n334 CHAPTER 2 \u25a0 Sorting\n EXPERIMENTS\n \n \n \n2.4.36  Performance driver I. Write a performance driver client program that uses in-\nsert to \ufb01ll a priority queue, then uses remove the maximum to remove half the keys, then \nuses insert to \ufb01ll it up again, then uses remove the maximum to remove all the keys, doing \nso multiple times on random sequences of keys of various lengths ranging from small to \nlarge; measures the time taken for each run; and prints out or plots the average running \ntimes.\n2.4.37  Performance driver II. Write a performance driver client program that uses in-\nsert to \ufb01ll a priority queue, then does as many remove the maximum and insert opera-\ntions as it can do in 1 second, doing so multiple times on random sequences of keys of \nvarious lengths ranging from small to large; and prints out or plots the average number \nof remove the maximum operations it was able to do.\n2.4.38  Exercise driver. ", "start": 346, "end": 347}, "502": {"text": "times on random sequences of keys of \nvarious lengths ranging from small to large; and prints out or plots the average number \nof remove the maximum operations it was able to do.\n2.4.38  Exercise driver. Write an exercise driver client program that uses the methods \nin our priority-queue interface of Algorithm 2.6 on dif\ufb01cult or pathological cases that \nmight turn up in practical applications. Simple examples include keys that are already \nin order, keys in reverse order, all keys the same, and sequences of keys having only two \ndistinct values.\n2.4.39  Cost of construction. Determine empirically the percentage of time heapsort \nspends in the construction phase for N = 103, 106, and 109.\n2.4.40  Floyd\u2019s method. Implement a version of heapsort based on Floyd\u2019s sink-to-the-\nbottom-and-then-swim idea, as described in the text. Count the number of compares \nused by your program and the number of compares used by the standard implementa-\ntion, for randomly ordered distinct keys with N = 103, 106, and 109.\n2.4.41    Multiway heaps. Implement a version of heapsort based on complete heap-\nordered 3-ary and 4-ary trees, as described in the text. Count the number of compares \nused by each and the number of compares used by the standard implementation, for \nrandomly ordered distinct keys with N = 103, 106, and 109.\n2.4.42  Preorder heaps. Implement a version of heapsort based on the idea of repre -\nsenting the heap-ordered tree in preorder rather than in level order. Count the number \nof compares used by your program and the number of compares used by the standard \nimplementation, for randomly ordered keys with N = 103, 106, and 109.\n3352.5 \u25a0 ", "start": 347, "end": 347}, "503": {"text": "order. Count the number \nof compares used by your program and the number of compares used by the standard \nimplementation, for randomly ordered keys with N = 103, 106, and 109.\n3352.5 \u25a0 Priority Queues\n 2.5  APPLICATIONS\n \n \n \nSorting algorithms and priority queues are widely used in a broad variety of ap -\nplications. Our purpose in this section is to brie\ufb02y survey some of these applications, \nconsider ways in which the ef\ufb01cient methods that we have considered play a critical role \nin such applications, and discuss some of the steps needed to make use of our sort and \npriority-queue code.\nA prime reason why sorting is so useful is that it is much easier to search for an item \nin a sorted array than in an unsorted one. For over a century, people found it easy to \nlook up someone\u2019s phone number in a phone book where items are sorted by last name. \nNow digital music players organize song \ufb01les by artist name or song title; search engines \ndisplay search results in descending order of importance; spreadsheets display columns \nsorted by a particular \ufb01eld; matrix-processing packages sort the real eigenvalues of a \nsymmetric matrix in descending order; and so forth. Other tasks are also made easier \nonce an array is in sorted order: from looking up an item in the sorted index in the back \nof this book; to removing duplicates from a long list such as a mailing list, a list of vot-\ners, or a list of websites; to performing statistical calculations such as removing outliers, \n\ufb01nding the median, or computing percentiles.\nSorting also arises as a critical subproblem in many applications that appear to have \nnothing to do with sorting at all. Data compression, computer graphics, computational \nbiology, supply-chain management, combinatorial optimization, social choice, and \nvoting are but a few of ", "start": 347, "end": 348}, "504": {"text": "many applications that appear to have \nnothing to do with sorting at all. Data compression, computer graphics, computational \nbiology, supply-chain management, combinatorial optimization, social choice, and \nvoting are but a few of many examples. The algorithms that we have considered in this \nchapter play a critical role in the development of effective algorithms in each of the later \nchapters in this book.\nMost important is the system sort, so we begin by considering a number of practical \nconsiderations that come into play when building a sort for use by a broad variety of \nclients. While some of these topics are speci\ufb01c to Java, they each re\ufb02ect challenges that \nneed to be met in any system.\nOur primary purpose is to demonstrate that, even though we have used mechanisms \nthat are relatively simple, the sorting implementations that we are studying are widely \napplicable. The list of proven applications of fast sorting algorithms is vast, so we can \nconsider just a small fraction of them: some scienti\ufb01c, some algorithmic, and some \ncommercial. You will \ufb01nd many more examples in the exercises, and many more than \nthat on the booksite. Moreover, we will often refer back to this chapter to effectively ad-\ndress the problems that we later consider in this book!\n336\n  \n \nSorting various types of data Our implementations  sort arrays of Comparable\nobjects. This Java convention allows us to use Java\u2019s callback mechanism to sort arrays \nof objects of any type that implements the Comparable interface. As described in \nSection 2.1, implementing Comparable amounts to de\ufb01ning a    compareTo() method \nthat implements a  natural ordering for the type. We can use our code immediately to \nsort arrays of type String, Integer, Double, and other types such as File and URL, \nbecause these data types all implement Comparable. Being able to use the same code \nfor all of those types is ", "start": 348, "end": 349}, "505": {"text": "code immediately to \nsort arrays of type String, Integer, Double, and other types such as File and URL, \nbecause these data types all implement Comparable. Being able to use the same code \nfor all of those types is convenient, but typical applications involve working with data \ntypes that are de\ufb01ned for use within the application. Accordingly it is common to im -\nplement a compareTo() method for user-de\ufb01ned data types, so that they implement \nComparable, thus enabling client code to sort arrays of that type (and build priority \nqueues of values of that type). \n T r a n s a c t i o n  e x a m p l e .  A prototypical breeding ground for sorting applications is \ncommercial data processing. For example, imagine that a company engaged in internet \ncommerce maintains a record for each transaction involving a customer account that \ncontains all of the pertinent information, such as the customer name, date, amount, \nand so forth. Nowadays, a successful company needs to be able to handle millions and \nmillions of such transactions. As we saw in Exercise 2.1.21, it is reasonable to decide \nthat a natural ordering of such transactions is that they be ordered by amount, which \nwe can implement by adding an appropriate compareTo() method in the class de\ufb01ni-\ntion. With such a de\ufb01nition, we could process an array a[] of Transactions by, for ex-\nample, \ufb01rst sorting it with the call Quick.sort(a). Our sorting methods know nothing \nabout our Transaction data type, but Java\u2019s Comparable interface allows us to de\ufb01ne \na natural ordering so that we can use any of our methods to sort Transaction objects. \nAlternatively, we might specify that Transaction objects are to be ordered by date by \nimplementing compareTo() to compare the Date \ufb01elds. Since Date objects are them-\nselves Comparable, we can just invoke ", "start": 349, "end": 349}, "506": {"text": "objects. \nAlternatively, we might specify that Transaction objects are to be ordered by date by \nimplementing compareTo() to compare the Date \ufb01elds. Since Date objects are them-\nselves Comparable, we can just invoke the compareTo() method in Date rather than \nhaving to implement it from scratch. It is also reasonable to consider ordering this data \nby it customer \ufb01eld; arranging to allow clients the \ufb02exibility to switch among multiple \ndifferent orders is an interesting challenge that we will soon consider.\npublic int compareTo(Transaction that) \n{  return this.when.compareTo(that.when);  }\nAlternate compareTo() implementation for sorting transactions by date\n3372.5 \u25a0 Applications\n  \n  P o i n t e r  s o r t i n g .  The approach we are using is known in the classical literature as \npointer sorting, so called because we process references to items and do not move the \ndata itself. In programming languages such as C and C++, programmers explicitly de-\ncide whether to manipulate data or pointers to data; in Java, pointer manipulation is \nimplicit. Except for primitive numeric types, we always manipulate references to ob -\njects (pointers), not the objects themselves. Pointer sorting adds a level of indirection: \nthe array contains references to the objects to be sorted, not the objects themselves. We \nbrie\ufb02y consider some associated issues, in the context of sorting. With multiple refer -\nence arrays, we can have multiple different sorted representations of different parts of a \nsingle body of data (perhaps using multiple keys, as described below). \nKeys are immutable. It stands to reason that an array might not remain sorted if a \nclient is allowed to change the values of keys after the sort. Similarly, a priority queue \ncan hardly be expected to operate properly if the client can change the values of keys \nbetween operations. In Java, it is wise to ensure that key values do not ", "start": 349, "end": 350}, "507": {"text": "values of keys after the sort. Similarly, a priority queue \ncan hardly be expected to operate properly if the client can change the values of keys \nbetween operations. In Java, it is wise to ensure that key values do not change by using \nimmutable keys. Most of the standard data types that you are likely to use as keys, such \nas String,  Integer, Double, and File, are immutable.\nExchanges are inexpensive. Another advantage of using references is that we avoid the \ncost of moving full items. The cost saving is signi\ufb01cant for arrays with large items (and \nsmall keys) because the compare needs to access just a small part of the item, and most \nof the item is not even touched during the sort. The reference approach makes the cost \nof an exchange roughly equal to the cost of a compare for general situations involving \narbitrarily large items (at the cost of the extra space for the references). Indeed, if the \nkeys are long, the exchanges might even wind up being less costly than the compare. \nOne way to study the performance of algorithms that sort arrays of numbers is to sim-\nply look at the total number of compares and exchanges they use, implicitly making the \nassumption that the cost of exchanges is the same as the cost of compares. Conclusions \nbased on this assumption are likely to apply to a broad class of applications in Java, \nbecause we are sorting reference objects.\nAlternate orderings. There are many applications where we want to use differ -\nent orders for the objects that we are sorting, depending on the situation. The Java \n  Comparator interface allows us to build multiple orders within a single class. It has \na single public method    compare() that compares two objects. If we have a data type \nthat implements this interface, we can pass a Comparator to sort() (which passes it to \nless()) as in the example on the next page. The Comparator mechanism allows ", "start": 350, "end": 350}, "508": {"text": "that compares two objects. If we have a data type \nthat implements this interface, we can pass a Comparator to sort() (which passes it to \nless()) as in the example on the next page. The Comparator mechanism allows us to \nsort arrays of any type of object, using any total order that we wish to de\ufb01ne for them. \nUsing a Comparator instead of working with Comparable types better separates the \nde\ufb01nition of the type from the de\ufb01nition of what it means to compare two objects of \n338 CHAPTER 2 \u25a0 Sorting\n  \n \nthat type. Indeed, there are typically many possible ways to compare objects, and the \nComparator mechanism allows us to choose among them. For instance, to sort an ar -\nray a[] of strings without regard to whether characters are uppercase or lowercase you \ncan just call Insertion.sort(a, String.CASE_INSENSITIVE_ORDER) which makes \nuse of the CASE_INSENSITIVE_ORDER comparator de\ufb01ned in Java\u2019s String class. As\nyou can imagine, the precise rules for ordering strings are complicated and quite differ-\nent for various natural languages, so Java has many String comparators.\n I t e m s  w i t h  m u l t i p l e  k e y s .  In typical applications, items have multiple instance variables \nthat might need to serve as sort keys. In our transaction example, one client may need \nto sort the transaction list by customer (for example, to bring together all transactions \ninvolving each customer); another client might need to sort the list by amount (for \nexample, to identify high-value transactions); and other clients might need to use other \n\ufb01elds as sort keys. The  Comparator mechanism is precisely what we need to allow this \n\ufb02exibility. We can de\ufb01ne multiple comparators, as in the alternate implementation of \nTransaction shown on the bottom of the ", "start": 350, "end": 351}, "509": {"text": "sort keys. The  Comparator mechanism is precisely what we need to allow this \n\ufb02exibility. We can de\ufb01ne multiple comparators, as in the alternate implementation of \nTransaction shown on the bottom of the next page. With this de\ufb01nition, a client can \nsort an array of Transaction objects by time with the call \nInsertion.sort(a, new Transaction.WhenOrder()) \nor by amount with the call \nInsertion.sort(a, new Transaction.HowMuchOrder()). \nThe sort does each compare through a  callback to the compare() method in \nTransaction that is speci\ufb01ed by the client code. T o avoid the cost of making a new \nComparator object for each sort, we could use public final instance variables to de-\n\ufb01ne the comparators (as Java does for CASE_INSENSITIVE_ORDER). \npublic static void sort(Object[] a, Comparator c) \n{\n   int N = a.length;\n   for (int i = 1; i < N; i++)\n      for (int j = i; j > 0 && less(c, a[j], a[j-1]); j--)\n         exch(a, j, j-1); \n}\nprivate static boolean less(Comparator c, Object v, Object w) \n{  return c.compare (v, w) < 0;  }\nprivate static void exch(Object[] a, int i, int j) \n{  Object t = a[i]; a[i] = a[j]; a[j] = t; }\nInsertion sorting with a Comparator\n3392.5 \u25a0 Applications\n Priority queues with  comparators. The same \ufb02exibility to use comparators is also \nuseful for priority queues. Extending our standard implementation in Algorithm 2.6 \nto support comparators involves the following steps:\n\u25a0 Import java.util.Comparator.\n\u25a0 Add to MaxPQ an instance variable comparator and a ", "start": 351, "end": 352}, "510": {"text": "\nuseful for priority queues. Extending our standard implementation in Algorithm 2.6 \nto support comparators involves the following steps:\n\u25a0 Import java.util.Comparator.\n\u25a0 Add to MaxPQ an instance variable comparator and a constructor that takes a \ncomparator as argument and initializes comparator to that value.\n\u25a0 Add code to less() that checks whether comparator is null (and uses it if it is \nnot null).\nFor example, with these changes, you could build different priority queues with \nTransaction keys, using the time, place, or account number for the ordering. If you \nremove the Key extends Comparable<Key> phrase from MinPQ, you even can support \nkeys with no natural order.\nimport java.util.Comparator;\npublic class    Transaction \n{\n   ...\n   private final String who;\n   private final Date when;\n   private final double amount;\n   ...\n   public static class WhoOrder implements Comparator<Transaction>\n   {\n      public int compare(Transaction v, Transaction w)\n      {  return v.who.compareTo(w.when);  }\n   }\n   public static class WhenOrder implements Comparator<Transaction>\n   {\n      public int compare(Transaction v, Transaction w)\n      {  return v.when.compareTo(w.when);  }\n   }\n   public static class HowMuchOrder implements Comparator<Transaction>\n   {\n      public int compare(Transaction v, Transaction w)\n      {  \n         if (v.amount < w.amount) return -1;\n         if (v.amount > w.amount) return +1;\n         return 0;  \n      }\n   } \n}\n I n s e r t i o n  s o r t i n g  w i t h  a  Comparator\n340 CHAPTER 2 \u25a0 Sorting\n  \n    S t a b i l i t y .  A sorting method is stable if it preserves the relative order of equal keys in the \narray. This property is frequently important. For example, consider an internet com-\nmerce application where we have to process ", "start": 352, "end": 353}, "511": {"text": "l i t y .  A sorting method is stable if it preserves the relative order of equal keys in the \narray. This property is frequently important. For example, consider an internet com-\nmerce application where we have to process a large number of events that have loca-\ntions and timestamps. T o begin, suppose that we store events in an array as they arrive, \nso they are in order of the timestamp in the array. Now suppose that the application \nrequires that the transactions be separated out by location for further processing. One \neasy way to do so is to sort the array by location. If the sort is unstable, the transac-\ntions for each city may not necessarily be in order by timestamp after the sort. Often, \nprogrammers who are unfamiliar with stability are surprised, when they \ufb01rst encounter \nthe situation, by the way an unstable algorithm seems to scramble the data. Some of \nthe sorting methods that we have considered in this chapter are stable ( insertion sort \nand  mergesort); many are not (selection sort, shellsort, quicksort, and heapsort). There \nare ways to trick any sort into stable behavior (see Exercise 2.5.18), but using a stable \nalgorithm is generally preferable when stability is an essential requirement. It is easy \nto take stability for granted; actually, no practical method in common use achieves \nstability without using signi\ufb01cant extra time or space (researchers have developed al-\ngorithms that do so, but applications programmers have judged them too complicated \nto be useful).\nChicago  09:00:00\nPhoenix  09:00:03\nHouston  09:00:13\nChicago  09:00:59\nHouston  09:01:10\nChicago  09:03:13\nSeattle  09:10:11\nSeattle  09:10:25\nPhoenix  09:14:25\nChicago ", "start": 353, "end": 353}, "512": {"text": "09:00:59\nHouston  09:01:10\nChicago  09:03:13\nSeattle  09:10:11\nSeattle  09:10:25\nPhoenix  09:14:25\nChicago  09:19:32\nChicago  09:19:46\nChicago  09:21:05\nSeattle  09:22:43\nSeattle  09:22:54\nChicago  09:25:52\nChicago  09:35:21\nSeattle  09:36:14\nPhoenix  09:37:44\nChicago 09:00:00\nChicago 09:00:59\nChicago 09:03:13\nChicago 09:19:32\nChicago 09:19:46\nChicago 09:21:05\nChicago 09:25:52\nChicago 09:35:21\nHouston 09:00:13\nHouston 09:01:10\nPhoenix 09:00:03\nPhoenix 09:14:25\nPhoenix 09:37:44\nSeattle 09:10:11\nSeattle 09:10:25\nSeattle 09:22:43\nSeattle 09:22:54\nSeattle 09:36:14\nChicago 09:25:52\nChicago 09:03:13\nChicago 09:21:05\nChicago 09:19:46\nChicago 09:19:32\nChicago 09:00:00\nChicago 09:35:21\nChicago 09:00:59\nHouston 09:01:10\nHouston 09:00:13\nPhoenix 09:37:44\nPhoenix 09:00:03\nPhoenix 09:14:25\nSeattle 09:10:25\nSeattle 09:36:14\nSeattle 09:22:43\nSeattle ", "start": 353, "end": 353}, "513": {"text": "09:00:13\nPhoenix 09:37:44\nPhoenix 09:00:03\nPhoenix 09:14:25\nSeattle 09:10:25\nSeattle 09:36:14\nSeattle 09:22:43\nSeattle 09:10:11\nSeattle 09:22:54\nsorted by time sorted by location (not stable) sorted by location (stable)\nno \nlonger\nsorted\nby time\nstill\nsorted\nby time\nStability when sorting on a second key\n3412.5 \u25a0 Applications\n Which sorting algorithm should I use? We have considered numerous sor ting \nalgorithms in this chapter, so this question is natural. Knowing which algorithm is best \npossible depends heavily on details of the application and implementation, but we \nhave studied some general-purpose methods that can be nearly as effective as the best \npossible for a wide variety of applications.\nThe table at the bottom of this page is a general guide that summarizes the impor -\ntant characteristics of the sort algorithms that we have studied in this chapter. In all \ncases but shellsort (where the growth rate is only an estimate), insertion sort (where \nthe growth rate depends on the order of the input keys), and both versions of quicksort \n(where the growth rate is probabilitic and may depend on the distribution of input key \nvalues), multiplying these growth rates by appropriate constants gives an effective way \nto predict running time. The constants involved are partly algorithm-dependent (for \nexample, heapsort uses twice the number of compares as mergesort and both do many \nmore array accesses than quicksort) but are primarily dependent on the implementa-\ntion, the Java compiler, and your computer, which determine the number of machine \ninstructions that are executed and the time that each requires. Most important, since \nthey are constants, you can generally predict the running time for large N by running \nexperiments for smaller N and ", "start": 353, "end": 354}, "514": {"text": "which determine the number of machine \ninstructions that are executed and the time that each requires. Most important, since \nthey are constants, you can generally predict the running time for large N by running \nexperiments for smaller N and extrapolating, using our standard doubling protocol.\nalgorithm stable? in place? order of growth to sort N items notesrunning time extra space\nselection sort no yes N 2 1\ninsertion sort yes yes between\nN and N 2 1 depends on order\nof items\nshellsort no yes N log N ?\nN 6/5 ? 1\nquicksort no yes N log N lg N probabilistic\nguarantee\n3-way quicksort no yes between\nN and N log N lg N\nprobabilistic,\nalso depends on\ndistribution of\ninput keys\nmergesort yes no N log N N\nheapsort no yes N log N 1\n P e r f o r m a n c e  c h a r a c t e r i s t i c s  o f  s o r t i n g  a l g o r i t h m s\n342 CHAPTER 2 \u25a0 Sorting\n Property T.  Quicksort is the fastest general-purpose sort.\nEvidence: This hypothesis is supported by countless implementations of quick-\nsort on countless computer systems since its invention decades ago. Generally, the \nreason that quicksort is fastest is that it has only a few instructions in its inner \nloop (and it does well with  cache memories because it most often references data \nsequentially) so that its running time is ~c N lg N with the value of c smaller than \nthe corresponding constants for other linearithmic sorts. With 3-way partitioning, \nquicksort becomes linear for certain key distributions likely to arise in practice, \nwhere other sorts are linearithmic.\n \n \n \nThus, in most practical situations, quicksort ", "start": 354, "end": 355}, "515": {"text": "linearithmic sorts. With 3-way partitioning, \nquicksort becomes linear for certain key distributions likely to arise in practice, \nwhere other sorts are linearithmic.\n \n \n \nThus, in most practical situations, quicksort is the method of choice. Still, given the \nbroad reach of sorting and the broad variety of computers and systems, a \ufb02at statement \nlike this is dif\ufb01cult to justify. For example, we have already seen one notable exception: \nif stability is important and space is available, mergesort might be best. We will see oth-\ner exceptions in Chapter 5. With tools like SortCompare and a considerable amount of \ntime and effort, you can do a more detailed study of comparative performance of these \nalgorithms and the re\ufb01nements that we have discussed for your computer, as discussed \nin several exercises at the end of this section. Perhaps the best way to interpret Prop-\nerty T is as saying that you certainly should seriously consider using quicksort in any \nsort application where running time is important. \n S o r t i n g  p r i m i t i v e  t y p e s .  In some performance-critical applications, the focus may be \non sorting numbers, so it is reasonable to avoid the costs of using references and sort \nprimitive types instead. For example, consider the difference between sorting an array \nof double values and sorting an array of Double values. In the former case, we exchange \nthe numbers themselves and put them in order in the array; in the latter, we exchange \nreferences to Double objects, which contain the numbers. If we are doing nothing more \nthan sorting a huge array of numbers, we avoid paying the cost of storing an equal \nnumber of references plus the extra cost of accessing the numbers through the refer -\nences, not to mention the cost of invoking compareTo() and less() methods. We can \ndevelop ", "start": 355, "end": 355}, "516": {"text": "numbers, we avoid paying the cost of storing an equal \nnumber of references plus the extra cost of accessing the numbers through the refer -\nences, not to mention the cost of invoking compareTo() and less() methods. We can \ndevelop ef\ufb01cient versions of our sort codes for such purposes by replacing Comparable\nwith the primitive type name, and rede\ufb01ning less() or just replacing calls to less()\nwith code like a[i] < a[j] (see Exercise 2.1.26). \nJava system sort. As an example of applying the information given in the table on \npage 342, consider Java\u2019s primary system sort method, java.util.Arrays.sort(). \nWith overloading of argument types, this name actually represents a collection of \nmethods:\n3432.5 \u25a0 Applications\n \u25a0 A different method for each primitive type\n\u25a0 A method for data types that implement Comparable\n\u25a0 \n \nA method that uses a Comparator\nJava\u2019s systems programmers have chosen to use quicksort (with 3-way partitioning) \nto implement the primitive-type methods, and mergesort for reference-type methods. \n T h e  p r i m a r y  p r a c t i c a l  i m p l i c a t i o n s  o f  t h e s e  c h o i c e s  a r e ,  a s  j u s t  d i s c u s s e d ,  t o  t r a d e  s p e e d  \nand memory usage (for primitive types) for stability (for reference types). \nThe algorithms and ideas  that we have been considering are an essential part of \nmany modern systems, including Java. When developing Java programs to address an \napplication, you are likely to \ufb01nd that Java\u2019s Arrays.sort() implementations (perhaps \nsupplemented by your own implementation(s) of compareTo() and/or compare()) \nwill ", "start": 355, "end": 356}, "517": {"text": "including Java. When developing Java programs to address an \napplication, you are likely to \ufb01nd that Java\u2019s Arrays.sort() implementations (perhaps \nsupplemented by your own implementation(s) of compareTo() and/or compare()) \nwill meet your needs, because you will be using 3-way quicksort or mergesort, both \nproven classic algorithms. \nIn this book, we generally will use our own Quick.sort() (usually) or Merge.sort()\n(when stability is important and space is not) in sort clients. Y ou may feel free to use \nArrays.sort() unless you have a good reason to use another speci\ufb01c method.\n  R e d u c t i o n s  The idea that we can use sorting algorithms to solve other problems \nis an example of a basic technique in algorithm design known as reduction. We con-\nsider reduction in detail in Chapter 6 because of its importance in the theory of al -\ngorithms\u2014in the meantime, we will consider several practical examples. A reduction \nis a situation where an algorithm developed for one problem is used to solve another. \nApplications programmers are quite used to the concept of reduction (whether or not \nit is explicitly articulated)\u2014every time you make use of a method that solves problem \nB in order to solve problem A, you are doing a reduction from A to B. Indeed, one goal \nin implementing algorithms is to facilitate reductions by making the algorithms useful \nfor as wide a variety as possible of applications. We begin with a few elementary exam-\nples for sorting. Many of these take the form of algorithmic puzzles where a quadratic \nbrute-force algorithm is immediate. It is often the case that sorting the data \ufb01rst makes \nit easy to \ufb01nish solving the problem in linear additional time, thus reducing the total \ncost from quadratic to linearithmic. \n D u p l i c a t e s .  Are there any duplicate keys in ", "start": 356, "end": 356}, "518": {"text": "\nit easy to \ufb01nish solving the problem in linear additional time, thus reducing the total \ncost from quadratic to linearithmic. \n D u p l i c a t e s .  Are there any duplicate keys in an array of Comparable objects? How \nmany distinct keys are there? Which value appears most frequently? For small arrays, \nthese kinds of questions are easy to answer with a quadratic algorithm that compares \neach array entry with each other array entry. For large arrays, using a quadratic algo-\nrithm is not feasible. With sorting, you can answer these questions in linearithmic time: \n344 CHAPTER 2 \u25a0 Sorting\n \ufb01rst sort the array, then make a pass through the sorted array, taking note of duplicate \nkeys that appear consecutively in the ordered array. For example, the code fragment at \nright counts the distinct keys in an array. With simple modi\ufb01cations to this code, you \ncan answer the questions above and perform tasks such as printing all the distinct val-\nues, all the values that are duplicated, and so forth, even for huge arrays.\nRankings. A  permutation (or ranking) is an \narray of N integers where each of the integers \nbetween 0 and N-1 appears exactly once. The \nKendall tau distance  between two rankings is \nthe number of pairs that are in different order \nin the two rankings.  For example, the Kendall \ntau distance between 0 3 1 6 2 5 4  and \n1 0 3 6 4 2 5 is four because the pairs 0-1, \n3-1, 2-4, 5-4 are in different relative order in \nthe two rankings, but all other pairs are in the same relative order. This statistic is widely \nused: in sociology to study social choice and voting theory, in molecular biology ", "start": 356, "end": 357}, "519": {"text": "5-4 are in different relative order in \nthe two rankings, but all other pairs are in the same relative order. This statistic is widely \nused: in sociology to study social choice and voting theory, in molecular biology to \ncompare genes using expression pro\ufb01les, and in ranking search engine results on the \nweb, among many other applications. The Kendall tau distance between a permutation \nand the identity permutation (where each entry is equal to its index) is the number \nof inversions in the permutation, and a quadratic algorithm based on insertion sort \nis not dif\ufb01cult to devise (recall Proposition C in Section 2.1). Ef\ufb01ciently computing \nthe Kendall tau distance is an interesting exercise for a programmer (or a student!) \nwho is familiar with the classical sorting algorithms that we have studied (see Exercise \n2.5.19).\n  P r i o r i t y - q u e u e  r e d u c t i o n s .  In Section 2.4, we considered two examples of problems \nthat reduce to a sequence of operations on priority queues. TopM, on page 311, \ufb01nds the M \nitems in an input stream with the highest keys. Multiway, on page 322, merges M sorted \ninput streams together to make a sorted output stream. Both of these problems are eas-\nily addressed with a priority queue of size M.\n   M e d i a n  a n d    o r d e r    s t a t i s t i c s .  An important application related to sorting but for \nwhich a full sort is not required is the operation of \ufb01nding the median of a collection \nof keys (the value with the property that half the keys are no larger and half the keys \nare no smaller). This operation is a common computation in statistics and in various \nother data-processing applications. Finding the median ", "start": 357, "end": 357}, "520": {"text": "collection \nof keys (the value with the property that half the keys are no larger and half the keys \nare no smaller). This operation is a common computation in statistics and in various \nother data-processing applications. Finding the median is a special case of  selection: \n\ufb01nding the k th smallest of a collection of numbers. Selection has many applications in \nthe processing of experimental and other data. The use of the median and other order \nQuick.sort(a); \nint count = 1; // Assume a.length > 0. \nfor (int i = 1; i < a.length; i++)\n   if (a[i].compareTo(a[i-1]) != 0)   \n      count++;\n C o u n t i n g  t h e  d i s t i n c t  k e y s  i n  a[]\n3452.5 \u25a0 Applications\n statistics to divide an array into smaller groups is common. Often, only a small part of a \nlarge array is to be saved for further processing; in such cases, a program that can select, \nsay, the top 10 percent of the items of the array might be more appropriate than a full \nsort. Our TopM application of Section \n2.4 solves this problem for an unbound-\ned input stream, using a priority queue. \nAn effective alternative to TopM when \nyou have the items in an array is to just \nsort it: after the call Quick.sort(a) the k\nsmallest values in the array are in the \ufb01rst \nk array positions for all k less than the ar-\nray length. But this approach involves a \nsort, so the running time is linearithmic. \nCan we do better? Finding the k smallest \nvalues in an array is easy when k is very \nsmall or very \nlarge, but more \nchallenging  \nwhen k is a constant fraction of the array size, ", "start": 357, "end": 358}, "521": {"text": "linearithmic. \nCan we do better? Finding the k smallest \nvalues in an array is easy when k is very \nsmall or very \nlarge, but more \nchallenging  \nwhen k is a constant fraction of the array size, such as \ufb01nding \nthe median (k = N/2). Y ou might be surprised to learn that it is \npossible to solve this problem in linear time, as in the select()\nmethod above (this implementation requires a client  cast; for \nthe more pedantic code needed to avoid this requirement, see \nthe booksite). T o do the  job, select() maintains the variables \nlo and hi to delimit the subarray that contains the index k\nof the item to be selected and uses quicksort partitioning to \nshrink the size of the subarray. Recall that partition() rear-\nranges an array a[lo] through a[hi] and returns an integer \nj such that a[lo] through a[j-1] are less than or equal to \na[j], and a[j+1] through a[hi] are greater than or equal to \na[j]. Now, if k is equal to j, then we are done. Otherwise, if \nk < j, then we need to continue working in the left subarray \n(by changing the value of hi to j-1); if k > j, then we need \nto continue working in the right subarray (by changing lo to \nj+1). The loop maintains the invariant that no entry to the left \nof lo is larger and no entry to the right of hi is smaller than \nany element within a[lo..hi]. After partitioning, we preserve \nthis invariant and shrink the interval until it consists just of \npublic static Comparable \nselect(Comparable[] a, int k) \n{\n   StdRandom.shuffle(a);\n   int lo = 0, hi = a.length ", "start": 358, "end": 358}, "522": {"text": "preserve \nthis invariant and shrink the interval until it consists just of \npublic static Comparable \nselect(Comparable[] a, int k) \n{\n   StdRandom.shuffle(a);\n   int lo = 0, hi = a.length - 1;\n   while (hi > lo)\n   {\n      int j = partition(a, lo, hi);\n      if     (j == k)  return a[k];\n      else if (j > k)  hi = j - 1;\n      else if (j < k)  lo = j + 1;\n   }\n   return a[k]; \n}\n S e l e c t i n g  t h e  k smallest elements in a[]\nmedian\nlo i hi\nPartitioning to find the median\n346 CHAPTER 2 \u25a0 Sorting\n k. Upon termination, a[k] contains the ( k +1)st smallest entry, a[0] through a[k-1]\nare all smaller than (or equal to) a[k], and a[k+1] through the end of the array are all \ngreater than (or equal to) a[k]. To gain some insight into why this is a linear-time al -\ngorithm, suppose that partitioning divides the array exactly in half each time. Then the \nnumber of compares is N /H11001 N/2 /H11001 N/4 /H11001 N/8 /H11001 . . . , terminating when the k th small-\nest item is found. This sum is less than 2 N. As with quicksort, it takes a bit of math to \n\ufb01nd the true bound, which is a bit higher . Also as with quicksort, the analysis depends \non partitioning on a random item, so that the guarantee is probabilistic. \nProposition U.  Partitioning-based selection is a linear-time algorithm, on average.\nProof: An analysis similar to, but signi\ufb01cantly ", "start": 358, "end": 359}, "523": {"text": "partitioning on a random item, so that the guarantee is probabilistic. \nProposition U.  Partitioning-based selection is a linear-time algorithm, on average.\nProof: An analysis similar to, but signi\ufb01cantly more complex than, the proof of \nProposition K for quicksort leads to the result that the average number of com -\npares is ~ 2 N /H11001 2k ln(N/k) /H11001  2(N /H11002 k) ln(N/(N /H11002 k)), which is linear for any \nallowed value of k. For example, this formula says that \ufb01nding the median ( k = \nN/2) requires ~ (2 /H11001 2 ln 2)N compares, on the average.  Note that the worst case \nis quadratic but randomization protects against that possibility, as with quicksort.\nDesigning a selection algorithm that is guaranteed to use a linear number of compares \nin the worst case is a classic result in computational complexity, but it has not yet led to \na useful practical algorithm.\n3472.5 \u25a0 Applications\n  \nA brief survey of sorting applications Direct applications of sorting are fa-\nmiliar, ubiquitous, and far too numerous for us to list them all. Y ou sort your music by \nsong title or by artist name, your email or phone calls by time or origin, and your pho-\ntos by date. Universities sort student accounts by name or ID. Credit card companies \nsort millions or even billions of transactions by date or amount. Scientists sort not only \nexperimental data by time or other identi\ufb01er but also to enable detailed simulations \nof the natural world, from the motion of particles or heavenly bodies to the structure \nof materials to social interations and relationships. Indeed, it is dif\ufb01cult to identify a \ncomputational application that does not involve sorting! T o elaborate upon ", "start": 359, "end": 360}, "524": {"text": "from the motion of particles or heavenly bodies to the structure \nof materials to social interations and relationships. Indeed, it is dif\ufb01cult to identify a \ncomputational application that does not involve sorting! T o elaborate upon this point, \nwe describe in this section examples of applications that are more complicated than the \nreductions just considered, including several that we will examine in detail later in this \nbook.\nCommercial computing. The world is awash in information. Government organiza-\ntions, \ufb01nancial institutions, and commercial enterprises organize much of this infor -\nmation by sorting it. Whether the information is accounts to be sorted by name or \nnumber, transactions to be sorted by date or amount, mail to be sorted by postal code \nor address, \ufb01les to be sorted by name or date, or whatever, processing such data is sure \nto involve a sorting algorithm somewhere along the way. Typically, such information \nis organized in huge databases, sorted by multiple keys for ef\ufb01cient search. An effective \nstrategy that is widely used is to collect new information, add it to the database, sort it \non the keys of interest, and merge the sorted result for each key into the existing data-\nbase. The methods that we have discussed have been used effectively since the early days \nof computing to build a huge infrastructure of sorted data and methods for processing \nit that serve as the basis for all of this commercial activity. Arrays having millions or \neven billions of entries are routinely processed today\u2014without linearithmic sorting \nalgorithms, such arrays could not be sorted, making such processing extremely dif\ufb01cult \nor impossible.\nSearch for information. Keeping data in sorted order makes it possible to ef\ufb01ciently \nsearch through it using the classic binary search algorithm (see Chapter 1). Y ou will \nalso see that the same scheme makes it easy to quickly handle many other kinds of \nqueries. How many ", "start": 360, "end": 360}, "525": {"text": "ef\ufb01ciently \nsearch through it using the classic binary search algorithm (see Chapter 1). Y ou will \nalso see that the same scheme makes it easy to quickly handle many other kinds of \nqueries. How many items are smaller than a given item? Which items fall within a given \nrange? In Chapter 3, we consider such questions. We also consider in detail various \nextensions to sorting and binary search that allow us to intermix such queries with \noperations that insert and remove objects from the set, still guaranteeing logarithmic \nperformance for all operations. \n348 CHAPTER 2 \u25a0 Sorting\n  \n \n  \n O p e r a t i o n s  r e s e a r c h .  The \ufb01eld of  operations research (OR) develops and applies math-\nematical models for problem-solving and decision-making. We will see several exam-\nples in this book of relationships between OR and the study of algorithms, beginning \nhere with the use of sorting in a classic OR problem known as scheduling. Suppose that \nwe have N jobs to complete, where job j requires tj seconds of processing time. We need \nto complete all of the jobs but want to maximize customer satisfaction by minimizing \nthe average completion time of the jobs. The  shortest processing time \ufb01rst rule, where we \nschedule the jobs in increasing order of processing time, is known to accomplish this \ngoal. Therefore we can sort the jobs by processing time or put them on a minimum-\noriented priority queue. With various other constraints and restrictions, we get various \nother scheduling problems, which frequently arise in industrial applications and are \nwell-studied. As another example, consider the  load-balancing problem, where we have \nM identical processors and N jobs to complete, and our goal is to schedule all of the \njobs on the processors so that the time at which the last job completes is as early as pos-\nsible. This speci\ufb01c ", "start": 360, "end": 361}, "526": {"text": "\nM identical processors and N jobs to complete, and our goal is to schedule all of the \njobs on the processors so that the time at which the last job completes is as early as pos-\nsible. This speci\ufb01c problem is NP-hard (see Chapter 6) so we do not expect to \ufb01nd a \npractical way to compute an optimal schedule. One method that is known to produce \na good schedule is the    longest processing time \ufb01rst  rule,  where we consider the jobs in \ndescending order of processing time, assigning each job to the processor that becomes \navailable \ufb01rst. To implement this algorithm, we \ufb01rst sort the jobs in reverse order. Then \nwe maintain a priority queue of M processors, where the priority is the sum of the pro-\ncessing times of its jobs. At each step, we delete the processor with the minimum prior-\nity, add the next job to the processor, and reinsert that processor into the priority queue.\n E v e n t - d r i v e n  s i m u l a t i o n .  Many scienti\ufb01c applications involve simulation, where the \npoint of the computation is to model some aspect of the real world in order to be able to \nbetter understand it. Before the advent of computing, scientists had little choice but to \nbuild mathematical models for this purpose; such models are now well-complemented \nby computational models. Doing such simulations ef\ufb01ciently can be challenging, and \nuse of appropriate algorithms certainly can make the difference between being able \nto complete the simulation in a reasonable amount of time and being stuck with the \nchoice of accepting inaccurate results or waiting for the simulation to do the computa-\ntion necessary to get accurate results. We will consider in Chapter 6 a detailed example \nthat illustrates this point.\nNumerical computations. Scienti\ufb01c computing is often concerned with accuracy\n(how ", "start": 361, "end": 361}, "527": {"text": "simulation to do the computa-\ntion necessary to get accurate results. We will consider in Chapter 6 a detailed example \nthat illustrates this point.\nNumerical computations. Scienti\ufb01c computing is often concerned with accuracy\n(how close are we to the true answer?). Accuracy is extremely important when we are \nperforming millions of computations with estimated values such as the \ufb02oating-point \nrepresentation of real numbers that we commonly use on computers. Some numeri -\ncal algorithms use priority queues and sorting to control accuracy in calculations. For \n3492.5 \u25a0 Applications\n  \n \nexample, one way to do numerical integration (quadrature), where the goal is to esti -\nmate the area under a curve, is to maintain a priority queue with accuracy estimates for \na set of subintervals that comprise the whole interval. The process is to remove the least \naccurate subinterval, split it in half (thus achieving better accuracy for the two halves), \nand put the two halves back onto the priority queue, continuing until a desired toler -\nance is reached.\nCombinatorial search. A classic paradigm in arti\ufb01cial intelligence and in coping with \nintractable problems is to de\ufb01ne a set of con\ufb01gurations with well-de\ufb01ned moves from \none con\ufb01guration to the next and a priority associated with each move. Also de\ufb01ned \nis a start con\ufb01guration and a goal con\ufb01guration (which corresponds to having solved \nthe problem). The well-known  A* algorithm is a problem-solving process where we put \nthe start con\ufb01guration on the priority queue, then do the following until reaching the \ngoal: remove the highest-priority con\ufb01guration and add to the queue all con\ufb01gurations \nthat can be reached from that with one move (excluding the one just removed). As with \nevent-driven simulation, ", "start": 361, "end": 362}, "528": {"text": "\ngoal: remove the highest-priority con\ufb01guration and add to the queue all con\ufb01gurations \nthat can be reached from that with one move (excluding the one just removed). As with \nevent-driven simulation, this process is tailor-made for priority queues. It reduces solv-\ning the problem to de\ufb01ning an effective priority function. See Exercise 2.5.32 for an \nexample.\nBeyond such direct applications (and we have only indicated a small fraction of \nthose), sorting and priority queues are an essential abstraction in algorithm design, \nso they will surface frequently throughout this book. We next list some examples of \napplications from later in the book. All of these applications depend upon the ef\ufb01cient \nimplementations of sorting algorithms and the priority-queue data type that we have \nconsidered in this chapter.\n P r i m \u2019 s  a l g o r i t h m  a n d   D i j k s t r a \u2019 s  a l g o r i t h m  are classical algorithms from Chapter 4. \nThat chapter is about algorithms that process graphs, a fundamental model for items \nand edges that connect pairs of items. The basis for these and several other algorithms \nis graph search, where we proceed from item to item along edges. Priority queues play a \nfundamental role in organizing graph searches, enabling ef\ufb01cient algorithms.\n K r u s k a l \u2019 s  a l g o r i t h m  is another classic algorithm for graphs whose edges have weights \nthat depends upon processing the edges in order of their weight. Its running time is \ndominated by the cost of the sort.\n H u f f m a n  c o m p r e s s i o n  is a classic data compression algorithm that depends upon pro-\ncessing a set of items with integer weights by combining the two smallest to produce \na new ", "start": 362, "end": 362}, "529": {"text": "m a n  c o m p r e s s i o n  is a classic data compression algorithm that depends upon pro-\ncessing a set of items with integer weights by combining the two smallest to produce \na new one whose weight is the sum of its two constituents. Implementing this opera-\n350 CHAPTER 2 \u25a0 Sorting\n  \n  \ntion is immediate, using a priority queue. Several other data-compression schemes are \nbased upon sorting.\nString-processing algorithms, which are of critical importance in modern applica-\ntions in cryptology and in genomics, are often based on sorting (generally using one \nof the specialized string sorts discussed in Chapter 5). For example, we will discuss in \nChapter 6 algorithms for \ufb01nding the  longest repeated substring in a given string that is \nbased on \ufb01rst sorting suf\ufb01xes of the strings.\n3512.5 \u25a0 Applications\n  Q & A\n Q .  Is there a priority-queue data type in the Java library?\nA. Ye s , s e e   java.util.PriorityQueue.\n352 CHAPTER 2 \u25a0 Sorting\n EXERCISES\n2.5.1 Consider the following implementation of the  compareTo() method for String. \nHow does the third line help with ef\ufb01ciency?\npublic int compareTo(String that) \n{\n   if (this == that) return 0;  // this line\n   int n = Math.min(this.length(), that.length());\n   for (int i = 0; i < n; i++)\n   {\n      if      (this.charAt(i) < that.charAt(i)) return -1;\n      else if (this.charAt(i) > that.charAt(i)) return +1;\n   }\n   return this.length() - that.length(); \n}\n \n \n2.5.2 Write a program that reads a list of words from standard input and prints all two-\nword compound words in the list. For example, ", "start": 362, "end": 365}, "530": {"text": "}\n   return this.length() - that.length(); \n}\n \n \n2.5.2 Write a program that reads a list of words from standard input and prints all two-\nword compound words in the list. For example, if after, thought, and afterthought\nare in the list, then afterthought is a compound word.\n2.5.3  Criticize the following implementation of a class intended to represent account \nbalances. Why is compareTo() a \ufb02awed implementation of the Comparable interface?\npublic class Balance implements Comparable<Balance> \n{\n   ...\n   private double amount;\n   public int compareTo(Balance that)\n   {\n      if (this.amount < that.amount - 0.005) return -1;\n      if (this.amount > that.amount + 0.005) return +1;\n      return 0;\n   }\n   ... \n}\nDescribe a way to \ufb01x this problem.\n2.5.4 Implement a method String[] dedup(String[] a) that returns the objects in \na[] in sorted order, with duplicates removed.\n2.5.5 Explain why selection sort is not stable.\n3532.5 \u25a0 Applications\n  \n2.5.6 Implement a recursive version of select().\n2.5.7 About how many compares are required, on the average, to \ufb01nd the smallest of \nN items using select()?\n2.5.8 Write a program Frequency that reads strings from standard input and prints \nthe number of times each string occurs, in descending order of frequency.\n2.5.9 Develop a data type that allows you to write a client that can sort a \ufb01le such as the \none shown at right.\n2.5.10 Create a data type Version that represents a \nsoftware version number, such as 115.1.1, 115.10.1, \n115.10.2. Implement the Comparable interface so \nthat 115.1.1 ", "start": 365, "end": 366}, "531": {"text": "a data type Version that represents a \nsoftware version number, such as 115.1.1, 115.10.1, \n115.10.2. Implement the Comparable interface so \nthat 115.1.1 is less than 115.10.1, and so forth.\n2.5.11 One way to describe the result of a sorting al-\ngorithm is to specify a  permutation p[] of the num -\nbers 0 to a.length-1, such that p[i] speci\ufb01es where \nthe key originally in a[i] ends up. Give the permuta-\ntions that describe the results of insertion sort, selec-\ntion sort, shellsort, mergesort, quicksort, and heapsort \nfor an array of seven equal keys.\nEXERCISES  (continued)\ninput (DJI volumes for each day)\n  1-Oct-28     3500000\n  2-Oct-28     3850000\n  3-Oct-28     4060000\n  4-Oct-28     4330000\n  5-Oct-28     4360000\n ...\n 30-Dec-99   554680000\n 31-Dec-99   374049984\n  3-Jan-00   931800000\n  4-Jan-00  1009000000\n  5-Jan-00  1085500032\n  ...\noutput\n 19-Aug-40 130000\n 26-Aug-40 160000\n 24-Jul-40 200000\n 10-Aug-42 210000\n 23-Jun-42 210000\n ...\n 23-Jul-02 2441019904\n 17-Jul-02 2566500096\n 15-Jul-02 2574799872\n 19-Jul-02 2654099968\n 24-Jul-02 ", "start": 366, "end": 366}, "532": {"text": "23-Jul-02 2441019904\n 17-Jul-02 2566500096\n 15-Jul-02 2574799872\n 19-Jul-02 2654099968\n 24-Jul-02 2775559936\n354 CHAPTER 2 \u25a0 Sorting\n CREATIVE PROBLEMS\n \n2.5.12    Scheduling. Write a program SPT.java that reads job names and processing \ntimes from standard input and prints a schedule that minimizes average completion \ntime using the  shortest processing time \ufb01rst rule, as described on page 349.\n2.5.13  Load balancing. Write a program LPT.java that takes an integer M as a com-\nmand-line argument, reads job names and processing times from standard input and \nprints a schedule assigning the jobs to M processors that approximately minimizes the \ntime when the last job completes using the longest processing time \ufb01rst rule, as de-\nscribed on page 349.\n2.5.14  Sort by reverse domain. Write a data type Domain that represents domain names, \nincluding an appropriate compareTo() method where the natural order is in order of \nthe reverse domain name. For example, the reverse domain of cs.princeton.edu is \nedu.princeton.cs. This is useful for web log analysis. Hint: Use s.split(\"\\\\.\") to \nsplit the string s into tokens, delimited by dots. Write a client that reads domain names \nfrom standard input and prints the reverse domains in sorted order.\n2.5.15  Spam campaign. To  i n i t i a te  a n  i l l e g a l  s p a m  c a m p a i g n , yo u  h ave  a  l i s t  o f  e m a i l  \naddresses from various domains (the part of the email address that follows the @ \nsymbol). T o better forge the return ", "start": 366, "end": 367}, "533": {"text": "yo u  h ave  a  l i s t  o f  e m a i l  \naddresses from various domains (the part of the email address that follows the @ \nsymbol). T o better forge the return addresses, you want to send the email from an-\nother user at the same domain. For example, you might want to forge an email from \nwayne@princeton.edu to rs@princeton.edu. How would you process the email list \nto make this an ef\ufb01cient task?\n2.5.16  Unbiased election. In order to thwart bias against candidates whose names ap-\npear toward the end of the alphabet, California sorted the candidates appearing on its \n2003 gubernatorial ballot by using the following order of characters: \nR W Q O J M V A H B S G Z X N T C I E K U P D Y F L\nCreate a data type where this is the natural order and write a client California with a \nsingle static method main() that sorts strings according to this ordering. Assume that \neach string is composed solely of uppercase letters.\n2.5.17  Check  stability. Extend your check() method from Exercise 2.1.16 to call \nsort() for a given array and return true if sort() sorts the array in order in a stable \nmanner, false otherwise. Do not assume that sort() is restricted to move data only \nwith exch().\n3552.5 \u25a0 Applications\n   \n \n2.5.18    Force stability. Write a wrapper method that makes any sort stable by creating \na new key type that allows you to append each key\u2019s index to the key, call sort(), then \nrestore the original key after the sort.\n2.5.19     Kendall tau distance. Write a program KendallTau.java that computes the \nKendall tau distance between two  permutations in linearithmic time.\n2.5.20  Idle time. ", "start": 367, "end": 368}, "534": {"text": "the sort.\n2.5.19     Kendall tau distance. Write a program KendallTau.java that computes the \nKendall tau distance between two  permutations in linearithmic time.\n2.5.20  Idle time. Suppose that a parallel machine processes N jobs. Write a program \nthat, given the list of job start and \ufb01nish times, \ufb01nds the largest interval where the ma-\nchine is idle and the largest interval where the machine is not idle.\n2.5.21   Multidimensional sort. Write a Vector data type for use in having the sort-\ning methods sort multidimensional vectors of d integers, putting the vectors in order \nby \ufb01rst component, those with equal \ufb01rst component in order by second component, \nthose with equal \ufb01rst and second components in order by third component, and so \nforth.\n2.5.22  Stock market trading. Investors place buy and sell orders for a particular stock \non an electronic exchange, specifying a maximum buy or minimum sell price that they \nare willing to pay, and how many shares they wish to trade at that price. Develop a \nprogram that uses priority queues to match up buyers and sellers and test it through \nsimulation. Maintain two priority queues, one for buyers and one for sellers, executing \ntrades whenever a new order can be matched with an existing order or orders.\n2.5.23  Sampling for selection. Investigate the idea of using sampling to improve selec-\ntion. Hint: Using the median may not always be helpful.\n2.5.24    Stable priority queue. Develop a stable priority-queue implementation (which \nreturns duplicate keys in the same order in which they were inserted).\n2.5.25  Points in the plane. Write three static comparators for the Point2D data type \nof page 77, one that compares points by their x coordinate, one that compares them by \ntheir y coordinate, ", "start": 368, "end": 368}, "535": {"text": "inserted).\n2.5.25  Points in the plane. Write three static comparators for the Point2D data type \nof page 77, one that compares points by their x coordinate, one that compares them by \ntheir y coordinate, and one that compares them by their distance from the origin. Write \ntwo non-static comparators for the Point2D data type, one that compares them by \ntheir distance to a speci\ufb01ed point and one that compares them by their   polar angle with \nrespect to a speci\ufb01ed point.\n2.5.26  Simple polygon. Given N points in the plane, draw a simple polygon with N \nCREATIVE PROBLEMS  (continued)\n356 CHAPTER 2 \u25a0 Sorting\n  \npoints as vertices. Hint : Find the point p with the smallest y coordinate, breaking ties \nwith the smallest x coordinate. Connect the points in increasing order of the polar angle \nthey make with p.\n2.5.27   Sorting parallel arrays. When sorting parallel arrays, it is useful to have a version \nof a sorting routine that returns a permutation, say index[], of the indices in sorted \norder. Add a method indirectSort() to Insertion that takes an array of Comparable\nobjects a[] as argument, but instead of rearranging the entries of a[] returns an integer \narray index[] so that a[index[0]] through a[index[N-1]] are the items in ascending \norder.\n2.5.28  Sort \ufb01les by name. Write a program FileSorter that takes the name of a \ndirectory as a command-line argument and prints out all of the \ufb01les in the current \ndirectory, sorted by \ufb01le name. Hint : Use the File data type.\n2.5.29  Sort \ufb01les by size and date of last modi\ufb01cation. Write comparators for the type \nFile to order by increasing/decreasing order ", "start": 368, "end": 369}, "536": {"text": "name. Hint : Use the File data type.\n2.5.29  Sort \ufb01les by size and date of last modi\ufb01cation. Write comparators for the type \nFile to order by increasing/decreasing order of \ufb01le size, ascending/descending order \nof \ufb01le name, and ascending/descending order of last modi\ufb01cation date. Use these \ncomparators in a program LS that takes a command-line argument and lists the \ufb01les \nin the current directory according to a speci\ufb01ed order, e.g., \"-t\" to sort by timestamp. \nSupport multiple \ufb02ags to break ties. Be sure to use a stable sort.\n2.5.30   Boerner\u2019s theorem. True or false: If  you sor t each column of  a matr ix, then sor t \neach row, the columns are still sorted. Justify your answer.\n3572.5 \u25a0 Applications\n EXPERIMENTS\n2.5.31    Duplicates. Write a client that takes integers M, N, and T as command-line argu-\nments, then uses the code given in the text to perform T trials of the following experi-\nment: Generate N random int values between 0 and M \u2013 1 and count the number of \nduplicates. Run your program for T = 10 and N = 10 3, 10 4, 10 5, and 10 6, with  M = N /H11408 2, \nand N, and 2N. Probability theory says that the number of duplicates should be about\n(1 \u2013 e \u2013/H9251) where /H9251 /H11005 N /H11408 M\u2014print a table to help you con\ufb01rm that your experiments \nvalidate that formula. \n2.5.32      8 puzzle. The 8 puzzle ", "start": 369, "end": 370}, "537": {"text": "/H9251 /H11005 N /H11408 M\u2014print a table to help you con\ufb01rm that your experiments \nvalidate that formula. \n2.5.32      8 puzzle. The 8 puzzle is a game popularized by S. Loyd in the 1870s. It is \nplayed on a 3-by-3 grid with 8 tiles labeled 1 through 8 and a blank square. Y our goal \nis to rearrange the tiles so that they are in order. Y ou are permitted to slide one of the \navailable tiles horizontally or vertically (but not diagonally) into the blank square. Write \na program that solves the puzzle using the A* algorithm. Start by using as priority the \nsum of the number of moves made to get to this board position plus the number of tiles \nin the wrong position. (Note that the number of moves you must make from a given \nboard position is at least as big as the number of tiles in the wrong place.) Investigate \nsubstituting other functions for the number of tiles in the wrong position, such as the \nsum of the Manhattan distance from each tile to its correct position, or the sums of the \nsquares of these distances.\n2.5.33  Random transactions. Develop a generator that takes an argument N, generates \nN random Transaction objects (see Exercises 2.1.21 and 2.1.22), using assumptions \nabout the transactions that you can defend. Then compare the performance of shellsort, \nmergesort, quicksort, and heapsort for sorting N transactions, for N=103, 104, 105, and \n106.  \n358 CHAPTER 2 \u25a0 Sorting\n This page intentionally left blank \n 3.1 Symbol Tables   .  .  .  .  .  .  .  .  .  .  .  .  .  .  362\n3.2 ", "start": 370, "end": 372}, "538": {"text": "\u25a0 Sorting\n This page intentionally left blank \n 3.1 Symbol Tables   .  .  .  .  .  .  .  .  .  .  .  .  .  .  362\n3.2 Binary Search Trees   .  .  .  .  .  .  .  .  .  .  .  396\n3.3 Balanced Search Trees   .  .  .  .  .  .  .  .  .  424\n3.4 Hash Tables  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  458\n3.5 Applications.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  486\nTHREE\n   S e a r c h i n g M\n \n \nodern computing and the internet have made accessible a vast amount of \ninformation. The ability to ef\ufb01ciently search through this information is \nfundamental to processing it. This chapter describes classical searching algo-\nrithms that have proven to be effective in numerous diverse applications for decades. \nWithout algorithms like these, the development of the computational infrastructure \nthat we enjoy in the modern world would not have been possible.\nWe use the term symbol table to describe an abstract mechanism where we save in-\nformation (a  value) that we can later search for and retrieve by specifying a  key. The \nnature of the keys and the values depends upon the application. There can be a huge \nnumber of keys and a huge amount of information, so implementing an ef\ufb01cient sym-\nbol table is a signi\ufb01cant computational challenge. \nSymbol tables are sometimes called  dictionaries, by analogy with the time-honored \nsystem of providing de\ufb01nitions ", "start": 372, "end": 373}, "539": {"text": "information, so implementing an ef\ufb01cient sym-\nbol table is a signi\ufb01cant computational challenge. \nSymbol tables are sometimes called  dictionaries, by analogy with the time-honored \nsystem of providing de\ufb01nitions for words by listing them alphabetically in a reference \nbook. In an English-language dictionary, a key is a word and its values is the entry \nassociated with the word that contains the de\ufb01nition, pronunciation, and etymology. \nSymbol tables are also sometimes called  indices, by analogy with another time-honored \nsystem of providing access to terms by listing them alphabetically at the end of a book \nsuch as a textbook. In a book index, a key is a term of interest and its value is the list of \npage numbers that tell readers where to \ufb01nd that term in the book. \nAfter describing the basic APIs and two fundamental implementations, we consider \nthree classic data structures that can support ef\ufb01cient symbol-table implementations: \nbinary search trees, red-black trees, and hash tables. We conclude with several exten-\nsions and applications, many of which would not be feasible without the ef\ufb01cient algo-\nrithms that you will learn about in this chapter.\n361\n 3.1 SYMBOL TABLES\nThe primary purpose of a symbol table is to associate a value with a key.  The client can \ninsert key-value pairs into the symbol table with the expectation of later being able to \nsearch for the value associated with a given key, from among all of the key-value pairs \nthat have been put into the table. This chapter describes several ways to structure this \ndata so as to make ef\ufb01cient not just the  insert and  search operations, but several other \nconvenient operations as well. To implement a symbol table, we need to de\ufb01ne an un -\nderlying data structure and then specify algorithms for insert, search, ", "start": 373, "end": 374}, "540": {"text": "the  insert and  search operations, but several other \nconvenient operations as well. To implement a symbol table, we need to de\ufb01ne an un -\nderlying data structure and then specify algorithms for insert, search, and other opera-\ntions that create and manipulate the data structure. \nSearch is so important to so many computer applications that symbol tables are \navailable as high-level abstractions in many programming environments, including \nJava\u2014we shall discuss Java\u2019s symbol-table implementations in Section 3.5. The table \nbelow gives some examples of keys and values that you might use in typical applica-\ntions. We consider some illustrative reference clients soon, and Section 3.5 is devoted \nto showing you how to use symbol tables effectively in your own clients. We also use \nsymbol tables in developing other algorithms throughout the book.\nDefinition. A  symbol table is a data structure for key-value pairs that supports two \noperations: insert (put) a new pair into the table and search for (get) the value as-\nsociated with a given key.\napplication purpose of search key value\ndictionary find definition word definition\nbook index  find relevant pages term list of page numbers \nfile share  find song to download name of song computer ID     \naccount management process transactions account number transaction details     \nweb search  find relevant web pages keyword list of page names     \ncompiler  find type and value variable name type and value\nTypical symbol-table applications\n362\n  \nAPI The symbol table is a prototypical abstract data type (see Chapter 1): it repre-\nsents a well-de\ufb01ned set of values and operations on those values, enabling us to develop \nclients and implementations separately. As usual, we precisely de\ufb01ne the operations \nby specifying an applications programming interface (API) that provides the contract \nbetween client and implementation: \n  public class    ST<Key, Value> \nST() create a symbol table\nvoid ", "start": 374, "end": 375}, "541": {"text": "we precisely de\ufb01ne the operations \nby specifying an applications programming interface (API) that provides the contract \nbetween client and implementation: \n  public class    ST<Key, Value> \nST() create a symbol table\nvoid put(Key key, Value val) put key-value pair into the table\n(remove key from table if value is null)\nValue get(Key key) value paired with key\n(null if  key is absent)\nvoid delete(Key key) remove key (and its value) from table\nboolean contains(Key key) is there a value paired with key?\nboolean isEmpty() is the table empty?\nint size() number of key-value pairs in the table\nIterable<Key> keys() all the keys in the table\n A P I  f o r  a  g e n e r i c  b a s i c  s y m b o l  t a b l e\nBefore examining client code, we consider several design choices for our implementa-\ntions to make our code consistent, compact, and useful. \n G e n e r i c s .  As we did with sorting, we will consider the methods without specifying the \ntypes of the items being processed, using generics. For symbol tables, we emphasize the \nseparate roles played by keys and values in search by specifying the key and value types \nexplicitly instead of viewing keys as implicit in items as we did for priority queues in \nSection 2.4. After we have considered some of the characteristics of this basic API (for \nexample, note that there is no mention of order among the keys), we will consider an \nextension for the typical case when keys are Comparable, which enables numerous ad-\nditional methods. \n  D u p l i c a t e  k e y s .  We adopt the follow ing conventions in all of  our implementations:\n\u25a0 Only one value is associated with each key (no duplicate keys in a table).\n\u25a0 ", "start": 375, "end": 375}, "542": {"text": "D u p l i c a t e  k e y s .  We adopt the follow ing conventions in all of  our implementations:\n\u25a0 Only one value is associated with each key (no duplicate keys in a table).\n\u25a0 When a client puts a key-value pair into a table already containing that key (and \nan associated value), the new value replaces the old one. \nassociative array abstraction , where you can think of a These conventions de\ufb01ne the    \nsymbol table as being just like an array, where keys are indices and values are array \n3633.1 \u25a0 Symbol Tables\n  \n \nentries. In a conventional array, keys are integer indices that we use to quickly access ar-\nray values; in an associative array (symbol table), keys are of arbitrary type, but we can \nstill use them to quickly access values. Some programming languages (not Java) provide \nspecial support that allows programmers to use code such as st[key] for st.get(key)\nand st[key] = val for st.put(key, val) where key and val are objects of arbitrary \ntype.\nNull keys. Keys must not be null. As with many mechanisms in Java, use of a null key \nresults in an exception at runtime (see the third Q&A on page 387).\nNull  values. We also adopt the convention that no key can be associated w ith the value \nnull. This convention is directly tied to our speci\ufb01cation in the API that get() should \nreturn null for keys not in the table, effectively associating the value null with every \nkey not in the table. This convention has two (intended) consequences: First, we can \ntest whether or not the symbol table de\ufb01nes a value associated with a given key by test-\ning whether get() returns null. Second, we can use the operation of calling put()\nwith null as its second (value) argument to implement deletion, as described in the ", "start": 375, "end": 376}, "543": {"text": "de\ufb01nes a value associated with a given key by test-\ning whether get() returns null. Second, we can use the operation of calling put()\nwith null as its second (value) argument to implement deletion, as described in the \nnext paragraph.\n D e l e t i o n .  Deletion in symbol tables generally involves one of two strategies: lazy dele-\ntion, where we associate keys in the table with null, then perhaps remove all such keys \nat some later time; and eager deletion, where we remove the key from the table imme-\ndiately. As just discussed, the code put(key, null) is an easy (lazy) implementation \nof delete(key). When we give an (eager) implementation of delete(), it is intended \nto replace this default. In our symbol-table implementations that do not use the default \ndelete(), the put() implementations on the booksite begin with the defensive code\nif (val == null) {  delete(key); return; } \nto ensure that no key in the table is associated with null. For economy, we do not in-\nclude this code in the book (and we do not call put() with a null value in client code).\nShorthand methods. For clarity in client code, we include the methods contains()\nand isEmpty() in the API, with the default one-line implementations shown here. \nFor economy, we do not \nrepeat this code, but we \nassume it to be present \nin all implementations of \nthe symbol-table API and \nuse these methods freely \nin client code.\nmethod default implementation\n   void delete(Key key) put(key, null); \nboolean contains(key) return get(key) != null; \nboolean isEmpty() return size() == 0;\nDefault implementations\n364 CHAPTER 3 \u25a0 Searching\n  \n \nIteration. To  e n a b l e  c l i e n t s  to  p ro ce s s  a ", "start": 376, "end": 377}, "544": {"text": "isEmpty() return size() == 0;\nDefault implementations\n364 CHAPTER 3 \u25a0 Searching\n  \n \nIteration. To  e n a b l e  c l i e n t s  to  p ro ce s s  a l l  t h e  ke y s  a n d  v a l u e s  i n  t h e  t a b l e , w e  m i g h t  \nadd the phrase implements Iterable<Key> to the \ufb01rst line of the API to specify that \nevery implementation must implement an iterator() method that returns an iterator \nhaving appropriate implementations of hasNext() and next(), as described for stacks \nand queues in Section 1.3. For symbol tables, we adopt a simpler alternative approach, \nwhere we specify a keys() method that returns an Iterable<Key> object for clients \nto use to iterate through the keys. Our reason for doing so is to maintain consistency \nwith methods that we will de\ufb01ne for ordered symbol tables that allow clients to iterate \nthrough a speci\ufb01ed subset of keys in the table.\n   K e y  e q u a l i t y .  Determining whether or not a given key is in a symbol table is based on \nthe concept of object equality, which we discussed at length in  Section 1.2  (see page \n102). Java\u2019s convention that all objects inherit an equals() method and its implementa-\ntion of equals() both for standard types such as Integer, Double, and String and \nfor more complicated types such as File and URL is a head start\u2014when using these \ntypes of data, you can just use the built-in implementation.  For example, if x and y are \nString values, then x.equals(y) is true if and only if x and y have the same length \nand are identical in each character position. For such client-de\ufb01ned keys, ", "start": 377, "end": 377}, "545": {"text": "example, if x and y are \nString values, then x.equals(y) is true if and only if x and y have the same length \nand are identical in each character position. For such client-de\ufb01ned keys, you need to \noverride equals(), as discussed in Section 1.2. Yo u  c a n  u s e  o u r  i m p l e m e n t a t i o n  o f  \nequals() for Date (page 103) as a template to develop equals() for a type of your own. \nAs discussed for priority queues on page 320, a best practice is to make Key types  immu-\ntable, because consistency cannot otherwise be guaranteed. \n3653.1 \u25a0 Symbol Tables\n     O r d e r e d  s y m b o l  t a b l e s   \n \nIn typical applications, keys are Comparable objects, so \nthe option exists of using the code a.compareTo(b) to compare two keys a and b.\nSeveral symbol-table implementations take advantage of order among the keys that is \nimplied by Comparable to provide ef\ufb01cient implementations of the put() and get()\noperations. More important, in such implementations, we can think of the symbol ta-\nble as keeping the keys in order  and consider a signi\ufb01cantly expanded API that de\ufb01nes \nnumerous natural and useful operations involving relative key order. For example, sup-\npose that your keys are times of the day. Y ou might be interested in knowing the earliest \nor the latest time, the set of keys that fall between two given times, and so forth. In most \ncases, such operations are not dif\ufb01cult to implement with the same data structures and \nmethods underlying the put() and get() implementations.  Speci\ufb01cally, for applica-\ntions where keys are Comparable, we implement in this ", "start": 377, "end": 378}, "546": {"text": "dif\ufb01cult to implement with the same data structures and \nmethods underlying the put() and get() implementations.  Speci\ufb01cally, for applica-\ntions where keys are Comparable, we implement in this chapter the following API:\n  public class    ST<Key extends Comparable<Key>, Value> \nST() create an ordered symbol table\nvoid put(Key key, Value val) put key-value pair into the table\n(remove key from table if value is null)\nValue get(Key key) value paired with key\n(null if key is absent)\nvoid delete(Key key) remove key (and its value) from table\nboolean contains(Key key) is there a value paired with key?\nboolean isEmpty() is the table empty?\nint size() number of key-value pairs\nKey min() smallest key\nKey max() largest key\nKey floor(Key key) largest key less than or equal to key \nKey ceiling(Key key) smallest key greater than or equal to key\nint rank(Key key) number of keys less than key \nKey select(int k) key of rank k\nvoid deleteMin() delete smallest key\nvoid deleteMax() delete largest key\nint size(Key lo, Key hi) number of keys in [lo..hi]\nIterable<Key> keys(Key lo, Key hi) keys in [lo..hi], in sorted order\nIterable<Key> keys() all keys in the table, in sorted order\n A P I  f o r  a  g e n e r i c  o r d e r e d  s y m b o l  t a b l e\n366 CHAPTER 3 \u25a0 Searching\n  \nYo u r  s i g n a l  t h a t  o n e  o f  o u r  p ro g r a m s  i s  i m p l e m e n t i n g  t h i s  A P I  i s  t ", "start": 378, "end": 379}, "547": {"text": "e  o f  o u r  p ro g r a m s  i s  i m p l e m e n t i n g  t h i s  A P I  i s  t h e  p re s e n ce  o f  t h e  \nKey extends Comparable<Key> generic type variable in the class declaration, which \nspeci\ufb01es that the code depends upon the keys being Comparable and implements the \nricher set of operations. T ogether, these operations de\ufb01ne for client programs an or-\ndered symbol table.\nMinimum and maximum. Perhaps the most natural queries for a set of ordered keys \nare to ask for the smallest and largest keys. We have already encountered these opera-\ntions, in our discussion of priority queues in Section 2.4. In ordered symbol tables, \nwe also have methods to delete the maximum \nand minimum keys (and their associated val-\nues). With this capability, the symbol table can \noperate like the IndexMinPQ() class that we \ndiscussed in Section 2.4. The primary differ -\nences are that equal keys are allowed in prior -\nity queues but not in symbol tables and that \nordered symbol tables support a much larger \nset of operations. \n    F l o o r  a n d   c e i l i n g .  Given a key, it is often use-\nful to be able to perform the \ufb02oor operation \n(\ufb01nd the largest key that is less than or equal to \nthe given key) and the ceiling operation (\ufb01nd \nthe smallest key that is greater than or equal to \nthe given key). The nomenclature comes from \nfunctions de\ufb01ned on real numbers (the \ufb02oor \nof a real number x is the largest integer that is \nsmaller than or equal to x and the ceiling of \na real number ", "start": 379, "end": 379}, "548": {"text": "nomenclature comes from \nfunctions de\ufb01ned on real numbers (the \ufb02oor \nof a real number x is the largest integer that is \nsmaller than or equal to x and the ceiling of \na real number x is the smallest integer that is \ngreater than or equal to x).\n    R a n k  a n d   s e l e c t i o n .  The basic operations for determining where a new key \ufb01ts in the \norder are the rank operation (\ufb01nd the number of keys less than a given key) and the \nselect operation (\ufb01nd the key with a given rank). T o test your understanding of their \nmeaning, con\ufb01rm for yourself that both i == rank(select(i)) for all i between 0\nand size()-1 and all keys in the table satisfy key == select(rank(key)). We have \nalready encountered the need for these operations, in our discussion of sort applica-\ntions in Section 2.5. For symbol tables, our challenge is to perform these operations \nquickly, intermixed with insertions, deletions, and searches.\n09:00:00  Chicago \n09:00:03  Phoenix \n09:00:13  Houston \n09:00:59  Chicago \n09:01:10  Houston \n09:03:13  Chicago \n09:10:11  Seattle \n09:10:25  Seattle \n09:14:25  Phoenix \n09:19:32  Chicago \n09:19:46  Chicago \n09:21:05  Chicago \n09:22:43  Seattle \n09:22:54  Seattle \n09:25:52  Chicago \n09:35:21  Chicago \n09:36:14  Seattle \n09:37:44  Phoenix \nkeys values\nget(09:00:13)\nceiling(09:30:00)\nkeys(09:15:00, ", "start": 379, "end": 379}, "549": {"text": "\n09:25:52  Chicago \n09:35:21  Chicago \n09:36:14  Seattle \n09:37:44  Phoenix \nkeys values\nget(09:00:13)\nceiling(09:30:00)\nkeys(09:15:00, 09:25:00)\nsize(09:15:00, 09:25:00) is 5\nrank(09:10:25) is 7\nfloor(09:05:00)\nmin()\nselect(7)\nmax()\nExamples of ordered symbol-table operations\n3673.1 \u25a0 Symbol Tables\n  \n   R a n g e  q u e r i e s .  How many keys fall within a given range (between two given keys)? \nWhich keys fall in a given range? The two-argument size() and keys() methods that \nanswer these questions are useful in many applications, particularly in large databases. \nThe capability to handle such queries is one prime reason that ordered symbol tables \nare so widely used in practice.\nExceptional cases. When a method is to return a key and there is no key \ufb01tting the de-\nscription in the table, our convention is to throw an exception (an alternate approach, \nwhich is also reasonable, would be to return null  in such cases). For example, min(), \nmax(), deleteMin(), deleteMax(), floor(), and ceiling() all throw exceptions if \nthe table is empty, as does select(k) if k is less than 0 or not less than size(). \nShorthand methods. As we have already seen with isEmpty() and contains() in our \nbasic API, we keep some redundant methods in the API for clarity in client code. For \neconomy in the text, we assume that the following default implementations are includ-\ned in any implementation of the ordered symbol-table API unless otherwise speci\ufb01ed:\nmethod default implementation\n         void ", "start": 379, "end": 380}, "550": {"text": "clarity in client code. For \neconomy in the text, we assume that the following default implementations are includ-\ned in any implementation of the ordered symbol-table API unless otherwise speci\ufb01ed:\nmethod default implementation\n         void deleteMin() delete(min());\n         void deleteMax() delete(max());\n           int size(Key lo, Key hi)if (hi.compareTo(lo) < 0) \n   return 0; \nelse if (contains(hi))\n   return rank(hi) - rank(lo) + 1; \nelse\n   return rank(hi) - rank(lo);\nIterable<Key> keys() return keys(min(), max());\nDefault implementations of redundant order-based symbol-table methods\n  K e y  e q u a l i t y  ( r e v i s i t e d ) .  The best practice in Java is to make compareTo() consistent \nwith equals() in all Comparable types. That is, for every pair of values a and b in \nany given Comparable type, it should be the case that (a.compareTo(b) == 0) and \na.equals(b) have the same value. T o avoid any potential ambiguities, we avoid the use \nof equals() in ordered symbol-table implementations. Instead, we use compareTo()\nexclusively to compare keys: we take the boolean expression a.compareTo(b) == 0 to \n368 CHAPTER 3 \u25a0 Searching\n mean \u201cAre a and b equal ?\u201d Typically, such a test marks the successful end of a search for \na in the symbol table (by \ufb01nding b). As you saw with sorting algorithms, Java provides \nstandard implementations of compareTo() for many commonly used types of keys, \nand it is not dif\ufb01cult to develop a compareTo() implementation for your own data \ntypes (see Section 2.5). \n  C o s t  m o d e l .  Whether we use equals() (for ", "start": 380, "end": 381}, "551": {"text": "not dif\ufb01cult to develop a compareTo() implementation for your own data \ntypes (see Section 2.5). \n  C o s t  m o d e l .  Whether we use equals() (for symbol tables \nwhere keys are not Comparable) or compareTo() (for or -\ndered symbol tables with Comparable keys), we use the term \ncompare to refer to the operation of comparing a symbol-\ntable entry against a search key. In most symbol-table imple-\nmentations, this operation is in the inner loop. In the few \ncases where that is not the case, we also count array accesses.\nSymbol-table implementations are generally character-\nized by their underlying data structures and their implemen-\ntations of get() and put(). We do not always provide im -\nplementations of all of the other methods in the text because \nmany of them make good exercises to test your understanding of the underlying data \nstructures. T o distinguish implementations, we add a descriptive pre\ufb01x to ST that refers \nto the implementation in the class name of symbol-table implementations. In clients, \nwe use ST to call on a reference implementation unless we wish to refer to a speci\ufb01c \nimplementation. Y ou will gradually develop a better feeling for the rationale behind the \nmethods in the APIs in the context of the numerous clients and symbol-table imple-\nmentations that we present and discuss throughout this chapter and throughout the \nrest of this book. We also discuss alternatives to the various design choices that we have \ndescribed here in the Q&A and exercises.\nSearching     cost model. \nWhen studying symbol-table \nimplementations, we count\ncompares (equality tests or \nkey comparisons). In (rare) \ncases where compares are not \nin the inner loop, we count \narray accesses. \n3693.1 \u25a0 Symbol Tables\n Sample clients While we defer detailed consideration of applications to Section \n3.5, ", "start": 381, "end": 382}, "552": {"text": "(rare) \ncases where compares are not \nin the inner loop, we count \narray accesses. \n3693.1 \u25a0 Symbol Tables\n Sample clients While we defer detailed consideration of applications to Section \n3.5, it is worthwhile to consider some client code before considering implementations. \nAccordingly, we now consider two clients: a test client that we use to trace algorithm \nbehavior on small inputs and a performance client that we use to motivate the develop-\nment of ef\ufb01cient implementations.\nTe s t  c l i e n t . For tracing our algorithms on small inputs we assume that all of our im -\nplementations use the test client below, which takes a sequence of strings from standard \ninput, builds a symbol table that associates the value i with the ith string in the input, \nand then prints the table. In the traces in the text, we assume that the input is a sequence \nof single-character strings. Most often, we \nuse the string \"S E A R C H E X A M P L E\". \nBy our conventions, this client associates \nthe key S with the value 0, the key R with \nthe value 3, and so forth. But E is associated \nwith the value 12 (not 1 or 6) and A with \nthe value 8 (not 2) because our associative-\narray convention implies that each key is \nassociated with the value used in the most \nrecent call to put(). For basic (unordered) \nimplementations, the order of the keys in \nthe output of this test client is not speci\ufb01ed \n(it depends on characteristics of the imple-\nmentation); for an ordered symbol table \nthe test client prints the keys in sorted order. \nThis client is an example of an indexing cli-\nent, a special case of a fundamental symbol-\ntable application that we discuss in Section \n3.5.\nkeys\nvalues\nS ", "start": 382, "end": 382}, "553": {"text": "client prints the keys in sorted order. \nThis client is an example of an indexing cli-\nent, a special case of a fundamental symbol-\ntable application that we discuss in Section \n3.5.\nkeys\nvalues\nS  E  A  R  C  H  E  X  A  M  P  L  E\n0  1  2  3  4  5  6  7  8  9 10 11 12\noutput for\nbasic symbol table\n(one possibility)\nL  11\nP  10\nM  9\nX  7\nH  5\nC  4\nR  3\nA  8\nE  12\nS  0\noutput for\nordered\nsymbol table\nA  8\nC  4\nE  12\nH  5\nL  11\nM  9\nP  10\nR  3\nS  0\nX  7\nKeys, values, and output for test client\npublic static void main(String[] args) \n{\n   ST<String, Integer> st;\n   st = new ST<String, Integer>();\n   for (int i = 0; !StdIn.isEmpty(); i++)\n   {\n      String key = StdIn.readString();\n      st.put(key, i);\n   }\n   for (String s : st.keys())\n      StdOut.println(s + \" \" + st.get(s)); \n}\nBasic symbol-table test client\n370 CHAPTER 3 \u25a0 Searching\n  \n \nPerformance client. FrequencyCounter (on the next page) is a symbol-table client \nthat \ufb01nds the number of occurrences of each string (having at least as many characters \nas a given threshold length) in a sequence of strings from standard input, then iterates \nthrough the keys to \ufb01nd the one that ", "start": 382, "end": 383}, "554": {"text": "\ufb01nds the number of occurrences of each string (having at least as many characters \nas a given threshold length) in a sequence of strings from standard input, then iterates \nthrough the keys to \ufb01nd the one that occurs the most frequently. This client is an exam-\nple of a dictionary client, an application that we discuss in more detail in Section 3.5. \nThis client answers a simple question: Which word (no shorter than a given length) oc-\ncurs most frequently in a given text? Throughout this chapter, we measure performance \nof this client with three reference inputs: the \ufb01rst \ufb01ve lines of C. Dickens\u2019s  Ta l e  of  Tw o  \nCities (tinyTale.txt), the full text of the book (tale.txt), and a popular database of \n1 million sentences taken at random from the web that is known as the  Leipzig Corpora \nCollection (leipzig1M.txt). For example, here is the content of tinyTale.txt:\n% more tinyTale.txt \nit was the best of times it was the worst of times \nit was the age of wisdom it was the age of foolishness \nit was the epoch of belief it was the epoch of incredulity \nit was the season of light it was the season of darkness \nit was the spring of hope it was the winter of despair\n S m a l l  t e s t  i n p u t\nThis text has 60 words taken from 20 distinct words, four of which occur ten times (the \nhighest frequency). Given this input, FrequencyCounter will print out any of it, was, \nthe, or of (the one chosen may vary, depending upon characteristics of the symbol-\ntable implementation), followed by the frequency, 10.\nTo  s t u d y  p e r f o r m a n ce  f o r  t h e ", "start": 383, "end": 383}, "555": {"text": "vary, depending upon characteristics of the symbol-\ntable implementation), followed by the frequency, 10.\nTo  s t u d y  p e r f o r m a n ce  f o r  t h e  l a r g e r  i n p u t s , i t  i s  c l e a r  t h a t  t wo  m e a s u re s  a re  o f  i n te r-\nest: Each word in the input is used as a search key once, so the total number of words \nin the text is certainly relevant. Second, each distinct word in the input is put into the \ntinyTale.txt tale.txt leipzig1M.txt\nwords distinct words distinct words distinct\nall words 60 20 135,635 10,679 21,191,455 534,580\nat least 8 letters 3 3 14,350 5,737 4,239,597 299,593\nat least 10 letters 2 2 4,582 2,260 1,610,829 165,555\nCharacteristics of larger test input streams\n3713.1 \u25a0 Symbol Tables\n A symbol-table client\npublic class  FrequencyCounter \n{\n   public static void main(String[] args)\n   {\n      int minlen = Integer.parseInt(args[0]);   // key-length cutoff\n      ST<String, Integer> st = new ST<String, Integer>();\n      while (!StdIn.isEmpty())\n      {  // Build symbol table and count frequencies.\n         String word = StdIn.readString();\n         if (word.length() < minlen) continue;  // Ignore short keys.\n         if (!st.contains(word)) st.put(word, 1);\n         else                    st.put(word, st.get(word) + 1);\n      }\n      // Find a key with the highest frequency count.\n      String ", "start": 383, "end": 384}, "556": {"text": "short keys.\n         if (!st.contains(word)) st.put(word, 1);\n         else                    st.put(word, st.get(word) + 1);\n      }\n      // Find a key with the highest frequency count.\n      String max = \"\";\n      st.put(max, 0);\n      for (String word : st.keys())\n         if (st.get(word) > st.get(max))\n            max = word;\n      StdOut.println(max + \" \" + st.get(max));\n   } \n}\n \nThis ST client counts the frequency of occurrence of the strings in standard input, then prints out \none that occurs with highest frequency. The command-line argument speci\ufb01es a lower bound on the \nlength of keys considered. \n% java FrequencyCounter 1 < tinyTale.txt \nit 10\n% java FrequencyCounter 8 < tale.txt \nbusiness 122\n% java FrequencyCounter 10 < leipzig1M.txt \ngovernment 24763\n372 CHAPTER 3 \u25a0 Searching  \nsymbol table (and the total number of distinct words in the input gives the size of the \ntable after all keys have been inserted), so the total number of distinct words in the \ninput stream is certainly relevant. We need to know both of these quantities in order \nto estimate the running time of FrequencyCounter (for a start, see Exercise 3.1.6). \nWe defer details until we consider some algor ithms, but you should have in mind a \ngeneral idea of the needs of typical applications like this one. For example, running \nFrequencyCounter on leipzig1M.txt for words of length 8 or more involves millions \nof searches in a table with hundreds of thousands of keys and values. A server on the \nweb might need to handle billions of transactions on tables with millions of keys and \nvalues.\nThe basic question that this client and these examples raise is the following: Can \nwe develop a symbol-table implementation that can handle ", "start": 384, "end": 385}, "557": {"text": "on the \nweb might need to handle billions of transactions on tables with millions of keys and \nvalues.\nThe basic question that this client and these examples raise is the following: Can \nwe develop a symbol-table implementation that can handle a huge number of get() \noperations on a large table, which itself was built with a large number of intermixed \nget() and put() operations? If you are only doing a few searches, any implementation \nwill do, but you cannot make use of a client like FrequencyCounter for large prob -\nlems without a good symbol-table implementation. FrequencyCounter is surrogate \nfor a very common situation. Speci\ufb01cally, it has the following characteristics, which are \nshared by many other symbol-table clients:\n\u25a0 Search and insert operations are intermixed.\n\u25a0 The number of distinct keys is not small.\n\u25a0 Substantially more searches than inserts are likely. \n\u25a0 Search and insert patterns, though unpredictable, are not random.\nOur goal is to develop symbol-table implementations that make it feasible to use such \nclients to solve typical practical problems.\nNext, we consider two elementary implementations and their performance for \nFrequencyCounter. Then, in the next several sections, you will learn classic imple -\nmentations that can achieve excellent performance for such clients, even for huge input \nstreams and tables.\n3733.1 \u25a0 Symbol Tables\n Sequential search in an  unordered linked list One straightforward option \nfor the underlying data structure for a symbol table is a linked list of nodes that contain \nkeys and values, as in the code on the facing page. To implement get(), we scan through \nthe list, using equals() to compare the search key with the key in each node in the list. \nIf we \ufb01nd the match, we return the associated value; if not, we return null. To imple-\nment put(), we also scan through the list, using equals() to compare the client key \nwith the key in each node ", "start": 385, "end": 386}, "558": {"text": "\ufb01nd the match, we return the associated value; if not, we return null. To imple-\nment put(), we also scan through the list, using equals() to compare the client key \nwith the key in each node in the list. If we \ufb01nd the match, we update the value associ -\nated with that key to be the value given in the second argument; if not, we create a new \nnode with the given key and value and insert it at the beginning of the list. This method \nis known as sequential search: we search by considering the keys in the table one after \nanother, using equals() to test for a match with our search key.\nAlgorithm 3.1 (SequentialSearchST) is an implementation of our basic symbol-\ntable API that uses standard list-processing mechanisms, which we have used for el-\nementary data structures in Chapter 1. We have left the implementations of size(),  \nkeys(), and eager delete() for exercises. Y ou are encouraged to work these exercises \nto cement your understanding of the linked-list data structure and the basic symbol-\ntable API. \nCan this linked-list-based implementation handle applications that need large ta-\nbles, such as our sample clients? As we have noted, analyzing symbol-table algorithms \nis more complicated than analyzing sorting algorithms because of the dif\ufb01culty of \nTrace of linked-list ST implementation for standard indexing client \nred nodes\nare new\nblack nodes\nare accessed\nin search\nfirst\nS0\nS0E1\nS0E1A2\nS0E1A2R3\nS0E1A2R3C4\nS0E1A2R3C4H5\nS0E 6A2R3C4H5\nS0E6A2R3C4H5\nS0E6A 8R3C4H5\nX7\nX7\nM9\nP1 ", "start": 386, "end": 386}, "559": {"text": "search\nfirst\nS0\nS0E1\nS0E1A2\nS0E1A2R3\nS0E1A2R3C4\nS0E1A2R3C4H5\nS0E 6A2R3C4H5\nS0E6A2R3C4H5\nS0E6A 8R3C4H5\nX7\nX7\nM9\nP1 0\nL1 1\nL1 1\ncircled entries are\nchanged values\ngray nodes\nare untouched\nS0E6A8R3C4H5X7\nM9 S0E6A8R3C4H5X7\nP1 0 M9 S0E6A8R3C4H5X7\nP1 0 M9 S0E 12A8R3C4H5X7\nkey value\nS  0\nE  1\nA  2\nR  3\nC  4\nH  5\nE  6\nX  7\nA  8\nM  9\nP 10\nL 11\nE 12\n374 CHAPTER 3 \u25a0 Searching\n ALGORITHM 3.1   Sequential search (in an unordered linked list)\npublic class  SequentialSearchST<Key, Value> \n{\n   private Node first;        // first node in the linked list\n   private class Node\n   {  // linked-list node\n      Key key;\n      Value val;\n      Node next;\n      public Node(Key key, Value val, Node next)\n      {\n         this.key  = key;\n         this.val  = val;\n         this.next = next;\n      }\n   }\n   public Value get(Key key)\n   {  // Search for key, return ", "start": 386, "end": 387}, "560": {"text": "val, Node next)\n      {\n         this.key  = key;\n         this.val  = val;\n         this.next = next;\n      }\n   }\n   public Value get(Key key)\n   {  // Search for key, return associated value.\n      for (Node x = first; x != null; x = x.next)\n         if (key.equals(x.key))\n            return x.val;    // search hit\n      return null;           // search miss\n   }\n   public void put(Key key, Value val)\n   {  // Search for key. Update value if found; grow table if new.\n      for (Node x = first; x != null; x = x.next)\n         if (key.equals(x.key))\n         {  x.val = val; return;  }      // Search hit: update val.\n      first = new Node(key, val, first); // Search miss: add new node.\n   } \n}\nThis ST implementation uses a private Node inner class to keep the keys and values in an unordered \nlinked list. The get() implementation searches the list sequentially to \ufb01nd whether the key is in the \ntable (and returns the associated value if so). The put() implementation also searches the list sequen-\ntially to check whether the key is in the table. If so, it updates the associated value; if not, it creates a \nnew node with the given key and value and inserts it at the beginning of the list. Implementations of \nsize(), keys(), and eager delete() are left for exercises.\n3753.1 \u25a0 Symbol Tables characterizing the sequence of operations that might be invoked by a given client. As \nnoted for FrequencyCounter, the most common situation is that, while search and \ninsert patterns are unpredictable, they certainly are not random. For this reason, we pay \ncareful attention to worst-case performance. For economy, we use the term  search hit to \nrefer to a ", "start": 387, "end": 388}, "561": {"text": "that, while search and \ninsert patterns are unpredictable, they certainly are not random. For this reason, we pay \ncareful attention to worst-case performance. For economy, we use the term  search hit to \nrefer to a successful search and  search miss to refer to an unsuccessful search.\nProposition A. Search misses and  insertions in an (unordered) linked-list sym-\nbol table having N key-value pairs both require N compares, and search hits N\ncompares in the worst case. In particular, inserting N distinct keys into an initially \nempty linked-list symbol table uses ~N 2/2 compares.\nProof: When searching for a key that is not in the list, we test every key in the table \nagainst the search key. Because of our policy of disallowing duplicate keys, we need \nto do such a search before each insertion.\nCorollary. Inserting N distinct keys into an initially empty linked-list symbol table \nuses ~N 2/2 compares.\nIt is true that searches for keys that are in the table need not take linear time. One useful \nmeasure is to compute the total cost of searching for all of the keys in the table, divided \nby N. This quantity is precisely the expected number of compares required for a search \nunder the condition that searches for each key in the table are equally likely. We refer to \nsuch a search as a  random search hit. Though client search patterns are not likely to be \nrandom, they often are well-described by this model. It is easy to show that the average \nnumber of compares for a random search hit is ~ N/2: the get() method in Algo-\nrithm 3.1 uses 1 compare to \ufb01nd the \ufb01rst key, 2 compares to \ufb01nd the second key, and \nso forth, for an average cost of (1 + 2 + ... + N )/ N = ", "start": 388, "end": 388}, "562": {"text": "1 compare to \ufb01nd the \ufb01rst key, 2 compares to \ufb01nd the second key, and \nso forth, for an average cost of (1 + 2 + ... + N )/ N = (N /H11001 1)/2 ~ N/2.\nThis analysis strongly indicates that a linked-list implementation with sequential \nsearch is too slow for it to be used to solve huge problems such as our reference inputs \nwith clients like FrequencyCounter. The total number of compares is proportional to \nthe product of the number of searches and the number of inserts, which is more than \n10 9 for Ta l e  of  Tw o  Ci t i e s and more than 1014 for the Leipzig Corpora. \nAs usual, to validate analytic results, we need to run experiments. As an example, \nwe study the operation of FrequencyCounter with command-line argument 8 for \ntale.txt, which involves 14,350 put() operations (recall that every word in the input \nleads to a put(), to update its frequency, and we ignore the cost of easily avoided calls \n376 CHAPTER 3 \u25a0 Searching\n  \nto contains()). The symbol table grows to 5,737 keys, so about one-third of the opera-\ntions increase the size of the table; the rest are searches. T o visualize the performance, we \nuse VisualAccumulator (see page 95) to plot two points corresponding to each put()\noperation as follows: for the i th  put() operation we plot a gray point with x coordinate \ni and y coordinate the number of key compares it uses and a red point  with x coordi-\nnate i and y coordinate the cumulative average number of key compares used for the \n\ufb01rst i put() operations. As with any scienti\ufb01c data, there is a great deal of information \nin this data for us to investigate (this plot has ", "start": 388, "end": 389}, "563": {"text": "average number of key compares used for the \n\ufb01rst i put() operations. As with any scienti\ufb01c data, there is a great deal of information \nin this data for us to investigate (this plot has 14,350 gray points and 14,350 red points). \nIn this context, our primary interest is that the plot validates our hypothesis that about \nhalf the list is accessed for the average put() operation. The actual total is slightly lower \nthan half, but this fact (and the precise shape of the curves) is perhaps best explained by \ncharacteristics of the application, not our algorithms (see Exercise 3.1.36).\nWhile detailed characterization of performance for particular clients can be com -\nplicated, speci\ufb01c hypotheses are easy to formulate and to test for FrequencyCount\nwith our reference inputs or with randomly ordered inputs, using a client like the \nDoublingTest client that we introduced in Chapter 1. We will reserve such testing \nfor exercises and for the more sophisticated implementations that follow . If you are not \nalready convinced that we need faster implementations, be sure to work these exercises \n(or just run FrequencyCounter with SequentialSearchST on leipzig1M.txt!).\nCosts for java FrequencyCounter 8 < tale.txt using SequentialSearchST\n5737\n0\n0 14350operations\ncompares 2246\n3773.1 \u25a0 Symbol Tables\n    B i n a r y  s e a r c h  i n  a n   o r d e r e d  a r r a y  Next, we consider a full implementation \nof our ordered symbol-table API. The underlying data structure is a pair of    parallel \narrays, one for the keys and one for the values. Algorithm 3.2 (BinarySearchST) on \nthe facing page keeps Comparable keys in order in the array, then uses array indexing \nto enable fast implementation of ", "start": 389, "end": 390}, "564": {"text": "\narrays, one for the keys and one for the values. Algorithm 3.2 (BinarySearchST) on \nthe facing page keeps Comparable keys in order in the array, then uses array indexing \nto enable fast implementation of get() and other operations.\nThe heart of the implementation is the  rank() method, which returns the number \nof keys smaller than a given key. For get(), the rank tells us precisely where the key is \nto be found if it is in the table (and, if it is not there, that it is not in the table). \nFor put(), the rank tells us precisely where to update the value when the key is in the \ntable, and precisely where to put the key when the key is not in the table. We move all \nlarger keys over one position to make room (working from back to front) and insert the \ngiven key and value into the proper positions in their respective arrays. Again, studying \nBinarySearchST in conjunction with a trace of our test client is an instructive intro-\nduction to this data structure.\nThis code maintains parallel arrays of keys and values (see Exercise 3.1.12 for an \nalternative). As with our implementations of generic stacks and queues in Chapter 1, \nthis code carries the inconvenience of having to create a Key array of type Comparable \nand a Value array of type Object, and to cast them back to Key[] and Value[] in the \nconstructor. As usual, we can use array resizing so that clients do not have to be con -\ncerned with the size of the array (noting, as you shall see, that this method is too slow \nto use with large arrays).\nTrace of  ordered-array ST implementation for standard indexing client\n                      keys[]                               vals[]\n           0  1  2  3  4  5  6  7  8  9    N    0 ", "start": 390, "end": 390}, "565": {"text": "implementation for standard indexing client\n                      keys[]                               vals[]\n           0  1  2  3  4  5  6  7  8  9    N    0  1  2  3  4  5  6  7  8  9\n S   0     S                               1    0\n E   1     E  S                            2    1  0 \n A   2     A  E  S                         3    2  1  0 \n R   3     A  E  R  S                      4    2  1  3  0 \n C   4     A  C  E  R  S                   5    2  4  1  3  0\n H   5     A  C  E  H  R  S                6    2  4  1  5  3  0 \n E   6     A  C  E  H  R  S                6    2  4  6  5  3  0 \n X   7     A  C  E  H  R  S  X             7    2  4  6  5  3  0  7    \n A   8     A  C  E  H  R  S  X             7    8  4  6  5  3  0  7         \n M   9     A  C  E  H  M  R  S  X          8    8  4  6  5  9  3  0  7 ", "start": 390, "end": 390}, "566": {"text": "9     A  C  E  H  M  R  S  X          8    8  4  6  5  9  3  0  7     \n P  10     A  C  E  H  M  P  R  S  X       9    8  4  6  5  9 10  3  0  7  \n L  11     A  C  E  H  L  M  P  R  S  X   10    8  4  6  5 11  9 10  3  0  7 \n E  12     A  C  E  H  L  M  P  R  S  X   10    8  4 12  5 11  9 10  3  0  7\n           A  C  E  H  L  M  P  R  S  X         8  4 12  5 11  9 10  3  0  7\nentries in gray \ndid not move circled entries are\nchanged values\nentries in black \nmoved to the rightentries in red\nwere inserted\nkey value\n378 CHAPTER 3 \u25a0 Searching\n ALGORITHM 3.2   Binary search (in an ordered array)\npublic class  BinarySearchST<Key extends Comparable<Key>, Value> \n{\n   private Key[] keys;\n   private Value[] vals;\n   private int N;\n   public BinarySearchST(int capacity)\n   {   // See Algorithm 1.1 for standard array-resizing code.\n       keys = (Key[]) new Comparable[capacity];\n       vals = (Value[]) new Object[capacity];\n   }   \n   public int size()\n   { ", "start": 390, "end": 391}, "567": {"text": "// See Algorithm 1.1 for standard array-resizing code.\n       keys = (Key[]) new Comparable[capacity];\n       vals = (Value[]) new Object[capacity];\n   }   \n   public int size()\n   {  return N;  }\n   public Value get(Key key)\n   {\n      if (isEmpty()) return null;\n      int i = rank(key);\n      if (i < N && keys[i].compareTo(key) == 0) return vals[i];\n      else                                      return null;\n   }\n   public int rank(Key key)\n   // See page 381.\n   public void put(Key key, Value val) \n   {  // Search for key. Update value if found; grow table if new.\n      int i = rank(key);\n      if (i < N && keys[i].compareTo(key) == 0)\n      {  vals[i] = val; return;  }\n      for (int j = N; j > i; j--)\n      {  keys[j] = keys[j-1]; vals[j] = vals[j-1];  }\n      keys[i] = key; vals[i] = val;\n      N++;\n   }\n   public void delete(Key key) \n   // See Exercise 3.1.16 for this method.\n}\nThis ST implementation keeps the keys and values in parallel arrays. The put() implementation \nmoves larger keys one position to the right before growing the table as in the array-based stack imple-\nmentation in Section 1.3. Array-resizing code is omitted here.\n3793.1 \u25a0 Symbol Tables public int rank(Key key, int lo, int hi) \n{\n   if (hi < lo) return lo;\n   int mid = lo + (hi - lo) / 2;\n   int cmp = key.compareTo(keys[mid]);\n   if      (cmp < 0)\n        return rank(key, lo, mid-1);\n   else if (cmp > 0)\n ", "start": 391, "end": 392}, "568": {"text": "+ (hi - lo) / 2;\n   int cmp = key.compareTo(keys[mid]);\n   if      (cmp < 0)\n        return rank(key, lo, mid-1);\n   else if (cmp > 0)\n        return rank(key, mid+1, hi);\n   else return mid; \n}\nRecursive binary search\n B i n a r y  s e a r c h .  The reason that we keep keys in an ordered array is so that we can \nuse array indexing to dramatically reduce the number of compares required for each \nsearch, using the classic and venerable binary search algorithm that we used as an exam-\nple in Chapter 1. We maintain indices into \nthe sorted key array that delimit the subar-\nray that might contain the search key. T o \nsearch, we compare the search key against \nthe key in the middle of the subarray.  If the \nsearch key is less than the key in the middle, \nwe search in the left half of the subarray; if \nthe search key is greater than the key in the \nmiddle, we search in the right half of the \nsubarray; otherwise the key in the middle is \nequal to the search key. The rank() code on \nthe facing page uses binary search to com-\nplete the symbol-table implementation just discussed. This implementation is worthy \nof careful study. T o study it, we start with the equivalent recursive code at left. A call to \nrank(key, 0, N-1) does the same sequence of compares as a call to the nonrecursive \nimplementation in Algorithm 3.2, but this alternate version better exposes the struc-\nture of the algorithm, as discussed in Section 1.1. This recursive rank() preserves the \nfollowing properties:\n\u25a0 If key is in the table, rank() returns its index in the table, which is the same as \nthe number of keys in the ", "start": 392, "end": 392}, "569": {"text": "in Section 1.1. This recursive rank() preserves the \nfollowing properties:\n\u25a0 If key is in the table, rank() returns its index in the table, which is the same as \nthe number of keys in the table that are smaller than key.\n\u25a0 If key is not in the table, rank() also returns the number of keys in the table \nthat are smaller than key.\nTaking the time to conv ince yourself  that the nonrecursive rank() in Algorithm 3.2 \noperates as expected (either by proving that it is equivalent to the recursive version or \nby proving directly that the loop always terminates with the value of lo precisely equal \nto the number of keys in the table that are smaller than key) is a worthwhile exercise for \nany programmer. (Hint : Note that lo starts at 0 and never decreases.) \nOther operations. Since the keys are kept in an ordered array, most of the order-based \noperations are compact and straightforward, as you can see from the code on page 382. For \nexample, a call to select(k) just returns keys[k].We have left delete() and  floor()\nas exercises. Y ou are encouraged to study the implementation of  ceiling() and the \ntwo-argument keys() and to work these exercises to cement your understanding of the \nordered symbol-table API and this implementation.\n380 CHAPTER 3 \u25a0 Searching\n ALGORITHM 3.2  (continued) Binary search in an ordered array (iterative)\npublic int  rank(Key key) \n{\n   int lo = 0, hi = N-1;\n   while (lo <= hi)\n   {\n      int mid = lo + (hi - lo) / 2;\n      int cmp = key.compareTo(keys[mid]);\n      if      (cmp < 0) hi = mid - 1;\n      else if (cmp > 0) lo = mid + 1;\n      else return mid;\n   }\n ", "start": 392, "end": 393}, "570": {"text": "cmp = key.compareTo(keys[mid]);\n      if      (cmp < 0) hi = mid - 1;\n      else if (cmp > 0) lo = mid + 1;\n      else return mid;\n   }\n   return lo; \n}\nThis method uses the classic method described in the text to compute the number of keys in the table \nthat are smaller than key. Compare key with the key in the middle: if it is equal, return its index; if it \nis less, look in the left half; if it is greater, look in the right half.\nloop exits with lo > hi: return  7  \nentries in black \nare a[lo..hi]\nentry in red is a[mid]\nsuccessful search for P\nloop exits with keys[mid] = P: return \nlo hi mid\nunsuccessful search for Q\nlo hi mid\n                      keys[]\n           0  1  2  3  4  5  6  7  8  9\n0  9  4    A  C  E  H  L  M  P  R  S  X\n5  9  7    A  C  E  H  L  M  P  R  S  X\n5  6  5    A  C  E  H  L  M  P  R  S  X\n6  6  6    A  C  E  H  L  M  P  R  S  X\n0  9  4    A  C  E  H  L  M  P  R  S  X\n5  9  7    A  C  E  H  L  M  P  R  S  X\n5  6  5    A  C  E  H  L  M ", "start": 393, "end": 393}, "571": {"text": "X\n5  9  7    A  C  E  H  L  M  P  R  S  X\n5  6  5    A  C  E  H  L  M  P  R  S  X\n7  6  6    A  C  E  H  L  M  P  R  S  X\nTrace of  binary search for rank in an ordered array\n3813.1 \u25a0 Symbol Tables  A L G O R I T H M  3 . 2 (continued) Ordered symbol-table operations for binary search\npublic  Key min() \n{  return keys[0];  }\npublic Key max() \n{  return keys[N-1];  }\npublic Key select(int k) \n{  return keys[k];  }\npublic Key ceiling(Key key) \n{\n   int i = rank(key);\n   return keys[i]; \n}\npublic Key floor(Key key) \n// See Exercise 3.1.17.\npublic Key delete(Key key) \n// See Exercise 3.1.16.\npublic Iterable<Key> keys(Key lo, Key hi) \n{\n   Queue<Key> q = new Queue<Key>();\n   for (int i = rank(lo); i < rank(hi); i++)\n   q.enqueue(keys[i]);\n   if (contains(hi)) \n      q.enqueue(keys[rank(hi)]);\n   return q; \n}\nThese methods, along with the methods of Exercise 3.1.16 and Exercise 3.1.17, complete the imple-\nmentation of our (ordered) symbol-table API using binary search in an ordered array. The min(), \nmax(), and select() methods are trivial, just amounting to returning the appropriate key from its \nknown position in the array. The rank() method, which is the basis of binary search, plays a central \nrole in the others. ", "start": 393, "end": 394}, "572": {"text": "\nmax(), and select() methods are trivial, just amounting to returning the appropriate key from its \nknown position in the array. The rank() method, which is the basis of binary search, plays a central \nrole in the others. The floor() and delete() implementations, left for exercises, are more compli-\ncated, but still straightforward. \n382 CHAPTER 3 \u25a0 Searching Analysis of binary search The recursive implementation of rank() also leads \nto an immediate argument that binary search guarantees fast search, because it cor -\nresponds to a recurrence relation that describes an upper bound on the number of \ncompares.\n \n \n \n P r o p o s i t i o n  B .   Binary search in an ordered array with N keys uses no more than \nlg N /H11001 1 compares for a search (successful or unsuccessful).\nProof: This analysis is similar to (but simpler than) the analysis of mergesort \n(Proposition F in Chapter 2). Let C(N) be the number of compares needed to \nsearch for a key in a symbol table of size N. We have C(0) = 0, C(1) = 1, and for N > \n0 we can write a recurrence relationship that directly mirrors the recursive method:\nC(N ) /H11349 C(\u23a3N/2\u23a6)  /H11001 1\nWhether the search goes to the left or to the right, the size of the subarray is no \nmore than \u23a3N/2\u23a6, and we use one compare to check for equality and to choose \nwhether to go left or right. When N is one less than a power of 2 (say N = 2n/H110021), \nthis recurrence is not dif\ufb01cult to solve. First, since \u23a3N/2\u23a6 = 2n/H110021/H110021, ", "start": 394, "end": 395}, "573": {"text": "2 (say N = 2n/H110021), \nthis recurrence is not dif\ufb01cult to solve. First, since \u23a3N/2\u23a6 = 2n/H110021/H110021, we have\nC(2n /H110021) /H11349 C(2n/H110021/H110021) /H11001 1\nApplying the same equation to the \ufb01rst term on the right, we have\nC(2n /H110021) /H11349 C(2n/H110022/H110021) /H11001 1 /H11001 1\nRepeating the previous step n /H11002 2 additional times gives\nC(2n /H110021) /H11349 C(20) /H11001 n\nwhich leaves us with the solution\nC(N )  =  C(2n ) /H11349 n  /H11001 1 <  lg N /H11001 1\nExact solutions for general N are more complicated, but it is not dif\ufb01cult to extend \nthis argument to establish the stated property for all values of N (see  Exercise \n3.1.20). With binary search, we achieve a logarithmic-time search guarantee.\nThe implementation just given for ceiling() is based on a single call to rank(), and \nthe default two-argument size() implementation calls rank() twice, so this proof also \nestablishes that these operations (and  floor()) are supported in guaranteed logarith-\nmic time (min(), max(), and select() are constant-time operations).\n3833.1 \u25a0 Symbol Tables\n Despite its guaranteed logarithmic search, BinarySearchST\nstill does not enable us to use clients like FrequencyCounter to \nsolve huge problems, because the put() method is too slow. Bi-\nnary search reduces the number of compares, but not the running ", "start": 395, "end": 396}, "574": {"text": "search, BinarySearchST\nstill does not enable us to use clients like FrequencyCounter to \nsolve huge problems, because the put() method is too slow. Bi-\nnary search reduces the number of compares, but not the running \ntime, because its use does not change the fact that the number of \narray accesses required to build a symbol table in an ordered ar -\nray is quadratic in the size of the array when keys are randomly \nordered (and in typical practical situations where the keys, while \nnot random, are well-described by this model).\nProposition B (continued). Inserting a new key into an or -\ndered array of size N uses  ~ 2 N array accesses in the worst \ncase, so inserting N keys into an initially empty table uses ~ \nN 2 array accesses in the worst case.\nProof: Same as for Proposition A.\nFor Ta l e  of  Tw o  Ci t i e s, with over 10 4 distinct keys, the cost to build \nthe table is nearly 10 8 array accesses, and for the Leipzig project, \nwith over 106 distinct keys, the cost to build the table is over 10 11 \narray accesses. While not quite prohibitive on modern computers, these costs are ex-\ntremely (and unnecessarily) high. \nReturning to the cost of the put() operations for FrequencyCounter for words of \nlength 8 or more, we see a reduction in the average cost from 2,246 compares (plus \narray accesses) per operation for SequentialSearchST to 484 for BinarySearchST. \nAs before, this cost is even better than would be predicted by analysis, and the extra \nimprovement is likely again explained by properties of the application (see Exercise \n3.1.36). This improvement is impressive, but we can do much better, as you shall see.\nmethod order of growth\nof running time\nput() N\nget() ", "start": 396, "end": 396}, "575": {"text": "again explained by properties of the application (see Exercise \n3.1.36). This improvement is impressive, but we can do much better, as you shall see.\nmethod order of growth\nof running time\nput() N\nget() log N\ndelete() N\ncontains() log N\nsize() 1\nmin() 1\nmax() 1\nfloor() log N\nceiling() log N\nrank() log N\nselect() 1\ndeleteMin() N\ndeleteMax() 1\nBinarySearchST costs \nCosts for java FrequencyCounter 8 < tale.txt using BinarySearchST\n5737\n0\n0 14350operations\ncost\n484\n384 CHAPTER 3 \u25a0 Searching\n Preview Binary search is typically far better than sequential search and is the meth-\nod of choice in numerous practical applications. For a static table (with no insert op-\nerations allowed), it is worthwhile to initialize and sort the table, as in the version of \nbinary search that we considered in Chapter 1 (see page 99). Even when the bulk of \nthe key-value pairs is known before the bulk of the searches (a common situation in \napplications), it is worthwhile to add to BinarySearchST a constructor that initial-\nizes and sorts the table (see Exercise 3.1.12). Still, binary search is infeasible for use \nin many other applications. For example, it fails for our Leipzig Corpora application \nbecause searches and inserts are intermixed and the table size is too large. As we have \nemphasized, typical modern search clients require symbol tables that can support fast \nimplementations of both search and insert. That is, we need to be able to build huge \ntables where we can insert (and perhaps remove) key-value pairs in unpredictable pat-\nterns, intermixed with searches. \nThe table below summarizes performance characteristics for the elementary sym-\nbol-table implementations considered in ", "start": 396, "end": 397}, "576": {"text": "to build huge \ntables where we can insert (and perhaps remove) key-value pairs in unpredictable pat-\nterns, intermixed with searches. \nThe table below summarizes performance characteristics for the elementary sym-\nbol-table implementations considered in this section. The table entries give the leading \nterm of the cost (number of array accesses for binary search, number of compares for \nthe others), which implies the order of growth of the running time.\n \nThe central question is whether we can devise algorithms and data structures that \nachieve logarithmic performance for both search and insert. The answer is a resound -\ning yes! Providing that answer is the main thrust of this chapter. Along with the fast sort \ncapability discussed in Chapter 2, fast symbol-table search/insert is one of the most \nimportant contributions of algorithmics and one of the most important steps toward \nthe development of the rich computational infrastructure that we now enjoy. \nHow can we achieve this goal? To support ef\ufb01cient insertion, it seems that we need a \nlinked structure. But a singly linked list forecloses the use of binary search, because the \nef\ufb01ciency of binary search depends on our ability to get to the middle of any subarray \nalgorithm\n(data structure)\nworst-case cost\n(after N inserts) \naverage-case cost\n(after N random inserts) efficiently\nsupport ordered\noperations?search insert search hit insert\nsequential search\n(unordered linked list) N N N/2 N no\nbinary search\n(ordered array) lg N 2N lg N N yes\nCost summary for basic symbol-table implementations\n3853.1 \u25a0 Symbol Tables\n quickly via indexing (and the only way to get to the middle of a singly linked list is to \nfollow links).  T o combine the ef\ufb01ciency of binary search with the \ufb02exibility of linked \nstructures, we need more complicated data structures.  That combination is provided \nboth ", "start": 397, "end": 398}, "577": {"text": "linked list is to \nfollow links).  T o combine the ef\ufb01ciency of binary search with the \ufb02exibility of linked \nstructures, we need more complicated data structures.  That combination is provided \nboth by binary search trees, the subject of the next two sections, and by hash tables, the \nsubject of Section 3.4.\nWe consider six sy mbol-table implementations in this chapter, so a br ief  prev iew is \nin order.  The table below is a list of the data structures, along with the primary reasons \nin favor of and against using each for an application. They appear in the order in which \nwe consider them.\nWe w ill get into more detail on proper ties of  the algor ithms and implementations \nas we discuss them, but the brief characterizations in this table will help you keep them \nin a broader context as you learn them. The bottom line is that we have several fast \nsymbol-table implementations that can be and are used to great effect in countless \napplications.\nunderlying\ndata structure  implementation pros cons\nlinked list\n(sequential \nsearch)\nSequentialSearchST best for tiny STs slow for large STs \nordered array\n(binary search) BinarySearchST \noptimal search\nand space,\norder-based ops\nslow insert \nbinary\nsearch tree\nBST\neasy to \nimplement,\norder-based ops \nno guarantees\nspace for links\nbalanced\nBST\nRedBlackBST\noptimal search \nand insert,\norder-based ops\nspace for links \nhash table SeparateChainingHashST \nLinearProbingHashST\nfast search/insert\nfor common types\nof data\nneed hash for each type\nno order-based ops \nspace for links/empty\nPros and cons of symbol-table implementations\n386 CHAPTER 3 \u25a0 Searching\n Q&A \nQ. Why not use an  Item type that implements Comparable for symbol tables, in the \nsame ", "start": 398, "end": 399}, "578": {"text": "order-based ops \nspace for links/empty\nPros and cons of symbol-table implementations\n386 CHAPTER 3 \u25a0 Searching\n Q&A \nQ. Why not use an  Item type that implements Comparable for symbol tables, in the \nsame way as we did for priority queues in Section 2.4, instead of having separate keys \nand values ?\nA. That is also a reasonable option. These two approaches illustrate two different ways \nto associate information with keys\u2014we can do so implicitly by building a data type that \nincludes the key or explicitly by separating keys from values. For symbol tables, we have \nchosen to highlight the associative array abstraction. Note also that a client speci\ufb01es just \na key in search, not a key-value aggregation.\nQ.  Why bother with equals() ? Why not just use compareTo() throughout?\nA. Not all data types lead to key values that are easy to compare, even though having a \nsymbol table still might make sense. T o take an extreme example, you may wish to use  \npictures or songs as keys. There is no natural way to compare them, but we can certainly \ntest equality (with some work).\n Q .  Why not allow keys to take the value null?\nA. We assume that Key is an Object because we use it to invoke compareTo() or \nequals(). But a call like a.compareTo(b) would cause a null pointer exception if a is \nnull. By ruling out this possibility, we allow for simpler client code.\nQ.  Why not use a method like the less() method that we used for sorting?\nA. Equality plays a special role in symbol tables, so we also would need a method for \ntesting equality. T o avoid proliferation of methods that have essentially the same func-\ntion, we adopt the built-in Java methods equals() and compareTo().\nQ.  Why not declare key[] as Object[] (instead of Comparable[]) in BinarySearchST\nbefore casting, ", "start": 399, "end": 399}, "579": {"text": "methods that have essentially the same func-\ntion, we adopt the built-in Java methods equals() and compareTo().\nQ.  Why not declare key[] as Object[] (instead of Comparable[]) in BinarySearchST\nbefore casting, in the same way that val[] is declared as Object?\nA. Good question. If you do so, you will get a   ClassCastException because keys need \nto be Comparable (to ensure that entries in key[] have a compareTo() method). Thus, \ndeclaring key[] as Comparable[] is required. Delving into the details of program-\nming-language design to explain the reasons would take us somewhat off topic. We use \nprecisely this idiom (and nothing more complicated) in any code that uses Comparable\ngenerics and arrays throughout this book.\n3873.1 \u25a0 Symbol Tables\n  \nQ.  What if we need to associate multiple values with the same key? For example, if we \nuse Date as a key in an application, wouldn\u2019t we have to process equal keys?\nA. Maybe, maybe not. For example, you can\u2019t have two trains arrive at the station on \nthe same track at the same time (but they could arrive on different tracks at the same \ntime). There are two ways to handle the situation: use some other information to dis-\nambiguate or make the value a Queue of values having the same key. We consider ap -\nplications in detail in Section 3.5.\nQ. Presorting the table as discussed on page 385 seems like a good idea. Why relegate \nthat to an exercise (see Exercise 3.1.12)?\nA. Indeed, this may be the method of choice in some applications. But adding a slow \ninsert method to a data structure designed for fast search \u201cfor convenience\u201d is a per -\nformance trap, because an unsuspecting client might intermix searches and inserts in \na huge table and experience quadratic performance. Such traps are ", "start": 399, "end": 400}, "580": {"text": "\ninsert method to a data structure designed for fast search \u201cfor convenience\u201d is a per -\nformance trap, because an unsuspecting client might intermix searches and inserts in \na huge table and experience quadratic performance. Such traps are all too common, \nso that \u201cbuyer beware\u201d is certainly appropriate when using software developed by oth-\ners, particularly when interfaces are too wide. This problem becomes acute when a \nlarge number of methods are included \u201cfor convenience\u201d leaving performance traps \nthroughout, while a client might expect ef\ufb01cient implementations of all methods. Java\u2019s \nArrayList class is an example (see Exercise 3.5.27). \nQ&A (continued)\n388 CHAPTER 3 \u25a0 Searching\n EXERCISES \n3.1.1 Write a client that creates a symbol table mapping letter grades to numerical \nscores, as in the table below, then reads from standard input a list of letter grades and \ncomputes and prints the GPA (the average of the numbers corresponding to the grades).\nA+ A A- B+ B B- C+ C C- D F\n4.33 4.00 3.67 3.33 3.00 2.67 2.33 2.00 1.67 1.00 0.00\n3.1.2  Develop a symbol-table implementation ArrayST that uses an (unordered) array \nas the underlying data structure to implement our basic symbol-table API.\n3.1.3 Develop a symbol-table implementation OrderedSequentialSearchST that \nuses an ordered linked list as the underlying data structure to implement our ordered \nsymbol-table API.\n3.1.4 Develop Time and Event ADTs that allow processing of data as in the example \nillustrated on page 367.\n3.1.5  Implement size(), delete(),  and keys() for  SequentialSearchST.\n3.1.6  Give the number of calls to ", "start": 400, "end": 401}, "581": {"text": "data as in the example \nillustrated on page 367.\n3.1.5  Implement size(), delete(),  and keys() for  SequentialSearchST.\n3.1.6  Give the number of calls to put() and get() issued by FrequencyCounter, as a \nfunction of the number W of words and the number D of distinct words in the input.\n3.1.7 What is the average number of distinct keys that FrequencyCounter will \ufb01nd \namong N random nonnegative integers less than 1,000, for N=10, 102, 103, 104, 105, and \n106?\n3.1.8 What is the most frequently used word of ten letters or more in Ta l e  of  Tw o  Ci t i e s?\n3.1.9 Add code to FrequencyCounter to keep track of the last call to put(). Print the \nlast word inserted and the number of words that were processed in the input stream \nprior to this insertion. Run your program for tale.txt with length cutoffs 1, 8, and 10.\n3.1.10 Give a trace of the process of inserting the keys E A S Y Q U E S T I O N into an \ninitially empty table using SequentialSearchST. How many compares are involved?\n3.1.11 Give a trace of the process of inserting the keys E A S Y Q U E S T I O N into \nan initially empty table using BinarySearchST. How many compares are involved?\n3.1.12  Modify BinarySearchST to maintain one array of Item objects that contain \nkeys and values, rather than two parallel arrays. Add a constructor that takes an array of \n3893.1 \u25a0 Symbol Tables\n Item values as argument and uses mergesort to sort the array.\n3.1.13 Which of the symbol-table implementations in this section would you use for \nan application ", "start": 401, "end": 402}, "582": {"text": "an array of \n3893.1 \u25a0 Symbol Tables\n Item values as argument and uses mergesort to sort the array.\n3.1.13 Which of the symbol-table implementations in this section would you use for \nan application that does 10 3 put() operations and 10 6 get() operations, randomly \nintermixed? Justify your answer.\n3.1.14 Which of the symbol-table implementations in this section would you use for \nan application that does 10 6 put() operations and 10 3 get() operations, randomly \nintermixed? Justify your answer.\n3.1.15 Assume that searches are 1,000 times more frequent than insertions for a \nBinarySearchST client. Estimate the percentage of the total time that is devoted to \ninsertions, when the number of searches is 103, 10 6, and 10 9.\n3.1.16  Implement the delete() method for BinarySearchST.\n3.1.17  Implement the floor() method for BinarySearchST.\n3.1.18  Prove that the rank() method in BinarySearchST is correct.\n3.1.19 Modify FrequencyCounter to print all of the values having the highest fre-\nquency of occurrence, not just one of them. Hint : Use a Queue.\n3.1.20  Complete the proof of Proposition B (show that it holds for all values of N).\nHint : Start by showing that C(N) is monotonic: C(N) /H11349 C(N+1) for all N > 0.\nEXERCISES  (continued)\n390 CHAPTER 3 \u25a0 Searching\n CREATIVE PROBLEMS \n \n3.1.21    Memory usage. Compare the memory usage of BinarySearchST with that of \nSequentialSearchST for N key-value pairs, under the assumptions described in Sec-\ntion 1.4. Do not count the memory for the ", "start": 402, "end": 403}, "583": {"text": "\n3.1.21    Memory usage. Compare the memory usage of BinarySearchST with that of \nSequentialSearchST for N key-value pairs, under the assumptions described in Sec-\ntion 1.4. Do not count the memory for the keys and values themselves, but do count \nreferences to them. For BinarySearchST, assume that array resizing is used, so that the \narray is between 25 percent and 100 percent full.\n3.1.22    Self-organizing search. A self-organizing search algorithm is one that rearrang-\nes items to make those that are accessed frequently likely to be found early in the search.  \nModify your search implementation for Exercise 3.1.2 to perform the following action \non every search hit: move the key-value pair found to the beginning of the list, moving \nall pairs between the beginning of the list and the vacated position to the right one posi-\ntion.  This procedure is called the move-to-front heuristic.\n3.1.23   Analysis of binary search. Prove that the maximum number of compares used \nfor a binary search in a table of size N is precisely the number of bits in the binary rep-\nresentation of N, because the operation of shifting 1 bit to the right converts the binary \nrepresentation of N into the binary representation of  \u23a3N/2\u23a6.\n3.1.24    Interpolation search. Suppose that arithmetic operations are allowed on keys \n(for example, they may be Double or Integer values). Write a version of binary search \nthat mimics the process of looking near the beginning of a dictionary when the word \nbegins with a letter near the beginning of the alphabet. Speci\ufb01cally, if kx is the key value \nsought, klo is the key value of the \ufb01rst key in the table, and khi is the key value of the \nlast key in the ", "start": 403, "end": 403}, "584": {"text": "Speci\ufb01cally, if kx is the key value \nsought, klo is the key value of the \ufb01rst key in the table, and khi is the key value of the \nlast key in the table, look \ufb01rst \u23a3(kx /H11002 klo)/(khi /H11002 klo)\u23a6-way through the table, not half-\nway. T est your implementation against BinarySearchST for FrequencyCounter using \nSearchCompare.\n3.1.25    Software caching. Since the default implementation of contains() calls get(), \nthe inner loop of FrequencyCounter \nif (!st.contains(word)) st.put(word, 1); \nelse                    st.put(word, st.get(word) + 1);\nleads to two or three searches for the same key. T o enable clear client code like this \nwithout sacri\ufb01cing ef\ufb01ciency, we can use a technique known as software  caching, where \nwe save the location of the most recently accessed key in an instance variable. Modify \nSequentialSearchST and BinarySearchST to take advantage of this idea.\n3913.1 \u25a0 Symbol Tables\n  \n \n3.1.26    Frequency count from a dictionary. Modify FrequencyCounter to take the \nname of a dictionary \ufb01le as its argument, count frequencies of the words from standard \ninput that are also in that \ufb01le, and print two tables of the words with their frequencies, \none sorted by frequency, the other sorted in the order found in the dictionary \ufb01le.\n3.1.27  Small tables. Suppose that a BinarySearchST client has S search operations \nand N distinct keys. Give the order of growth of S such that the cost of building the table \nis the same as the cost of all the searches.\n3.1.28  Ordered insertions. Modify BinarySearchST so that inserting a key that is larg-\ner ", "start": 403, "end": 404}, "585": {"text": "of growth of S such that the cost of building the table \nis the same as the cost of all the searches.\n3.1.28  Ordered insertions. Modify BinarySearchST so that inserting a key that is larg-\ner than all keys in the table takes constant time (so that building a table by calling put()\nfor keys that are in order takes linear time).\n3.1.29  Test client. Write a test client TestBinarySearch.java for use in test-\ning the implementations of min(), max(), floor(), ceiling(), select(), rank(), \ndeleteMin(), deleteMax(), and keys() that are given in the text. Start with the stan-\ndard indexing client given on page 370. Add code to take additional command-line argu-\nments, as appropriate.\n3.1.30   Certi\ufb01cation. Add assert statements to BinarySearchST to check algorithm \ninvariants and data structure integrity after every insertion and deletion. For example, \nevery index i should always be equal to rank(select(i)) and the array should always \nbe in order.\nCREATIVE PROBLEMS  (continued)\n392 CHAPTER 3 \u25a0 Searching\n EXPERIMENTS\n \n \n3.1.31  Performance driver. Write a performance driver program that uses put() to \n\ufb01ll a symbol table, then uses get() such that each key in the table is hit an average of \nten times and there is about the same number of misses, doing so multiple times on \nrandom sequences of string keys of various lengths ranging from 2 to 50 characters; \nmeasures the time taken for each run; and prints out or plots the average running times.\n3.1.32  Exercise driver. Write an exercise driver program that uses the methods in our \nordered symbol-table API on dif\ufb01cult or pathological cases that might turn up in prac-\ntical applications.  Simple examples include key sequences that are already in order, key ", "start": 404, "end": 405}, "586": {"text": "an exercise driver program that uses the methods in our \nordered symbol-table API on dif\ufb01cult or pathological cases that might turn up in prac-\ntical applications.  Simple examples include key sequences that are already in order, key \nsequences in reverse order, key sequences where all keys are the same, and keys consist-\ning of only two distinct values.\n3.1.33  Driver for self-organizing search. Write a driver program for self-organizing \nsearch implementations (see Exercise 3.1.22 ) that uses get() to \ufb01ll a symbol table \nwith N keys, then does 10 N successful searches according to a prede\ufb01ned probability \ndistribution. Use this driver to compare the running time of your implementation from \nExercise 3.1.22 with BinarySearchST for N = 10 3, 104, 105, and 106 using the prob-\nability distribution where search hits the i th smallest key with probability 1/2 i .\n3.1.34   Zipf\u2019s law. Do the previous exercise for the probability distribution where \nsearch hits the i th smallest key with probability 1/(i HN) where  HN is a Harmonic num-\nber (see page 185). This distribution is called Zipf\u2019s law. Compare the move-to-front heu-\nristic with the optimal arrangement for the distributions in the previous exercise, which \nis to keep the keys in increasing order (decreasing order of their expected frequency).\n3.1.35  Performance validation I. Run doubling tests that use the \ufb01rst N words of Ta l e  \nof Two Cities for various values of N to test the hypothesis that the running time of \nFrequencyCounter is quadratic when it uses SequentialSearchST for its symbol \ntable.\n3.1.36    Performance validation II. Explain why the performance of BinarySearchST\nand SequentialSearchST for FrequencyCounter is even better ", "start": 405, "end": 405}, "587": {"text": "\nFrequencyCounter is quadratic when it uses SequentialSearchST for its symbol \ntable.\n3.1.36    Performance validation II. Explain why the performance of BinarySearchST\nand SequentialSearchST for FrequencyCounter is even better than predicted by \nanalysis.\n3.1.37  Put/get ratio. Determine empirically the ratio of the amount of time that \nBinarySearchST spends on put() operations to the time that it spends on get() op-\nerations when FrequencyCounter is used to \ufb01nd the frequency of occurrence of values \n3933.1 \u25a0 Symbol Tables\n in 1 million random M-bit int values, for M = 10, 20, and 30. Answer the same question \nfor tale.txt and compare the results.\n3.1.38    Amortized cost plots. Develop instrumentation for FrequencyCounter, \nSequentialSearchST, and BinarySearchST so that you can produce plots like the \nones in this section showing the cost of each put() operation during the computation.\n3.1.39    Actual timings. Instrument FrequencyCounter to use Stopwatch and StdDraw\nto make a plot where the x-axis is the number of calls on get() or put() and the y-axis \nis the total running time, with a point plotted of the cumulative time after each call. \nRun your program for Ta l e  of  Tw o  Ci t i e s using SequentialSearchST and again using \nBinarySearchST and discuss the results. Note : Sharp jumps in the curve may be ex-\nplained by  caching, which is beyond the scope of this question.\n3.1.40  Crossover to binary search. Find the values of N for which binary search in a \nsymbol table of size N becomes 10, 100, and 1,000 times faster than sequential search.   \nPredict the values with analysis and verify them experimentally.\n3.1.41  Crossover to interpolation search. ", "start": 405, "end": 406}, "588": {"text": "\nsymbol table of size N becomes 10, 100, and 1,000 times faster than sequential search.   \nPredict the values with analysis and verify them experimentally.\n3.1.41  Crossover to interpolation search. Find the values of N for which interpolation \nsearch in a symbol table of size N becomes 1, 2, and 10 times faster than binary search, \nassuming the keys to be random 32-bit integers (see Exercise 3.1.24). Predict the values \nwith analysis, and verify them experimentally. \nEXPERIMENTS  (continued)\n394 CHAPTER 3 \u25a0 Searching\n This page intentionally left blank \n 3.2    BINARY SEARCH TREES\n \n \nIn this section , we will examine a symbol-table implementation that combines the \n\ufb02exibility of insertion in a linked list with the ef\ufb01ciency of search in an ordered array. \nSpeci\ufb01cally, using two links per node (instead of the one link per node found in linked \nlists) leads to an ef\ufb01cient symbol-table implementation based on the binary search tree \ndata structure, which quali\ufb01es as one of the most fundamental al-\ngorithms in computer science.\nTo  b e g i n , w e  d e \ufb01 n e  b a s i c  t e r m i n o l o g y. We  a re  wo r k i n g  w i t h  d a t a  \nstructures made up of nodes that contain links that are either null\nor references to other nodes.  In a  binary tree, we have the restric -\ntion that every node is pointed to by just one other node, which is \ncalled its parent (except for one node, the  root, which has no nodes \npointing to it), and that each node has exactly two links, which are \ncalled its left and ", "start": 406, "end": 408}, "589": {"text": "one other node, which is \ncalled its parent (except for one node, the  root, which has no nodes \npointing to it), and that each node has exactly two links, which are \ncalled its left and right links, that point to nodes called its left child\nand right child, respectively. Although links point to nodes, we can \nview each link as pointing to a binary tree, the tree whose root is the referenced node. \nThus, we can de\ufb01ne a binary tree as a either a  null link or a node with a left link and \na right link, each references to (disjoint) subtrees that are themselves binary trees. In a \nbinary search tree, each node also has a key and a value, with an ordering restriction to \nsupport ef\ufb01cient search.\nDefinition. A     binary search tree  (BST) is a binary tree where each node has a \nComparable key (and an associated value) and satis\ufb01es the restriction that the key \nin any node is larger than the keys in all nodes in that node\u2019s left subtree and small-\ner than the keys in all nodes in that node\u2019s right subtree.\nWe  draw BSTs w ith keys in the nodes and use terminolog y \nsuch as \u201cA is the left child of E\u201d that associates nodes with keys. \nLines connecting the nodes represent links, and we give the \nvalue associated with a key in black, beside the nodes (sup-\npressing the value as dictated by context). Each node\u2019s links \nconnect it to nodes below it on the page, except for null links, \nwhich are short segments at the bottom. As usual, our exam-\nples use the single-letter keys that are generated by our index-\ning test client.\nright child\nof root\na left link\na subtree\nroot\nnull links\nAnatomy of a binary tree\nvalue\nassociated\nwith R\nparent ", "start": 408, "end": 408}, "590": {"text": "single-letter keys that are generated by our index-\ning test client.\nright child\nof root\na left link\na subtree\nroot\nnull links\nAnatomy of a binary tree\nvalue\nassociated\nwith R\nparent of A and R\nleft link\nof E\nkeys smaller than E keys larger than E\nkey\nA\nC\nE\nH\nR\nS\nX\n9\nAnatomy of a binary search tree\n396\n  \n \nBasic implementation Algorithm 3.3 de\ufb01nes the BST data structure that we \nuse throughout this section to implement the ordered symbol-table API. We begin by \nconsidering this classic data structure de\ufb01nition and the characteristic associated im -\nplementations of the get() (search) and put() (insert) methods.\n R e p r e s e n t a t i o n .  We de\ufb01ne a pr ivate nested class to de\ufb01ne nodes in BSTs, just as we \ndid for linked lists. Each node contains a key, a value, a left link, a right link, and a node \ncount (when relevant, we include node counts in red above the node \nin our drawings). The left link points to a BST for items with smaller \nkeys, and the right link points to a BST for items with larger keys. \nThe instance variable N gives the node count in the subtree rooted at \nthe node. This \ufb01eld facilitates the implementation of various ordered \nsymbol-table operations, as you will see. The private size() method \nin Algorithm 3.3 is implemented to assign the value 0 to null links, \nso that we can maintain this \ufb01eld by making sure that the invariant\nsize(x) = size(x.left) + size(x.right) + 1\nholds for every node x in the tree.\nA BST represents a set of keys (and associated values), and there \nare ", "start": 408, "end": 409}, "591": {"text": "that the invariant\nsize(x) = size(x.left) + size(x.right) + 1\nholds for every node x in the tree.\nA BST represents a set of keys (and associated values), and there \nare many different BSTs that represent the same set. If we project the \nkeys in a BST such that all keys in each node\u2019s left subtree appear to \nthe left of the key in the node and all keys in each node\u2019s right subtree \nappear to the right of the key in the node, then we always get the keys \nin sorted order. We take advantage of the \ufb02exibility inherent in having \nmany BSTs represent this sorted order to develop ef\ufb01cient algorithms \nfor building and using BSTs. \n S e a r c h .   As usual, when we search for a key in a symbol table, we have one of two \npossible outcomes. If a node containing the key is in the table, we have a   search hit, so \nwe return the associated value. Otherwise, we have a  search miss (and return null). A \nrecursive algorithm to search for a key in a BST follows immediately from the recursive \nstructure: if the tree is empty, we have a search miss; if the search key is equal to the key \nat the root, we have a search hit. Otherwise, we search (recursively) in the appropriate \nsubtree, moving left if the search key is smaller, right if it is larger. The recursive get()\nmethod on page 399 implements this algorithm directly. It takes a node (root of a subtree) \nas \ufb01rst argument and a key as second argument, starting with the root of the tree and \nthe search key. The code maintains the invariant that no parts of the tree other than the \nsubtree rooted at the current node can have a node whose key is equal to the search key. \nJust as the size of the ", "start": 409, "end": 409}, "592": {"text": "\nthe search key. The code maintains the invariant that no parts of the tree other than the \nsubtree rooted at the current node can have a node whose key is equal to the search key. \nJust as the size of the interval in binary search shrinks by about half on each iteration, \nA\nA C E H M R S X\nC\nE\nH\nM\nR\nS\nX\nA\nA C E H M R S X\nC\nE\nH\nM\nR\nS\nX\n2\n6\n5\n8\n8\n1\n1\n1\n1\n1 1\n3\n2\n22\n2\nnode count N\nTwo BSTs that represent\nthe same set of keys\n3973.2 \u25a0 Binary Search Trees\n ALGORITHM 3.3   Binary search tree symbol table\npublic class  BST<Key extends Comparable<Key>, Value> \n{\n   private Node root;               // root of BST\n   private class  Node\n   {\n      private Key key;              // key\n      private Value val;            // associated value\n      private Node left, right;     // links to subtrees\n      private int N;                // # nodes in subtree rooted here\n      public Node(Key key, Value val, int N)\n      {  this.key = key; this.val = val; this.N = N; }\n   }\n   public int size()\n   {  return size(root);  }\n   private int size(Node x)\n   {\n      if (x == null) return 0;\n      else           return x.N;\n   }\n   public Value get(Key key)\n   // See page 399.\n   public void put(Key key, Value val)\n   // See page 399.\n   // See page 407 for min(), max(), floor(), and ceiling(). \n  // See page 409 for select() and rank().\n   // See page 411 ", "start": 409, "end": 410}, "593": {"text": "key, Value val)\n   // See page 399.\n   // See page 407 for min(), max(), floor(), and ceiling(). \n  // See page 409 for select() and rank().\n   // See page 411 for delete(), deleteMin(), and deleteMax().\n   // See page 413 for keys().\n}\nThis implementation of the ordered symbol-table API uses a binary search tree built from Node ob-\njects that each contain a key, associated value, two links, and a node count N. Each Node is the root \nof a subtree containing N nodes, with its left link pointing to a Node that is the root of a subtree with \nsmaller keys and its right link pointing to a Node that is the root of a subtree with larger keys. The \ninstance variable root points to the Node at the root of the BST (which has all the keys and associ-\nated values in the symbol table). Implementations of other methods appear throughout this section.\n398 CHAPTER 3 \u25a0 Searching  A L G O R I T H M  3 . 3 (continued) Search and insert for BSTs\npublic  Value get(Key key) \n{  return get(root, key);  }\nprivate Value get(Node x, Key key) \n{  // Return value associated with key in the subtree rooted at x;\n   // return null if key not present in subtree rooted at x.\n   if (x == null) return null;\n   int cmp = key.compareTo(x.key);\n   if      (cmp < 0) return get(x.left, key);\n   else if (cmp > 0) return get(x.right, key);\n   else return x.val; \n}\npublic void put(Key key, Value val) \n{  // Search for key. Update value if found; grow table if new.\n   root = put(root, key, val); \n}\nprivate Node put(Node x, Key key, Value val) \n{\n ", "start": 410, "end": 411}, "594": {"text": "val) \n{  // Search for key. Update value if found; grow table if new.\n   root = put(root, key, val); \n}\nprivate Node put(Node x, Key key, Value val) \n{\n   // Change key\u2019s value to val if key in subtree rooted at x.\n   // Otherwise, add new node to subtree associating key with val.\n   if (x == null) return new Node(key, val, 1);\n   int cmp = key.compareTo(x.key);\n   if      (cmp < 0) x.left  = put(x.left,  key, val);\n   else if (cmp > 0) x.right = put(x.right, key, val);\n   else x.val = val;\n   x.N = size(x.left) + size(x.right) + 1;\n   return x; \n}\nThese implementations of get() and put() for the symbol-table API are characteristic recursive \nBST methods that also serve as models for several other implementations that we consider later in \nthe chapter. Each method can be understood as both working code and a proof by induction of the \ninductive hypothesis in the opening comment.\n3993.2 \u25a0 Binary Search Trees  \nthe size of the subtree rooted at the current node when searching in a BST shrinks when \nwe go down the tree (by about half, ideally, but at least by one). The procedure stops \neither when a node containing the search key is found (search hit) or when the current \nsubtree becomes empty (search miss). Starting at the top, the search procedure at each \nnode involves a recursive invocation for one of that node\u2019s children, so the search de-\n\ufb01nes a path through the tree.  For a search hit, the path terminates at the node contain-\ning the key.  For a search miss, the path terminates at a null link.\n I n s e r t .  The search code in Algorithm 3.3 ", "start": 411, "end": 412}, "595": {"text": "For a search hit, the path terminates at the node contain-\ning the key.  For a search miss, the path terminates at a null link.\n I n s e r t .  The search code in Algorithm 3.3 is almost as simple as binary search; that \nsimplicity is an essential feature of BSTs. A more important essential feature of BSTs is \nthat insert is not much more dif\ufb01cult to implement than search. Indeed, a search for a \nkey not in the tree ends at a null link, and all that we need to do is replace that link with \na new node containing the key (see the diagram on the next page). The recursive put()\nmethod in Algorithm 3.3 accomplishes this task using logic similar to that we used for \nthe recursive search: if the tree is empty, we return a new node containing the key and \nvalue; if the search key is less than the key at the root, we set the left link to the result \nof inserting the key into the left subtree; otherwise, we set the right link to the result of \ninserting the key into the right subtree. \nR is less than S\nso look to the left \nblack nodes could\nmatch the search key \ngray nodes cannot\nmatch the search key \nfound R\n(search hit)\nso return value\nR is greater than E\nso look to the right \nA\nC\nE\nH\nM\nR\nS\nX\nA\nC\nE\nH\nM\nR\nS\nX\nA\nC\nE\nH\nM\nR\nS\nX\nT is less than X\nso look to the left \nlink is null\nso T is not in tree\n(search miss)\nT is greater than S\nso look to the right \nA\nC\nE\nH\nM\nR\nS\nX\nA\nC\nE\nH\nM\nR\nS\nX\nSearch ", "start": 412, "end": 412}, "596": {"text": "null\nso T is not in tree\n(search miss)\nT is greater than S\nso look to the right \nA\nC\nE\nH\nM\nR\nS\nX\nA\nC\nE\nH\nM\nR\nS\nX\nSearch hit (left) and search miss (right) in a BST\nsuccessful search for R unsuccessful search for T\n400 CHAPTER 3 \u25a0 Searching\n  R e c u r s i o n .  It is worthwhile to take the time to \nunderstand the dynamics of these recursive im-\nplementations. Y ou can think of the code before\nthe recursive calls as happening on the way down\nthe tree: it compares the given key against the \nkey at each node and moves right or left accord-\ningly. Then,  think of  the code after the recursive \ncalls as happening on the way up the tree. For \nget() this amounts to a series of return state-\nments, but for put(), it corresponds to resetting \nthe link of each parent to its child on the search \npath and to incrementing the counts on the way \nup the path. In simple BSTs, the only new link \nis the one at the bottom, but resetting the links \nhigher up on the path is as easy as the test to \navoid setting them. Also, we just need to incre -\nment the node count on each node on the path, \nbut we use more general code that sets each to \none plus the sum of the counts in its subtrees. \nLater in this section and in the next section, we \nshall study more advanced algorithms that are \nnaturally expressed with this same recursive \nscheme but that can change more links on the search paths and need the more general \nnode-count-update code. Elementary BSTs are often implemented with nonrecursive \ncode (see Exercise 3.2.12)\u2014we use recursion in our implementations both to make ", "start": 412, "end": 413}, "597": {"text": "links on the search paths and need the more general \nnode-count-update code. Elementary BSTs are often implemented with nonrecursive \ncode (see Exercise 3.2.12)\u2014we use recursion in our implementations both to make it \neasy for you to convince yourself that the code is operating as described and to prepare \nthe groundwork for more sophisticated algorithms.\nA careful study of the trace for our standard indexing client that is shown on the \nnext page will give you a feeling for the way in which binary search trees grow. New \nnodes are attached to null links at the bottom of the tree; the tree structure is not oth-\nerwise changed. For example, the root has the \ufb01rst key inserted, one of the children of \nthe root has the second key inserted, and so forth. Because each node has two links, the \ntree tends to grow out, rather than just down. Moreover, only the keys on the path from \nthe root to the sought or inserted key are examined, so the number of keys examined \nbecomes a smaller and smaller fraction of the number of keys in the tree as the tree size \nincreases.\nsearch for L ends\nat this null link \nreset links and \nincrement counts\non the way up \ncreate new node 1\n3\n2\n4\n3\n5\n4\n8\n7\n10\n9\nA\nC\nE\nH\nM\nP\nR\nS\nX\nA\nC\nE\nH\nL\nM\nP\nR\nS\nX\nA\nC\nE\nH\nL\nM\nP\nR\nS\nX\nInsertion into a BST\ninserting L\n4013.2 \u25a0 Binary Search Trees\n A\nC\nE\nH\nR\nS\nX\nA\nC\nE\nH\nR\nS\nA\nC\nE\nH\nR\nS\nA\nC\nE\nR\nS\nA\nE\nR\nA\nE\nS\nS\nE\nS\nS\n6\nS ", "start": 413, "end": 414}, "598": {"text": "L\n4013.2 \u25a0 Binary Search Trees\n A\nC\nE\nH\nR\nS\nX\nA\nC\nE\nH\nR\nS\nA\nC\nE\nH\nR\nS\nA\nC\nE\nR\nS\nA\nE\nR\nA\nE\nS\nS\nE\nS\nS\n6\nS   0\nE   1\nA   2\nR   3\nC   4\nH   5\nE   6\nX   7\nred nodes\nare new\nblack nodes\nare accessed\nin search\nchanged\nvalue\nchanged\nvalue\nchanged\nvalue\ngray nodes\nare untouched\nA\nC\nE\nH\nM\nP\nR\nS\nX\nA\nC\nE\nH\nM\nR\nS\nX\nA\nC\nE\nH\nR\nS\nX\nA\nC\nE\nH\nL\nM\nP\nR\nS\nX\nA\nC\nE\nH\nL\nM\nP\nR\nS\nX12\n8\nA   8\nM   9\nP  10\nL  11\nE  12\nBST trace for standard indexing client\nkey value key value\n402 CHAPTER 3 \u25a0 Searching\n   A n a l y s i s  The running times of algorithms on binary \nsearch trees depend on the shapes of the trees, which, in turn, \ndepend on the order in which keys are inserted. In the best \ncase, a tree with N nodes could be  perfectly balanced, with ~ lg \nN nodes between the root and each null link. In the worst case \nthere could be N nodes on the search path. The balance in typi-\ncal trees turns out to be much closer to the best case than the ", "start": 414, "end": 415}, "599": {"text": "\nN nodes between the root and each null link. In the worst case \nthere could be N nodes on the search path. The balance in typi-\ncal trees turns out to be much closer to the best case than the \nworst case.\nFor many applications, the following simple model is rea -\nsonable: We assume that the keys are (uniformly) random, or, \nequivalently, that they are inserted in random order. Analysis \nof this model stems from the observation that BSTs are dual \nto  quicksort. The node at the root of the tree corresponds to \nthe \ufb01rst partitioning item in quicksort (no keys to the left are \nlarger, and no keys to the right are smaller) and the subtrees are \nbuilt recursively, corresponding to quicksort\u2019s recursive subar-\nray sorts. This observation leads us to the analysis of properties \nof the trees.\n P r o p o s i t i o n  C .   Search hits in a BST built from N random keys require ~ 2 ln N\n(about 1.39 lg N) compares, on the average.\nProof: The number of compares used for a search hit ending at a given node is \n1 plus the depth. Adding the depths of all nodes, we get a quantity known as the \ninternal path length of the tree. Thus, the desired quantity is 1 plus the average in -\nternal path length of the BST, which we can analyze with the same argument that \nwe used for Proposition K in Section 2.3: Let CN be the total internal path length \nof a BST built from inserting N randomly ordered distinct keys, so that the average \ncost of a search hit is 1 /H11001CN / N. We have C0 = C1 = 0 and for N > 1 we can write a \nrecurrence relationship that directly mirrors the recursive BST structure:\nCN = ", "start": 415, "end": 415}, "600": {"text": "hit is 1 /H11001CN / N. We have C0 = C1 = 0 and for N > 1 we can write a \nrecurrence relationship that directly mirrors the recursive BST structure:\nCN = N /H11002 1 /H11001 (C0 /H11001 CN/H110021) / N + (C1 /H11001 CN/H110022)/N /H11001  . . .  (CN/H110021 /H11001 C0 )/N\nThe N /H11002 1 term takes into account that the root contributes 1 to the path length \nof each of the other N /H11002 1 nodes in the tree; the rest of the expression accounts \nfor the subtrees, which are equally likely to be any of the N sizes. After rearranging \nterms, this recurrence is nearly identical to the one that we solved in Section 2.3 \nfor quicksort, and we can derive the approximation CN ~ 2N  ln N.\nA\nH\nS\nR\nX\nC\nE\nX\nS\nR\nC\nE\nH\nA\nA\nC\nE\nH\nR\nS\nX\nBST possibilities\nbest case\ntypical case\nworst case\n4033.2 \u25a0 Binary Search Trees\n  \nProposition D. Insertions and search misses  in a BST built from  N random keys \nrequire ~ 2 ln N (about 1.39 lg N) compares, on the average.\nProof: Insertions and search misses take one more compare, on the average, than \nsearch hits. This fact is not dif\ufb01cult to establish by induction (see Exercise 3.2.16).\n \nProposition C says that we should expect the BST search cost for random keys to be \nabout 39 percent higher than that for binary search. Proposition D says that ", "start": 415, "end": 416}, "601": {"text": "by induction (see Exercise 3.2.16).\n \nProposition C says that we should expect the BST search cost for random keys to be \nabout 39 percent higher than that for binary search. Proposition D says that the extra \ncost is well worthwhile, because the cost of inserting a new key is also expected to be \nlogarithmic\u2014\ufb02exibility not available with binary search in an ordered array, where the \nnumber of array accesses required for an insertion is typically linear. As with quicksort, \nthe standard deviation of the number of compares is known to be low, so that these \n  f o r m u l a s  b e c o m e  i n c r e a s i n g l y  a c c u r a t e  a s  N increases. \nExperiments. How well does our random-key model match what is found in typical \nsymbol-table clients? As usual, this question has to be studied carefully for particular \npractical applications, because of the large potential variation in performance. Fortu-\nnately, for many clients, the model is quite good for BSTs.\nFor our example study of the cost of the put() operations for FrequencyCounter\nfor words of length 8 or more, we see a reduction in the average cost from 484 array \naccesses or compares per operation for BinarySearchST to 13 for BST, again providing \na quick validation of the logarithmic performance predicted by the theoretical model. \nMore extensive experiments for larger inputs are illustrated in the table on the next \npage. On the basis of Propositions C and D, it is reasonable to predict that this number \nshould be about twice the natural logarithm of the table size, because the preponder -\nance of operations are searches in a nearly full table. This prediction has at least the \nfollowing inherent inaccuracies:\n\u25a0 Many operations are for smaller tables.\n\u25a0 The keys are ", "start": 416, "end": 416}, "602": {"text": "logarithm of the table size, because the preponder -\nance of operations are searches in a nearly full table. This prediction has at least the \nfollowing inherent inaccuracies:\n\u25a0 Many operations are for smaller tables.\n\u25a0 The keys are not random.\n\u25a0 \n \nThe table size may be too small for the approximation 2 ln N to be accurate.\nNevertheless, as you can see in the table, this prediction is accurate for our \nFrequencyCounter test cases to within a few compares. Actually, most of the differ -\nence can be explained by re\ufb01ning the mathematics in the approximation (see Exercise \n3.2.35). \n404 CHAPTER 3 \u25a0 Searching\n tale.txt leipzig1M.txt\nwords distinct \ncompares \nwords distinct \ncompares\nmodel actual model actual\nall words 135,635 10,679 18.6 17.5 21,191,455 534,580 23.4 22.1\n8+ letters 14,350 5,737 17.6 13.9 4,239,597 299,593 22.7 21.4\n10+ letters 4,582 2,260 15.4 13.1 1,610,829 165,555 20.5 19.3\n A v e r a g e  n u m b e r  o f  c o m p a r e s  p e r  put() for FrequencyCounter using BST\nCosts for java FrequencyCounter 8 < tale.txt using BST\n20\n0\n0 14350operations\ncost\n13.9\nscale magnified by a factor of 250\n compared to previous figures\nTypical BST, built from 256 random keys\n4053.2 \u25a0 Binary Search Trees\n Order-based methods and deletion An important reason that BSTs are widely \nused is that they allow us ", "start": 416, "end": 418}, "603": {"text": "250\n compared to previous figures\nTypical BST, built from 256 random keys\n4053.2 \u25a0 Binary Search Trees\n Order-based methods and deletion An important reason that BSTs are widely \nused is that they allow us to keep the keys in order . As such, they can serve as the basis \nfor implementing the numerous methods in our ordered symbol-table API (see page \n366) that allow clients to access key-value pairs not just by providing the key, but also by \nrelative key order. Next, we consider implementations of the various methods in our \nordered symbol-table API.\n M i n i m u m  a n d    m a x i m u m .  If the left link of the root is null, the smallest key in a BST \nis the key at the root; if the left link is not null, the smallest key in the BST is the smallest \nkey in the subtree rooted at the node referenced by the left link. This statement is both \na description of the recursive min() method on page 407 and an inductive proof that it \n\ufb01nds the smallest key in the BST. The computation is equivalent to a simple iteration \n(move left until \ufb01nding a null link), but we use recursion for consistency. We might \nhave the recursive method return a Key instead of a Node, but we will later have a need \nto use this method to access the Node containing \nthe minimum key. Finding the maximum key is \nsimilar, moving to the right instead of to the left.\n F l o o r  a n d    c e i l i n g .  If a given key key is less than\nthe key at the root of a BST, then the \ufb02oor of key\n(the largest key in the BST less than or equal to \nkey) must be in the left subtree. If key is greater \nthan the key at the root, then the \ufb02oor ", "start": 418, "end": 418}, "604": {"text": "the \ufb02oor of key\n(the largest key in the BST less than or equal to \nkey) must be in the left subtree. If key is greater \nthan the key at the root, then the \ufb02oor of key\ncould be in the right subtree, but only if there is \na key smaller than or equal to key in the right \nsubtree; if not (or if key is equal to the key at the \nroot), then the key at the root is the \ufb02oor of key. \nAgain, this description serves both as the basis \nfor the recursive floor() method and for an in-\nductive proof that it computes the desired result. \nInterchanging right and left (and less and greater) \ngives ceiling().\n   S e l e c t i o n .  Selection in a BST works in a man-\nner analogous to the  partition-based method of \nselection in an array that we studied in Section \n2.5. We maintain in BST nodes the variable N that \ncounts the number of keys in the subtree rooted \nat that node precisely to support this operation. \nfloor(G)in left\nsubtree is null\nresult\nfinding floor(G)\nG is greater than E so \nfloor(G) could be\non the right \nG is less than S so \nfloor(G) must be\non the left\nA\nC\nE\nH\nM\nR\nS\nX\nA\nC\nE\nH\nM\nR\nS\nX\nA\nC\nE\nH\nM\nR\nS\nX\nA\nC\nE\nH\nM\nR\nS\nX\nComputing the floor function\n406 CHAPTER 3 \u25a0 Searching\n   ALGORITHM 3.3  (continued) Min, max, floor, and ceiling in BSTs\npublic Key min() \n{  \n   return min(root).key; \n}\nprivate ", "start": 418, "end": 419}, "605": {"text": "function\n406 CHAPTER 3 \u25a0 Searching\n   ALGORITHM 3.3  (continued) Min, max, floor, and ceiling in BSTs\npublic Key min() \n{  \n   return min(root).key; \n}\nprivate Node min(Node x) \n{\n   if (x.left == null) return x;\n   return min(x.left); \n}\npublic Key floor(Key key) \n{  \n   Node x = floor(root, key);\n   if (x == null) return null;\n   return x.key; \n}\nprivate Node floor(Node x, Key key) \n{  \n   if (x == null) return null;\n   int cmp = key.compareTo(x.key);\n   if (cmp == 0) return x;\n   if (cmp < 0)  return floor(x.left, key);\n   Node t = floor(x.right, key);\n   if (t != null) return t;\n   else           return x; \n}\nEach client method calls a corresponding private method that takes an additional link (to a Node) \nas argument and returns null or a Node containing the desired Key via the recursive procedure de-\nscribed in the text. The max() and ceiling() methods are the same as min() and floor() (respec-\ntively) with right and left (and < and >) interchanged. \n4073.2 \u25a0 Binary Search Trees Suppose that we seek the key of rank k (the key \nsuch that precisely k other keys in the BST are \nsmaller). If the number of keys t in the left sub-\ntree is larger than k, we look (recursively) for the \nkey of rank k in the left subtree; if t is equal to k, \nwe return the key at the root; and if t is smaller \nthan k, we look (recursively) for the key of rank \nk /H11002 t /H11002 1 in the right subtree. ", "start": 419, "end": 420}, "606": {"text": "\nwe return the key at the root; and if t is smaller \nthan k, we look (recursively) for the key of rank \nk /H11002 t /H11002 1 in the right subtree. As usual, this de-\nscription serves both as the basis for the recursive \nselect() method on the facing page and for a \nproof by induction that it works as expected. \n  R a n k .  The inverse method rank() that returns \nthe rank of a given key is similar: if the given \nkey is equal to the key at the root, we return the \nnumber of keys t in the left subtree; if the given \nkey is less than the key at the root, we return the \nrank of the key in the left subtree (recursively \ncomputed); and if the \ngiven key is larger than \nthe key at the root, we re-\nturn t plus one (to count \nthe key at the root) plus \nthe rank of the key in the \nright subtree (recursively \ncomputed). \n \nDelete the minimum/maximum. The most dif\ufb01cult BST op-\neration to implement is the delete() method that removes a \nkey-value pair from the symbol table. As a warmup, consider \ndeleteMin() (remove the key-value pair with the smallest key). \nAs with put() we write a recursive method that takes a link to \na Node as argument and returns a link to a Node, so that we can \nre\ufb02ect changes to the tree by assigning the result to the link used \nas argument. For deleteMin() we go left until \ufb01nding a Node\nthat has a null left link and then replace the link to that node by \nits right link (simply by returning the right link in the recursive \nmethod). The deleted node, with no link now pointing to it, is \n8 keys in left subtree \nso search ", "start": 420, "end": 420}, "607": {"text": "the link to that node by \nits right link (simply by returning the right link in the recursive \nmethod). The deleted node, with no link now pointing to it, is \n8 keys in left subtree \nso search for key of\nrank 3 on the left\ncount N \n8\n2 keys in left subtree so \nsearch for key of rank\n3-2-1 = 0 on the right\n2\n0 keys in left subtree \nand searching for\nkey of rank 0\nso return H\n2 keys in left subtree\nso search for key of \nrank 0 on the left\n2\nA\nC\nE\nH\nM\nR\nS\nX\nA\nC\nE\nH\nM\nR\nS\nX\nA\nC\nE\nH\nM\nR\nS\nX\nA\nC\nE\nH\nM\nR\nS\nX\nfinding select(3)\nthe key of rank 3\nSelection in a BST\ngo left until\nreaching null\nleft link\nreturn that\nnode\u2019s right link\navailable for\ngarbage collection\n5\n7\nupdate links and node counts\nafter recursive calls\nA\nC\nE\nH\nM\nR\nS\nX\nA\nC\nE\nH\nM\nR\nS\nX\nC\nE\nH\nM\nR\nS\nX\nDeleting the minimum in a BST\n408 CHAPTER 3 \u25a0 Searching\n ALGORITHM 3.3  (continued) Selection and rank in BSTs\npublic Key select(int k) \n{  \n   return select(root, k).key; \n}\nprivate Node select(Node x, int k) \n{   // Return Node containing key of rank k.\n    if (x == null) return null;\n    int t = size(x.left);\n    if      (t > k) return select(x.left, ", "start": 420, "end": 421}, "608": {"text": "x, int k) \n{   // Return Node containing key of rank k.\n    if (x == null) return null;\n    int t = size(x.left);\n    if      (t > k) return select(x.left,  k);\n    else if (t < k) return select(x.right, k-t-1);\n    else            return x; \n}\npublic int rank(Key key) \n{  return rank(key, root);  }\nprivate int rank(Key key, Node x) \n{  // Return number of keys less than x.key in the subtree rooted at x.\n   if (x == null) return 0;\n   int cmp = key.compareTo(x.key);\n   if      (cmp < 0) return rank(key, x.left);\n   else if (cmp > 0) return 1 + size(x.left) + rank(key, x.right);\n   else              return size(x.left); \n}\nThis code uses the same recursive scheme that we have been using throughout this chapter to imple-\nment the select() and rank() methods. It depends on using the private size() method given at the \nbeginning of this section that gives the number of subtrees rooted at each node. \n4093.2 \u25a0 Binary Search Trees available for garbage collection. Our standard recursive setup accomplishes, after the \ndeletion, the task of setting the appropriate link in the parent and updating the counts \nin all nodes in the path to the root. The symmetric method works for deleteMax(). \n D e l e t e .  We can proceed in a similar manner to de -\nlete any node that has one child (or no children), but \nwhat can we do to delete a node that has two chil-\ndren? We are left with two links, but have a place in \nthe parent node for only one of them. An answer to \nthis dilemma, \ufb01rst proposed by  T. Hibbard ", "start": 421, "end": 422}, "609": {"text": "two chil-\ndren? We are left with two links, but have a place in \nthe parent node for only one of them. An answer to \nthis dilemma, \ufb01rst proposed by  T. Hibbard in 1962, \nis to delete a node x by replacing it with its successor. \nBecause x has a right child, its successor is the node \nwith the smallest key in its right subtree. The replace-\nment preserves order in the tree because there are no \nkeys between x.key and the successor\u2019s key. We can \naccomplish the task of replacing x by its successor in \nfour (!) easy steps: \n\u25a0 Save a link to the node to be deleted in t. \n\u25a0 Set x to point to its successor min(t.right). \n\u25a0 Set the right link of x (which is supposed to \npoint to the BST containing all the keys larger \nthan x.key) to deleteMin(t.right), the link \nto the BST containing all the keys that are larger \nthan x.key after the deletion. \n\u25a0 Set the left link of x (which was null) to t.left\n(all the keys that are less than both the deleted \nkey and its successor).\nOur standard recursive setup accomplishes, after the \nrecursive calls, the task of setting the appropriate link \nin the parent and decrementing the node counts in \nthe nodes on the path to the root (again, we accom-\nplish the task of updating the counts by setting the counts in each node on the search \npath to be one plus the sum of the counts in its children). While this method does the \njob, it has a \ufb02aw that might cause performance problems in some practical situations. \nThe problem is that the choice of using the successor is arbitrary and not symmetric. \nWhy not use the predecessor? In practice, it is worthwhile to choose at random between \nthe predecessor and the successor.  See Exercise 3.2.42 ", "start": 422, "end": 422}, "610": {"text": "\nThe problem is that the choice of using the successor is arbitrary and not symmetric. \nWhy not use the predecessor? In practice, it is worthwhile to choose at random between \nthe predecessor and the successor.  See Exercise 3.2.42 for details.\nsearch for key E\nnode to delete\ndeleteMin(t.right)\nt\n5\n7\nx\nsuccessor\nmin(t.right)\nt.left\nx\nupdate links and\nnode counts after\nrecursive calls\nA\nC\nE\nH\nM\nR\nS\nX\nA\nC\nE\nH\nM\nR\nS\nX\nA\nC\nH\nA\nC\nH\nM\nR\nM\nR\nS\nX\nE\nS\nX\ndeleting E\nDeletion in a BST\ngo right, then\ngo left until\nreaching null\nleft link\n410 CHAPTER 3 \u25a0 Searching\n   ALGORITHM 3.3  (continued) Deletion in BSTs\npublic void deleteMin() \n{  \n   root = deleteMin(root); \n}\nprivate Node deleteMin(Node x) \n{\n   if (x.left == null) return x.right;\n   x.left = deleteMin(x.left);\n   x.N = size(x.left) + size(x.right) + 1;\n   return x; \n}\npublic void delete(Key key) \n{  root = delete(root, key);  }\nprivate Node delete(Node x, Key key) \n{\n   if (x == null) return null;\n   int cmp = key.compareTo(x.key);\n   if      (cmp < 0) x.left  = delete(x.left,  key);\n   else if (cmp > 0) x.right = delete(x.right, key);\n   else \n   {\n      if (x.right == null) return x.left;\n      if (x.left == null) return x.right;\n      Node t ", "start": 422, "end": 423}, "611": {"text": "(cmp > 0) x.right = delete(x.right, key);\n   else \n   {\n      if (x.right == null) return x.left;\n      if (x.left == null) return x.right;\n      Node t = x;\n      x = min(t.right);  // See page 407.\n      x.right = deleteMin(t.right);\n      x.left = t.left;\n   }\n   x.N = size(x.left) + size(x.right) + 1;\n   return x; \n}\nThese methods implement eager Hibbard deletion in BSTs, as described in the text on the facing \npage. The delete() code is compact, but tricky. Perhaps the best way to understand it is to read \nthe description at left, try to write the code yourself on the basis of the description, then compare \nyour code with this code. This method is typically effective, but performance in large-scale applica -\ntions can become a bit problematic (see Exercise 3.2.42). The deleteMax() method is the same as \ndeleteMin() with right and left interchanged.\n4113.2 \u25a0 Binary Search Trees    \n \n \n  R a n g e  q u e r i e s .  To  i m p l e m e n t  t h e  keys() method that returns the keys in a given \nrange, we begin with a basic recursive BST traversal method, known as  inorder traversal. \nTo  i l l u s t r a te  t h e  m e t h o d , w e  co n s i d e r  t h e  t a s k  o f  p r i n t i n g  a l l  t h e  ke y s  i n  a  B S T  i n  o rd e r. \nTo  d o  s o, p r i n t  a l l ", "start": 423, "end": 424}, "612": {"text": "l l  t h e  ke y s  i n  a  B S T  i n  o rd e r. \nTo  d o  s o, p r i n t  a l l  t h e  ke y s  i n  t h e  l e f t  s u b t re e  ( w h i c h  a re  l e s s  t h a n  t h e  ke y  a t  t h e  ro o t  by  \nde\ufb01nition of BSTs), then print the key at the root, then \nprint all the keys in the right subtree (which are greater \nthan the key at the root by de\ufb01nition of BSTs), as in the \ncode at left. As usual, the description serves as an argu -\nment by induction that this code prints the keys in order. \nTo  i m p l e m e n t  t h e  t wo - a r g u m e n t  keys() method that re-\nturns to a client all the keys in a speci\ufb01ed range, we modi-\nfy this code to add each key that is in the range to a Queue, \nand to skip the recursive calls for subtrees that cannot \ncontain keys in the range. As with BinarySearchST, the \nfact that we gather the keys in a Queue is hidden from the client. The intention is that \nclients should process all the keys in the range of interest using Java\u2019s foreach construct \nrather than needing to know what data structure we use to implement Iterable<Key>. \nAnalysis. How ef\ufb01cient are the order-based operations in BSTs? To study this question, \nwe consider the  tree height (the maximum depth of any node in the tree). Given a tree, \nits height determines the worst-case cost of all BST ", "start": 424, "end": 424}, "613": {"text": "the order-based operations in BSTs? To study this question, \nwe consider the  tree height (the maximum depth of any node in the tree). Given a tree, \nits height determines the worst-case cost of all BST operations (except for range search \nwhich incurs additional cost proportional to the number of keys returned). \nProposition E. In a  BST, all operations take time proportional to the height of the \ntree, in the worst case.\nProof: All of these methods go down one or two paths in the tree. The length of \nany path is no more than the height, by de\ufb01nition.\nWe expect the    tree height (the worst-case cost) to be higher than the average    \n \ninternal \npath length that we de\ufb01ned on page 403 (which averages in the short paths as well), but \nhow much higher? This question may seem to you to be similar to the questions an-\nswered by Proposition C and Proposition D, but it is far more dif\ufb01cult to answer, \ncertainly beyond the scope of this book. The average height of a BST built from random \nkeys was shown to be logarithmic by  J. Robson in 1979, and  L. Devroye later showed \nthat the value approaches 2.99 lg N for large N. Thus, if the insertions in our applica-\ntion are well-described by the random-key model, we are well on the way toward our \ngoal of developing a symbol-table implementation that supports all of these operations \nprivate void print(Node x) \n{\n   if (x == null) return;\n   print(x.left);\n   StdOut.println(x.key);\n   print(x.right); \n}\nPrinting the keys in a BST in order\n412 CHAPTER 3 \u25a0 Searching\n ALGORITHM 3.3  (continued) Range searching in BSTs\npublic Iterable<Key> keys() \n{  return ", "start": 424, "end": 425}, "614": {"text": "print(x.right); \n}\nPrinting the keys in a BST in order\n412 CHAPTER 3 \u25a0 Searching\n ALGORITHM 3.3  (continued) Range searching in BSTs\npublic Iterable<Key> keys() \n{  return keys(min(), max());  }\npublic Iterable<Key> keys(Key lo, Key hi) \n{\n    Queue<Key> queue = new Queue<Key>();\n    keys(root, queue, lo, hi);\n    return queue; \n}\nprivate void keys(Node x, Queue<Key> queue, Key lo, Key hi) \n{\n   if (x == null) return;\n   int cmplo = lo.compareTo(x.key);\n   int cmphi = hi.compareTo(x.key);\n   if (cmplo < 0) keys(x.left, queue, lo, hi);\n   if (cmplo <= 0 && cmphi >= 0) queue.enqueue(x.key);\n   if (cmphi > 0) keys(x.right, queue, lo, hi); \n}\nTo  e n q u e u e  a l l  t h e  ke y s  f ro m  t h e  t re e  ro o te d  a t  a  g ive n  n o d e  t h a t  f a l l  i n  a  g ive n  r a n g e  o n to  a  q u e u e , w e  \n(recursively) enqueue all the keys from the left subtree (if any of them could fall in the range), then \nenqueue the node at the root (if it falls in the range), then (recursively) enqueue all the keys from the \nright subtree (if any of them could fall in the range).\nblack keys are\nin the range\nred keys are used in compares\nbut are not in the range\nA\nC\nE\nH\nL\nM\nP\nR\nS\nX\nsearching ", "start": 425, "end": 425}, "615": {"text": "fall in the range).\nblack keys are\nin the range\nred keys are used in compares\nbut are not in the range\nA\nC\nE\nH\nL\nM\nP\nR\nS\nX\nsearching in the range [F..T]\nRange search in a BST\n4133.2 \u25a0 Binary Search Trees  \nin logarithmic time. We can expect that no path in a tree built from random keys is \nlonger than 3 lg N, but what can we expect if the keys are not random? In the next sec-\ntion, you will see why this question is moot in practice because of balanced BSTs, which \nguarantee that the BST height will be logarithmic regardless of the order in which keys \nare inserted. \nIn summary, BSTs are not dif\ufb01cult to implement and can provide fast search and insert \nfor practical applications of all kinds if the key insertions are well-approximated by the \nrandom-key model. For our examples (and for many practical applications) BSTs make \nthe difference between being able to accomplish the task at hand and not being able to \ndo so. Moreover, many programmers choose BSTs for symbol-table implementations \nbecause they also support fast rank, select, delete, and range query operations. Still, as \nwe have emphasized, the bad worst-case performance of BSTs may not be tolerable in \nsome situations. Good performance of the basic BST implementation is dependent on \nthe keys being suf\ufb01ciently similar to random keys that the tree is not likely to contain \nmany long paths. With quicksort, we were able to randomize; with a symbol-table API, \nwe do not have that freedom, because the client controls the mix of operations. Indeed, \nthe worst-case behavior is not unlikely in practice\u2014it arises when a client inserts keys \nin order or in reverse order, a sequence of operations that some client certainly might \nattempt ", "start": 425, "end": 426}, "616": {"text": "because the client controls the mix of operations. Indeed, \nthe worst-case behavior is not unlikely in practice\u2014it arises when a client inserts keys \nin order or in reverse order, a sequence of operations that some client certainly might \nattempt in the absence of any explicit warnings to avoid doing so. This possibility is a \nprimary reason to seek better algorithms and data structures, which we consider next. \nalgorithm\n(data structure)\nworst-case cost\n(after N inserts) \naverage-case cost\n(after N random inserts) efficiently\nsupport ordered\noperations?search insert search hit insert\nsequential search\n(unordered linked list) N N N/2 N no\nbinary search\n(ordered array) lg N N lg N N/2 yes\nbinary tree search\n(BST) N N 1.39 lg N 1.39 lg N yes\nCost summary for basic symbol-table implementations (updated)\n414 CHAPTER 3 \u25a0 Searching\n Q&A\n \nQ.  I\u2019ve seen BSTs before, but not using recursion. What are the tradeoffs?\nA. Generally,  recursive implementations are a bit easier to verify for correctness; non-\nrecursive implementations are a bit more ef\ufb01cient. See Exercise 3.2.13 for an imple-\nmentation of get(), the one case where you might notice the improved ef\ufb01ciency. If \ntrees are unbalanced, the depth of the  function-call stack could be a problem in a recur-\nsive implementation. Our primary reason for using recursion is to ease the transition \nto the balanced BST implementations of the next section, which de\ufb01nitely are easier to \nimplement and debug with recursion.\nQ. Maintaining the node count \ufb01eld in Node seems to require a lot of code. Is it really \nnecessary? Why not maintain a single instance variable containing the number of nodes \nin the tree to implement the size() client method?\nA. The    rank() and select() ", "start": 426, "end": 427}, "617": {"text": "to require a lot of code. Is it really \nnecessary? Why not maintain a single instance variable containing the number of nodes \nin the tree to implement the size() client method?\nA. The    rank() and select() methods need to have the size of the subtree rooted at \neach node. If you are not using these ordered operations, you can streamline the code \nby eliminating this \ufb01eld (see Exercise 3.2.12). Keeping the node count correct for all \nnodes is admittedly error-prone, but also a good check for debugging. Y ou might also \nuse a recursive method to implement size() for clients, but that would take linear\ntime to count all the nodes and is a dangerous choice because you might experience \npoor performance in a client program, not realizing that such a simple operation is so \nexpensive.\n4153.2 \u25a0 Binary Search Trees\n EXERCISES\n \n \n \n \n3.2.1 Draw the BST that results when you insert the keys E A S Y Q U E S T I O N, \nin that order (associating the value i with the ith key, as per the convention in the text) \ninto an initially empty tree. How many compares are needed to build the tree?\n3.2.2 Inserting the keys in the order A X C S E R H into an initially empty BST gives \na worst-case tree where every node has one null link, except one at the bottom, which \nhas two null links.  Give \ufb01ve other orderings of these keys that produce worst-case trees.\n3.2.3 Give \ufb01ve orderings of the keys A X C S E R H that, when inserted into an initially \nempty BST, produce the best-case tree.\n3.2.4 Suppose that a certain BST has keys that are integers between 1 and 10, and we \nsearch for 5. Which sequence below cannot be ", "start": 427, "end": 428}, "618": {"text": "initially \nempty BST, produce the best-case tree.\n3.2.4 Suppose that a certain BST has keys that are integers between 1 and 10, and we \nsearch for 5. Which sequence below cannot be the sequence of keys examined?\na. 10, 9, 8, 7, 6, 5\nb. 4, 10, 8, 7, 53\nc. 1, 10, 2, 9, 3, 8, 4, 7, 6, 5\nd. 2, 7, 3, 8, 4, 5\ne. 1, 2, 10, 4, 8, 5 \n3.2.5 Suppose that we have an estimate ahead of time of how often search keys are \nto be accessed in a BST, and the freedom to insert them in any order that we desire.   \nShould the keys be inserted into the tree in increasing order, decreasing order of likely \nfrequency of access, or some other order? Explain your answer.\n3.2.6 Add to BST a method height() that computes the height of the tree. Develop two \nimplementations: a recursive method (which takes linear time and space proportional \nto the height), and a method like size() that adds a \ufb01eld to each node in the tree (and \ntakes linear space and constant time per query).\n3.2.7 Add to BST a recursive method avgCompares() that computes the average num-\nber of compares required by a random search hit in a given BST (the internal path \nlength of the tree divided by its size, plus one). Develop two implementations: a re-\ncursive method (which takes linear time and space proportional to the height), and a \nmethod like size() that adds a \ufb01eld to each node in the tree (and ", "start": 428, "end": 428}, "619": {"text": "size, plus one). Develop two implementations: a re-\ncursive method (which takes linear time and space proportional to the height), and a \nmethod like size() that adds a \ufb01eld to each node in the tree (and takes linear space and \nconstant time per query).\n3.2.8 Write a static method optCompares() that takes an integer argument N and com-\nputes the number of compares required by a random search hit in an optimal (perfectly \n416 CHAPTER 3 \u25a0 Searching\n balanced) BST, where all the null links are on the same level if the number of links is a \npower of 2 or on one of two levels otherwise.\n3.2.9 Draw all the different BST shapes that can result when N keys are inserted into an \ninitially empty tree, for N = 2, 3, 4, 5, and 6. \n3.2.10  Write a test client TestBST.java for use in testing the implementations of \nmin(), max(), floor(), ceiling(), select(), rank(), delete(), deleteMin(), \ndeleteMax(), and keys() that are given in the text. Start with the standard index-\ning client given on page 370. Add code to take additional command-line arguments, as \nappropriate.\n3.2.11 How many binary tree shapes of N nodes are there with height N?  How many \ndifferent ways are there to insert N distinct keys into an initially empty BST that result \nin a tree of height N? (See Exercise 3.2.2.)\n3.2.12  Develop a BST implementation that omits rank() and select() and does not \nuse a count \ufb01eld in Node.\n3.2.13  Give  nonrecursive implementations of get() and put() for BST.\nPartial solution : Here is an implementation of get():\npublic Value get(Key key) \n{\n   Node x = root;\n   while ", "start": 428, "end": 429}, "620": {"text": "Node.\n3.2.13  Give  nonrecursive implementations of get() and put() for BST.\nPartial solution : Here is an implementation of get():\npublic Value get(Key key) \n{\n   Node x = root;\n   while (x != null)\n   {\n      int cmp = key.compareTo(x.key);\n      if (cmp == 0) return x.val;\n      else if (cmp < 0) x = x.left;\n      else if (cmp > 0) x = x.right;\n   }\n   return null; \n}\nThe implementation of put() is more complicated because of the need to save a point-\ner to the parent node to link in the new node at the bottom. Also, you need a separate \npass to check whether the key is already in the table because of the need to update the \ncounts. Since there are many more searches than inserts in performance-critical imple-\nmentations, using this code for get() is justi\ufb01ed; the corresponding change for put()\nmight not be noticed.\n4173.2 \u25a0 Binary Search Trees\n  \n3.2.14 Give nonrecursive implementations of min(), max(), floor(), ceiling(), \nrank(), and select().\n3.2.15 Give the sequences of nodes examined when the methods in BST are used to \ncompute each of the following quantities for the tree drawn at right.\na. floor(\"Q\")\nb. select(5)\nc. ceiling(\"Q\")\nd. rank(\"J\")\ne. size(\"D\", \"T\")\n f. keys(\"D\", \"T\")\nexternal path length of a tree to be the sum of the number of nodes on 3.2.16  De\ufb01ne the     \nthe paths from the root to all null links. Prove that the difference between the external \nand internal path lengths in any binary tree with N nodes is 2N (see Proposition C).\n3.2.17 Draw the sequence of ", "start": 429, "end": 430}, "621": {"text": "from the root to all null links. Prove that the difference between the external \nand internal path lengths in any binary tree with N nodes is 2N (see Proposition C).\n3.2.17 Draw the sequence of BSTs that results when you delete the keys from the tree \nof Exercise 3.2.1, one by one, in the order they were inserted.\n3.2.18 Draw the sequence of BSTs that results when you delete the keys from the tree \nof Exercise 3.2.1, one by one, in alphabetical order.\n3.2.19 Draw the sequence of BSTs that results when you delete the keys from the tree \nof Exercise 3.2.1, one by one, by successively deleting the key at the root.\n3.2.20 Prove that the running time of the two-argument keys() in a BST with N nodes \nis at most proportional to the tree height plus the number of keys in the range.\n3.2.21 Add a BST method randomKey() that returns a random key from the symbol \ntable in time proportional to the tree height, in the worst case.\n3.2.22 Prove that if a node in a BST has two children, its successor has no left child and \nits predecessor has no right child.\n3.2.23 Is delete() commutative? (Does deleting x, then y give the same result as de-\nleting y, then x?)\n3.2.24 Prove that no compare-based algorithm can build a BST using fewer than \nlg(N !) ~ N lg N compares.\nEXERCISES  (continued)\nA\nD\nE\nJ\nM\nQ\nS\nT\n418 CHAPTER 3 \u25a0 Searching\n CREATIVE PROBLEMS\n \n3.2.25  Perfect balance. Write a program that inserts a set of keys into an initially emp-\nty BST such that ", "start": 430, "end": 431}, "622": {"text": "(continued)\nA\nD\nE\nJ\nM\nQ\nS\nT\n418 CHAPTER 3 \u25a0 Searching\n CREATIVE PROBLEMS\n \n3.2.25  Perfect balance. Write a program that inserts a set of keys into an initially emp-\nty BST such that the tree produced is equivalent to binary search, in the sense that the \nsequence of compares done in the search for any key in the BST is the same as the se-\nquence of compares used by binary search for the same set of keys.\n3.2.26  Exact probabilities. Find the probability that each of the trees in Exercise 3.2.9 \nis the result of inserting N random distinct elements into an initially empty tree. \n3.2.27  Memory usage. Compare the memory usage of BST with the memory usage of \nBinarySearchST and SequentialSearchST for N key-value pairs, under the assump-\ntions described in Section 1.4 (see Exercise 3.1.21). Do not count the memory for \nthe keys and values themselves, but do count references to them. Then draw a diagram \nthat depicts the precise memory usage of a BST with String keys and Integer values \n(such as the ones built by FrequencyCounter), and then estimate the memory usage \n(in bytes) for the BST built when FrequencyCounter uses BST for Ta l e  of  Tw o  Ci t i e s.\n3.2.28  Sofware  caching. Modify BST to keep the most recently accessed Node in an \ninstance variable so that it can be accessed in constant time if the next put() or get()\nuses the same key (see Exercise 3.1.25).\n3.2.29  Binary tree check. Write a recursive method isBinaryTree() that takes a Node\nas argument and returns true if the subtree count \ufb01eld N is consistent in the data struc-\nture rooted at that node, ", "start": 431, "end": 431}, "623": {"text": "3.1.25).\n3.2.29  Binary tree check. Write a recursive method isBinaryTree() that takes a Node\nas argument and returns true if the subtree count \ufb01eld N is consistent in the data struc-\nture rooted at that node, false otherwise. Note : This check also ensures that the data \nstructure has no cycles and is therefore a binary tree (!). \n3.2.30  Order check. Write a recursive method isOrdered() that takes a Node and two \nkeys min and max as arguments and returns true if all the keys in the tree are between \nmin and max; min and max are indeed the smallest and largest keys in the tree, respec-\ntively; and the BST ordering property holds for all keys in the tree; false otherwise. \n3.2.31  Equal key check. Write a method hasNoDuplicates() that takes a Node as ar-\ngument and returns true if there are no equal keys in the binary tree rooted at the argu-\nment node, false otherwise. Assume that the test of the previous exercise has passed.\n3.2.32      Certi\ufb01cation. Write a method isBST() that takes a Node as argument and re-\nturns true if the argument node is the root of a binary search tree, false otherwise. \nHint : This task is also more dif\ufb01cult than it might seem, because the order in which you \ncall the methods in the previous three exercises is important. \n4193.2 \u25a0 Binary Search Trees\n Solution : \nprivate boolean isBST() \n{  \n   if (!isBinaryTree(root)) return false;\n   if (!isOrdered(root, min(), max())) return false;\n   if (!hasNoDuplicates(root)) return false;\n   return true; \n}\n3.2.33  Select/rank check. Write a method that checks, for all i from 0 to size()-1, ", "start": 431, "end": 432}, "624": {"text": "false;\n   if (!hasNoDuplicates(root)) return false;\n   return true; \n}\n3.2.33  Select/rank check. Write a method that checks, for all i from 0 to size()-1,  \nwhether i is equal to rank(select(i)) and, for all keys in the BST, whether key is \nequal to select(rank(key)). \n3.2.34    Threading. Yo u r  g o a l  i s  t o  s u p p o r t  a n  e x t e n d e d  A P I  ThreadedST that supports \nthe following additional operations in constant time:\nKey next(Key key) key that follows key (null if key is the maximum)\nKey prev(Key key) key that precedes key (null if key is the minimum)\nTo  d o  s o, a d d  \ufb01 e l d s  pred and succ to Node that contain links to the predecessor and \nsuccessor nodes, and modify put(), deleteMin(), deleteMax(), and delete() to \nmaintain these \ufb01elds.\n3.2.35    Re\ufb01ned analysis. Re\ufb01ne the mathematical model to better explain the experi -\nmental results in the table given in the text. Speci\ufb01cally, show that the average number \nof compares for a successful search in a tree built from random keys approaches the \nlimit 2 ln N /H11001 2/H9253 \u2013 3 /H11015 1.39 lg N \u2013 1.85 as N increases, where /H9253 /H11005 .57721... is Euler\u2019s \nconstant. Hint : Referring to the quicksort analysis in Section 2.3, use the fact that the \nintegral of 1/x approaches ln N /H11001 /H9253.\n3.2.36  Iterator. ", "start": 432, "end": 432}, "625": {"text": ": Referring to the quicksort analysis in Section 2.3, use the fact that the \nintegral of 1/x approaches ln N /H11001 /H9253.\n3.2.36  Iterator. Is it possible to write a nonrecursive version of keys() that uses space \nproportional to the tree height (independent of the number of keys in the range)?\n3.2.37    Level-order traversal. Write a method printLevel() that takes a Node as argu-\nment and prints the keys in the subtree rooted at that node in level order (in order of \ntheir distance from the root, with nodes on each level in order from left to right). Hint : \nUse a Queue.\nCREATIVE PROBLEMS  (continued)\n420 CHAPTER 3 \u25a0 Searching\n  3.2.38    Tree drawing. Add a method draw() to BST that draws BST \ufb01gures in the style \nof the text. Hint : Use instance variables to hold node coordinates, and use a recursive \nmethod to set the values of these variables.\n4213.2 \u25a0 Binary Search Trees\n EXPERIMENTS\n3.2.39    Average case. Run empirical studies to estimate the average and standard de -\nviation of the number of compares used for search hits and for search misses in a BST \nbuilt by running 100 trials of the experiment of inserting N random keys into an ini -\ntially empty tree, for N = 10 4, 10 5, and 10 6. Compare your results against the formula \nfor the average given in Exercise 3.2.35.\n3.2.40  Height. Run empirical studies to estimate average BST height by running 100 \ntrials of the experiment of inserting N random keys into an initially empty tree, for N = \n104, 105, and 10 6. Compare your results against the 2.99 lg ", "start": 432, "end": 434}, "626": {"text": "by running 100 \ntrials of the experiment of inserting N random keys into an initially empty tree, for N = \n104, 105, and 10 6. Compare your results against the 2.99 lg N estimate that is described \nin the text.\n3.2.41  Array representation. Develop a BST implementation that represents the BST \nwith three arrays (preallocated to the maximum size given in the constructor): one with \nthe keys, one with array indices corresponding to left links, and one with array indices \ncorresponding to right links. Compare the performance of your program with that of \nthe standard implementation.\n3.2.42      Hibbard deletion degradation. Write a program that takes an integer N from the \ncommand line, builds a random BST of size N, then enters into a loop where it deletes \na random key (using the code delete(select(StdRandom.uniform(N)))) and then \ninserts a random key, iterating the loop N 2 times. After the loop, measure and print the \naverage length of a path in the tree (the internal path length divided by N, plus 1).  Run \nyour program for N = 102, 103, and 10 4 to test the somewhat counterintuitive hypoth-\nesis that this process increases the average path length of the tree to be proportional to \nthe square root of N. Run the same experiments for a delete() implementation that \nmakes a random choice  whether to use the predecessor or the successor node.\n3.2.43  Put/get ratio. Determine empirically the ratio of the amount of time that \nBST spends on put() operations to the time that it spends on get() operations when \nFrequencyCounter is used to \ufb01nd the frequency of occurrence of values in 1 million \nrandomly-generated integers.\n3.2.44  Cost plots. Instrument BST so that you can produce plots like the ones in this \nsection ", "start": 434, "end": 434}, "627": {"text": "\nFrequencyCounter is used to \ufb01nd the frequency of occurrence of values in 1 million \nrandomly-generated integers.\n3.2.44  Cost plots. Instrument BST so that you can produce plots like the ones in this \nsection showing the cost of each put() operation during the computation (see Exer-\ncise 3.1.38).\n3.2.45  Actual timings. Instrument FrequencyCounter to use Stopwatch and StdDraw\nto make a plot where the x axis is the number of calls on get() or put() and the  y axis \n422 CHAPTER 3 \u25a0 Searching\n  \nis the total running time, with a point plotted of the cumulative time after each call. \nRun your program for Ta l e  of  Tw o  Ci t i e s using SequentialSearchST and again using \nBinarySearchST and again using BST and discuss the results. Note : Sharp jumps in \nthe curve may be explained by   caching, which is beyond the scope of this question (see \nExercise 3.1.39).\n3.2.46  Crossover to binary search trees. Find the values of N for which using a binary \nsearch tree to build a symbol table of N random double keys becomes 10, 100, and \n1,000 times faster than binary search.  Predict the values with analysis and verify them \nexperimentally.\n3.2.47  Average search time. Run empirical studies to compute the average and stan -\ndard deviation of the average length of a path to a random node (internal path length \ndivided by tree size, plus 1) in a BST built by insertion of N random keys into an initially \nempty tree, for N from 100 to 10,000. Do 1,000 trials for each tree size. Plot the results in \na Tufte plot, like the one at the bottom of this page, \ufb01t with a curve plotting ", "start": 434, "end": 435}, "628": {"text": "100 to 10,000. Do 1,000 trials for each tree size. Plot the results in \na Tufte plot, like the one at the bottom of this page, \ufb01t with a curve plotting the function \n1.39 lg N \u2013 1.85 (see Exercise 3.2.35 and Exercise 3.2.39).\n1.39 lg N \u2212 1.85\nAverage path length to a random node in a BST built from random keys\n20\n0\n10000operations\ncompares\n16\n100\n4233.2 \u25a0 Binary Search Trees\n 3.3    BALANCED SEARCH TREES\nThe algorithms in the previous section work well for a wide variety of applications, but \nthey have poor worst-case performance.  We introduce in this section a type of binary \nsearch tree where costs are guaranteed to be logarithmic, no matter what sequence of \nkeys is used to construct them. Ideally, we would like to keep our binary search trees \nperfectly balanced. In an N-node tree, we would like the height to be ~lg N so that we \ncan guarantee that all searches can be completed in ~lg N compares, just as for binary \nsearch (see Proposition B). Unfortunately, maintaining perfect balance for dynamic \ninsertions is too expensive. In this section, we consider a data structure that slightly re-\nlaxes the perfect balance requirement to provide guaranteed logarithmic performance \nnot just for the insert and search operations in our symbol-table API but also for all of \nthe ordered operations (except range search).\n  2 - 3  s e a r c h  t r e e s  The primary step to get the \ufb02exibility that we need to guarantee \nbalance in search trees is to allow the nodes in our trees to hold more than one key.  Spe-\nci\ufb01cally, referring to the nodes in a standard BST as ", "start": 435, "end": 436}, "629": {"text": "\ufb02exibility that we need to guarantee \nbalance in search trees is to allow the nodes in our trees to hold more than one key.  Spe-\nci\ufb01cally, referring to the nodes in a standard BST as 2-nodes (they hold two links and \none key), we now also allow 3-nodes, which hold three links and two keys. Both 2-nodes \nand 3-nodes have one link for each of the intervals  subtended by its keys.\nDefinition. A  2-3 search tree is a tree that is either empty or \n\u25a0 A  2-node, with one key (and associated value) and two links, \na left link to a 2-3 search tree with smaller keys, and a right \nlink to a 2-3 search tree with larger keys\n\u25a0 A 3-node, with two keys (and associated values) and three\nlinks, a left link to a 2-3 search tree with smaller keys, a mid-\ndle link to a 2-3 search tree with keys between the node\u2019s \nkeys, and a right link to a 2-3 search tree with larger keys\nAs usual, we refer to a link to an empty tree as a null link.\nA  perfectly balanced   2-3 search tree is one whose null links are all the same distance \nfrom the root. T o be concise, we use the term 2-3 tree to refer to a perfectly balanced 2-3 \nsearch tree (the term denotes a more general structure in other contexts). Later, we shall \nsee ef\ufb01cient ways to de\ufb01ne and implement the basic operations on 2-nodes, 3-nodes, \nand 2-3 trees; for now, let us assume that we can manipulate them conveniently and see \nhow we can use them as search trees. \nE J\nH ", "start": 436, "end": 436}, "630": {"text": "2-nodes, 3-nodes, \nand 2-3 trees; for now, let us assume that we can manipulate them conveniently and see \nhow we can use them as search trees. \nE J\nH L\n2-node3-node\nnull link\nM\nR\nP S XA C\nAnatomy of a 2-3 search tree\n424\n  S e a r c h .  The search algorithm for keys in a 2-3 tree directly generalizes the search al-\ngorithm for BSTs.  T o determine whether a key is in the tree, we compare it against the \nkeys at the root. If it is equal to any of them, we have a search hit; otherwise, we follow \nthe link from the root to the subtree corresponding to the interval of key values that \ncould contain the search key. If that link is null, we have a search miss; otherwise we \nrecursively search in that subtree. \n I n s e r t  i n t o  a  2 - n o d e .  To  i n s e r t  a  n e w  n o d e  i n  a  2 - 3  \ntree, we might do an unsuccessful search and then \nhook on the node at the bottom, as we did with \nBSTs, but the new tree would not remain perfectly \nbalanced. The primary reason that 2-3 trees are \nuseful is that we can do insertions and still main-\ntain perfect balance. It is easy to accomplish this \ntask if the node at which the search terminates is \na 2-node: we just replace the node with a 3-node \ncontaining its key and the new key to be inserted. If \nthe node where the search terminates is a 3-node, \nwe have more work to do. \nsearch for K ends here\nreplace 2-node ", "start": 436, "end": 437}, "631": {"text": "3-node \ncontaining its key and the new key to be inserted. If \nthe node where the search terminates is a 3-node, \nwe have more work to do. \nsearch for K ends here\nreplace 2-node with\nnew 3-node containing K\nE J\nH L\nM\nR\nP S XA C\nE J\nH\nM\nR\nP S XK LA C\ninserting K\nInsert into a 2-node\nfound H so return value (search hit)\nH is less than M so\nlook to the left\nH is between E and J so\nlook in the middle\nB is between A and C so look in the middle\nB is less than M so\nlook to the left\nB is less than E\nso look to the left\nlink is null so B is not in the tree (search miss)\nE J\nH L\nM\nR\nP S XA C\nE J\nH L\nM\nR\nP S XA C\nE J\nH L\nM\nR\nP S XA C\nE J\nH L\nM\nR\nP S XA C\nE J\nH L\nM\nR\nP S XA C\nE J\nH L\nM\nR\nP S XA C\nsuccessful search for H unsuccessful search for B\nSearch hit (left) and search miss (right) in a 2-3 tree\n4253.3 \u25a0 Balanced Search Trees\n Insert into a tree consisting of a single 3-node. As a \ufb01rst warmup before considering \nthe general case, suppose that we want to insert into a tiny 2-3 tree consisting of just a \nsingle 3-node. Such a tree has two keys, but no room for a new key in its one node. T o be \nable to perform the insertion, we temporarily put the ", "start": 437, "end": 438}, "632": {"text": "2-3 tree consisting of just a \nsingle 3-node. Such a tree has two keys, but no room for a new key in its one node. T o be \nable to perform the insertion, we temporarily put the new key into a 4-node, a natural \nextension of our node type that has three keys and four links. Creating the 4-node is \nconvenient because it is easy to convert it into a 2-3 tree made up of three 2-nodes, one \nwith the middle key (at the root), one with the smallest of \nthe three keys (pointed to by the left link of the root), and \none with the largest of the three keys (pointed to by the \nright link of the root). Such a tree is a 3-node BST and also \na perfectly balanced 2-3 search tree, with all the null links \nat the same distance from the root. Before the insertion, the \nheight of the tree is 0; after the insertion, the height of the \ntree is 1. This case is simple, but it is worth considering be-\ncause it illustrates height growth in 2-3 trees.\nInsert into a 3-node whose parent is a 2-node. As a second warmup, suppose that the \nsearch ends at a 3-node at the bottom whose parent is a 2-node. In this case, we can still \nmake room for the new key while maintaining perfect balance in the tree , by making a \ntemporary 4-node as just described, then splitting the \n4-node as just described, but then, instead of creat-\ning a new node to hold the middle key, moving the \nmiddle key to the node\u2019s parent. Y ou can think of the \ntransformation as replacing the link to the old 3-node \nin the parent by the middle key with links on either \nside to the new 2-nodes. ", "start": 438, "end": 438}, "633": {"text": "key to the node\u2019s parent. Y ou can think of the \ntransformation as replacing the link to the old 3-node \nin the parent by the middle key with links on either \nside to the new 2-nodes. By our assumption, there \nis room for doing so in the parent: the parent was a \n2-node (with one key and two links) and becomes \na 3-node (with two keys and three links). Also, this \ntransformation does not affect the de\ufb01ning properties \nof (perfectly balanced) 2-3 trees. The tree remains or-\ndered because the middle key is moved to the parent, \nand it remains perfectly balanced: if all null links are \nthe same distance from the root before the insertion, \nthey are all the same distance from the root after the \ninsertion. Be certain that you understand this trans-\nformation\u2014it is the crux of 2-3 tree dynamics.  \nmake a 4-node\nno room for S\nsplit 4-node into\nthis 2-3 tree\nA E\n A E S \nA\nE\nS\ninserting S\nInsert into a single 3-node\nsplit 4-node into two 2-nodes\npass middle key to parent\nreplace 3-node with\ntemporary 4-node\ncontaining Z\nreplace 2-node\nwith new 3-node\ncontaining\nmiddle key\nS X Z\nS Z\nE J\nH L\nL\nM\nR\nPA C\nsearch for Z ends\nat this 3-nodeE J\nH L\nM\nR\nP S XA C\nE J\nH\nM\nP\nR X\nA C\ninserting Z\nInsert into a 3-node whose parent is a 2-node\n426 CHAPTER 3 \u25a0 Searching\n Insert into a 3-node whose parent is a 3-node. ", "start": 438, "end": 439}, "634": {"text": "J\nH\nM\nP\nR X\nA C\ninserting Z\nInsert into a 3-node whose parent is a 2-node\n426 CHAPTER 3 \u25a0 Searching\n Insert into a 3-node whose parent is a 3-node. Now \nsuppose that the search ends at a node whose parent is \na 3-node. Again, we make a temporary 4-node as just \ndescribed, then split it and insert its middle key into \nthe parent. The parent was a 3-node, so we replace it \nwith a temporary new 4-node containing the middle \nkey from the 4-node split. Then, we perform precisely \nthe same transformation on that node . That is, we split \nthe new 4-node and insert its middle key into its par-\nent. Extending to the general case is clear: we con-\ntinue up the tree, splitting 4-nodes and inserting their \nmiddle keys in their parents until reaching a 2-node, \nwhich we replace with a 3-node that does not need to \nbe further split, or until reaching a 3-node at the root. \nSplitting the root. If we have 3-nodes along the \nwhole path from the insertion point to the root, we \n \nend up with a tempo-\nrary 4-node at the root. \nIn this case we can pro-\nceed in precisely the \nsame way as for inser -\ntion into a tree consist-\ning of a single 3-node. \nWe split the tempo -\nrary 4-node into three \n2-nodes, increasing the height of the tree by 1. Note that \nthis last transformation preserves perfect balance be-\ncause it is performed at the root.\nLocal transformations. Splitting a temporary 4-node \nin a 2-3 tree involves one of six transformations, sum-\nmarized at the bottom of the next page. The 4-node may ", "start": 439, "end": 439}, "635": {"text": "performed at the root.\nLocal transformations. Splitting a temporary 4-node \nin a 2-3 tree involves one of six transformations, sum-\nmarized at the bottom of the next page. The 4-node may \nbe the root; it may be the left child or the right child of a \n2-node; or it may be the left child, middle child, or right \nchild of a 3-node. The basis of the 2-3 tree insertion al -\ngorithm is that all of these transformations are purely lo-\ncal: no part of the tree needs to be examined or modi\ufb01ed \nother than the speci\ufb01ed nodes and links. The number of \nsplit 4-node into two 2-nodes\npass middle key to parent\nsplit 4-node into two 2-nodes\npass middle key to parent\nadd middle key E to 2-node\nto make new 3-node\nadd middle key C to 3-node\nto make temporary 4-node\nadd new key D to 3-node\nto make temporary 4-node\nA C D\nA D\nsearch for D ends\nat this 3-node E J\nH L\nM\nR\nP S XA C\nE J\nH L\nM\nR\nP S X\nC E J\nH L\nM\nR\nP S X\nA D H L\nC J R\nP S X\nE M\ninserting D\nInsert into a 3-node whose parent is a 3-node\nsplit 4-node into two 2-nodes\npass middle key to parent\nsplit 4-node into\nthree 2-nodes\nincreasing tree\nheight by 1\nadd middle key C to 3-node\nto make temporary 4-node\nA C D\nA D\nsearch for D ends\nat this 3-node E J\nH ", "start": 439, "end": 439}, "636": {"text": "2-nodes\nincreasing tree\nheight by 1\nadd middle key C to 3-node\nto make temporary 4-node\nA C D\nA D\nsearch for D ends\nat this 3-node E J\nH LA C\nE J\nH L\nC E J\nH L\nA D H L\nC J\nE\nadd new key D to 3-node\nto make temporary 4-node\ninserting D\nSplitting the root\n4273.3 \u25a0 Balanced Search Trees\n links changed for each trans-\nformation is bounded by a \nsmall constant. In particular, \nthe transformations are effec-\ntive when we \ufb01nd the speci\ufb01ed \npatterns anywhere in the tree, \nnot just at the bottom. Each of \nthe transformations passes up \none of the keys from a 4-node \nto that node\u2019s parent in the tree \nand then restructures links ac-\ncordingly, without touching \nany other part of the tree.\n G l o b a l  p r o p e r t i e s .  Moreover, \nthese local transformations \npreserve the global properties that the tree is ordered and perfectly balanced: the num-\nber of links on the path from the root to any null link is the same. For reference, a com-\nplete diagram illustrating this point for the case that the 4-node is the middle child of a \n3-node is shown above. If the length of every path from a root to a null link is h before \nthe transformation, then it is h after the transformation. Each transformation preserves \nthis property, even while splitting the 4-node into two 2-nodes and while changing the \nparent from a 2-node to a 3-node or from a 3-node into a temporary 4-node. When \nthe root splits into three 2-nodes, the length of every path from the root to ", "start": 439, "end": 440}, "637": {"text": "\nparent from a 2-node to a 3-node or from a 3-node into a temporary 4-node. When \nthe root splits into three 2-nodes, the length of every path from the root to a null link \nincreases by 1. If you are not fully convinced, work Exercise 3.3.7, which asks you to \n... ... ... ... ......\n... ... ... ... ...\nb c d\na e\nbetween\na and b\nless\nthan a\nbetween\nb and c\nbetween\nd and e\ngreater\nthan e\nbetween\nc and d\nbetween\na and b\nless\nthan a\nbetween\nb and c\nbetween\nd and e\ngreater\nthan e\nbetween\nc and d\nb\n...\nd\na c e\nSplitting a  4-node is a local transformation\nthat preserves order and perfect balance \nb\nright\nmiddle\nleft\nright\nleft\nb db c d\na ca\na b c\nd\nca\nb d\na b c\nca\nroot\nparent is a 2-node\nparent is a 3-node\nSplitting a temporary 4-node in a 2-3 tree (summary) \nc e\nb d\nc d e\na b\nb c d\na e\na b d\na c e\na b c\nd e\nca\nb d e\n428 CHAPTER 3 \u25a0 Searching\n  extend the diagrams at the top of the previous page for the other \ufb01ve cases to illustrate \nthe same point. Understanding that every local transformation preserves order and \nperfect balance in the whole tree is the key to understanding the algorithm.  \nUnlike standard BSTs, which grow down from the top, 2-3 trees grow up from the \nbottom. If you take the time to carefully study the \ufb01gure on the next ", "start": 440, "end": 441}, "638": {"text": "to understanding the algorithm.  \nUnlike standard BSTs, which grow down from the top, 2-3 trees grow up from the \nbottom. If you take the time to carefully study the \ufb01gure on the next page, which gives \nthe sequence of 2-3 trees that is produced by our standard indexing test client and the \nsequence of 2-3 trees that is produced when the same keys are inserted in increasing or-\nder, you will have a good understanding of the way that 2-3 trees are built. Recall that in \na BST, the increasing-order sequence for 10 keys results in a worst-case tree of height 9. \nIn the 2-3 trees, the height is 2. \nThe preceding description is suf\ufb01cient to de\ufb01ne a symbol-table implementation \nwith 2-3 trees as the underlying data structure. Analyzing 2-3 trees is different from \nanalyzing BSTs because our primary interest is in worst-case performance, as opposed \nto average-case performance (where we analyze expected performance under the ran -\ndom-key model). In symbol-table implementations, we normally have no control over \nthe order in which clients insert keys into the table and worst-case analysis is one way \nto provide performance guarantees. \nProposition F.    Search and insert operations in a 2-3 tree with N keys are guaran-\nteed to visit at most lg N nodes.\nProof: The    height of an N-node 2-3 tree is between  \u23a3log3 N\u23a6 = \u23a3(lg N)/(lg 3)\u23a6 (if \nthe tree is all 3-nodes) and  \u23a3lg N\u23a6 (if the tree is all 2-nodes) (see Exercise 3.3.4). \nThus, we can guarantee good worst-case performance with 2-3 trees. ", "start": 441, "end": 441}, "639": {"text": "\u23a3lg N\u23a6 (if the tree is all 2-nodes) (see Exercise 3.3.4). \nThus, we can guarantee good worst-case performance with 2-3 trees. The amount of \ntime required at each node by each of the operations is bounded by a constant, and \nboth operations examine nodes on just one path, so the total cost of any search or insert \nis guaranteed to be logarithmic. As you can see from comparing the 2-3 tree depicted \nat the bottom of page 431 with the BST formed from the same keys on page 405, a perfectly \nbalanced 2-3 tree strikes a remarkably \ufb02at posture. For example, the height of a 2-3 \ntree that contains 1 billion keys is between 19 and 30. It is quite remarkable that we can \nguarantee to perform arbitrary search and insertion operations among 1 billion keys by \nexamining at most 30 nodes.\nHowever, we are only part of the way to an implementation.  Although it is possible \nto write code that performs transformations on distinct data types representing 2- and \n3-nodes, most of the tasks that we have described are inconvenient to implement in \n4293.3 \u25a0 Balanced Search Trees\n S\nS\nS\nPA\nE\nA\nE S\nR S\nE\nA S\nC\nA E\nM\nE R\nH P\nH\nE\nR S\nS X\nA C\nE R\nA C\nH\nE R\nA C\nA\nL\nC\nA\nA C\nE H\nS X\nE R\nA C H M\nS XA C\nH\nC M\nE L\nA\nH\nC M\nE L\nM\nE R\nP S XA C H L\nA E L M\nP R\nP ", "start": 441, "end": 442}, "640": {"text": "H M\nS XA C\nH\nC M\nE L\nA\nH\nC M\nE L\nM\nE R\nP S XA C H L\nA E L M\nP R\nP S X\nC H\nA E\nC H\nM R\nH\nC\nLA E\nP\nM R\nH\nC\nLA E\n standard indexing client  same keys in increasing order\nE\nA\nR\nC\nH\nX\nM\nP\nL\nC\nE\nH\nL\nM\nP\nR\nS\nX\ninsert S insert A\n2-3 construction traces\n430 CHAPTER 3 \u25a0 Searching\n this direct representation because there are numerous different cases to be handled. \nWe would need to maintain two different t y pes of  nodes, compare search keys against \neach of the keys in the nodes,  copy links and other information from one type of node \nto another, convert nodes from one type to another, and so forth.  Not only is there a \nsubstantial amount of code involved, but the overhead incurred could make the algo-\nrithms slower than standard BST search and insert.  The primary purpose of balancing \nis to provide insurance against a bad worst case, but we would prefer the overhead cost \nfor that insurance to be low. Fortunately, as you will see, we can do the transformations \nin a uniform way using little overhead. \n         \nTypical 2-3 tree built from random keys\n4313.3 \u25a0 Balanced Search Trees\n  \n   R e d - b l a c k  B S T s  The insertion algorithm for 2-3 trees just described is not dif\ufb01cult \nto understand; now, we will see that it is also not dif\ufb01cult to implement. We will con -\nsider a simple representation known as a red-black BST that leads to a natural imple-\nmentation. ", "start": 442, "end": 444}, "641": {"text": "dif\ufb01cult \nto understand; now, we will see that it is also not dif\ufb01cult to implement. We will con -\nsider a simple representation known as a red-black BST that leads to a natural imple-\nmentation. In the end, not much code is required, but understanding how and why the \ncode gets the job done requires a careful look.\nEncoding 3-nodes. The basic idea behind red-black \nBSTs is to encode 2-3 trees by starting with standard \nBSTs (which are made up of 2-nodes) and adding extra \ninformation to encode 3-nodes. We think of the links \nas being of two different types: red links, which bind \ntogether two 2-nodes to represent 3-nodes, and black \nlinks, which bind together the 2-3 tree.  Speci\ufb01cally, \nwe represent 3-nodes as two 2-nodes connected by a \nsingle red link that  leans left (one of the 2-nodes is the \nleft child of the other). One advantage of using such a \nrepresentation is that it allows us to use our get() code \nfor standard BST  search without modi\ufb01cation . Given \nany 2-3 tree, we can immediately derive a corresponding BST, just by converting each \nnode as speci\ufb01ed. We refer to BSTs that represent 2-3 trees in this way as red-black BSTs. \nAn equivalent  de\ufb01nition. Another way to proceed is to de\ufb01ne red-black BSTs as BSTs \nhaving red and black links and satisfying the following three restrictions:\n\u25a0 Red links lean left.\n\u25a0 No node has two red links connected to it.\n\u25a0 \n \nThe tree has  perfect black balance : every path from the root to a null link has the \nsame number of black links.\nThere is a 1-1 ", "start": 444, "end": 444}, "642": {"text": "lean left.\n\u25a0 No node has two red links connected to it.\n\u25a0 \n \nThe tree has  perfect black balance : every path from the root to a null link has the \nsame number of black links.\nThere is a 1-1 correspondence between red-black BSTs de\ufb01ned in this way and 2-3 trees.\nA   1-1 correspondence. If we draw the red links horizontally in a red-black BST, all of \nthe null links are the same distance from the root, and if we then collapse together the \nnodes connected by red links, the result is a 2-3 tree. Conversely, if we draw 3-nodes in \nA red-black tree with horizontal red links is a 2-3 tree\n... ... ...\n... ...\n...\nEncoding a 3-node with two 2-nodes\n connected by a left-leaning red link\na b3-node\nbetween\na and b\nless\nthan a\ngreater\nthan b\na\nb\nbetween\na and b\nless\nthan a\ngreater\nthan b\n432 CHAPTER 3 \u25a0 Searching\n  \n \na 2-3 tree as two 2-nodes connected by \na red link that leans left, then no node \nhas two red links connected to it, and \nthe tree has perfect black balance, since \nthe black links correspond to the 2-3 \ntree links, which are perfectly balanced \nby de\ufb01nition. Whichever way we choose \nto de\ufb01ne them, red-black BSTs are both \nBSTs and 2-3 trees. Thus, if we can im-\nplement the 2-3 tree insertion algorithm \nby maintaining the 1-1 correspondence, \nthen we get the best of both worlds: the \nsimple and ef\ufb01cient search method from \nstandard BSTs and the ef\ufb01cient inser -\ntion-balancing method from 2-3 trees. ", "start": 444, "end": 445}, "643": {"text": "1-1 correspondence, \nthen we get the best of both worlds: the \nsimple and ef\ufb01cient search method from \nstandard BSTs and the ef\ufb01cient inser -\ntion-balancing method from 2-3 trees. \n  C o l o r   r e p r e s e n t a t i o n .  For convenience, since \neach node is pointed to by precisely one link \n(from its parent), we encode the color of links \nin nodes, by adding a boolean instance variable \ncolor to our Node data type, which is true if \nthe link from the parent is red and false if tit \nis black. By convention, null links are black .\nFor clarity in our code, we de\ufb01ne constants \nRED and BLACK for use in setting and testing \nthis variable. We use a private method isRed()\nto test the color of a node\u2019s link to its parent. \nWhen we refer to the color of a node, we are \nreferring to the color of the link pointing to it, \nand vice versa.\n   R o t a t i o n s .  The implementation that we will \nconsider might allow right-leaning red links or \ntwo red links in a row during an operation, but \nit always corrects these conditions before com-\npletion, through judicious use of an operation \ncalled rotation that switches the orientation of \n1-1 correspondence between red-black BSTs and 2-3 trees\nX\nSH\nP\nJ R\nE\nA\nM\nC\nL\nXSH P\nJ RE\nA\nM\nC L\nred-black BST\nhorizontal red links\n2-3 tree\nE J\nH L\nM\nR\nP S XA C\nprivate static final boolean RED   = true;\nprivate static final boolean BLACK = false;\nprivate class Node\n{\n  Key key;          // key\n  Value val; ", "start": 445, "end": 445}, "644": {"text": "J\nH L\nM\nR\nP S XA C\nprivate static final boolean RED   = true;\nprivate static final boolean BLACK = false;\nprivate class Node\n{\n  Key key;          // key\n  Value val;        // associated data\n  Node left, right; // subtrees\n  int N;            // # nodes in this subtree\n  boolean color;    // color of link from\n                    //   parent to this node\n  Node(Key key, Value val, int N, boolean color)\n  {\n     this.key   = key;\n     this.val   = val;\n     this.N     = N;\n     this.color = color;\n  }\n}\nprivate boolean isRed(Node x)\n{\n  if (x == null) return false;\n  return x.color == RED;\n}\nJ\nG\nE\nA D\nC\nNode representation for red-black BSTs\nhh.left.color\nis RED h.right.color\nis BLACK\n4333.3 \u25a0 Balanced Search Trees\n red links. First, suppose that we have a right-leaning red link that \nneeds to be rotated to lean to the left (see the diagram at left). This \noperation is called a left rotation. We organize the computation as \na method that takes a link to a red-black BST as argument and, as-\nsuming that link to be to a Node h whose right link is red, makes the \nnecessary adjustments and returns a link to a node that is the root of \na red-black BST for the same set of keys whose left link is red. If you \ncheck each of the lines of code against the before/after drawings in \nthe diagram, you will \ufb01nd this operation is easy to understand: we \nare switching from having the smaller of the two keys at the root to \nhaving the larger of the two keys at the root. Implementing a right \nrotation that converts a left-leaning red link to a right-leaning ", "start": 445, "end": 446}, "645": {"text": "understand: we \nare switching from having the smaller of the two keys at the root to \nhaving the larger of the two keys at the root. Implementing a right \nrotation that converts a left-leaning red link to a right-leaning one \namounts to the same code, with left and right interchanged (see the \ndiagram at right below). \nResetting the link in the parent after a rotation. Whether left or \nright, every rotation leaves us with a link. W e always use the link \nreturned by rotateRight() or rotateLeft() to reset the appro-\npriate link in the parent (or the root of \nthe tree). That may be a right or a left \nlink, but we can always use it to reset \nthe link in the parent. This link may be \nred or black\u2014both rotateLeft() and \nrotateRight() preserve its color by setting x.color to \nh.color. This might allow two red links in a row to occur \nwithin the tree, but our algorithms will also use rotations \nto correct this condition when it arises. For example, the \ncode\nh = rotateLeft(h);\nrotates left a right-leaning red link that is to the right of \nnode h, setting h to point to the root of the resulting sub-\ntree (which contains all the same nodes as the subtree \npointed to by h before the rotation, but a different root). \nThe ease of writing this type of code is the primary reason \nwe use recursive implementations of BST methods, as it \nmakes doing rotations an easy supplement to normal in-\nsertion, as you will see.  \nLeft rotate (right link of h)\nNode rotateLeft(Node h)\n{\n   Node x = h.right;\n   h.right = x.left;\n   x.left = h;\n   x.color = h.color;\n   h.color = RED;\n   x.N = h.N;\n   h.N = 1 + size(h.left)\n ", "start": 446, "end": 446}, "646": {"text": "x = h.right;\n   h.right = x.left;\n   x.left = h;\n   x.color = h.color;\n   h.color = RED;\n   x.N = h.N;\n   h.N = 1 + size(h.left)\n           + size(h.right);\n   return x;\n}\nh\nx\nx\nh\nE\nS\nbetweenless\nE and Sthan E\ngreater\nthan S\nE\nS\nbetween\nE and S\ncould be right or left,\nred or black\nless\nthan E greater\nthan S\nNode rotateRight(Node h)\n{\n   Node x = h.left;\n   h.left = x.right;\n   x.right = h;\n   x.color = h.color;\n   h.color = RED;\n   x.N = h.N;\n   h.N = 1 + size(h.left)\n           + size(h.right);\n   return x;\n}\nx\nh\nh\nx\nE\nS\nbetweenless\nS and Ethan E\ngreater\nthan S\nE\nS\nbetween\nS and E\nless\nthan E greater\nthan S\nRight rotate (left link of h)\n434 CHAPTER 3 \u25a0 Searching\n  \n \n \nWe can use rotations to help maintain the 1-1 correspondence between \n2-3 trees and red-black BSTs as new keys are inserted because they pre-\nserve the two de\ufb01ning properties of red-black BSTs: order and perfect black \nbalance. That is, we can use rotations on a red-black BST without having \nto worry about losing its order or its perfect black balance. Next, we see \nhow to use rotations to preserve the other two de\ufb01ning properties of red-\nblack BSTs (no consecutive red links on any path and no right-leaning red \nlinks). We warm up with some easy cases.\nInsert into a single 2-node. A red-black BST with 1 key is just a single \n2-node. Inserting ", "start": 446, "end": 447}, "647": {"text": "red links on any path and no right-leaning red \nlinks). We warm up with some easy cases.\nInsert into a single 2-node. A red-black BST with 1 key is just a single \n2-node. Inserting the second key immediately shows the need for having \na rotation operation. If the new key is smaller than the key in the tree, we \njust make a new (red) node with the new key and we are done: we have \na red-black BST that is equivalent to a single 3-node. But if the new key \nis larger than the key in the tree, then attaching a new (red) node gives a \nright-leaning red link, and the code root = rotateLeft(root);  com-\npletes the insertion by rotating the red link to the left and updating the \ntree root link. The result in both cases is the red-black representation of a \nsingle 3-node, with two keys, one left-leaning red link, and black height 1.\nInsert into a 2-node at the bottom. We inser t keys into a red-black BST \nas usual into a BST, adding a new node at the bottom (respecting the or -\nder), but always connected to its parent with a red link. If the parent is a \n2-node, then the same two cases just discussed are effective. If the new node \nis attached to the left link, the parent simply becomes a 3-node; if it is at-\ntached to a right link, we have a 3-node leaning the wrong way, but a left \nrotation \ufb01nishes the job.\nInsert into a tree with two keys (in a 3-node). This case reduces to three \nsubcases: the new key is either less than both keys in the tree, between them, \nor greater than both of them. Each of the cases introduces a node with two \nred links connected to it; our goal ", "start": 447, "end": 447}, "648": {"text": "\nsubcases: the new key is either less than both keys in the tree, between them, \nor greater than both of them. Each of the cases introduces a node with two \nred links connected to it; our goal is to correct this condition.\n\u25a0 \n \nThe simplest of the three cases is when the new key is larger than \nthe two in the tree and is therefore attached on the rightmost link of \nthe 3-node, making a balanced tree with the middle key at the root, \nconnected with red links to nodes containing a smaller and a larger \nkey. If we \ufb02ip the colors of those two links from red to black, then we \nhave a balanced tree of height 2 with three nodes, exactly what we \nneed to maintain our 1-1 correspondence to 2-3 trees. The other two \ncases eventually reduce to this case.\nsearch ends\nat this null link\nred link to\n new node\ncontaining a\nconverts 2-node\nto 3-node \nsearch ends\nat this null link\nattached new node\nwith red link\nrotated left\nto make a \nlegal 3-node \na\nb\na\na\nb\nb\na\nb\nroot\nroot\nroot\nroot\nleft\nright\nInsert into a single\n2-node (two cases)\nE\nA\nE\nR\nS\nR\nS\nA\nC\nE\nR\nS\nC\nA\nadd new\nnode here\nright link red\nso rotate left\ninsert C\nInsert into a 2-node\nat the bottom\n4353.3 \u25a0 Balanced Search Trees\n \u25a0 If the new key is \nsmaller than the two \nkeys in the tree and \ngoes on the left link, \nthen we have two \nred links in a row, \nboth leaning to the \nleft, which we can \nreduce to the previ-\nous ", "start": 447, "end": 448}, "649": {"text": "than the two \nkeys in the tree and \ngoes on the left link, \nthen we have two \nred links in a row, \nboth leaning to the \nleft, which we can \nreduce to the previ-\nous case (middle \nkey at the root, con-\nnected to the others \nby two red links) by \nrotating the top link \nto the right.\n\u25a0 If the new key goes \nbetween the two \nkeys in the tree, we \nagain have two red links in a row, a right-leaning one below \na left-leaning one, which we can reduce to the previous case \n(two red links in a row, to the left) by rotating left the bot-\ntom link.\nIn summary, we achieve the desired result by doing zero, \none, or two rotations followed by \ufb02ipping the colors of the \ntwo children of the root.  As with 2-3 trees, be certain that \nyou understand these transformations , as they are the key to \nred-black tree dynamics.\n  F l i p p i n g  c o l o r s .  To  \ufb02 i p  t h e  co l o r s  o f  t h e  t wo  re d  c h i l d re n  \nof a node, we use a method flipColors(), shown at left. In \naddition to \ufb02ipping the colors of the children from red to \nblack, we also \ufb02ip the color of the parent from black to red. \nA critically important characteristic of this operation is that, \nlike rotations, it is a local transformation that preserves per-\nfect black balance in the tree. Moreover, this convention im-\nmediately leads us to a full implementation, as we describe \nnext.\nsearch ends\nat this null link\nsearch ends\nat this null link\nattached new\nnode with\nred link\na\nc\nb\nattached ", "start": 448, "end": 448}, "650": {"text": "tree. Moreover, this convention im-\nmediately leads us to a full implementation, as we describe \nnext.\nsearch ends\nat this null link\nsearch ends\nat this null link\nattached new\nnode with\nred link\na\nc\nb\nattached new\nnode with\nred link\nrotated left \nrotated\nright \nrotated\nright \ncolors flipped\nto black \ncolors flipped\nto black \nsearch ends\nat this\nnull link\nattached new\nnode with\nred link\ncolors flipped\nto black \na\nc\nb\nb\nc\na\nb\nc\na\nb\nc\na\nb\nc\na\nb\nc\na\nc\na\nc\nb\nsmaller between\na\nb\na\nb\nc\na\nb\nc\nlarger\nInsert into a single 3-node (three cases)\nvoid flipColors(Node h)\n{\n   h.color = RED;\n   h.left.color = BLACK;\n   h.right.color = BLACK;\n}\nh\nA\nE\nbetween\nA and E\nless\nthan A\nS\nbetween\nE and S\ncould be left\nor right link\nred link attaches\nmiddle node\nto parent\nblack links split\nto 2-nodes\ngreater\nthan S\nA\nE\nbetweenless\nA and Ethan A\nS\nbetween\nE and S\ngreater\nthan S\nFlipping colors to split a 4-node\n436 CHAPTER 3 \u25a0 Searching\n  \n \n \nKeeping the root black. In the case just considered (insert into a single 3-node), the \ncolor \ufb02ip will color the root red. This can also happen in larger trees. Strictly speaking, \na red root  implies that the root is part of a 3-node, but that is not the case, so we color \nthe root black after each insertion. Note ", "start": 448, "end": 449}, "651": {"text": "also happen in larger trees. Strictly speaking, \na red root  implies that the root is part of a 3-node, but that is not the case, so we color \nthe root black after each insertion. Note that the black \nheight of the tree increases by 1 whenever the color of \nthe color of the root is \ufb02ipped from black to red.  \n I n s e r t  i n t o  a  3 - n o d e  a t  t h e  b o t t o m .  Now suppose that \nwe add a new node at the bottom that is connected to a \n3-node. The same three cases just discussed arise. Either \nthe new link is connected to the right link of the 3-node \n(in which case we just \ufb02ip colors) or to the left link of \nthe 3-node (in which case we need to rotate the top link \nright and \ufb02ip colors) or to the middle link of the 3-node \n(in which case we rotate left the bottom link, then rotate \nright the top link, then \ufb02ip colors). Flipping the colors \nmakes the link to the middle node red, which amounts \nto passing it up to its parent, putting us back in the same \nsituation with respect to the parent, which we can \ufb01x by \nmoving up the tree. \nPassing a red link up the tree. The 2-3 tree insertion \nalgorithm calls for us to split the 3-node, passing the \nmiddle key up to be inserted into its parent, continuing \nuntil encountering a 2-node or the root. In every case \nwe have considered, we precisely accomplish this objec-\ntive: after doing any necessary rotations, we \ufb02ip colors, \nwhich turns the middle node to red. From the point of \nview of the parent of that node, ", "start": 449, "end": 449}, "652": {"text": "considered, we precisely accomplish this objec-\ntive: after doing any necessary rotations, we \ufb02ip colors, \nwhich turns the middle node to red. From the point of \nview of the parent of that node, that link becoming red \ncan be handled in precisely the same manner as if the \nred link came from attaching a new node: we pass up a \nred link to the middle node.  The three cases summa -\nrized in the diagram on the next page precisely capture \nthe operations necessary in a red-black tree to implement the key operation in 2-3 tree \ninsertion: to insert into a 3-node, create a temporary 4-node, split it, and pass a red link \nto the middle key up to its parent. Continuing the same process, we pass a red link up \nthe tree until reaching a 2-node or the root. \nH\nE\nR\nS\nA\nC\nS\nS\nR\nE\nH\nadd new\nnode here\nE\nR\nS\nA\nC\nright link red\nso rotate left\ntwo lefts in a row\nso rotate right\nE\nH\nR\nA\nC\nboth children red\nso flip colors\nS\nE\nH\nR\nA\nC\nA\nC\ninserting H\nInsert into a 3-node at the bottom\nE\nR SA C\nE\nH R SA C\nSH\nE R\nA C\n4373.3 \u25a0 Balanced Search Trees\n In summary, we can maintain our 1-1 \ncorrespondence between 2-3 trees and \nred-black BSTs during insertion by judi -\ncious use of three simple operations: left \nrotate, right rotate, and color \ufb02ip. We can \naccomplish the insertion by performing \nthe following operations, one after the \nother, on each ", "start": 449, "end": 450}, "653": {"text": "-\ncious use of three simple operations: left \nrotate, right rotate, and color \ufb02ip. We can \naccomplish the insertion by performing \nthe following operations, one after the \nother, on each node as we pass up the tree \nfrom the point of insertion:\n\u25a0 If the right child is red and the left \nchild is black, rotate left.\n\u25a0 If both the left child and its left \nchild are red, rotate right.\n\u25a0 \n \n \nIf both children are red, \ufb02ip colors.\nIt certainly is worth your while to check that this sequence handles each of the cases \njust described. Note that the \ufb01rst operation handles both the rotation necessary to lean \nthe 3-node to the left when the parent is a 2-node and the rotation necessary to lean the \nbottom link to the left when the new red link is the middle link in a 3-node. \nImplementation Since the balancing operations are to be performed on the way \nup the tree from the point of insertion, implementing them is easy in our standard \nrecursive implementation: we just do them after the recursive calls, as shown in Algo-\nrithm 3.4. The three operations listed in the previous paragraph each can be accom -\nplished with a single if statement that tests the colors of two nodes in the tree. Even \nthough it involves a small amount of code, this implementation would be quite dif\ufb01cult \nto understand without the two layers of abstraction that we have developed (2-3 trees \nand red-black BSTs) to implement it. At a cost of testing three to \ufb01ve node colors (and \nperhaps doing a rotation or two or \ufb02ipping colors when a test succeeds), we get BSTs \nthat have nearly perfect balance. \nThe traces for our standard indexing client and for the same keys inserted in increas-\ning order are given on page 440. Considering these examples simply ", "start": 450, "end": 450}, "654": {"text": "a test succeeds), we get BSTs \nthat have nearly perfect balance. \nThe traces for our standard indexing client and for the same keys inserted in increas-\ning order are given on page 440. Considering these examples simply in terms of our \nthree operations on red-black trees, as we have been doing, is an instructive exercise. \nAnother instructive exercise is to check the correspondence with 2-3 trees that the algo-\nrithm maintains (using the \ufb01gure for the same keys given on page 430). In both cases, \nyou can test your understanding of the algorithm by considering the transformations \n(two color \ufb02ips and two rotations) that are needed when P is inserted into the red-black \nBST  (see Exercise 3.3.12).\nflip\ncolors\nright\nrotate\nleft\nrotate\nPassing a red link up a red-black BST\nh\nh h\n438 CHAPTER 3 \u25a0 Searching\n ALGORITHM 3.4   Insert for red-black BSTs\npublic class    RedBlackBST<Key extends Comparable<Key>, Value> \n{  \n   private Node root;\n   private class Node // BST node with color bit (see page 433)\n   private boolean isRed(Node h)    // See page 433.\n   private Node rotateLeft(Node h)  // See page 434.\n   private Node rotateRight(Node h) // See page 434.\n   private void flipColors(Node h)  // See page 436.\n   private int size()               // See page 398.\n   public void put(Key key, Value val)\n   {  // Search for key. Update value if found; grow table if new. \n      root = put(root, key, val);\n      root.color = BLACK;\n   }\n   private Node put(Node h, Key key, Value val)\n   {\n      if (h == null)  // Do standard insert, with red link ", "start": 450, "end": 451}, "655": {"text": "put(root, key, val);\n      root.color = BLACK;\n   }\n   private Node put(Node h, Key key, Value val)\n   {\n      if (h == null)  // Do standard insert, with red link to parent.\n         return new Node(key, val, 1, RED);\n      int cmp = key.compareTo(h.key);\n      if      (cmp < 0) h.left  = put(h.left,  key, val);\n      else if (cmp > 0) h.right = put(h.right, key, val);\n      else h.val = val;\n      if (isRed(h.right) && !isRed(h.left))    h = rotateLeft(h);\n      if (isRed(h.left) && isRed(h.left.left)) h = rotateRight(h);\n      if (isRed(h.left) && isRed(h.right))     flipColors(h);\n      h.N = size(h.left) + size(h.right) + 1;\n      return h;\n   } \n}\n \nThe code for the recursive put() for red-black BSTs is identical to put() in elementary BSTs except \nfor the three if statements after the recursive calls, which provide near-perfect balance in the tree \nby maintaining a 1-1 correspondence with 2-3 trees, on the way up the search path. The \ufb01rst rotates \nleft any right-leaning 3-node (or a right-leaning red link at the bottom of a temporary 4-node); the \nsecond rotates right the top link in a temporary 4-node with two left-leaning red links; and the third \n\ufb02ips colors to pass a red link up the tree (see text).\n4393.3 \u25a0 Balanced Search Trees S\nE\nA S\nE\nA\nPA\nH\nC M\nE L\nA\nH\nC M\nE L\nE\nA\nR\nC\nH\nX\nM\nP\nL\nC\nE\nH\nL\nM\nP\nR\nS\nX\nE\nR\nS\nL\nM\nP\nR\nS\nX\nA\nH\nC\nE\nR\nS\nC\nA\nE\nH\nA\nC\nE\nS\nA\nC\nA ", "start": 451, "end": 452}, "656": {"text": "Trees S\nE\nA S\nE\nA\nPA\nH\nC M\nE L\nA\nH\nC M\nE L\nE\nA\nR\nC\nH\nX\nM\nP\nL\nC\nE\nH\nL\nM\nP\nR\nS\nX\nE\nR\nS\nL\nM\nP\nR\nS\nX\nA\nH\nC\nE\nR\nS\nC\nA\nE\nH\nA\nC\nE\nS\nA\nC\nA E\nA\nC\nS\nX\nM\nR\nE\nA H\nC\nS\nX\nR\nE\nA\nC H\nP\nR\nS\nX\nM\nE\nA\nC H\nP\nR\nSH\nX\nM\nE\nA\nC L\nS\nR\nE\nA\nC H\nL\nH\nC\nA E\nS\nR\nM\nL P\nA\nH\nC\nE\nR\nM\nL P\nH\nC\nA E\nRed-black BST construction traces\n standard indexing client  same keys in increasing order\ninsert S insert A\n440 CHAPTER 3 \u25a0 Searching\n  D e l e t i o n  Since put() in Algorithm 3.4 is already one of the most intricate \nmethods that we consider in this book, and the implementations of deleteMin(), \ndeleteMax(), and delete() for red-black BSTs are a bit more complicated, we defer \ntheir full implementations to exercises. Still, the basic approach is worthy of study. T o \ndescribe it, we begin by returning to 2-3 trees. As with insertion, we can de\ufb01ne a se-\nquence of local transformations that allow us to delete a node while still maintaining \nperfect balance. The process is somewhat more complicated ", "start": 452, "end": 453}, "657": {"text": "by returning to 2-3 trees. As with insertion, we can de\ufb01ne a se-\nquence of local transformations that allow us to delete a node while still maintaining \nperfect balance. The process is somewhat more complicated than for insertion, because \nwe do the transformations both on the way down the search path, \nwhen we introduce temporary 4-nodes (to allow for a node to be \ndeleted), and also on the way up the search path, where we split any \nleftover 4-nodes (in the same manner as for insertion).\n  T o p - d o w n  2 - 3 - 4  t r e e s .  As a \ufb01rst warmup for deletion, we con -\nsider a simpler algorithm that does transformations both on the \nway down the path and on the way up the path: an insertion algo-\nrithm for 2-3-4 trees, where the temporary 4-nodes that we saw in \n2-3 trees can persist in the tree. The insertion algorithm is based on \ndoing transformations on the way down the path to maintain the \ninvariant that the current node is not a 4-node (so we are assured \nthat there will be room to insert the new key at the bottom) and \ntransformations on the way up the path to balance any 4-nodes \nthat may have been created. The transformations on the way down \nare precisely the same transformations that we used for splitting \n4-nodes in 2-3 trees. If the root is a 4-node, we split it into three \n2-nodes, increasing the height of the tree by 1. On the way down \nthe tree, if we encounter a 4-node with a 2-node as parent, we split \nthe 4-node into two 2-nodes and pass the middle key to the par -\nent, making it a 3-node; ", "start": 453, "end": 453}, "658": {"text": "\nthe tree, if we encounter a 4-node with a 2-node as parent, we split \nthe 4-node into two 2-nodes and pass the middle key to the par -\nent, making it a 3-node; if we encounter a 4-node with a 3-node as \nparent, we split the 4-node into two 2-nodes and pass the middle \nkey to the parent, making it a 4-node. We do not need to worry \nabout encountering a 4-node with a 4-node as parent by virtue of \nthe invariant. At the bottom, we have, again by virtue of the invariant, a 2-node or a \n3-node, so we have room to insert the new key. T o implement this algorithm with red-\nblack BSTs, we\n\u25a0 Represent 4-nodes as a balanced subtree of three 2-nodes, with both the left and \nright child connected to the parent with a red link\n\u25a0 Split 4-nodes on the way down the tree with color \ufb02ips\n\u25a0 Balance 4-nodes on the way up the tree with rotations, as for insertion\nat the root\non the way down\nat the bottom\nTransformations for insert\nin top-down 2-3-4 trees\n4413.3 \u25a0 Balanced Search Trees\n Remarkably, you can implement top-down 2-3-4 trees by moving one line of code in \nput() in Algorithm 3.4: move the colorFlip() call (and accompanying test) to be-\nfore the recursive calls (between the test for null and the comparison). This algorithm \nhas some advantages over 2-3 trees in applications where multiple processes have access \nto the same tree, because it always is operating within a link or two of the current node. \nThe deletion algorithms that we describe next are based on a similar scheme and are \neffective for ", "start": 453, "end": 454}, "659": {"text": "applications where multiple processes have access \nto the same tree, because it always is operating within a link or two of the current node. \nThe deletion algorithms that we describe next are based on a similar scheme and are \neffective for these trees as well as for 2-3 trees.\nDelete the minimum. As a second warmup \nfor deletion, we consider the operation of \ndeleting the minimum from a 2-3 tree. The \nbasic idea is based on the observation that we \ncan easily delete a key from a 3-node at the \nbottom of the tree, but not from a 2-node. \nDeleting the key from a 2-node leaves a node \nwith no keys; the natural thing to do would \nbe to replace the node with a null link, but \nthat operation would violate the perfect bal-\nance condition. So, we adopt the following \napproach: to ensure that we do not end up on \na 2-node, we perform appropriate transfor -\nmations on the way down the tree to preserve \nthe invariant that the current node is not a \n2-node (it might be a 3-node or a tempo-\nrary 4-node). First, at the root, there are two \npossibilities: if the root is a 2-node and both \nchildren are 2-nodes, we can just convert the \nthree nodes to a 4-node; otherwise we can \nborrow from the right sibling  if necessary to ensure that the left child of the root is not \na 2-node. Then, on the way down the tree, one of the following cases must hold: \n\u25a0 If the left child of the current node is not a 2-node, there is nothing to do.\n\u25a0 If the left child is a 2-node and its immediate sibling is not a 2-node, move a key \nfrom the sibling to the left child.\n\u25a0 If the left child and its ", "start": 454, "end": 454}, "660": {"text": "there is nothing to do.\n\u25a0 If the left child is a 2-node and its immediate sibling is not a 2-node, move a key \nfrom the sibling to the left child.\n\u25a0 If the left child and its immediate sibling are 2-nodes, then combine them with \nthe smallest key in the parent to make a 4-node, changing the parent from a \n3-node to a 2-node or from a 4-node to a 3-node.\nFollowing this process as we traverse left links to the bottom, we wind up on a 3-node \nor a 4-node with the smallest key, so we can just remove it, converting the 3-node to a \na b c\nb\nca\nc\na b d e\nb\na\nb c\na b d ea\nc d e\nb f g c f g\nc d e\nat the root\non the way down\nat the bottom\nTransformations for delete the minimum\na b c\na b c\nd e\nca\nb d e\n442 CHAPTER 3 \u25a0 Searching\n  \n2-node or the 4-node to a 3-node. Then, on the way up the tree, we split any unused \ntemporary 4-nodes. \nDelete. The same transformations along the search path just described for deleting the \nminimum are effective to ensure that the current node is not a 2-node during a search \nfor any key. If the search key is at the bottom, we can just remove it. If the key is not \nat the bottom, then we have to exchange it with its successor as in regular BSTs. Then, \nsince the current node is not a 2-node, we have reduced the problem to deleting the \nminimum in a subtree whose root is not a 2-node, and we can use the procedure just \ndescribed for that subtree. After the deletion, as usual, we ", "start": 454, "end": 455}, "661": {"text": "2-node, we have reduced the problem to deleting the \nminimum in a subtree whose root is not a 2-node, and we can use the procedure just \ndescribed for that subtree. After the deletion, as usual, we split any remaining 4-nodes \non the search path on the way up the tree.\nSeveral of the exercises  at the end of this section are devoted to examples and \nimplementations related to these deletion algorithms. People with an interest in devel-\noping or understanding implementations need to master the details covered in these \nexercises. People with a general interest in the study of algorithms need to recognize \nthat these methods are important because they represent the \ufb01rst symbol-table imple-\nmentation that we have seen where search, insert, and delete are all guaranteed to be \nef\ufb01cient, as we will establish next.\n4433.3 \u25a0 Balanced Search Trees\n Properties of red-black BSTs Studying the properties of red-black BSTs is a \nmatter of checking the correspondence with 2-3 trees and then applying the analysis of \n2-3 trees. The end result is that all symbol-table operations in red-black BSTs are guaran-\nteed to be logarithmic in the size of the tree (except for range search, which additionally \ncosts time proportional to the number of keys returned). We repeat and emphasize this \npoint because of its importance. \n  A n a l y s i s .  First, we establish that red-black BSTs, while not perfectly balanced, are al-\nways nearly so, regardless of the order in which the keys are inserted. This fact immedi-\nately follows from the 1-1 correspondence with 2-3 trees and the de\ufb01ning property of \n2-3 trees (perfect balance).\nProposition G. The  height of a red-black BST with N nodes is no more than 2 lg N.\nProof sketch: The worst case is a ", "start": 455, "end": 456}, "662": {"text": "de\ufb01ning property of \n2-3 trees (perfect balance).\nProposition G. The  height of a red-black BST with N nodes is no more than 2 lg N.\nProof sketch: The worst case is a 2-3 tree that is all 2-nodes except that the leftmost \npath is made up of 3-nodes. The path taking left links from the root is twice as long \nas the paths of length ~ lg N that involve just 2-nodes. It is possible, but not easy, to \ndevelop key sequences that cause the construction of red-black BSTs whose average \npath length is the worst-case 2 lg N. If you are mathematically inclined, you might \nenjoy exploring this issue by working Exercise 3.3.24. \nThis upper bound is conservative: experiments involving both random insertions and \ninsertion sequences found in typical applications support the hypothesis that each \nsearch in a red-black BST of N nodes uses about 1.00 lg N \u2013 .5 compares, on the aver-\nage. Moreover, you are not likely to encounter a substantially higher average number of \ncompares in practice.\nTypical red-black BST built from random keys (null links omitted)\n444 CHAPTER 3 \u25a0 Searching\n Property H. The  average length of a path from the root to a node in a red-black \nBST with N nodes is ~1.00 lg N. \nEvidence: Typical trees, such as the one at the bottom of  the previous page (and \neven the one built by inserting keys in increasing order at the bottom of this page) \nare quite well-balanced, by comparison with typical BSTs (such as the tree depicted \non page 405). The table at the top of this page shows that path lengths (search costs) \nfor our FrequencyCounter application are about 40 percent lower than from el-\nementary BSTs, as expected. ", "start": 456, "end": 457}, "663": {"text": "\non page 405). The table at the top of this page shows that path lengths (search costs) \nfor our FrequencyCounter application are about 40 percent lower than from el-\nementary BSTs, as expected. This performance has been observed in countless ap-\nplications and experiments since the invention of red-black BSTs. \n \nFor our example study of the cost of the put() operations for FrequencyCounter for \nwords of length 8 or more, we see a further reduction in the average cost, again pro -\nviding a quick validation of the logarithmic performance predicted by the theoretical \nmodel, though this validation is less surprising than for BSTs because of the guarantee \nprovided by Property G. The total savings is less than the 40 per cent savings in the \nsearch cost because we count rotations and color \ufb02ips as well as compares.\ntale.txt leipzig1M.txt\nwords distinct compares words distinct compares\nmodel actual model actual\nall words 135,635 10,679 13.6 13.5 21,191,455 534,580 19.4 19.1\n8+ letters 14,350 5,737 12.6 12.1 4,239,597 299,593 18.7 18.4\n10+ letters 4,582 2,260 11.4 11.5 1,610,829 165,555 17.5 17.3\nAverage number of compares per put() for FrequencyCounter using RedBlackBST\nRed-black BST built from ascending keys (null links omitted)\n4453.3 \u25a0 Balanced Search Trees\n  \n \n \nThe get() method in red-black BSTs does not examine the node color, so the balanc-\ning mechanism adds no overhead; search is faster than in elementary BSTs because \nthe tree is balanced. Each key is inserted just once, but may be ", "start": 457, "end": 458}, "664": {"text": "red-black BSTs does not examine the node color, so the balanc-\ning mechanism adds no overhead; search is faster than in elementary BSTs because \nthe tree is balanced. Each key is inserted just once, but may be involved in many, many \nsearch operations, so the end result is that we get search times that are close to optimal \n(because the trees are nearly balanced and no work for balancing is done during the \nsearches) at relatively little cost (unlike binary search, insertions are guaranteed to be \nlogarithmic). The inner loop of the search is a compare followed by updating a link, \nwhich is quite short, like the inner loop of binary search (compare and index arithme-\ntic). This implementation is the \ufb01rst we have seen with logarithmic guarantees for both \nsearch and insert, and it has a tight inner loop, so its use is justi\ufb01ed in a broad variety of \napplications, including library implementations. \nOrdered symbol-table API. One of the most appealing features of red-black BSTs is \nthat the complicated code is limited to the put() (and deletion) methods. Our code for \nthe minimum/maximum, select, rank, \ufb02oor, ceiling and range queries in standard BSTs \ncan be used without any change, since it operates on BSTs and has no need to refer to the \nnode color. Algorithm 3.4, together with these methods (and the deletion methods), \nleads to a complete implementation of our ordered symbol-table API. Moreover, all of \nthe methods bene\ufb01t from the near-perfect balance in the tree because they all require \ntime proportional to the tree height, at most. Thus  Proposition G , in combination \nwith Proposition E, suf\ufb01ces to establish a logarithmic performance guarantee for all \nof them.\nCosts for java FrequencyCounter 8 < tale.txt using RedBlackBST\n20\n0\n0 ", "start": 458, "end": 458}, "665": {"text": "Proposition G , in combination \nwith Proposition E, suf\ufb01ces to establish a logarithmic performance guarantee for all \nof them.\nCosts for java FrequencyCounter 8 < tale.txt using RedBlackBST\n20\n0\n0 14350operations\ncompares\n12\n446 CHAPTER 3 \u25a0 Searching\n Proposition I. In a red- black BST, the following operations take logarithmic time \nin the worst case: search, insertion, \ufb01nding the minimum, \ufb01nding the maximum, \n\ufb02oor, ceiling, rank, select, delete the minimum, delete the maximum, delete, and \nrange count.\nProof: We have just discussed get(), put(), and the deletion operations. For the \nothers, the code from Section 3.2 can be used verbatim (it just ignores the node \ncolor). Guaranteed logarithmic performance follows from Propositions E and G, \nand the fact that each algorithm performs a constant number of operations on each \nnode examined.\nOn re\ufb02ection, it is quite remarkable that we are able to achieve such guarantees. In a \nworld awash with information, where people maintain tables with trillions or quadril-\nlions of entries, the fact is that we can guarantee to complete any one of these opera-\ntions in such tables with just a few dozen compares.\nalgorithm\n(data structure)\nworst-case cost\n(after N inserts) \naverage-case cost\n(after N random inserts) efficiently\nsupport ordered\noperations?search insert search hit insert\nsequential search\n(unordered linked list) N N N/2 N no\nbinary search\n(ordered array) lg N N lg N N/2 yes\nbinary tree search\n(BST) N N 1.39 lg N 1.39 lg N yes\n2-3 tree search\n(red-black BST) 2 lg N 2 lg N 1.00 lg N 1.00 ", "start": 458, "end": 459}, "666": {"text": "search\n(BST) N N 1.39 lg N 1.39 lg N yes\n2-3 tree search\n(red-black BST) 2 lg N 2 lg N 1.00 lg N 1.00 lg N yes\nCost summary for symbol-table implementations (updated)\n4473.3 \u25a0 Balanced Search Trees\n Q&A\n \nQ. Why not let the 3-nodes lean either way and also allow 4-nodes in the trees?\nA. Those are \ufb01ne alternatives, used by many for decades. Y ou can learn about several of \nthese alternatives in the exercises. The left-leaning convention reduces the number of \ncases and therefore requires substantially less code.\nQ.  Why not use an array of Key values to represent 2-, 3-, and 4-nodes with a single \nNode type?\nA. Good question. That is precisely what we do for  B-trees (see Chapter 6), where we \nallow many more keys per node. For the small nodes in 2-3 trees, the overhead for the \narray is too high a price to pay.\nQ. When we split a 4-node, we sometimes set the color of the right node to RED in \nrotateRight() and then immediately set it to BLACK in flipColors(). Isn\u2019t that \nwasteful?\nA. Ye s , a n d  we  a l s o  s o m e t i m e s  u n n e ce s s a r i l y  re co l o r  t h e  m i d d l e  n o d e . In  t h e  g r a n d  \nscheme of things, resetting a few extra bits is not in the same league with the improve-\nment from linear to logarithmic that we get for all operations, but in performance-crit-\nical applications, you can put the code for rotateRight() ", "start": 459, "end": 460}, "667": {"text": "things, resetting a few extra bits is not in the same league with the improve-\nment from linear to logarithmic that we get for all operations, but in performance-crit-\nical applications, you can put the code for rotateRight() and flipColors() inline \nand eliminate those extra tests. We use those methods for deletion, as well, and \ufb01nd \nthem slightly easier to use, understand, and maintain by making sure that they preserve \nperfect black balance. \n448 CHAPTER 3 \u25a0 Searching\n EXERCISES\n \n3.3.1 Draw the 2-3 tree that results when you insert the keys E A S Y Q U T I O N in \nthat order into an initially empty tree.\n3.3.2 Draw the 2-3 tree that results when you insert the keys Y L P M X H C R A E S\nin that order into an initially empty tree.\n3.3.3 Find an insertion order for the keys S E A R C H X M that leads to a 2-3 tree \nof height 1.\n3.3.4  Prove that the height of a 2-3 tree with N keys is between ~ \u23a3log3 N\u23a6 \n .63 lg N (for a tree that is all 3-nodes) and ~ \u23a3lg N\u23a6 (for a tree that is all \n2-nodes).\n3.3.5  The \ufb01gure at right shows all the structurally different 2-3 trees with N\nkeys, for N from 1 up to 6 (ignore the order of the subtrees). Draw all the \nstructurally different trees for N = 7, 8, 9, and 10.\n3.3.6 Find the probability that each of the 2-3 trees in Exercise 3.3.5 is the \nresult of the insertion of ", "start": 460, "end": 461}, "668": {"text": "7, 8, 9, and 10.\n3.3.6 Find the probability that each of the 2-3 trees in Exercise 3.3.5 is the \nresult of the insertion of N random distinct keys into an initially empty tree.\n3.3.7  Draw diagrams like the one at the top of page 428 for the other \ufb01ve cases \nin the diagram at the bottom of that page.\n3.3.8 Show all possible ways that one might represent a 4-node with three \n2-nodes bound together with red links (not necessarily left-leaning).\n3.3.9 Which of the following are red-black BSTs?\n3.3.10 Draw the red-black BST that results when you insert items with the keys \nE A S Y Q U T I O N in that order into an initially empty tree.\n3.3.11 Draw the red-black BST that results when you insert items with the keys \nY L P M X H C R A E S in that order into an initially empty tree.\nD\nY\nH\nA\nC\nF\nH\nG Z\nA\nC\nD\nE\nD\nY\nH ZA\nB\nE\nY\nTA\nC\nH\n(i) (ii) (iii) (iv)\n4493.3 \u25a0 Balanced Search Trees\n  \n3.3.12  Draw the red-black BST that results after each transformation (color \ufb02ip or \nrotation) during the insertion of P for our standard indexing client.\n3.3.13 True or false: If  you inser t keys in increasing order into a red-black BST, the tree \nheight is monotonically increasing.\n3.3.14 Draw the red-black BST that results when you insert letters A through K in order \ninto an initially empty tree, then describe what happens in general when trees are built \nby insertion of keys in ", "start": 461, "end": 462}, "669": {"text": "increasing.\n3.3.14 Draw the red-black BST that results when you insert letters A through K in order \ninto an initially empty tree, then describe what happens in general when trees are built \nby insertion of keys in ascending order (see also the \ufb01gure in the text). \n3.3.15 Answer the previous two questions \nfor the case when the keys are inserted in de-\nscending order. \n3.3.16 Show the result of inserting n into the \nred-black BST drawn at right (only the search \npath is shown, and you need to include only \nthese nodes in your answer).\n3.3.17 Generate two random 16-node red-\nblack BSTs. Draw them (either by hand or \nwith a program). Compare them with the \n(unbalanced) BSTs built with the same keys.\n3.3.18 Draw all the structurally different red-black BSTs with N keys, for N from 2 up \nto 10 (see Exercise 3.3.5).\n3.3.19 With 1 bit per node for color, we can represent 2-, 3-, and 4-nodes.  How many \nbits per node would we need to represent 5-, 6-, 7-, and 8-nodes with a binary tree?\n3.3.20 Compute the internal path length in a perfectly balanced BST of N nodes, when \nN is a power of 2 minus 1.\n3.3.21 Create a test client TestRB.java, based on your solution to Exercise 3.2.10.\n3.3.22 Find a sequence of keys to insert into a BST and into a red-black BST such that \nthe height of the BST is less than the height of the red-black BST, or prove that no such \nsequence is possible.\nj\nt\ns\nq\nu\nr\no\nm\np\nk\nl\nEXERCISES ", "start": 462, "end": 462}, "670": {"text": "BST such that \nthe height of the BST is less than the height of the red-black BST, or prove that no such \nsequence is possible.\nj\nt\ns\nq\nu\nr\no\nm\np\nk\nl\nEXERCISES  (continued)\n450 CHAPTER 3 \u25a0 Searching\n CREATIVE PROBLEMS\n \n \n3.3.23  2-3 trees without balance restriction. Develop an implementation of the basic \nsymbol-table API that uses 2-3 trees that are not necessarily balanced as the underlying \ndata structure. Allow 3-nodes to lean either way. Hook the new node onto the bottom \nwith a black link when inserting into a 3-node at the bottom. Run experiments to de-\nvelop a hypothesis estimating the average path length in a tree built from N random \ninsertions.\n3.3.24    Worst case for red-black BSTs. Show how to construct a red-black BST dem-\nonstrating that, in the worst case, almost all the paths from the root to a null link in a \nred-black BST of N nodes are of length 2 lg N.\n3.3.25    Top-dow n 2-3-4 trees. Develop an implementation of the basic symbol-table \nAPI that uses balanced 2-3-4 trees as the underlying data structure, using the red-black \nrepresentation and the insertion method described in the text, where 4-nodes are split \nby \ufb02ipping colors on the way down the search path and balancing on the way up.\n3.3.26  Single top-down pass. Develop a modi\ufb01ed version of your solution to Exer-\ncise 3.3.25 that does not use recursion. Complete all the work splitting and balancing \n4-nodes (and balancing 3-nodes) on the way down the tree, \ufb01nishing with an insertion \nat ", "start": 462, "end": 463}, "671": {"text": "3.3.25 that does not use recursion. Complete all the work splitting and balancing \n4-nodes (and balancing 3-nodes) on the way down the tree, \ufb01nishing with an insertion \nat the bottom.\n3.3.27  Allow right-leaning red links. Develop a modi\ufb01ed version of your solution to \nExercise 3.3.25 that allows right-leaning red links in the tree.\n3.3.28    Bottom-up 2-3-4 trees. Develop an implementation of the basic symbol-table \nAPI that uses balanced 2-3-4 trees as the underlying data structure, using the red-black \nrepresentation and a bottom-up insertion method based on the same recursive approach \nas Algorithm 3.4. Yo u r  i n s e r t i o n  m e t h o d  s h o u l d  s p l i t  o n l y  t h e  s e q u e n ce  o f  4 - n o d e s  ( i f  \nany) on the bottom of the search path.\n3.3.29  Optimal storage. Modify RedBlackBST so that it does not use any extra storage \nfor the color bit, based on the following trick: T o color a node red, swap its two links. \nThen, to test whether a node is red, test whether its left child is larger than its right child.  \nYo u  h ave  t o  m o d i f y  t h e  co m p a re s  t o  a cco m m o d a t e  t h e  p o s s i b l e  l i n k  s w a p, a n d  t h i s  t r i c k  \nreplaces bit compares with key compares that are presumably more expensive, ", "start": 463, "end": 463}, "672": {"text": "o s s i b l e  l i n k  s w a p, a n d  t h i s  t r i c k  \nreplaces bit compares with key compares that are presumably more expensive, but it \nshows that the bit in the nodes can be eliminated, if necessary.\n3.3.30  Sofware  caching. Modify RedBlackBST to keep the most recently accessed Node\nin an instance variable so that it can be accessed in constant time if the next put() or \n4513.3 \u25a0 Balanced Search Trees\n  \n \nget() uses the same key (see Exercise 3.1.25).\n3.3.31  Tree drawing. Add a method draw() to RedBlackBST that draws red-black \nBST \ufb01gures in the style of the text (see Exercise 3.2.38)\n3.3.32     AV L  t r e e s .  An AV L  t r e e is a BST where the height of every node and that of \nits sibling differ by at most 1. (The oldest balanced tree algorithms are based on using \nrotations to maintain height balance in AVL trees.) Show that coloring red links that \ngo from nodes of even height to nodes of odd height in an A VL tree gives a (perfectly \nbalanced) 2-3-4 tree, where red links are not necessarily left-leaning. Extra credit : De-\nvelop an implementation of the symbol-table API that uses this as the underlying data \nstructure. One approach is to keep a height \ufb01eld in each node, using rotations after the \nrecursive calls to adjust the height as necessary; another is to use the red-black represen-\ntation and use methods like moveRedLeft() and moveRedRight() in Exercise 3.3.39 \nand Exercise 3.3.40.\n3.3.33   Certi\ufb01cation. Add ", "start": 463, "end": 464}, "673": {"text": "represen-\ntation and use methods like moveRedLeft() and moveRedRight() in Exercise 3.3.39 \nand Exercise 3.3.40.\n3.3.33   Certi\ufb01cation. Add to RedBlackBST a method is23() to check that no node is \nconnected to two red links and that there are no right-leaing red links and a method \nisBalanced() to check that all paths from the root to a null link have the same number \nof black links. Combine these methods with code from isBST() in Exercise 3.2.32  to \ncreate a method isRedBlackBST() that checks that the tree is a red-black BST.\n3.3.34  All 2-3 trees. Write code to generate all structurally different 2-3 trees of height \n2, 3, and 4. There are 2, 7, and 122 such trees, respectively.  (Hint : Use a symbol table.)\n3.3.35  2-3 trees. Write a program TwoThreeST.java that uses two node types to im-\nplement 2-3 search trees directly.\n3.3.36  2-3-4-5-6-7-8 trees. Describe algorithms for search and insertion in balanced \n2-3-4-5-6-7-8 search trees.\n3.3.37  Memoryless. Show that red-black BSTs are not memoryless: for example, if you \ninsert a key that is smaller than all the keys in the tree and then immediately delete the \nminimum, you may get a different tree.\n3.3.38  Fundamental theorem of  rotations. Show that any BST can be transformed into \nany other BST on the same set of keys by a sequence of left and right rotations.\nCREATIVE PROBLEMS  (continued)\n452 ", "start": 464, "end": 464}, "674": {"text": "tree.\n3.3.38  Fundamental theorem of  rotations. Show that any BST can be transformed into \nany other BST on the same set of keys by a sequence of left and right rotations.\nCREATIVE PROBLEMS  (continued)\n452 CHAPTER 3 \u25a0 Searching\n 3.3.39     Delete the minimum. Implement the deleteMin() operation for red-black \nBSTs by maintaining the correspondence with the transformations given in the text for \nmoving down the left spine of the tree while maintaining the invariant that the current \nnode is not a 2-node.\nSolution:\n   private Node moveRedLeft(Node h)\n   {  // Assuming that h is red and both h.left and h.left.left\n      // are black, make h.left or one of its children red.\n      flipColors(h);\n      if (isRed(h.right.left))\n      {\n         h.right = rotateRight(h.right);\n         h = rotateLeft(h);\n      }\n      return h;\n   }\n   public void deleteMin()\n   {\n      if (!isRed(root.left) && !isRed(root.right))\n         root.color = RED;\n      root = deleteMin(root);\n      if (!isEmpty()) root.color = BLACK;\n   }\n   private Node deleteMin(Node h)\n   {\n      if (h.left == null)\n         return null;\n      if (!isRed(h.left) && !isRed(h.left.left))\n         h = moveRedLeft(h);\n      h.left = deleteMin(h.left);\n      return balance(h);\n   }\nThis code assumes a balance() method that consists of the line of code\nif (isRed(h.right)) h = rotateLeft(h);\n4533.3 \u25a0 Balanced Search Trees\n followed by the last \ufb01ve lines of the recursive put() in Algorithm 3.4 and a \nflipColors() implementation that complements the three colors, instead of the \nmethod given in the text for insertion. For deletion, we set the ", "start": 464, "end": 466}, "675": {"text": "\ufb01ve lines of the recursive put() in Algorithm 3.4 and a \nflipColors() implementation that complements the three colors, instead of the \nmethod given in the text for insertion. For deletion, we set the parent to BLACK and the \ntwo children to RED.\n3.3.40     Delete the maximum. Implement the deleteMax() operation for red-black \nBSTs. Note that the transformations involved differ slightly from those in the previous \nexercise because red links are left-leaning.\nSolution:\n   private Node moveRedRight(Node h)\n   {  // Assuming that h is red and both h.right and h.right.left\n      // are black, make h.right or one of its children red.\n      flipColors(h)\n      if (!isRed(h.left.left))\n         h = rotateRight(h);\n      return h;\n    }\n   public void deleteMax()\n   {\n      if (!isRed(root.left) && !isRed(root.right))\n         root.color = RED;\n      root = deleteMax(root);\n      if (!isEmpty()) root.color = BLACK;\n   }\n   private Node deleteMax(Node h)\n   {\n      if (isRed(h.left))\n          h = rotateRight(h);\n      if (h.right == null)\n         return null;\n      if (!isRed(h.right) && !isRed(h.right.left))\n         h = moveRedRight(h);\n      h.right = deleteMax(h.right);\n      return balance(h);\n   }\nCREATIVE PROBLEMS  (continued)\n454 CHAPTER 3 \u25a0 Searching\n 3.3.41   Delete. Implement the delete() operation for red-black BSTs, combining the \nmethods of the previous two exercises with the delete() operation for BSTs.\nSolution :\n   public void delete(Key key)\n   {\n      if (!isRed(root.left) && !isRed(root.right))\n         root.color = RED;\n      root = delete(root, key);\n      if (!isEmpty()) root.color ", "start": 466, "end": 467}, "676": {"text": ":\n   public void delete(Key key)\n   {\n      if (!isRed(root.left) && !isRed(root.right))\n         root.color = RED;\n      root = delete(root, key);\n      if (!isEmpty()) root.color = BLACK;\n   }\n   private Node delete(Node h, Key key)\n   {\n      if (key.compareTo(h.key) < 0)\n      {\n         if (!isRed(h.left) && !isRed(h.left.left))\n            h = moveRedLeft(h);\n         h.left =  delete(h.left, key);\n      }\n      else\n      {\n         if (isRed(h.left))\n            h = rotateRight(h);\n         if (key.compareTo(h.key) == 0 && (h.right == null))\n            return null;\n         if (!isRed(h.right) && !isRed(h.right.left))\n            h = moveRedRight(h);\n         if (key.compareTo(h.key) == 0)\n         {\n            h.val = get(h.right, min(h.right).key);\n            h.key = min(h.right).key;\n            h.right = deleteMin(h.right);\n         }\n         else h.right = delete(h.right, key);\n      }\n      return balance(h);\n   }\n4553.3 \u25a0 Balanced Search Trees\n EXPERIMENTS\n3.3.42  Count red nodes. Write a program that computes the percentage of red nodes \nin a given red-black BST. T est your program  by running at least 100 trials of the experi-\nment of inserting N random keys into an initially empty tree, for N = 10 4, 10 5, and 10 6, \nand formulate an hypothesis.\n3.3.43    Cost plots. Instrument RedBlackBST so that you can produce plots like the \nones in this section showing the cost of each put() operation during the computation \n(see Exercise 3.1.38).\n3.3.44  Average search ", "start": 467, "end": 468}, "677": {"text": "RedBlackBST so that you can produce plots like the \nones in this section showing the cost of each put() operation during the computation \n(see Exercise 3.1.38).\n3.3.44  Average search time. Run empirical studies to compute the average and stan -\ndard deviation of the average length of a path to a random node (internal path length \ndivided by tree size) in a red-black BST built by insertion of N random keys into an \ninitially empty tree, for N from 1 to 10,000. Do at least 1,000 trials for each tree size. \nPlot the results in a  Tufte plot, like the one at the bottom of this page, \ufb01t with a curve \nplotting the function lg N \u2013  .5.\n3.3.45  Count rotations. Instrument your program for Exercise 3.3.43 to plot the \nnumber of rotations and node splits that are used to build the trees. Discuss the results.\n3.3.46  Height. Instrument your program for Exercise 3.3.43 to plot the height of \nred-black BSTs. Discuss the results.\nAverage path length to a random node in a red-black BST built from random keys \nlg N \u2212 .5\n20\n0\n10000operations\ncompares\n13\n100\n456 CHAPTER 3 \u25a0 Searching\n This page intentionally left blank \n 3.4    HASH TABLES\nIf keys are small integers, we can use an array to implement an unordered symbol table, \nby interpreting the key as an array index so that we can store the value associated with \nkey i in array entry i, ready for immediate access.  In this section, we consider hashing, \nan extension of this simple method that handles more complicated types of keys. We \nreference key-value pairs using arrays by doing arithmetic operations to transform keys \ninto array indices.\nSearch algorithms that ", "start": 468, "end": 470}, "678": {"text": "In this section, we consider hashing, \nan extension of this simple method that handles more complicated types of keys. We \nreference key-value pairs using arrays by doing arithmetic operations to transform keys \ninto array indices.\nSearch algorithms that use hashing consist of two separate parts. The \ufb01rst part is to \ncompute a    hash function that transforms the search key into an array index.  Ideally, \ndifferent keys would map to different indices. This ideal is gener-\nally beyond our reach, so we have to face the possibility that two \nor more different keys may hash to the same array index. Thus, \nthe second part of a hashing search is a   collision-resolution process \nthat deals with this situation. After describing ways to compute \nhash functions, we shall consider two different approaches to col-\nlision resolution: separate chaining and linear probing. \nHashing is a classic example of a  time-space tradeoff. If there \nwere no memory limitation, then we could do any search with \nonly one memory access by simply using the key as an index in \na (potentially huge) array.  This ideal often cannot be achieved, \nhowever, because the amount of memory required is prohibitive \nwhen the number of possible key values is huge.  On the other \nhand, if there were no time limitation, then we can get by with \nonly a minimum amount of memory by using sequential search \nin an unordered array. Hashing provides a way to use a reasonable \namount of both memory and time to strike a balance between \nthese two extremes. Indeed, it turns out that we can trade off time \nand memory in hashing algorithms by adjusting parameters, not by rewriting code. T o \nhelp choose values of the parameters, we use classical results from probability theory.\nProbability theory is a triumph of mathematical analysis that is beyond the scope of \nthis book, but the hashing algorithms we consider that take advantage of the knowl-\nedge gained from that ", "start": 470, "end": 470}, "679": {"text": "parameters, we use classical results from probability theory.\nProbability theory is a triumph of mathematical analysis that is beyond the scope of \nthis book, but the hashing algorithms we consider that take advantage of the knowl-\nedge gained from that theory are quite simple, and widely used. With hashing, you can \nimplement search and insert for symbol tables that require constant (amortized) time \nper operation in typical applications, making it the method of choice for implementing \nbasic symbol tables in many situations.\n0\n1\n2\n3\nM-1\nb pqr\na xyz\nd uvw\nc ijk\ncollision\nkey hash\na  2  xyz\nb  0  pqr\nc  3  ijk\nd  2  uvw\n..\n.\nvalue\nHashing: the crux of the problem\n458\n  \n \n  H a s h  f u n c t i o n s  The \ufb01rst problem that we face is the computation of the hash \nfunction, which transforms keys into array indices. If we have an array that can hold \nM key-value pairs, then we need a hash function that can transform any given key into \nan index into that array: an integer in the range [0, M \u2013 1]. We seek a hash function \nthat both is easy to compute and uniformly distributes the keys: for each key, every \ninteger between 0 and M \u2013 1 should be equally likely (independently for \nevery key). This ideal is somewhat mysterious; to understand hashing, \nit is worthwhile to begin by thinking carefully about how to implement \nsuch a function.\nThe hash function depends on the key type.  Strictly speaking, we need \na different hash function for each key type that we use. If the key involves a \nnumber, such as a social security number, we could start with that num-\nber; if the key involves a ", "start": 470, "end": 471}, "680": {"text": "speaking, we need \na different hash function for each key type that we use. If the key involves a \nnumber, such as a social security number, we could start with that num-\nber; if the key involves a string, such as a person\u2019s name, we need to con-\nvert the string into a number; and if the key has multiple parts, such as   \na mailing address, we need to combine the parts somehow. For many \ncommon types of keys, we can make use of default implementations pro-\nvided by Java. W e brie\ufb02y discuss potential implementations for various \ntypes of keys so that you can see what is involved because you do need to \nprovide implementations for key types that you create. \nTy pical ex ample. Suppose that we have an application where the keys \nare U.S. social security numbers. A social security number such as \n123-45-6789 is a nine-digit number divided into three \ufb01elds. The \ufb01rst \n\ufb01eld identi\ufb01es the geographical area where the number was issued (for \nexample, social security numbers whose \ufb01rst \ufb01eld is 035 are from Rhode \nIsland and numbers whose \ufb01rst \ufb01eld is 214 are from Maryland) and the \nother two \ufb01elds identify the individual. There are a billion (109) different \nsocial security numbers, but suppose that our application will need to \nprocess just a few hundred keys, so that we could use a hash table of size \nM = 1,000. One possible approach to implementing a hash function is \nto use three digits from the key. Using three digits from the third \ufb01eld is \nlikely to be preferable to using the three digits in the \ufb01rst \ufb01eld (since customers may not \nbe uniformly dispersed over geographic areas), but a better approach is to use all nine \ndigits to make an int value, ", "start": 471, "end": 471}, "681": {"text": "\nlikely to be preferable to using the three digits in the \ufb01rst \ufb01eld (since customers may not \nbe uniformly dispersed over geographic areas), but a better approach is to use all nine \ndigits to make an int value, then consider hash functions for integers, described next.\nPositive integers. The most commonly used method for hashing integers is called \n   modular hashing  : we choose the array size M to be prime and, for any positive inte-\nger key k, compute the remainder when dividing k by M. This function is very easy to \ncompute (k % M, in Java) and is effective in dispersing the keys evenly between 0 and \n212     12      18\n618     18      36\n302      2      11\n940     40      67\n702      2      23\n704      4      25\n612     12      30\n606      6      24\n772     72      93\n510     10      25\n423     23      35\n650     50      68\n317     17      26\n907      7      34\n507      7      22\n304      4      13\n714     14      35\n857     57      81\n801      1      25\n900      0      27\n413     13      25\n701      1      22\n418     18      30\n601      1      19\nkey hash\n(M = 100)\nhash\n(M = 97)\nModular hashing\n4593.4 \u25a0 Hash Tables\n  \n \n \nM \u2013 1. If M is not prime, it may be the case that not all of the bits of the key play a role, \nwhich amounts to missing an opportunity to disperse the values evenly. ", "start": 471, "end": 472}, "682": {"text": "Tables\n  \n \n \nM \u2013 1. If M is not prime, it may be the case that not all of the bits of the key play a role, \nwhich amounts to missing an opportunity to disperse the values evenly. For example, \nif the keys are base-10 numbers and M is 10 k, then only the k least signi\ufb01cant digits are \nused. As a simple example where such a choice might be problematic, suppose that the \nkeys are telephone area codes and M = 100. For historical reasons, most area codes in \nthe United States have middle digit 0 or 1, so this choice strongly favors the values less \nthan 20, where the use of the prime value 97 better disperses them (a prime value not \nclose to 100 would do even better). Similarly, IP addresses that are used in the internet \nare binary numbers that are not random for similar historical reasons as for telephone \narea codes, so we need to use a table size that is a prime (in particular, not a power of 2) \n i f  w e  w a n t  t o  u s e  m o d u l a r  h a s h i n g  t o  d i s p e r s e  t h e m .  \nFloating-point numbers. If the keys are real numbers between 0 and 1, we might just \nmultiply by M and round off to the nearest integer to get an index between 0 and M \u2013 1. \nAlthough this approach is intuitive, it is defective because it gives more weight to the \nmost signi\ufb01cant bits of the keys; the least signi\ufb01cant bits play no role. One way to ad-\ndress this situation is to use modular hashing on the binary representation of the key \n(this is what Java does).\nStrings. Modular hashing works for long keys such as strings, too: ", "start": 472, "end": 472}, "683": {"text": "signi\ufb01cant bits play no role. One way to ad-\ndress this situation is to use modular hashing on the binary representation of the key \n(this is what Java does).\nStrings. Modular hashing works for long keys such as strings, too: we simply treat \nthem as huge integers.  For example, the code at left computes a modular hash func-\ntion for a String s: recall that charAt() returns a char value in Java, which is a 16-bit \nnonnegative integer. If R is greater than any character value, this computation would \nbe equivalent to treating the String as \nan N-digit base-R integer, computing the \nremainder that results when dividing that \nnumber by M. A classic algorithm known \nas  Horner\u2019s method gets the job done with \nN multiplications, additions, and modulus \noperations. If the value of R is suf\ufb01ciently \nsmall that no over\ufb02ow occurs, the result is an integer between 0 and M \u2013 1, as desired. \nThe use of a small prime integer such as 31 ensures that the bits of all the characters \nplay a role. Java\u2019s default implementation for String uses a method like this.\n C o m p o u n d  k e y s .  If the key type has multiple integer \ufb01elds, we can typically mix them \ntogether in the way just described for String values. For example, suppose that search \nkeys are of type Date, which has three integer \ufb01elds: day (two-digit day), month (two-\ndigit month), and year (four-digit year).We compute the number\nint hash = (((day * R + month) % M ) * R + year) % M;\nint hash = 0; \nfor (int i = 0; i < s.length(); i++)\n   hash = (R * hash + s.charAt(i)) % M;\n H ", "start": 472, "end": 472}, "684": {"text": "* R + year) % M;\nint hash = 0; \nfor (int i = 0; i < s.length(); i++)\n   hash = (R * hash + s.charAt(i)) % M;\n H a s h i n g  a  s t r i n g  k e y\n460 CHAPTER 3 \u25a0 Searching\n  \nwhich, if the value of R is suf\ufb01ciently small that no over\ufb02ow occurs, is an integer be-\ntween 0 and M \u2013 1, as desired. In this case, we could save the cost of the inner % M opera-\ntion by choosing a moderate prime value such as 31 for R. As with strings, this method \ngeneralizes to handle any number of \ufb01elds. \n J a v a  c o n v e n t i o n s .  Java helps us address the basic problem that every type of data needs \na hash function by ensuring that every data type inherits a method called hashCode()\nthat returns a 32-bit integer. The implementation of  hashCode() for a data type must \nbe consistent with equals. That is, if a.equals(b) is true, then a.hashCode() must have \nthe same numerical value as b.hashCode(). Conversely, if the hashCode() values are \ndifferent, then we know that the objects are not equal. If the hashCode() values are \nthe same, the objects may or may not be equal, and we must use equals() to decide \nwhich condition holds. This convention is a basic requirement for clients to be able to \nuse hashCode() for symbol tables. Note that it implies that you must override both \nhashCode() and equals() if you need to hash with a user-de\ufb01ned type. The default \nimplementation returns the machine address of the key object, which is seldom what \nyou want. Java provides hashCode() implementations that override the defaults for \nmany common types ", "start": 472, "end": 473}, "685": {"text": "to hash with a user-de\ufb01ned type. The default \nimplementation returns the machine address of the key object, which is seldom what \nyou want. Java provides hashCode() implementations that override the defaults for \nmany common types (including String, Integer, Double, File, and URL).\nConverting a hashCode() to an array index. Since our goal is an array index, not a \n32-bit integer, we combine hashCode() with modular hashing in our implementations \nto produce integers between 0 and M \u2013 1, as follows:\nprivate int hash(Key x) \n{  return (x.hashCode() & 0x7fffffff) % M;  } \nThis code masks off the sign bit (to turn the 32-bit number into a 31-bit nonnegative \ninteger) and then computes the remainder when dividing by M, as in modular hashing. \nProgrammers commonly use a prime number for the hash table size M when using code \nlike this, to attempt to make use of all the \nbits of the hash code. Note: To avoid con-\nfusion, we omit all of these calculations in \nour hashing examples and use instead the \nhash values in the table at right. \nUser-de\ufb01ned hashCode(). Client code expects that hashCode() disperses the keys \nuniformly among the possible 32-bit result values. That is, for any object x, you can \nwrite x.hashCode() and, in principle, expect to get any one of the 2 32 possible 32-bit \nvalues with equal likelihood. Java\u2019s hashCode() implementations for String, Integer, \nDouble, File, and URL aspire to this functionality; for your own type, you have to \ntry to do it on your own. The Date example that we considered on page 460 illustrates \n Hash values for keys in examples\nS   E   A   R   C   H   X   M   P   L\n2   0   0 ", "start": 473, "end": 473}, "686": {"text": "own. The Date example that we considered on page 460 illustrates \n Hash values for keys in examples\nS   E   A   R   C   H   X   M   P   L\n2   0   0   4   4   4   2   4   3   3\n6  10   4  14   5   4  15   1  14   6\nkey\nhash (M = 5)\nhash (M = 16)\n4613.4 \u25a0 Hash Tables\n  \none way to proceed: make integers from \nthe instance variables and use modular \nhashing. In Java, the convention that all \ndata types inherit a hashCode() method \nenables an even simpler approach: use the \nhashCode() method for the instance vari-\nables to convert each to a 32-bit int value \nand then do the arithmetic, as illustrated \nat left for Transaction. For primitive-\ntype instance variables, note that a cast to \na wrapper type is necessary to access the \nhashCode() method. Again, the precise \nvalues of the multiplier ( 31 in our exam-\nple) is not particularly important.\nSoftware  caching. If computing the hash \ncode is expensive, it may be worthwhile to \ncache the hash for each key. That is, we maintain an instance variable hash in the key \ntype that contains the value of hashCode() for each key object (see Exercise 3.4.25). \nOn the \ufb01rst call to hashCode(), we have to compute the full hash code (and set the val-\nue of hash), but subsequent calls on hashCode() simply return the value of hash. Java \nuses this technique to reduce the cost of computing hashCode() for String objects.\nIn summary, we have three primary requirements in implementing a good hash \nfunction for a given data type:\n\u25a0 It should be consistent\u2014equal keys ", "start": 473, "end": 474}, "687": {"text": "Java \nuses this technique to reduce the cost of computing hashCode() for String objects.\nIn summary, we have three primary requirements in implementing a good hash \nfunction for a given data type:\n\u25a0 It should be consistent\u2014equal keys must produce the same hash value.\n\u25a0 It should be ef\ufb01cient to compute.\n\u25a0 It should uniformly distribute the keys.\nSatisfying these requirements simultaneously is a job for experts. As with many built-in \ncapabilities, Java programmers who use hashing assume that hashCode() does the job, \nabsent any evidence to the contrary.\nStill, you should be vigilant whenever using hashing in situations where good perfor-\nmance is critical, because a bad hash function is a classic example of a performance bug: \neverything will work properly, but much more slowly than expected. Perhaps the easiest \nway to ensure uniformity is to make sure that all the bits of the key play an equal role in \ncomputing every hash value; perhaps the most common mistake in implementing hash \nfunctions is to ignore signi\ufb01cant numbers of the key bits. Whatever the implementa-\ntion, it is wise to test any hash function that you use, when performance is important. \nWhich takes more time: computing a hash function or comparing two keys? Does your \npublic class  Transaction \n{\n   ...\n   private final String who;\n   private final Date when;\n   private final double amount;\n   public int hashCode()\n   {\n       int hash = 17;\n       hash = 31 * hash + who.hashCode();\n       hash = 31 * hash + when.hashCode();  \n       hash = 31 * hash\n           + ((Double) amount).hashCode();  \n       return hash;\n   }\n   ... \n}\n I m p l e m e n t i n g hashCode() in a user-defined type\n462 CHAPTER 3 \u25a0 Searching\n hash function spread a typical set of keys uniformly among the values between 0 and \nM \u2013 1? ", "start": 474, "end": 475}, "688": {"text": "l e m e n t i n g hashCode() in a user-defined type\n462 CHAPTER 3 \u25a0 Searching\n hash function spread a typical set of keys uniformly among the values between 0 and \nM \u2013 1? Doing simple experiments that answer these questions can protect future clients \nfrom unfortunate surprises. For example, the histogram above shows that our hash()\nimplementation using the hashCode() from Java\u2019s String data type produces a rea-\nsonable dispersion of the words for our Ta l e  of  Tw o  Ci t i e s example. \nUnderlying this discussion is a fundamental assumption that we make when using \nhashing, an idealized model that we do not actually expect to achieve, but that guides \nour thinking when implementing hashing algorithms:\nAssumption J (  uniform hashing assumption). The hash functions that we use uni-\nformly and independently distribute keys among the integer values between 0 and \nM \u2013 1.\nDiscussion: With all of the arbitrary choices that we have made, we certainly do not \nhave hash functions that uniformly and independently distribute keys in this strict \nmathematical sense. Indeed, the idea of implementing consistent functions that are \nguaranteed to uniformly and independently distribute keys leads to deep theoreti-\ncal studies that tell us that computing such a function easily is likely to be a very \nelusive goal. In practice, as with random numbers generated by Math.random(), \nmost programmers are content to have hash functions that cannot easily be distin-\nguished from random ones. Few programmers check for independence, however, \nand this property is rarely satis\ufb01ed.\nDespite the difficulty of validating it, Assumption J  is a useful way to think \nabout hashing for two primary reasons. First, it is a worthy goal when designing hash \nfunctions that guides us away from making arbitrary decisions that might lead to an \nexcessive number of collisions. Second, even though we may not be able to validate ", "start": 475, "end": 475}, "689": {"text": "primary reasons. First, it is a worthy goal when designing hash \nfunctions that guides us away from making arbitrary decisions that might lead to an \nexcessive number of collisions. Second, even though we may not be able to validate the \nassumption itself, it does enable us to use mathematical analysis to develop hypotheses \nabout the performance of hashing algorithms that we can check with experiments.\nHash value frequencies for words in Tale of Two Cities (10,679 keys, M = 97)\n0 96\nfrequency\nkey value\n110 /H11015 10679/97\n4633.4 \u25a0 Hash Tables\n  \n  \n \n \nHashing with    separate chaining A hash function converts keys into array in-\ndices. The second component of a hashing algorithm is collision resolution: a strategy \nfor handling the case when two or more keys to be inserted hash to the same index. A \nstraightforward and general approach to collision resolution is to build, for each of the \nM array indices, a linked list of the key-value pairs whose keys hash to that index. This \nmethod is known as separate chaining because items that collide are chained together \nin separate linked lists. The basic idea is to choose M to be suf\ufb01ciently large that the lists \nare suf\ufb01ciently short to enable ef\ufb01cient search through a two-step process: hash to \ufb01nd \nthe list that could contain the key, then sequentially search through that list for the key.\nOne way to proceed is to ex-\npand SequentialSearchST (Al-\ngorithm 3.1) to implement sep-\narate chaining using linked-list \nprimitives (see Exercise 3.4.2). \nA simpler (though slightly less \nef\ufb01cient) way to proceed is to \nadopt a more general approach: \nwe build, for each of the M ar -\nray indices, a symbol table of the \nkeys that hash to that index, thus \nreusing ", "start": 475, "end": 476}, "690": {"text": "\nef\ufb01cient) way to proceed is to \nadopt a more general approach: \nwe build, for each of the M ar -\nray indices, a symbol table of the \nkeys that hash to that index, thus \nreusing code that we have already \ndeveloped. The implementa-\ntion SeparateChainingHashST\nin Algorithm 3.5 maintains an \narray of SequentialSearchST \nobjects and implements get()\nand put() by computing a \nhash function to choose which \nSequentialSearchST object can contain the key and then using get() and put() (re-\nspectively) from SequentialSearchST to complete the job. \nSince we have M lists and N keys, the average length of the lists is always N /H11408 M, no \nmatter how the keys are distributed among the lists. For example, suppose that all the \nitems fall onto the \ufb01rst list\u2014the average length of the lists is ( N + 0 + 0 + 0 +. . . + 0)/M =  \nN /H11408 M. However the keys are distributed on the lists, the sum of the list lengths is N and \nthe average is N /H11408 M. Separate chaining is useful in practice because each list is extremely \nlikely to have about N /H11408 M key-value pairs. In typical situations, we can verify this con-\nsequence of Assumption J and count on fast search and insert.\nHashing with separate chaining for standard indexing client \nst\nfirst\n0\n1\n2\n3\n4\nS 0X 7\nE 12\nfirst\nfirst\nfirst\nfirst\nA 8\nP 10L 11\nR 3C 4H 5M 9\nindependent\nSequentialSearchST\nobjects\nS  2   0\nE  0   1\nA  0   2\nR ", "start": 476, "end": 476}, "691": {"text": "11\nR 3C 4H 5M 9\nindependent\nSequentialSearchST\nobjects\nS  2   0\nE  0   1\nA  0   2\nR  4   3\nC  4   4\nH  4   5\nE  0   6\nX  2   7\nA  0   8\nM  4   9\nP  3  10\nL  3  11\nE  0  12\nnull\nkey hash value\n464 CHAPTER 3 \u25a0 Searching\n ALGORITHM 3.5   Hashing with separate chaining\npublic class  SeparateChainingHashST<Key, Value> \n{\n   private int N;                           // number of key-value pairs\n   private int M;                           // hash table size\n   private SequentialSearchST<Key, Value>[] st;  // array of ST objects\n   public SeparateChainingHashST()\n   {  this(997);  }\n   public SeparateChainingHashST(int M)\n   {  // Create M linked lists.\n      this.M = M;\n      st = (SequentialSearchST<Key, Value>[]) new SequentialSearchST[M];\n      for (int i = 0; i < M; i++)\n         st[i] = new SequentialSearchST();\n   }\n   private int hash(Key key)\n   {  return (key.hashCode() & 0x7fffffff) % M; }\n   public Value get(Key key)\n   {  return (Value) st[hash(key)].get(key);  }\n   public void put(Key key, Value val)\n   {  st[hash(key)].put(key, val);  }\n   public Iterable<Key> keys()\n   // See Exercise 3.4.19.\n}\nThis basic symbol-table implementation maintains an array of linked ", "start": 476, "end": 477}, "692": {"text": "Value val)\n   {  st[hash(key)].put(key, val);  }\n   public Iterable<Key> keys()\n   // See Exercise 3.4.19.\n}\nThis basic symbol-table implementation maintains an array of linked lists, using a hash function to \nchoose a list for each key. For simplicity, we use SequentialSearchST methods. We need a cast when \ncreating st[] because Java prohibits arrays with generics. The default constructor speci\ufb01es 997 lists, \nso that for large tables, this code is about a factor of 1,000 faster than SequentialSearchST. This \nquick solution is an easy way to get good performance when you have some idea of the number of \nkey-value pairs to be put() by a client. A more robust solution is to use array resizing to make sure \nthat the lists are short no matter how many key-value pairs are in the table (see page 474 and Exercise \n3.4.18).\n4653.4 \u25a0 Hash Tables Proposition K. In  a separate-chaining hash table with M lists and N keys, the prob-\nability (under Assumption J) that the number of keys in a list is within a small \nconstant factor of N/M is extremely close to 1.\nProof sketch: Assumption J  makes this an application of classical probability \ntheory. We sketch the proof, for readers who are familiar with basic probabilistic \nanalysis. The probability that a given list will contain exactly k keys is given by the \nbinomial distribution\n\u0011 \u0012\nN\nk3 $\n1\nM\u0011 \u0012\nM \u2212 1\nM\nk N \u2212 k\nby the following argument: Choose k out of the N keys. Those k keys hash to the \ngiven list with probability 1 /H11408 M, and the other N \u2013 k keys do not hash to the given \nlist with probability 1 \u2013 (1 /H11408 ", "start": 477, "end": 478}, "693": {"text": "keys. Those k keys hash to the \ngiven list with probability 1 /H11408 M, and the other N \u2013 k keys do not hash to the given \nlist with probability 1 \u2013 (1 /H11408 M ). In terms of /H9251 /H11005 N /H11408 M,  we can rewrite this expres-\nsion as\nN \n\u0011 \u0012\u0011 \u0012k3 $\nA\nN\nA\nN\nk N \u2212 k\n1 \u2212 \n \nwhich (for small /H9251) is closely\napproximated by the classical\n Poisson distribution\nAke A\nk!\nIt follows that the probability that a list has more than t /H9251 keys on it is bounded \nby the quantity (/H9251 e/t)t e \u2013/H9251. This probability is extremely small for practical ranges \nof the parameters. For example, if the average length of the lists is 10, the prob-\nability that we will hash to some list with more than 20 keys on it is less than (10 \ne/2)2 e \u201310 /H11015 0.0084, and if the average length of the lists is 20, the probability that \nwe will hash to some list with more than 40 keys on it is less than (20 e/2)2 e \u201320 \n 0.0000016. This concentration result does not guarantee that every list will be \nshort. Indeed it is known that, if /H9251 is a constant, the average length of the longest \nlist grows with log N / log log N. \n.125\n0\n0 10 20 30\n(10, .12572...)\nPoisson distribution (N = 104, M = 103, /H9251 = 10) \nBinomial distribution (N = 104, M = 103, /H9251 ", "start": 478, "end": 478}, "694": {"text": "30\n(10, .12572...)\nPoisson distribution (N = 104, M = 103, /H9251 = 10) \nBinomial distribution (N = 104, M = 103, /H9251 = 10) \n.125\n0\n0 10 20 30\n(10, .12511...)\n466 CHAPTER 3 \u25a0 Searching\n  \nThis classical mathematical analysis is compelling, but it is important to note that \nit completely depends on Assumption J. If the hash function is not uniform and inde -\npendent, the search and insert cost could be proportional to N, no better than with \nsequential search. Assumption J is much stronger than the corresponding assumption \nfor other probabilistic algorithms that we have seen, and much more dif\ufb01cult to verify. \nWith hashing, we are assuming that each and every key, no matter how complex, is \nequally likely to be hashed to one of M indices. We cannot afford to run experiments \nto test every possible key, so we would have to do more sophisticated experiments in -\nvolving random sampling from the set of possible keys used in an application, followed \nby statistical analysis. Better still, we can use the algorithm itself as part of the test, to \nvalidate both Assumption J and the mathematical results that we derive from it.\nProperty L. In a  separate-chaining hash table with M lists and N keys, the number \nof compares (equality tests) for search miss and insert is ~N/M.\nEvidence: Good performance of the algorithms in practice does not require the \nhash function to be fully uniform in the technical sense of Assumption J. Count-\nless programmers since the 1950s have seen the speedups predicted by Proposi-\ntion K, even for hash functions that are certainly not uniform. For example, the \ndiagram on page 468 shows that list length distribution for our FrequencyCounter \nexample ", "start": 478, "end": 479}, "695": {"text": "1950s have seen the speedups predicted by Proposi-\ntion K, even for hash functions that are certainly not uniform. For example, the \ndiagram on page 468 shows that list length distribution for our FrequencyCounter \nexample (using our hash() implementation based on the hashCode() from Java\u2019s \nString data type) precisely matches the theoretical model. One exception that has \nbeen documented on numerous occasions is poor performance due to hash func-\ntions not taking all of the bits of the keys into account. Otherwise, the preponder -\nance of the evidence from the experience of practical programmers puts us on \nsolid ground in stating that hashing with separate chaining using an array of size M\nspeeds up search and insert in a symbol table by a factor of M.\nTa b l e  s i z e .  In a separate-chaining implementation, our goal is to choose the table size  \nM to be suf\ufb01ciently small that we do not waste a huge area of contiguous memory \nwith empty chains but suf\ufb01ciently large that we do not waste time searching through \nlong chains. One of the virtues of separate chaining is that this decision is not critical: \nif more keys arrive than expected, then searches will take a little longer than if we had \nchosen a bigger table size ahead of time; if fewer keys are in the table, then we have ex-\ntra-fast search with some wasted space. When space is not a critical resource, M can be \nchosen suf\ufb01ciently large that search time is constant; when space is a critical resource, \nwe still can get a factor of M improvement in performance by choosing M to be as \n4673.4 \u25a0 Hash Tables\n large as we can afford. For our example FrequencyCounter study, we see in the \ufb01gure \nbelow a reduction in the average cost from thousands of compares per operation for \nSequentialSearchST to a small constant for SeparateChainingHashST, ", "start": 479, "end": 480}, "696": {"text": "we can afford. For our example FrequencyCounter study, we see in the \ufb01gure \nbelow a reduction in the average cost from thousands of compares per operation for \nSequentialSearchST to a small constant for SeparateChainingHashST, as expected. \nAnother option is to use array resizing to keep the lists short (see Exercise 3.4.18).\n D e l e t i o n .  To  d e l e te  a  ke y - v a l u e  p a i r, s i m p l y  h a s h  to  \ufb01 n d  t h e  SequentialSearchST\ncontaining the key, then invoke the delete() method for that table (see Exercise \n3.1.5). Reusing code in this way is preferable to reimplementing this basic operation \non linked lists.\nOrdered operations. The whole point of hashing is to uniformly disperse the keys, so \nany order in the keys is lost when hashing. If you need to quickly \ufb01nd the maximum \nor minimum key, \ufb01nd keys in a given range, or implement any of the other operations \nin the ordered symbol-table API on page 366, then hashing is not appropriate, since these \noperations will all take linear time.\nHashing with separate chaining is easy to implement and probably the fastest (and \nmost widely used) symbol-table implementation for applications where key order is \nnot important. When your keys are built-in Java types or your own type with well-\ntested implementations of hashCode(), Algorithm 3.5 provides a quick and easy path \nto fast search and insert. Next, we consider an alternative scheme for collision resolu -\ntion that is also effective.\n125\n0\n0 10 20 30\n= 10.711...)\nke /H9251\nk!\nList lengths for java FrequencyCounter 8 < tale.txt using SeparateChainingHashST ", "start": 480, "end": 480}, "697": {"text": "also effective.\n125\n0\n0 10 20 30\n= 10.711...)\nke /H9251\nk!\nList lengths for java FrequencyCounter 8 < tale.txt using SeparateChainingHashST   \nlist lengths (10,679 keys, M = 997)\nfrequency\nCosts for java FrequencyCounter 8 < tale.txt using SeparateChainingHashST (M = 997)\n3.9\n10\n0\n0 14350operations\nequality tests\ncumulative\naverage\n468 CHAPTER 3 \u25a0 Searching\n  H a s h i n g  w i t h     l i n e a r  p r o b i n g  Another approach to implementing hashing is to \nstore N key-value pairs in a hash table of size M > N, relying on empty entries in the \ntable to help with collision resolution. Such methods are called open-addressing hashing \nmethods.\nThe simplest open-addressing method is called linear probing: when there is a colli-\nsion (when we hash to a table index that is already occupied with a key different from \nthe search key), then we just check the next entry in the table (by incrementing the \nindex). Linear probing is characterized by identifying three possible outcomes:\n\u25a0 Key equal to search key: search hit\n\u25a0 Empty position (null key at indexed position): search miss\n\u25a0 Key not equal to search key: try next entry \nWe hash the key to a table index, check whether the search key matches the key there, \nand continue (incrementing the index, wrapping back to the beginning of the table \nif we reach the end) until \ufb01nding either the search key or an empty table entry. It is \ncustomary to refer to the operation of determining whether or not a given table entry \n0  1  2  3  4  5  6  7  8 ", "start": 480, "end": 481}, "698": {"text": "entry. It is \ncustomary to refer to the operation of determining whether or not a given table entry \n0  1  2  3  4  5  6  7  8  9  \n                  S                                            \n                  0                                            \n                  S           E\n                  0           1   \n            A     S           E\n            2     0           1\n            A     S           E           R\n            2     0           1           3    \n            A  C  S           E           R           \n            2  5  0           1           3      \n            A  C  S  H        E           R    \n            2  5  0  5        1           3   \n            A  C  S  H        E           R\n            2  5  0  5        6           3\n            A  C  S  H        E           R  X          \n            2  5  0  5        6           3  7         \n            A  C  S  H        E           R  X              \n            8  5  0  5        6           3  7      \n   M        A  C  S  H        E           R  X\n   9        8  5  0  5        6           3  7\nP  M        A  C  S  H        E           R  X  \n   9        8  5  0  5        6           3  7 \nP  M        A  C  S  H  L     E           R  X       \n   9        8  5  0  5        6           3 ", "start": 481, "end": 481}, "699": {"text": "3  7 \nP  M        A  C  S  H  L     E           R  X       \n   9        8  5  0  5        6           3  7 \nP  M        A  C  S  H  L     E           R  X        \n   9        8  5  0  5                    3  7  \n10 11 12 13 14 15\n11    12\n1110\n10\n10\nTrace of  linear-probing ST implementation for standard indexing client\nentries in gray \nare untouched\nprobe sequence \nwraps to 0\nentries in red\nare new\nkeys in black\nare probes\nS   6   0\nE  10   1\nA   4   2\nR  14   3\nC   5   4\nH   4   5\nE  10   6\nX  15   7\nA   4   8\nM   1   9\nP  14  10\nL   6  11\nE  10  12 keys[]\nvals[]\nkey hash value\n4693.4 \u25a0 Hash Tables\n ALGORITHM 3.6   Hashing with linear probing\npublic class  LinearProbingHashST<Key, Value> \n{\n   private int N;         // number of key-value pairs in the table\n   private int M = 16;    // size of linear-probing table                \n   private Key[] keys;    // the keys                                      \n   private Value[] vals;  // the values                       \n   public LinearProbingHashST()\n   {\n      keys = (Key[])   new Object[M];\n      vals = (Value[]) new Object[M];\n   }\n   private int hash(Key ", "start": 481, "end": 482}, "700": {"text": "Value[] vals;  // the values                       \n   public LinearProbingHashST()\n   {\n      keys = (Key[])   new Object[M];\n      vals = (Value[]) new Object[M];\n   }\n   private int hash(Key key)\n   {  return (key.hashCode() & 0x7fffffff) % M; }\n   private void resize()        // See page 474.\n   public void put(Key key, Value val)\n   {\n      if (N >= M/2) resize(2*M);  // double M (see text)\n      int i;\n      for (i = hash(key); keys[i] != null; i = (i + 1) % M)\n         if (keys[i].equals(key)) { vals[i] = val; return; }\n      keys[i] = key;\n      vals[i] = val;\n       N++;\n   }\n   public Value get(Key key)\n   {\n      for (int i = hash(key); keys[i] != null; i = (i + 1) % M)\n         if (keys[i].equals(key))\n             return vals[i];\n      return null;\n   } \n}\n \nThis symbol-table implementation keeps keys and values in parallel arrays (as in BinarySearchST) \nbut uses empty spaces (marked by null) to terminate clusters of keys. If a new key hashes to an empty \nentry, it is stored there; if not, we scan sequentially to \ufb01nd an empty position. T o search for a key, we \nscan sequentially starting at its hash index until \ufb01nding null (search miss) or the key (search hit). \nImplementation of keys() is left as Exercise 3.4.19.\n470 CHAPTER 3 \u25a0 Searching holds an item whose key is equal to the search key as a  probe. We use the term inter -\nchangeably with the term compare that we have been using, even though some probes \nare tests ", "start": 482, "end": 483}, "701": {"text": "CHAPTER 3 \u25a0 Searching holds an item whose key is equal to the search key as a  probe. We use the term inter -\nchangeably with the term compare that we have been using, even though some probes \nare tests for null. \nThe essential idea behind hashing with open addressing is this: rather than using mem-\nory space for references in linked lists, we use it for the empty entries in the hash table, \nwhich mark the ends of probe sequences. As you can see from LinearProbingHashST\n(Algorithm 3.6), applying this idea to implement the symbol-table API is quite \nstraightforward. We implement the table with  parallel arrays, one for the keys and one \nfor the values, and use the hash function as an index to access the data as just discussed.\nDeletion. How do we delete a key-value pair from a linear-probing table? If you think \nabout the situation for a moment, you will see that setting the key\u2019s table position to \nnull will not work, because that might prematurely terminate the search for a key that \nwas inserted into the table later. As an example, suppose that we try to delete C in this \nway in our trace example, then search for H. The \nhash value for H is 4, but it sits at the end of the \ncluster, in position 7. If we set position 5 to null, \nthen get() will not \ufb01nd H.  As a consequence, we \nneed to reinsert into the table all of the keys in the \ncluster to the right of the deleted key. This process \nis trickier than it might seem, so you are encour -\naged to trace through the code at right for an ex-\nample that exercises it (see Exercise 3.4.17).  \nAs with separate chaining, the performance of \nhashing with open addressing depends on the ratio \n/H11005 N /H11408 ", "start": 483, "end": 483}, "702": {"text": "right for an ex-\nample that exercises it (see Exercise 3.4.17).  \nAs with separate chaining, the performance of \nhashing with open addressing depends on the ratio \n/H11005 N /H11408 M, but we interpret it differently.  We refer \nto /H9251 as the    load factor of a hash table. For separate \nchaining, /H9251 is the average number of keys per list \nand is generally larger than 1; for linear probing, \n is the percentage of table entries that are occu -\npied; it cannot be greater than 1. In fact, we cannot \nlet the load factor reach 1 (completely full table) \nin LinearProbingHashST because a search miss \nwould go into an in\ufb01nite loop in a full table. In -\ndeed, for the sake of good performance, we use array resizing to guarantee that the load \nfactor is between one-eighth and one-half. This strategy is validated by mathematical \nanalysis, which we consider before we discuss implementation details. \npublic void delete(Key key) \n{\n   if (!contains(key)) return;\n   int i = hash(key);\n   while (!key.equals(keys[i]))\n      i = (i + 1) % M;\n   keys[i] = null;\n   vals[i] = null;\n   i = (i + 1) % M;\n   while (keys[i] != null)\n   {\n      Key   keyToRedo = keys[i];\n      Value valToRedo = vals[i];\n      keys[i] = null;\n      vals[i] = null;\n      N--;  \n      put(keyToRedo, valToRedo);\n      i = (i + 1) % M;\n   }\n   N--;   \n   if (N > 0 N == M/8) resize(M/2); \n}\n D e l e t i ", "start": 483, "end": 483}, "703": {"text": "valToRedo);\n      i = (i + 1) % M;\n   }\n   N--;   \n   if (N > 0 N == M/8) resize(M/2); \n}\n D e l e t i o n  f o r  l i n e a r  p r o b i n g\n4713.4 \u25a0 Hash Tables\n  C l u s t e r i n g .  The average cost of linear probing depends on the way in which the entries \nclump together into contiguous groups of occupied table entries, called clusters, when \nthey are inserted. For example, when the key C is inserted \nin our example, the result is a cluster ( A C S ) of length \n3, which means that four probes are needed to insert H\nbecause H hashes to the \ufb01rst position in the cluster. Short \nclusters are certainly a requirement for ef\ufb01cient perfor -\nmance. This requirement can be problematic as the table \n\ufb01lls, because long clusters are common. Moreover, since \nall table positions are equally likely to be the hash value \nof the next key to be inserted (under the uniform hash-\ning assumption), long clusters are more likely to increase \nin length than short ones, because a new key hashing to \nany entry in the cluster will cause the cluster to increase \nin length by 1 (and possibly much more, if there is just one table entry separating the \ncluster from the next one). Next, we turn to the challenge of quantifying the effect of \nclustering to predict performance in linear probing, and using that knowledge to set \ndesign parameters in our implementations.\nTable occupancy patterns (2,048 keys, tables laid out in 128-position rows)\nlong clusters are common\n = 1/2\n = 1/4\nkeys[0..127]\nkeys[8064..8192]\nlinear probing random\n9/64 ", "start": 483, "end": 484}, "704": {"text": "keys, tables laid out in 128-position rows)\nlong clusters are common\n = 1/2\n = 1/4\nkeys[0..127]\nkeys[8064..8192]\nlinear probing random\n9/64 chance of new key\nhitting this cluster\nkey lands here\nin that event\nand forms a much\nlonger cluster\nClustering in linear probing (M = 64)\nbefore\nafter\n472 CHAPTER 3 \u25a0 Searching\n Analysis of linear probing. Despite the relatively simple form of the results, precise \nanalysis of linear probing is a very challenging task.  Knuth\u2019s derivation of the following \nformulas in 1962 was a landmark in the analysis of algorithms:\nProposition M. In a linear-probing hash table with M lists and N =  /H9251 M keys, the \naverage number of probes (under Assumption J) required is\n1 \n\u0011 \u00122 1 \u2212 A\n11 +  1 \n\u0011 \u00122 (1 \u2212 A)2\n11 +  and~ ~\n \n \nfor search hits and search misses (or inserts), respectively. In particular, when  /H9251 \nis about 1/2, the average number of probes for a search hit is about 3/2 and for a \nsearch miss is about 5/2. These estimates lose a bit of precision as /H9251 approaches 1, \nbut we do not need them for that case, because we will only use linear probing for \n less than one-half.\nDiscussion: We compute the average by computing the cost of  a search miss star t-\ning at each position in the table, then dividing the total by M.  All search misses \ntake at least 1 probe, so we count the number of probes after the \ufb01rst. Consider the \nfollowing two extremes in a linear-probing table that is half full ( M = ", "start": 484, "end": 485}, "705": {"text": "M.  All search misses \ntake at least 1 probe, so we count the number of probes after the \ufb01rst. Consider the \nfollowing two extremes in a linear-probing table that is half full ( M = 2N): In the \nbest case, table positions with even indices could be empty, and table positions with \nodd indices could be occupied.  In the worst case, the \ufb01rst half of the table positions \ncould be empty, and the second half occupied. The average length of the clusters \nin both cases is N/(2N) = 1/2, but the average number of probes for a search miss \nis 1 (all searches take at least 1 probe) plus (0 + 1 + 0 + 1 +.  .  . )/(2 N) = 1/2 in the \nbest case, and is 1 plus ( N + (N \u2013 1)  + . . .)  /H11408 (2 N) ~ N/4 in the worst case. This \nargument generalizes to show that the average number of probes for a search miss \nis proportional to the squares of the lengths of the clusters: If a cluster is of length t, \nthen the expression (t + (t \u2013 1) + .  .  . + 2 + 1) / M = t(t + 1)/(2M) counts the con-\ntribution of that cluster to the grand total. The sum of the cluster lengths is N, so, \nadding this cost for all entries in the table, we \ufb01nd that the total average cost for a \nsearch miss is 1 + N /H11408 (2M) plus the sum of the squares of the lengths of the clusters, \ndivided by 2M.  Thus, given a table, we can quickly compute the average cost of a \nsearch miss in ", "start": 485, "end": 485}, "706": {"text": "/H11408 (2M) plus the sum of the squares of the lengths of the clusters, \ndivided by 2M.  Thus, given a table, we can quickly compute the average cost of a \nsearch miss in that table (see Exercise 3.4.21). In general, the clusters are formed \nby a complicated dynamic process (the linear-probing algorithm) that is dif\ufb01cult to \ncharacterize analytically, and quite beyond the scope of this book. \n4733.4 \u25a0 Hash Tables\n Proposition M tells us (under our usual Assumption J) that we can expect a search to \nrequire a huge number of probes in a nearly full table (as /H9251 approaches 1 the values of \nthe formulas describing the number of probes grow very large) but that the expected \nnumber of probes is between 1.5 and 2.5 if we can ensure that the load factor /H9251 is less \nthan 1/2. Next, we consider the use of array resizing for this purpose.\n   A r r a y  r e s i z i n g  We can use our standard ar ray-resizing technique from Chapter \n1 to ensure that the load factor never exceeds one-half. First, we need a new construc-\ntor for LinearProbingHashST that takes a \ufb01xed capacity as argument (add a line to \nthe constructor in Algorithm \n3.6 that sets M to the given value \nbefore creating the arrays). Next, \nwe need the resize() method \ngiven at left, which creates a new \nLinearProbingHashST of the giv -\nen size, puts all the keys and values \nin the table in the new one, then \nrehashes all the keys into the new \ntable. These additions allow us to \nimplement array doubling. The call \nto resize() in the \ufb01rst statement \nin put() ", "start": 485, "end": 486}, "707": {"text": "the table in the new one, then \nrehashes all the keys into the new \ntable. These additions allow us to \nimplement array doubling. The call \nto resize() in the \ufb01rst statement \nin put() ensures that the table is at \n \nmost one-half full. This code builds a hash table twice the size with the same keys, thus \nhalving the value of /H9251. As in other applications of array resizing, we also need to add\n if (N > 0 && N <= M/8) resize(M/2);\nas the last statement in delete() to ensure that the table is at least one-eighth full. \nThis ensures that the amount of memory used is always within a constant factor of the \nnumber of key-value pairs in the table. With array resizing, we are assured that /H9251 /H11349 1/2. \nSeparate chaining. The same method works to keep lists short (of average \nlength between 2 and 8) in separate chaining: replace LinearProbingHashST by \nSeparateChainingHashST in resize(), call resize(2*M) when (N >= M/2) in put(), \nand call resize(M/2) when (N > 0 && N <= M/8) in delete(). For separate chain-\ning, array resizing is optional and not worth your trouble if you have a decent estimate \nof the client\u2019s N: just pick a table size M based on the knowledge that search times are \nproportional to 1+ N/M. For linear probing, array resizing is necessary. A client that \ninserts more key-value pairs than you expect will encounter not just excessively long \nsearch times, but an in\ufb01nite loop when the table \ufb01lls.\nprivate void resize(int cap) \n{\n    LinearProbingHashST<Key, Value> t;\n    t = new LinearProbingHashST<Key, ", "start": 486, "end": 486}, "708": {"text": "\nsearch times, but an in\ufb01nite loop when the table \ufb01lls.\nprivate void resize(int cap) \n{\n    LinearProbingHashST<Key, Value> t;\n    t = new LinearProbingHashST<Key, Value>(cap);\n    for (int i = 0; i < M; i++)\n       if (keys[i] != null)\n           t.put(keys[i], vals[i]);\n    keys = t.keys;\n    vals = t.vals;\n    M    = t.M; \n}\n R e s i z i n g  a  l i n e a r - p r o b i n g  h a s h  t a b l e\n474 CHAPTER 3 \u25a0 Searching\n  A m o r t i z e d  a n a l y s i s .  From a theoretical standpoint, when we use array resizing, we \nmust settle for an  amortized bound, since we know that those insertions that cause the \ntable to double will require a large number of probes. \n \nhash table is built with array resizing, starting with Proposition N. Suppose a    \nan empty table. Under Assumption J, any sequence of t search, insert, and delete\nsymbol-table operations is executed in expected time proportional to t and with \nmemory usage always within a constant factor of the number of keys in the table.\nProof.: For both separate chaining and linear probing, this fact follows from a sim-\nple restatement of the amortized analysis for array growth that we \ufb01rst discussed in \nChapter 1, coupled with Proposition K and Proposition M.\n10\n0\n0 14350\n4.2\ncumulative\naverage\nCosts for java FrequencyCounter 8 < tale.txt using SeparateChainingHashST  (with doubling)\noperations\nequality tests\n10\n0\n0 14350operations\nequality tests 3.2\ncumulative\naverage\nCosts ", "start": 486, "end": 487}, "709": {"text": "for java FrequencyCounter 8 < tale.txt using SeparateChainingHashST  (with doubling)\noperations\nequality tests\n10\n0\n0 14350operations\nequality tests 3.2\ncumulative\naverage\nCosts for java FrequencyCounter 8 < tale.txt using LinearProbingHashST (with doubling)\n4753.4 \u25a0 Hash Tables\n  \nThe plots of the cumulative averages   for our FrequencyCounter example (shown at \nthe bottom of the previous page) nicely illustrate the dynamic behavior of array resiz-\ning in hashing. Each time the array doubles, the cumulative average increases by about \n1, because each key in the table needs to be rehashed; then it decreases because about \nhalf as many keys hash to each table position, with the rate of decrease slowing as the \ntable \ufb01lls again.\n  M e m o r y  As we have indicated, understanding memory usage is an important factor \nif we want to tune hashing algorithms for optimum performance. While such tuning \nis for experts, it is a worthwhile exercise to calculate a rough estimate of the amount of \nmemory required, by estimating the number of references used, as follows: Not counting \nthe memory for keys and values, our implementation SeparateChainingHashST uses \nmemory for M references to SequentialSearchST objects plus M SequentialSearchST\nobjects. Each SequentialSearchST object has the usual 16 bytes of object overhead \nplus one 8-byte reference (first), and there are a total of N Node objects, each with 24 \nbytes of object overhead plus 3 references (key, value, and next). This compares with \nan extra reference per node for binary search trees. With array resizing to ensure that \nthe table is between one-eighth and one-half full, linear probing uses between 4 N and \n16N references. Thus, choosing hashing on the basis of memory usage is not normally \njusti\ufb01ed. The ", "start": 487, "end": 488}, "710": {"text": "\nthe table is between one-eighth and one-half full, linear probing uses between 4 N and \n16N references. Thus, choosing hashing on the basis of memory usage is not normally \njusti\ufb01ed. The calculation is a bit different for primitive types (see Exercise 3.4.24)\nmethod space usage for N items\n(reference types)\nseparate chaining ~ 48 N + 64 M \nlinear probing between\n~32 N and ~128 N\nBSTs ~56 N\n S p a c e   u s a g e  i n  s y m b o l  t a b l e s\n476 CHAPTER 3 \u25a0 Searching\n Since the earliest days of computing , researchers have studied (and are study -\ning) hashing and have found many ways to improve the basic algorithms that we have \ndiscussed.  Y ou can \ufb01nd a huge literature on the subject. Most of the improvements \npush down the space-time curve: you can get the same running time for searches using \nless space or get faster searches using the same amount of space.  Other improvements \ninvolve better guarantees, on the expected worst-case cost of a search. Others involve \nimproved hash-function designs. Some of these methods are addressed in the exercises.\nDetailed comparison of separate chaining and linear probing depends on myriad \nimplementation details and on client space and time requirements. It is not normally \njusti\ufb01ed to choose separate chaining over linear probing on the basis of performance \n(see Exercise 3.5.31). In practice, the primary performance difference between the two \nmethods has to do with the fact that separate chaining uses a small block of memory \nfor each key-value pair, while linear probing uses two large arrays for the whole table. \nFor huge tables, these needs place quite different burdens on the memory management \nsystem. In modern systems, this sort of tradeoff is best addressed by experts in extreme ", "start": 488, "end": 489}, "711": {"text": "while linear probing uses two large arrays for the whole table. \nFor huge tables, these needs place quite different burdens on the memory management \nsystem. In modern systems, this sort of tradeoff is best addressed by experts in extreme \nperformance-critical situations. \nWith hashing, under generous assumptions, it is not unreasonable to expect to \nsupport the search and insert symbol-table operations in constant time, independent \nof the size of the table. This expectation is the theoretical optimum performance for \nany symbol-table implementation. Still, hashing is not a panacea, for several reasons, \nincluding: \n\u25a0 A good hash function for each type of key is required.\n\u25a0 The performance guarantee depends on the quality of the hash function.\n\u25a0  Hash functions can be dif\ufb01cult and expensive to compute.\n\u25a0 Ordered symbol-table operations are not easily supported.\nBeyond these basic considerations, we defer the comparison of hashing with the other \nsymbol-table methods that we have studied to the beginning of Section 3.5.\n4773.4 \u25a0 Hash Tables\n Q&A\n \n \n \nQ. How does Java implement hashCode() for Integer, Double, and Long?\nA. For Integer it just returns the 32-bit value. For Double and Long it returns the ex-\nclusive or of the \ufb01rst 32 bits with the second 32 bits of the standard machine representa-\ntion of the number. These choices may not seem to be very \nrandom, but they do serve the purpose of spreading out the \nvalues. \n Q .  When using array resizing, the size of the table is always \na power of 2. Isn\u2019t that a potential problem, because it only \nuses the least signi\ufb01cant bits of hashCode()?\nA. Ye s , p a r t i c u l a r l y  w i t h  t h e  d e f a u l t  i m p l e m e n t a t i o ", "start": 489, "end": 490}, "712": {"text": "Ye s , p a r t i c u l a r l y  w i t h  t h e  d e f a u l t  i m p l e m e n t a t i o n s . O n e  \nway to address this problem is to \ufb01rst distribute the key val-\nues using a prime larger than M, as in the following example:\nprivate int hash(Key x) \n{  \n   int t = x.hashCode() & 0x7fffffff; \n   if (lgM < 26) t = t % primes[lgM+5]; \n   return t % M; \n} \nThis code assumes that we maintain an instance variable \nlgM that is equal to lg M (by initializing to the appropri-\nate value, incrementing when doubling, and decrementing \nwhen halving) and an array primes[] of the smallest prime \ngreater than each power of 2 (see the table at right). The \nconstant 5 is an arbitrary choice\u2014we expect the \ufb01rst % to \ndistribute the values equally among the values less than the \nprime and the second to map about \ufb01ve of those values to \neach value less than M. Note that the point is moot for large \nM.\nQ. I\u2019ve forgotten. Why don\u2019t we implement hash(x) by returning x.hashCode() % M?\nA. We need a result between 0 and M-1, but in Java, the % function may be negative.\nQ. So, why not implement hash(x) by returning Math.abs(x.hashcode()) % M?\nPrimes for hash table sizes\nk/H9254 k (2k \u2212 /H9254k)\n 5     1             31\n 6     3             61\n 7     1            127\n 8     5            251\n 9     3            509\n10 ", "start": 490, "end": 490}, "713": {"text": "5     1             31\n 6     3             61\n 7     1            127\n 8     5            251\n 9     3            509\n10     3           1021\n11     9           2039\n12     3           4093\n13     1           8191\n14     3          16381\n15    19          32749\n16    15          65521\n17     1         131071\n18     5         262139\n19     1         524287\n20     3        1048573\n21     9        2097143\n22     3        4194301\n23    15        8388593\n24     3       16777213\n25    39       33554393\n26     5       67108859\n27    39      134217689\n28    57      268435399\n29     3      536870909\n30    35     1073741789\n31     1     2147483647\nprimes[k]\n478 CHAPTER 3 \u25a0 Searching\n  \nA. Nice try. Unfortunately, Math.abs() returns a negative result for the largest nega-\ntive number.  For many typical calculations, this over\ufb02ow presents no real problem, \nbut for hashing it would leave you with a program that is likely to crash after a few bil-\nlion inserts, an unsettling possibility. For example, s.hashCode() is /H11002231 for the Java \nString value \"polygenelubricants\". Finding other strings that hash to this value \n(and to 0) has turned into an amusing algorithm-puzzle pastime. \nQ.  Why not use BinarySearchST or RedBlackBST ", "start": 490, "end": 491}, "714": {"text": "value \"polygenelubricants\". Finding other strings that hash to this value \n(and to 0) has turned into an amusing algorithm-puzzle pastime. \nQ.  Why not use BinarySearchST or RedBlackBST instead of SequentialSearchST in \nAlgorithm 3.5?\nA. Generally, we set parameters so as to make the number of keys hashing to each value \nsmall, and elementary symbol tables are generally better for the small tables. In certain \nsituations, slight performance gains may be achieved with such hybrid methods, but \nsuch tuning is best left for experts.\nQ.  Is hashing faster than searching in red-black BSTs?\nA. It depends on the type of the key, which determines the cost of computing \nhashCode() versus the cost of compareTo(). For typical key types and for Java default \nimplementations, these costs are similar, so hashing will be signi\ufb01cantly faster, since it \nuses only a constant number of operations. But it is important to remember that this \nquestion is moot if you need ordered operations, which are not ef\ufb01ciently supported in \nhash tables. See Section 3.5 for further discussion.\nQ. Why not let the linear probing table get, say, three-quarters full?\nA. No particular reason. You can choose any value of /H9251, using Proposition M to esti-\nmate search costs. For /H9251 = 3/4, the average cost of search hits is 2.5 and search misses is \n8.5, but if you let /H9251 grow to 7/8, the average cost of a search miss is 32.5, perhaps more \nthan you want to pay. As /H9251 gets close to 1, the estimate in Proposition M becomes in-\nvalid, but you don\u2019t want your table to get that close to being full.\n4793.4 \u25a0 ", "start": 491, "end": 491}, "715": {"text": "you want to pay. As /H9251 gets close to 1, the estimate in Proposition M becomes in-\nvalid, but you don\u2019t want your table to get that close to being full.\n4793.4 \u25a0 Hash Tables\n EXERCISES\n \n \n \n3.4.1 Insert the keys E A S Y Q U T I O N in that order into an initially empty table \nof M = 5 lists, using separate chaining.  Use the hash function 11 k % M to transform \nthe kth letter of the alphabet into a table index.\n3.4.2  Develop an alternate implementation of SeparateChainingHashST that directly \nuses the linked-list code from SequentialSearchST.\n3.4.3  Modify your implementation of the previous exercise to include an integer \ufb01eld \nfor each key-value pair that is set to the number of entries in the table at the time that \npair is inserted.  Then implement a method that deletes all keys (and associated values)  \nfor which the \ufb01eld is greater than a given integer k. Note : This extra functionality is use-\nful in implementing the symbol table for a compiler.\n3.4.4 Write a program to \ufb01nd values of a and M, with M as small as possible, such that \nthe hash function  (a * k) %  M  for transforming the kth letter of the alphabet into a \ntable index produces distinct values (no collisions) for the keys S E A R C H X M P L. \nThe result is known as a   perfect hash function.\n3.4.5 Is the following implementation of hashCode() legal?\npublic int hashCode() \n{  return 17;  } \nIf so, describe the effect of using it. If not, explain why.\n3.4.6 Suppose that keys are t-bit integers. For a modular hash function with prime M, \nprove that each key ", "start": 491, "end": 492}, "716": {"text": "} \nIf so, describe the effect of using it. If not, explain why.\n3.4.6 Suppose that keys are t-bit integers. For a modular hash function with prime M, \nprove that each key bit has the property that there exist two keys differing only in that \nbit that have different hash values.\n3.4.7 Consider the idea of implementing modular hashing for integer keys with the \ncode  (a * k) %  M , where a is an arbitrary \ufb01xed prime.  Does this change mix up the \nbits suf\ufb01ciently well that you can use nonprime M?\n3.4.8 How many empty lists do you expect to see when you insert N keys into a hash \ntable with SeparateChainingHashST, for N=10, 102, 103, 104, 105, and 106? Hint : See \nExercise 2.5.31.\n3.4.9 Implement an eager delete() method for SeparateChainingHashST.\n3.4.10 Insert the keys E A S Y Q U T I O N in that order into an initially empty table \n480 CHAPTER 3 \u25a0 Searching\n  \nof size M =16 using linear probing. Use the hash function 11 k % M to transform the \nkth letter of the alphabet into a table index. Redo this exercise for M = 10.\n3.4.11 Give the contents of a linear-probing hash table that results when you insert the \nkeys E A S Y Q U T I O N in that order into an initially empty table of initial size M\n= 4 that is expanded with doubling whenever half full. Use the hash function 11 k % M\nto transform the kth letter of the alphabet into a table index.\n3.4.12 Suppose that the keys A through G, with the hash values given below, are inserted \nin some order into an initially empty ", "start": 492, "end": 493}, "717": {"text": "M\nto transform the kth letter of the alphabet into a table index.\n3.4.12 Suppose that the keys A through G, with the hash values given below, are inserted \nin some order into an initially empty table of size 7 using a linear-probing table (with \nno resizing for this problem). Which of the following could not possibly result from \ninserting these keys? \na.    E   F   G   A   C   B   D\nb.    C   E   B   G   F   D   A\nc.    B   D   F   A   C   E   G\nd.    C   G   B   A   D   E   F\ne.    F   G   B   D   A   C   E\nf.    G   E   C   A   D   B   F\nGive the minimum and the maximum number of probes that could be required to \nbuild a table of size 7 with these keys, and an insertion order that justi\ufb01es your answer.\n3.4.13 Which of the following scenarios leads to expected linear running time for a \nrandom search hit in a linear-probing hash table?\na. All keys hash to the same index.\nb. All keys hash to different indices.\nc. All keys hash to an even-numbered index.\nd. All keys hash to different even-numbered indices.\n3.4.14 Answer the previous question for search miss, assuming the search key is equally \nlikely to hash to each table position.\n3.4.15 How many compares could it take, in the worst case, to insert N keys into an \ninitially empty table, using linear probing with array resizing?\n3.4.16 Suppose that a linear-probing table of size 10 6 is half full, with occupied posi -\ntions chosen at random. Estimate the probability that all positions with ", "start": 493, "end": 493}, "718": {"text": "using linear probing with array resizing?\n3.4.16 Suppose that a linear-probing table of size 10 6 is half full, with occupied posi -\ntions chosen at random. Estimate the probability that all positions with indices divisible \n4813.4 \u25a0 Hash Tables\n  \nby 100 are occupied.\n3.4.17  Show the result of using the delete() method on page 471 to delete C from the \ntable resulting from using LinearProbingHashST with our standard indexing client \n(shown on page 469).\n3.4.18  Add a constructor to SeparateChainingHashST that gives the client the ability \nto specify the average number of probes to be tolerated for searches. Use array resizing \nto keep the average list size less than the speci\ufb01ed value, and use the technique described \non page 478 to ensure that the modulus for hash() is prime. \n3.4.19  Implement keys() for SeparateChainingHashST and LinearProbingHashST.\n3.4.20 Add a method to LinearProbingHashST that computes the average cost of a \nsearch hit in the table, assuming that each key in the table is equally likely to be sought. \n3.4.21  Add a method to LinearProbingHashST that computes the average cost of a \nsearch miss in the table, assuming a random hash function. Note : Y ou do not have to \ncompute any hash functions to solve this problem. \n3.4.22  Implement hashCode() for various types: Point2D, Interval, Interval2D,    \nand Date.  \n3.4.23 Consider modular hashing for string keys with R = 256 and M = 255. Show \nthat this is a bad choice because any permutation of letters within a string hashes to the \nsame value.\n3.4.24  Analyze the space usage of separate chaining, linear probing, and BSTs ", "start": 493, "end": 494}, "719": {"text": "255. Show \nthat this is a bad choice because any permutation of letters within a string hashes to the \nsame value.\n3.4.24  Analyze the space usage of separate chaining, linear probing, and BSTs for \ndouble keys. Present your results in a table like the one on page 476.\nEXERCISES  (continued)\n482 CHAPTER 3 \u25a0 Searching\n CREATIVE PROBLEMS\n \n \n3.4.25    Hash cache. Modify Transaction on page 462 to maintain an instance variable \nhash, so that hashCode() can save the hash value the \ufb01rst time it is called for each object \nand does not have to recompute it on subsequent calls. Note : This idea works only for \nimmutable types.\n3.4.26  Lazy delete for linear probing. Add to LinearProbingHashST a delete()\nmethod that deletes a key-value pair by setting the value to null (but not removing \nthe key) and later removing the pair from the table in resize(). Your primary chal-\nlenge is to decide when to call resize(). Note : Y ou should overwrite the null value if \na subsequent put() operation associates a new value with the key. Make sure that your \nprogram takes into account the number of such tombstone items, as well as the number \nof empty positions, in making the decision whether to expand or contract the table.\n3.4.27      Double probing. Modify SeparateChainingHashST to use a second hash func-\ntion and pick the shorter of the two lists. Give a trace of the process of inserting the keys \nE A S Y Q U T I O N  in that order into an initially empty table of size M =3 using \nthe function 11 k % M  (for the kth letter) as the \ufb01rst hash function and the function \n17 k % M (for the kth letter) as the second hash function. Give ", "start": 494, "end": 495}, "720": {"text": "using \nthe function 11 k % M  (for the kth letter) as the \ufb01rst hash function and the function \n17 k % M (for the kth letter) as the second hash function. Give the average number of \nprobes for random search hit and search miss in this table.\n3.4.28      Double hashing. Modify LinearProbingHashST to use a second hash function \nto de\ufb01ne the probe sequence. Speci\ufb01cally, replace (i + 1) % M (both occurrences) by \n(i + k) % M where k is a nonzero key-dependent integer that is relatively prime to M. \nNote : Y ou may meet the last condition by assuming that M is prime. Give a trace of the \nprocess of inserting the keys E A S Y Q U T I O N in that order into an initially empty \ntable of size M =11, using the hash functions described in the previous exercise. Give \nthe average number of probes for random search hit and search miss in this table. \n3.4.29  Deletion. Implement an eager delete() method for the methods described in \neach of the previous two exercises.\n3.4.30     Chi-square statistic. Add a method to SeparateChainingST to compute the /H9273 2\nstatistic for the hash table. With N keys and table size M, this number is de\ufb01ned by the \nequation \n/H9273 2\n   =   (M/N) ( (f0 /H11002 N/M)2 + (f1 /H11002 N/M)2 /H11001  . . .  (fM /H11002 1/H11002 N/M)2 )\n4833.4 \u25a0 Hash Tables\n where fi is the number of keys with hash value i. This statistic is one way of checking our \nassumption that ", "start": 495, "end": 496}, "721": {"text": "/H11002 1/H11002 N/M)2 )\n4833.4 \u25a0 Hash Tables\n where fi is the number of keys with hash value i. This statistic is one way of checking our \nassumption that the hash function produces random values. If so, this statistic, for N > \ncM, should be between  M /H11002 /H20881 M  and M + /H20881 M  with probability 1 /H11002 1/c.   \n3.4.31    Cuckoo hashing. Develop a symbol-table implementation that maintains two \nhash tables and two hash functions. Any given key is in one of the tables, but not both. \nWhen inserting a new key, hash to one of the tables; if the table position is occupied, \nreplace that key with the new key and hash the old key into the other table (again kick-\ning out a key that might reside there). If this process cycles, restart. Keep the tables less \nthan half full. This method uses a constant number of equality tests in the worst case \nfor search (trivial) and amortized constant time for insert.\n3.4.32  Hash attack. Find 2N strings, each of length 2N, that have the same hashCode()\nvalue, supposing that the hashCode() implementation for String is the following:\npublic int hashCode() \n{  \n   int hash = 0;\n   for (int i = 0; i < length(); i ++)\n      hash = (hash * 31) + charAt(i);\n   return hash; \n} \nStrong hint : Aa and BB have the same value. \n3.4.33  Bad hash function. Consider the following hashCode() implementation for \nString, which was used in early versions of Java:\npublic int hashCode() \n{  \n   int hash = 0;\n   int skip = Math.max(1, length()/8);\n ", "start": 496, "end": 496}, "722": {"text": "function. Consider the following hashCode() implementation for \nString, which was used in early versions of Java:\npublic int hashCode() \n{  \n   int hash = 0;\n   int skip = Math.max(1, length()/8);\n   for (int i = 0; i < length(); i += skip)\n      hash = (hash * 37) + charAt(i);\n   return hash; \n} \nExplain why you think the designers chose this implementation and then why you \nthink it was abandoned in favor of the one in the previous exercise.\nCREATIVE PROBLEMS  (continued)\n484 CHAPTER 3 \u25a0 Searching\n EXPERIMENTS\n \n3.4.34  Hash cost. Determine empirically the ratio of the time required for hash()\nto the time required for compareTo(), for as many commonly-used types of keys for \nwhich you can get meaningful results.\n3.4.35  Chi-square test. Use your solution from Exercise 3.4.30 to check the assump-\ntion that the hash functions for commonly-used key types produce random values.\n3.4.36  List length range. Write a program that inserts N random int keys into a table \nof size N / 100 using separate chaining, then \ufb01nds the length of the shortest and longest \nlists, for N = 10 3, 10 4, 10 5, 10 6.\n3.4.37  Hybrid. Run experimental studies to determine the effect of using RedBlackBST\ninstead of SequentialSearchST to handle collisions in SeparateChainingHashST. \nThis solution carries the advantage of guaranteeing logarithmic performance even for \na bad hash function and the disadvantage of necessitating maintenance of two different \nsymbol-table implementations. What are the practical effects?  \n3.4.38  Separate-chaining distribution. Write a program that inserts 10 5 random non-\nnegative integers less than 10 6 ", "start": 496, "end": 497}, "723": {"text": "of two different \nsymbol-table implementations. What are the practical effects?  \n3.4.38  Separate-chaining distribution. Write a program that inserts 10 5 random non-\nnegative integers less than 10 6 into a table of size 10 5 using linear probing, and that \nplots the total number of probes used for each 10 3 consecutive insertions. Discuss the \nextent to which your results validate Proposition K.\n3.4.39  Linear-probing distribution. Write a program that inserts N/2 random int keys \ninto a table of size N using linear probing, then computes the average cost of a search \nmiss in the resulting table from the cluster lengths, for N = 10 3, 10 4, 10 5, 10 6. Discuss \nthe extent to which your results validate Proposition M.\n3.4.40  Plots. Instrument LinearProbingHashST and SeparateChainingHashST to \nproduce plots like the ones shown in the text.\n3.4.41  Double probing. Run experimental studies to evaluate the effectiveness of dou-\nble probing (see Exercise 3.4.27).\n3.4.42  Double hashing. Run experimental studies to evaluate the effectiveness of dou-\nble hashing (see Exercise 3.4.28).\n3.4.43  Parking problem. (D. Knuth) Run experimental studies to validate the hypoth-\nesis that the number of compares needed to insert M random keys into a linear-probing \ntable of size M is ~cM 3/2, where c = /H20881/H9266/2.\n4853.4 \u25a0 Hash Tables\n 3.5  APPLICATIONS\n  \nFrom the early days of computing, when symbol tables allowed programmers to \nprogress from using numeric addresses in machine language to using symbolic names \nin assembly language, to modern applications of the new millennium, when symbolic \nnames ", "start": 497, "end": 498}, "724": {"text": "APPLICATIONS\n  \nFrom the early days of computing, when symbol tables allowed programmers to \nprogress from using numeric addresses in machine language to using symbolic names \nin assembly language, to modern applications of the new millennium, when symbolic \nnames have meaning across worldwide computer networks, fast search algorithms have \nplayed and continue to play an essential role in computation. Modern applications for \nsymbol tables include organization of scienti\ufb01c data, from searching for markers or \npatterns in genomic data to mapping the universe; organization of knowledge on the \nweb, from searching in online commerce to putting libraries online; and implement -\ning the internet infrastructure, from routing packets among machines on the web to \nshared \ufb01le systems and video streaming. Ef\ufb01cient search algorithms have enabled these \nand countless other important applications. We will consider several representative ex-\namples in this section:\n\u25a0  \n \nA dictionary client and an indexing client that enable fast and \ufb02exible access to \ninformation in comma-separated-value \ufb01les (and similar formats), which are \nwidely used to store data on the web\n\u25a0 An indexing client for building an inverted index of a set of \ufb01les\n\u25a0 A sparse-matrix data type that uses a symbol table to address problem sizes far \nbeyond what is possible with the standard implementation\nIn Chapter 6, we consider a symbol table that is appropriate for tables such as data -\nbases and \ufb01le systems that contain a vast number of keys, as large as can be reasonably \ncontemplated. \nSymbol tables also play a critical role in algorithms that we consider throughout the \nrest of the book. For example, we use symbol tables to represent graphs ( Chapter 4) \nand to process strings (Chapter 5).\nAs we have seen throughout this chapter, developing symbol-table implementations \nthat can guarantee fast performance for all operations is certainly a challenging task. \nOn the other hand, the implementations that we have considered are well-studied, ", "start": 498, "end": 498}, "725": {"text": "5).\nAs we have seen throughout this chapter, developing symbol-table implementations \nthat can guarantee fast performance for all operations is certainly a challenging task. \nOn the other hand, the implementations that we have considered are well-studied, \nwidely used, and available in many software environments (including Java libraries). \nFrom this point forward, you certainly should consider the symbol-table abstraction to \nbe a key component in your programmer\u2019s toolbox.\n486\n Which symbol-table implementation should I use? The table  at the bottom \nof this page summarizes the performance characteristics of the algorithms that we have \nconsidered in propositions and properties in this chapter (with the exception of the \nworst-case results for hashing, which are from the research literature and unlikely to \nbe experienced in practice). It is clear from the table that, for typical applications, your \ndecision comes down to a choice between hash tables and binary search trees.\nThe advantages of hashing over BST implementations are that the code is simpler \nand search times are optimal (constant), if the keys are of a standard type or are suf-\n\ufb01ciently simple that we can be con\ufb01dent of developing an ef\ufb01cient hash function for \nthem that (approximately) satis\ufb01es the uniform hashing assumption. The advantages \nof BSTs over hashing are that they are based on a simpler abstract interface (no hash \nfunction need be designed); red-black BSTs can provide guaranteed worst-case perfor-\nmance; and they support a wider range of operations (such as rank, select, sort, and \nrange search).  As a rule of thumb, most programmers will use hashing except when \none or more of these factors is important, when red-black BSTs are called for. In Chap-\nter 5, we will study one exception to this rule of thumb: when keys are long strings, \nwe can build data structures that are even more \ufb02exible than red-black BSTs ", "start": 498, "end": 499}, "726": {"text": "for. In Chap-\nter 5, we will study one exception to this rule of thumb: when keys are long strings, \nwe can build data structures that are even more \ufb02exible than red-black BSTs and even \nfaster than hashing.\nalgorithm\n(data structure)\nworst-case cost\n(after N inserts) \naverage-case cost\n(after N random inserts) key\ninterface\nmemory\n(bytes)search insert search hit insert\nsequential search\n(unordered list) N N N/2 N equals() 48 N\nbinary search\n(ordered array) lg N N lg N N/2 compareTo() 16 N\nbinary tree search\n(BST) N N 1.39 lg N 1.39 lg N compareTo() 64 N\n2-3 tree search\n(red-black BST) 2 lg N 2 lg N 1.00 lg N 1.00 lg N compareTo() 64 N\nseparate chaining\u2020\n(array of lists) < lg N < lg N N / (2M ) N / M\nequals() \nhashCode() 48 N + 64 M\nlinear probing\u2020\n(parallel arrays) c lg N c lg N < 1.50 < 2.50\nequals() \nhashCode()\nbetween\n32 N and 128 N\n\u2020 with uniform and independent hash function\nAsymptotic cost summary for symbol-table implementations\n4873.5 \u25a0 Applications\n Our symbol-table implementations are useful for a wide range of applications, but \nour algorithms are easily adapted to support several other options that are widely used \nand worth considering. \n P r i m i t i v e  t y p e s .  Suppose that we have a symbol table with integer keys and associ -\nated \ufb02oating-point numbers.  When we use our standard setup, the keys and values \nare stored as Integer and Double wrapper-type values, so we need two ", "start": 499, "end": 500}, "727": {"text": "have a symbol table with integer keys and associ -\nated \ufb02oating-point numbers.  When we use our standard setup, the keys and values \nare stored as Integer and Double wrapper-type values, so we need two extra memory \nreferences to access each key-value pair.  These references may be no problem in an ap-\nplication that involves thousands of searches on thousands of keys but may represent \nexcessive cost in an application that involves billions of searches on millions of keys. Us-\ning a primitive type instead of Key would save one reference per key-value pair. When \nthe associated value is also primitive, we can eliminate another reference. The situation \nis diagrammed at right for separate chaining; the \nsame tradeoffs hold for other implementations. For \nperformance-critical applications, it is worthwhile \nand not dif\ufb01cult to develop versions of our imple -\nmentations along these lines (see Exercise 3.5.4).\n  D u p l i c a t e  k e y s .  The possibility of duplicate keys \nsometimes needs special consideration in symbol-\ntable implementations. In many applications, it is \ndesirable to associate multiple values with the same \nkey. For example, in a transaction-processing sys -\ntem, numerous transactions may have the same \ncustomer key value. Our convention to disallow \nduplicate keys amounts to leaving duplicate-key \nmanagement to the client. We will consider an ex -\nample of such a client later in this section. In many \nof our implementations, we could consider the al-\nternative of leaving key-value pairs with duplicate \nkeys in the primary search data structure and to return any value with the given key for \na search. We might also add methods to return all values with the given key. Our BST \nand hashing implementations are not dif\ufb01cult to adapt to keep duplicate keys within \nthe data structure; doing so for red-black BSTs is just slightly more challenging (see ", "start": 500, "end": 500}, "728": {"text": "return all values with the given key. Our BST \nand hashing implementations are not dif\ufb01cult to adapt to keep duplicate keys within \nthe data structure; doing so for red-black BSTs is just slightly more challenging (see Ex-\nercise 3.5.9 and Exercise 3.5.10). Such implementations are common in the literature \n(including earlier editions of this book).\nMemory usage for separate chaining\ndata is stored in \nKey and Value objects\ndata is stored in \nlinked-list nodes\nstandard implementation\nprimitive-type implementation\n488 CHAPTER 3 \u25a0 Searching\n  \n \n J a v a  l i b r a r i e s .  Java\u2019s  java.util.TreeMap and  java.util.HashMap libraries are sym-\nbol-table implementations based on red-black BSTs and hashing with separate chaining \nrespectively. TreeMap does not directly support rank(), select(), and other opera -\ntions in our ordered symbol-table API, but it does support operations that enable ef\ufb01-\ncient implementation of these. HashMap is roughly equivalent to our LinearProbingST\nimplementation\u2014it uses array resizing to enforce a load factor of about 75 percent.\nTo be consistent and specific, we use in this book the symbol-table implementation \nbased on red-black BSTs from Section 3.3 or the one based on linear-probing hashing \nfrom Section 3.4. For economy and to emphasize client independence from speci\ufb01c \nimplementations, we use the name ST as shorthand for RedBlackBST for ordered sym-\nbol tables in client code and  the name HashST as shorthand for LinearProbingHashST\nwhen order is not important and hash functions are available. We adopt these conven-\ntions with full knowledge that speci\ufb01c applications might have demands that could call \nfor some variation or extension of one of these algorithms and data structures. Which \nsymbol table should you use? Whatever you decide, test your choice to be sure ", "start": 500, "end": 501}, "729": {"text": "that speci\ufb01c applications might have demands that could call \nfor some variation or extension of one of these algorithms and data structures. Which \nsymbol table should you use? Whatever you decide, test your choice to be sure that it is \ndelivering the performance that you expect. \n   S e t  A P I s  Some symbol-table clients do not need the values, just the ability to insert \nkeys into a table and to test whether a key is in the table. Because we disallow duplicate \nkeys, these operations correspond to the following API where we are just interested in \nthe set of keys in the table, not any associated values:\nYo u  c a n  t u r n  a ny  s y m b o l - t a b l e  i m p l e m e n t a t i o n  i n t o  a  SET implementation by ignoring \nvalues or by using a simple wrapper class (see Exercises 3.5.1 through 3.5.3). \npublic class SET<Key> \nSET() create an empty set\nvoid add(Key key) add key into the set\nvoid delete(Key key) remove key from the set\nboolean contains(Key key) is key in the set?\nboolean isEmpty() is the set empty?\nint size() number of keys in the set\nString toString() string representation of the set\nAPI for a basic set data type\n4893.5 \u25a0 Applications\n Extending SET to include union, intersection, complement, and other common math-\nematical set operations requires a more sophisticated API (for example, the comple-\nment operation requires some mechanism for specifying a universe of all possible keys) \nand provides a number of interesting algorithmic challenges, as discussed in Exercise \n3.5.17.  \nAs with ST, we have unordered and ordered versions of SET. If keys are Comparable, \nwe can include min(), max(), ", "start": 501, "end": 502}, "730": {"text": "a number of interesting algorithmic challenges, as discussed in Exercise \n3.5.17.  \nAs with ST, we have unordered and ordered versions of SET. If keys are Comparable, \nwe can include min(), max(), floor(), ceiling(), deleteMin(), deleteMax(), \nrank(), select(), and the two-argument versions of size() and get() to de\ufb01ne a \nfull API for ordered keys. T o match our ST conventions, we use the name SET in client \ncode for ordered sets and the name HashSET when order is not important.\nTo  i l l u s t r a t e  u s e s  o f  SET, we consider \ufb01lter clients that read a sequence of strings \nfrom standard input and write some of them to standard output. Such clients have \ntheir origin in early systems where main memory was far too small to hold all the data, \nand they are still relevant today, when we write programs that take their input from the \nweb. As example input, we use tinyTale.txt (see page 371). For readability, we preserve\nnewlines from the input to the \noutput in examples, even though \nthe code does not do so.\n  D e d u p .  The prototypical \ufb01lter \nexample is a SET or HashSET cli-\nent that removes duplicates in \nthe input stream. It is custom-\nary to refer to this operation as \ndedup. We maintain a set of the \nstring keys seen so far. If the next \nkey is in the set, ignore it; if it is \nnot in the set, add it to the set \nand print it. The keys appear on \nstandard output in the order they \nappear on standard input, with \nduplicates removed. This process \ntakes space proportional to the \nnumber of distinct keys in the \ninput stream (which is typically \nfar smaller than the total number ", "start": 502, "end": 502}, "731": {"text": "output in the order they \nappear on standard input, with \nduplicates removed. This process \ntakes space proportional to the \nnumber of distinct keys in the \ninput stream (which is typically \nfar smaller than the total number \nof keys). \npublic class  DeDup \n{\n   public static void main(String[] args)\n   {\n      HashSET<String> set;\n      set = new HashSET<String>();\n      while (!StdIn.isEmpty())\n      {\n         String key = StdIn.readString();\n         if (!set.contains(key))\n         {\n            set.add(key);\n            StdOut.println(key);\n         }\n      }\n   } \n}\n D e d u p  f i l t e r\n% java DeDup < tinyTale.txt \nit was the best of times worst \nage wisdom foolishness \nepoch belief incredulity \nseason light darkness \nspring hope winter despair\n490 CHAPTER 3 \u25a0 Searching\n   \n \n \n \n    W h i t e l i s t  a n d  b l a c k l i s t .  Another classic \ufb01lter uses keys in a separate \ufb01le to decide \nwhich keys from the input stream are passed to the output stream. This general process \nhas many natural applications. The simplest example is a whitelist, where any key that \nis in the \ufb01le is identi\ufb01ed as \u201cgood. \u201d The client might choose to pass through to standard \noutput any key that is not in the whitelist and to ignore any key that is in the whitelist \n(as in the example considered in our \ufb01rst program in Chapter 1); another client might \nchoose to pass through to standard output any key that is in the whitelist and to ig-\nnore any key that is not in the whitelist (as \nshown in the HashSET client WhiteFilter \nat right). For example, your email applica-\ntion might use such a \ufb01lter to allow you to \nspecify ", "start": 502, "end": 503}, "732": {"text": "key that is not in the whitelist (as \nshown in the HashSET client WhiteFilter \nat right). For example, your email applica-\ntion might use such a \ufb01lter to allow you to \nspecify the addresses of your friends and \nto direct it to consider emails from any -\none else as spam. We build a HashSET of \nthe keys in the speci\ufb01ed list, then read the \nkeys from standard input. If the next key \nis in the set, print it; if it is not in the set, \nignore it. A blacklist is the opposite, where \nany key that is in the \ufb01le is identi\ufb01ed as \n\u201cbad. \u201d Again, there are two natural \ufb01lters \nfor clients using a blacklist. In our email \nexample, you might specify the addresses \nof known spammers and direct the email \napplication to let through all mail not \nfrom one of those addresses. We can im-\nplement a HashSET client  BlackFilter \nthat implements this \ufb01lter by negating the \n\ufb01lter test in WhiteFilter. Typical practi-\ncal situations such as a credit card com-\npany using a blacklist to \ufb01lter out stolen \ncard numbers or an internet router using a \nwhitelist to implement a \ufb01rewall are likely \nto involve huge lists, unbounded input \nstreams, and strict response requirements. \nThe sorts of symbol-table implementa-\ntions that we have considered enable such \nchallenges to easily be met.\npublic class  WhiteFilter \n{\n   public static void main(String[] args)\n   {\n      HashSET<String> set;\n      set = new HashSET<String>();\n      In in = new In(args[0]);\n      while (!in.isEmpty())\n         set.add(in.readString());\n      while (!StdIn.isEmpty())\n      {\n         String word = StdIn.readString();\n         if (set.contains(word))\n ", "start": 503, "end": 503}, "733": {"text": "HashSET<String>();\n      In in = new In(args[0]);\n      while (!in.isEmpty())\n         set.add(in.readString());\n      while (!StdIn.isEmpty())\n      {\n         String word = StdIn.readString();\n         if (set.contains(word))\n            StdOut.println(word);\n      }\n   } \n}\n W h i t e l i s t  f i l t e r\n% more list.txt \nwas it the of\n% java WhiteFilter list.txt < tinyTale.txt \nit was the of it was the of \nit was the of it was the of \nit was the of it was the of \nit was the of it was the of \nit was the of it was the of\n% java BlackFilter list.txt < tinyTale.txt \nbest times worst times \nage wisdom age foolishness \nepoch belief epoch incredulity \nseason light season darkness \nspring hope winter despair\n4913.5 \u25a0 Applications\n Dictionary clients The most basic kind of symbol-table client builds a symbol \ntable with successive put operations in order to support get requests. Many applications \nalso take advantage of the idea that a symbol table is a dynamic dictionary, where it is \neasy to look up information and to update the information in the table. The following \nlist of familiar examples illustrates the utility of this approach:\n\u25a0 \n \n \nPhone book. When keys are people\u2019s names and values are their phone num-\nbers, a symbol table models a phone book. A very signi\ufb01cant difference from \na printed phone book is that we can add new names or change existing phone \nnumbers. We could also use the phone number as the key and the name as the \nvalue\u2014if you have never done so, try typing your phone number (with area \ncode) into the search \ufb01eld in your browser. \n\u25a0  \n  \n \nDictionary. Associating a word with its de\ufb01nition is a familiar concept that \ngives us the name ", "start": 503, "end": 504}, "734": {"text": "phone number (with area \ncode) into the search \ufb01eld in your browser. \n\u25a0  \n  \n \nDictionary. Associating a word with its de\ufb01nition is a familiar concept that \ngives us the name \u201cdictionary. \u201d For centuries people kept printed dictionaries in \ntheir homes and of\ufb01ces in order to check the de\ufb01nitions and spellings (values) \nof words (keys). Now, because of good symbol-table implementations, people \nexpect built-in spell checkers and immediate access to word de\ufb01nitions on their \ncomputers.\n\u25a0 Account information. People who own stock now regularly check the current \nprice on the web. Several services on the web associate a ticker symbol (key) with \nthe current price (value), usually along with a great deal of other information. \nCommercial applications of this sort abound, including \ufb01nancial institutions \nassociating account information with a name or account number or educational \ninstitutions associating grades with a student name or identi\ufb01cation number.\n\u25a0 Genomics. Symbols play a central role in modern genomics. The simplest ex-\nample is the use of the letters A, C, T, and G to represent the nucleotides found in \nthe DNA of living organisms. The next simplest is the correspondence between \ncodons (nucleotide triplets) and amino acids (TTA corresponds to leucine, TCT\nto serine, and so forth), then the correspondence between sequences of amino \nacids and proteins, and so forth. Researchers in genomics routinely use various \ntypes of symbol tables to organize this knowledge.\n\u25a0 \n \nExperimental data. From astrophysics to zoology, modern scientists are awash in \nexperimental data, and organizing and ef\ufb01ciently accessing this data are vital to \nunderstanding what it means. Symbol tables are a critical starting point, and ad-\nvanced data structures and algorithms that are based ", "start": 504, "end": 504}, "735": {"text": "awash in \nexperimental data, and organizing and ef\ufb01ciently accessing this data are vital to \nunderstanding what it means. Symbol tables are a critical starting point, and ad-\nvanced data structures and algorithms that are based on symbol tables are now \nan important part of scienti\ufb01c research .\n\u25a0 \n \nCompilers. One of the earliest uses of symbol tables was to organize information \nfor programming. At \ufb01rst, programs were simply sequences of numbers, but \nprogrammers very quickly found that using symbolic names for operations and \n492 CHAPTER 3 \u25a0 Searching\n memory locations (variable names) was far more convenient. Associating the \nnames with the numbers requires a symbol table. As the size of programs grew, \nthe cost of the symbol-table operations became a bottleneck in program devel-\nopment time, which led to the development of data structures and algorithms \nlike the ones we consider in this chapter.\n\u25a0 \n \nFile systems. We use symbol tables regularly to \norganize data on computer systems. Perhaps the \nmost prominent example is the \ufb01le system, where \nwe associate a \ufb01le name (key) with the location \nof its contents (value). Y our music player uses the \nsame system to associate song titles (keys) with \nthe location of the music itself (value).\n\u25a0 \n \n \nInternet DNS. The domain name system (DNS) \nthat is the basis for organizing information on \nthe internet associates URLs (keys) that humans \nunderstand (such as www.princeton.edu or \nwww.wikipedia.org) with IP addresses (values) \nthat computer network routers understand (such \nas 208.216.181.15 or 207.142.131.206). This \nsystem is the next-generation \u201cphone book. \u201d \nThus, humans can use names that are easy to re-\nmember and machines can ef\ufb01ciently process the \nnumbers. The number of symbol-table lookups \ndone ", "start": 504, "end": 505}, "736": {"text": "\nsystem is the next-generation \u201cphone book. \u201d \nThus, humans can use names that are easy to re-\nmember and machines can ef\ufb01ciently process the \nnumbers. The number of symbol-table lookups \ndone each second for this purpose on internet \nrouters around the world is huge, so perfor-\nmance is of obvious importance. Millions of new computers and other devices \nare put onto the internet each year, so these symbol tables on internet routers \nneed to be dynamic.\nDespite its scope, this list is still just a representative sample, intended to give you a \ufb02a-\nvor of the scope of applicability of the symbol-table abstraction. Whenever you specify \nsomething by name, there is a symbol table at work. Y our computer\u2019s \ufb01le system or the \nweb might do the work for you, but there is still a symbol table there somewhere.\nAs a speci\ufb01c example, we consider a symbol-table client that you can use to look up \ninformation that is kept in a table on a \ufb01le or a web page using the  comma-separated-\nvalue (.csv) \ufb01le format. This simple format achieves the (admittedly modest) goal of \nkeeping tabular data in a form that anyone can read (and is likely to be able to read in \nthe future) without needing to use a particular application: the data is in text form, \none row per line, with entries separated by commas.  Y ou can \ufb01nd on the booksite \ndomain key value\nphone \nbook name phone \nnumber\ndictionary word definition\naccount account \nnumber balance\ngenomics codon amino acid\ndata data/time results\ncompiler variable \nname\nmemory \nlocation\nfile share song name machine\n      website IP address\nTypical dictionary applications\n4933.5 \u25a0 Applications\n  \n \n \n \n \nnumerous .csv \ufb01les that are related to vari-\nous applications that ", "start": 505, "end": 506}, "737": {"text": "\nname\nmemory \nlocation\nfile share song name machine\n      website IP address\nTypical dictionary applications\n4933.5 \u25a0 Applications\n  \n \n \n \n \nnumerous .csv \ufb01les that are related to vari-\nous applications that we have described, \nincluding amino.csv (codon-to-amino-\nacid encodings), DJIA.csv (opening price, \nvolume, and closing price of the Dow Jones \nIndustrial Average, for every day in its his -\ntory), ip.csv (a selection of entries from \nthe DNS database), and upc.csv (the Uni-\nform Product Code bar codes that are wide-\nly used to identify consumer products). \nSpreadsheet and other data-processing \napplications programs can read and write \n.csv \ufb01les, and our example illustrates that \nyou can also write a Java program to process \nthe data any way that you would like.\nLookupCSV (on the facing page) builds a \nset of key-value pairs from a \ufb01le of comma-\nseparated values as speci\ufb01ed on the com-\nmand line and then prints out values corre-\nsponding to keys read from standard input. \nThe command-line arguments are the \ufb01le \nname and two integers, one specifying the \n\ufb01eld to serve as the key and the other speci-\nfying the \ufb01eld to serve as the value. \nThe purpose of this example is to il-\nlustrate the utility and \ufb02exibility of the \nsymbol-table abstraction. What website \nhas IP address 128.112.136.35? ( www. \ncs.princeton.edu) What amino acid  cor-\nresponds to the codon TCA ? (Serine) What \nwas the DJIA on October 29, 1929? (252.38) \nWhat product has UPC 0002100001086? \n(Kraft Parmesan) Y ou can easily look up \nthe ", "start": 506, "end": 506}, "738": {"text": "What \nwas the DJIA on October 29, 1929? (252.38) \nWhat product has UPC 0002100001086? \n(Kraft Parmesan) Y ou can easily look up \nthe answers to questions like these with \nLookupCSV and the appropriate .csv \ufb01les.\nPerformance is not much of an issue \nwhen handling interactive queries (since \nyour computer can look through millions \n% more amino.csv \nTTT,Phe,F,Phenylalanine \nTTC,Phe,F,Phenylalanine \nTTA,Leu,L,Leucine \nTTG,Leu,L,Leucine \nTCT,Ser,S,Serine \nTCC,Ser,S,Serine \n... \nGAA,Gly,G,Glutamic Acid \nGAG,Gly,G,Glutamic Acid \nGGT,Gly,G,Glycine \nGGC,Gly,G,Glycine \nGGA,Gly,G,Glycine \nGGG,Gly,G,Glycine\n% more DJIA.csv \n... \n20-Oct-87,1738.74,608099968,1841.01 \n19-Oct-87,2164.16,604300032,1738.74 \n16-Oct-87,2355.09,338500000,2246.73 \n15-Oct-87,2412.70,263200000,2355.09 \n... \n30-Oct-29,230.98,10730000,258.47 \n29-Oct-29,252.38,16410000,230.07 \n28-Oct-29,295.18,9210000,260.64 \n25-Oct-29,299.47,5920000,301.22 \n...\n% more ip.csv \n... \nwww.ebay.com,66.135.192.87 \nwww.princeton.edu,128.112.128.15 ", "start": 506, "end": 506}, "739": {"text": "\n28-Oct-29,295.18,9210000,260.64 \n25-Oct-29,299.47,5920000,301.22 \n...\n% more ip.csv \n... \nwww.ebay.com,66.135.192.87 \nwww.princeton.edu,128.112.128.15 \nwww.cs.princeton.edu,128.112.136.35 \nwww.harvard.edu,128.103.60.24 \nwww.yale.edu,130.132.51.8 \nwww.cnn.com,64.236.16.20 \nwww.google.com,216.239.41.99 \nwww.nytimes.com,199.239.136.200 \nwww.apple.com,17.112.152.32 \nwww.slashdot.org,66.35.250.151 \nwww.espn.com,199.181.135.201 \nwww.weather.com,63.111.66.11 \nwww.yahoo.com,216.109.118.65 \n...\n% more UPC.csv \n... \n0002058102040,,\"1 1/4\"\" STANDARD STORM DOOR\" \n0002058102057,,\"1 1/4\"\" STANDARD STORM DOOR\" \n0002058102125,,\"DELUXE STORM DOOR UNIT\" \n0002082012728,\"100/ per box\",\"12 gauge shells\" \n0002083110812,\"Classical CD\",\"'Bits and Pieces'\" \n002083142882,CD,\"Garth Brooks - Ropin' The Wind\" \n0002094000003,LB,\"PATE PARISIEN\" \n0002098000009,LB,\"PATE TRUFFLE COGNAC-M&H 8Z RW\" \n0002100001086,\"16 oz\",\"Kraft Parmesan\" \n0002100002090,\"15 pieces\",\"Wrigley's Gum\" \n0002100002434,\"One ", "start": 506, "end": 506}, "740": {"text": "TRUFFLE COGNAC-M&H 8Z RW\" \n0002100001086,\"16 oz\",\"Kraft Parmesan\" \n0002100002090,\"15 pieces\",\"Wrigley's Gum\" \n0002100002434,\"One pint\",\"Trader Joe's milk\" \n...\nTypical comma-separated-value ( .csv) files\n494 CHAPTER 3 \u25a0 Searching\n Dictionary lookup\npublic class  LookupCSV \n{\n   public static void main(String[] args)\n   {\n      In in = new In(args[0]);\n      int keyField = Integer.parseInt(args[1]);\n      int valField = Integer.parseInt(args[2]);\n      ST<String, String> st = new ST<String, String>();\n      while (in.hasNextLine())\n      {\n         String line = in.readLine();\n         String[] tokens = line.split(\",\");\n         String key = tokens[keyField];\n         String val = tokens[valField];\n         st.put(key, val);\n      }\n      while (!StdIn.isEmpty())\n      {\n         String query = StdIn.readString();\n         if (st.contains(query))\n            StdOut.println(st.get(query));\n      }\n   } \n}\nThis data-driven symbol-table client reads key-value pairs from a \ufb01le, then prints the values cor -\nresponding to the keys found on standard input. Both keys and values are strings. The separating \ndelimiter is taken as a command-line argument.\n% java LookupCSV ip.csv 1 0 \n128.112.136.35 \nwww.cs.princeton.edu\n% java LookupCSV DJIA.csv 0 3 \n29-Oct-29 \n230.07\n% java LookupCSV UPC.csv 0 2 \n0002100001086 \nKraft Parmesan\n% java LookupCSV amino.csv 0 3 \nTCC \nSerine\n4953.5 \u25a0 Applications of things in the time it takes to type a query), so fast implementations of ST are not no-\nticeable ", "start": 506, "end": 508}, "741": {"text": "Parmesan\n% java LookupCSV amino.csv 0 3 \nTCC \nSerine\n4953.5 \u25a0 Applications of things in the time it takes to type a query), so fast implementations of ST are not no-\nticeable when you use LookupCSV. However, when a program is doing the lookups (and \na huge number of them), performance matters. For example, an internet router might \nneed to look up millions of IP addresses per second. In this book, we have already seen \nthe need for good performance with FrequencyCounter, and we will see several other \nexamples in this section. \nExamples of similar but more sophisticated test clients for .csv \ufb01les are described \nin the exercises. For instance, we could make the dictionary dynamic by also allowing \nstandard-input commands to change the value associated with a key, or we could allow \nrange searching, or we could build multiple dictionaries for the same \ufb01le.\n I n d e x i n g  c l i e n t s  Dictionaries are char-\nacterized by the idea that there is one value \nassociated with each key, so the direct use of \nour ST data type, which is based on the asso-\nciative-array abstraction that assigns one value \nto each key, is appropriate. Each account num-\nber uniquely identi\ufb01es a customer, each UPC \nuniquely identi\ufb01es a product, and so forth. In \ngeneral, of course, there may be multiple val-\nues associated with a given key. For example, in \nour amino.csv example, each codon identi\ufb01es \none amino acid, but each amino acid is asso-\nciated with a list of codons, as in the example \naminoI.csv at right, where each line contains \nan amino acid and the list of codons associated \nwith it. W e use the term index to describe ", "start": 508, "end": 508}, "742": {"text": "with a list of codons, as in the example \naminoI.csv at right, where each line contains \nan amino acid and the list of codons associated \nwith it. W e use the term index to describe sym-\nbol tables that associate multiple values with \neach key. Here are some more examples: \n\u25a0 Commercial transactions. One way for \na company that maintains customer \naccounts to keep track of a day\u2019s transactions is to keep an index of the day\u2019s \ntransactions. The key is the account number; the value is the list of occurrences \nof that account number in the transaction list.\n\u25a0 Web search. When you type a keyword and get a list of websites containing that \nkeyword, you are using an index created by your web search engine. There is one \nvalue (the set of pages) associated with each key (the query), although the reality \nis a bit more complicated because we often specify multiple keys.\nA small index file (20 lines)\nAlanine,AAT,AAC,GCT,GCC,GCA,GCG\nArginine,CGT,CGC,CGA,CGG,AGA,AGG\nAspartic Acid,GAT,GAC\nCysteine,TGT,TGC\nGlutamic Acid,GAA,GAG\nGlutamine,CAA,CAG\nGlycine,GGT,GGC,GGA,GGG\nHistidine,CAT,CAC\nIsoleucine,ATT,ATC,ATA\nLeucine,TTA,TTG,CTT,CTC,CTA,CTG\nLysine,AAA,AAG\nMethionine,ATG\nPhenylalanine,TTT,TTC\nProline,CCT,CCC,CCA,CCG\nSerine,TCT,TCA,TCG,AGT,AGC\nStop,TAA,TAG,TGA\nThreonine,ACT,ACC,ACA,ACG\nTyrosine,TAT,TAC\nTryptophan,TGG\nValine,GTT,GTC,GTA,GTG\n ", "start": 508, "end": 508}, "743": {"text": "Acid,GAA,GAG\nGlutamine,CAA,CAG\nGlycine,GGT,GGC,GGA,GGG\nHistidine,CAT,CAC\nIsoleucine,ATT,ATC,ATA\nLeucine,TTA,TTG,CTT,CTC,CTA,CTG\nLysine,AAA,AAG\nMethionine,ATG\nPhenylalanine,TTT,TTC\nProline,CCT,CCC,CCA,CCG\nSerine,TCT,TCA,TCG,AGT,AGC\nStop,TAA,TAG,TGA\nThreonine,ACT,ACC,ACA,ACG\nTyrosine,TAT,TAC\nTryptophan,TGG\nValine,GTT,GTC,GTA,GTG\n  \naminoI.txt\nvalueskey\n\",\" separator\n496 CHAPTER 3 \u25a0 Searching\n \u25a0 \n  \n \nMovies and performers. The \ufb01le movies.txt on the booksite (excerpted below) \nis taken from the  Internet Movie Database (IMDB). Each line has a movie name \n(the key), followed by a list of performers in that movie (the value), separated by \nslashes.\nWe can easily build an index by putting the values to be associated w ith each key into a \nsingle data structure (a Queue, say) and then associating that key with that data struc -\nture as value. Extending LookupCSV along these lines is straightforward, but we leave \nthat as an exercise (see Exercise 3.5.12) \nand consider instead LookupIndex on \npage 499, which uses a symbol table to build \nan index from \ufb01les like aminoI.txt and \nmovies.txt (where the separator char -\nacter need not be a comma, as in a .csv \n\ufb01le, but can be speci\ufb01ed on the com -\nmand ", "start": 508, "end": 509}, "744": {"text": "\ufb01les like aminoI.txt and \nmovies.txt (where the separator char -\nacter need not be a comma, as in a .csv \n\ufb01le, but can be speci\ufb01ed on the com -\nmand line). After building the index, \nLookupIndex then takes key queries and \nprints the values associated with each key. \nMore interesting, LookupIndex also builds \nan inverted index associated with each \ufb01le, where  values and keys switch roles. In the \namino acid example, this gives the same functionality as Lookup (\ufb01nd the amino acid \nassociated with a given codon); in the movie-performer example it adds the ability to \n\ufb01nd the movies associated with any given performer, which is implicit in the data but \nwould be dif\ufb01cult to produce without a symbol table. Study this example carefully, as it \nprovides good insight into the essential nature of symbol tables.\nSmall portion of a large index file (250,000+ lines)\n...\nTin Men (1987)/DeBoy, David/Blumenfeld, Alan/...\nTirez sur le pianiste (1960)/Heymann, Claude/...\nTitanic (1997)/Mazin, Stan/...DiCaprio, Leonardo/...\nTitus (1999)/Weisskopf, Hermann/Rhys, Matthew/...\nTo Be or Not to Be (1942)/Verebes, Ern\u00f6 (I)/...\nTo Be or Not to Be (1983)/.../Brooks, Mel (I)/...\nTo Catch a Thief (1955)/Par\u00eds, Manuel/...\nTo Die For (1995)/Smith, Kurtwood/.../Kidman, Nicole/...\n...\n  \nmovies.txt\nvalueskey\n\"/\" separator\ndomain key value\ngenomics amino acid list of codons\ncommercial account number list of transactions\nweb search search key list of web pages\nIMDB ", "start": 509, "end": 509}, "745": {"text": "Kurtwood/.../Kidman, Nicole/...\n...\n  \nmovies.txt\nvalueskey\n\"/\" separator\ndomain key value\ngenomics amino acid list of codons\ncommercial account number list of transactions\nweb search search key list of web pages\nIMDB movie list of performers\nTypical indexing applications\n4973.5 \u25a0 Applications\n    I n v e r t e d  i n d e x .  The term inverted index is normally applied to a situation where values \nare used to locate keys. We have a large amount of data and want to know where certain \nkeys of interest occur. This application is another prototypical example of a symbol-\ntable client that uses an intermixed sequence of calls to get() and put(). Again, we as-\nsociate each key with a SET of locations, where the occurrences of the key can be found. \nThe nature and use of the location depend on the application: in a book, a location \nmight be a page number; in a program, a location might be a line number; in genomics, \na location might be a position in a genetic sequence; and so forth: \n\u25a0 Internet Movie DataBase (IMDB). In the example just considered, the input is \nan index that associates each movie with a list of performers. The inverted index \nassociates each performer with a list of movies.\n\u25a0 Book index. Every textbook has an index where you look up a term and get \nthe page numbers containing that term. While creating a good index generally \ninvolves work by the book author to eliminate common and irrelevant words, \na document preparation \nsystem will certainly use a \nsymbol table to help auto-\nmate the process. An interest-\ning special case is known as a \nconcordance, which associates \neach word in a text with the \nset of positions in the text \nwhere that word occurs (see \nExercise 3.5.20).\n\u25a0 ", "start": 509, "end": 510}, "746": {"text": "interest-\ning special case is known as a \nconcordance, which associates \neach word in a text with the \nset of positions in the text \nwhere that word occurs (see \nExercise 3.5.20).\n\u25a0 Compiler. In a large program \nthat uses a large number of symbols, it is useful to know where each name is \nused. Historically, an explicit printed symbol table was one of the most impor-\ntant tools used by programmers to keep track of where symbols are used in their \nprograms. In modern systems, symbol tables are the basis of software tools that \nprogrammers use to manage names.\n\u25a0 \n \n \nFile search. Modern operating systems provide you with the ability to type a term \nand to learn the names of \ufb01les containing that term. The key is the term; the \nvalue is the set of \ufb01les containing that term.\n\u25a0  \n \n \nGenomics. In a typical (if oversimpli\ufb01ed) scenario in genomics research, a \nscientist wants to know the positions of a given genetic sequence in an existing \ngenome or set of genomes. Existence or proximity of certain sequences may be \nof scienti\ufb01c signi\ufb01cance. The starting point for such research is an index like a \nconcordance, but modi\ufb01ed to take into account the fact that genomes are not \nseparated into words (see Exercise 3.5.15).\ndomain key value\nIMDB performer set of movies\nbook term set of pages\ncompiler  identifier set of places used\nfile search search term set of files\ngenomics subsequence set of locations\nTypical inverted indices\n498 CHAPTER 3 \u25a0 Searching\n  I n d e x  ( a n d  i n v e r t e d  i n d e x )  l o o k u p\npublic class  LookupIndex \n{\n   public static void ", "start": 510, "end": 511}, "747": {"text": "I n d e x  ( a n d  i n v e r t e d  i n d e x )  l o o k u p\npublic class  LookupIndex \n{\n   public static void main(String[] args)\n   {\n      In in = new In(args[0]);   // index database\n      String sp = args[1];       // separator\n      ST<String, Queue<String>> st = new ST<String, Queue<String>>();\n      ST<String, Queue<String>> ts = new ST<String, Queue<String>>();\n      while (in.hasNextLine())\n      {\n         String[] a = in.readLine().split(sp);\n         String key = a[0];\n         for (int i = 1; i < a.length; i++)\n         {\n            String val = a[i];\n            if (!st.contains(key)) st.put(key, new Queue<String>());\n            if (!ts.contains(val)) ts.put(val, new Queue<String>());\n            st.get(key).enqueue(val);\n            ts.get(val).enqueue(key);\n         }\n      }\n      while (!StdIn.isEmpty())\n      {\n         String query = StdIn.readLine();\n         if (st.contains(query))\n           for (String s : st.get(query))\n              StdOut.println(\"  \" + s);\n         if (ts.contains(query))\n           for (String s : ts.get(query))\n              StdOut.println(\"  \" + s);\n     }\n   } \n}\n \nThis data-driven symbol-table client reads key-value pairs \nfrom a \ufb01le, then prints the values corresponding to the keys \nfound on standard input. Keys are strings; values are lists \nof strings. The separating delimiter is taken as a command-\nline argument.\n% java LookupIndex aminoI.txt \",\" \nSerine\n  TCT\n  TCA\n  TCG\n  AGT\n  AGC \nTCG\n  Serine\n% java LookupIndex movies.txt \"/\" \nBacon, Kevin\n  Mystic ", "start": 511, "end": 511}, "748": {"text": "\",\" \nSerine\n  TCT\n  TCA\n  TCG\n  AGT\n  AGC \nTCG\n  Serine\n% java LookupIndex movies.txt \"/\" \nBacon, Kevin\n  Mystic River (2003)\n  Friday the 13th (1980)\n  Flatliners (1990)\n  Few Good Men, A (1992)\n  ...\nTin Men (1987)\n  Blumenfeld, Alan\n  DeBoy, David\n  ...\n4993.5 \u25a0 Applications FileIndex (on the facing page) takes \ufb01le names from the command line and uses a \nsymbol table to build an inverted index associating every word in any of the \ufb01les with \na SET of \ufb01le names where the word can be found, then takes keyword queries from \nstandard input, and produces its associated list of \ufb01les. This process is similar to that \nused by familiar software tools for searching the web or for searching for information \non your computer; you type a keyword to get a list of places where that keyword occurs. \nDevelopers of such tools typically embellish the process by paying careful attention to\n\u25a0 The form of the query\n\u25a0  The set of \ufb01les/pages that are indexed\n\u25a0  The order in which \ufb01les are listed in the response\nFor example, you are certainly used to typing queries that contain multiple keywords   \nto a web search engine (which is based on indexing a large fraction of the pages on the \nweb) that provides answers in order of relevance or importance (to you or to an adver-\ntiser). The exercises at the end of this section address some of these embellishments. We \nwill consider various algorithmic issues related to web search later, but the symbol table \nis certainly at the heart of the process.\nAs with LookupIndex, you are certainly encouraged to download FileIndex from \nthe booksite and use it ", "start": 511, "end": 512}, "749": {"text": "consider various algorithmic issues related to web search later, but the symbol table \nis certainly at the heart of the process.\nAs with LookupIndex, you are certainly encouraged to download FileIndex from \nthe booksite and use it to index some text \ufb01les on your computer or some websites of \ninterest, to gain further appreciation for the utility of symbol tables. If you do so, you \nwill \ufb01nd that it can build large indices for huge \ufb01les with little delay, because each put\noperation and get request is taken care of immediately. Providing this immediate re-\nsponse for huge dynamic tables is one of the classic triumphs of algorithmic technology. \n500 CHAPTER 3 \u25a0 Searching\n  F i l e  i n d e x i n g\nimport java.io.File;\npublic class  FileIndex \n{\n   public static void main(String[] args)\n   {\n      ST<String, SET<File>> st = new ST<String, SET<File>>();\n      for (String filename : args)\n      {\n         File file = new File(filename);\n         In in = new In(file);\n         while (!in.isEmpty())\n         {\n            String word = in.readString();\n            if (!st.contains(word)) st.put(word, new SET<File>());\n            SET<File> set = st.get(word);\n            set.add(file);\n         }\n      }\n      while (!StdIn.isEmpty())\n      {\n         String query = StdIn.readString();\n         if (st.contains(query))\n             for (File file : st.get(query))\n               StdOut.println(\"  \" + file.getName());\n      }\n    } \n}\nThis symbol-table client indexes a set of \ufb01les. We search for each word in each \ufb01le in a symbol table, \nmaintaining a SET of \ufb01le names that contain the word. Names for In can also refer to web pages, so \nthis code can also be used to build an inverted index of web pages.\n% more ex1.txt ", "start": 512, "end": 513}, "750": {"text": "\nmaintaining a SET of \ufb01le names that contain the word. Names for In can also refer to web pages, so \nthis code can also be used to build an inverted index of web pages.\n% more ex1.txt \nit was the best of times\n% more ex2.txt \nit was the worst of times\n% more ex3.txt \nit was the age of wisdom\n% more ex4.txt \nit was the age of foolishness\n% java FileIndex ex*.txt \nage\n  ex3.txt\n  ex4.txt \nbest\n  ex1.txt \nwas\n  ex1.txt\n  ex2.txt\n  ex3.txt\n  ex4.txt\n5013.5 \u25a0 Applications  \n S p a r s e  v e c t o r s  Our next example illustrates the importance of symbol tables in sci-\nenti\ufb01c and mathematical calculations. We describe a fundamental and familiar calcula-\ntion that becomes a bottleneck in typical practical applications, then show how using a \nsymbol table can remove the bottleneck and enable solution of vastly larger problems. \nIndeed, this particular calculation was at the core of the PageRank algorithm that was \ndeveloped by S. Brin and L. Page and led to the emergence of Google in the early 2000s \n(and is a well-known mathematical abstraction \nthat is useful in many other contexts).\nThe basic calculation that we consider is ma-\ntrix-vector multiplication  : given a matrix and a \nvector, compute a result vector whose i th entry \nis the dot product  of the given vector and the i th \nrow of the matrix. For simplicity, we consider the \ncase when the matrix is square with N rows and \nN columns and the vectors are of size N. This \noperation is elementary to code in Java, requir -\ning time proportional to N 2, for the N multipli-\ncations to compute each of the ", "start": 513, "end": 514}, "751": {"text": "\nN columns and the vectors are of size N. This \noperation is elementary to code in Java, requir -\ning time proportional to N 2, for the N multipli-\ncations to compute each of the N entries in the \nresult vector, which also matches the space proportional to N 2 that is required to store \nthe matrix.\nIn practice, it is very often the case that N is huge. For example, in the Google appli-\ncation cited above, N is the number of pages on the web. At the time PageRank was de-\nveloped, that was in the tens or hundreds of billions and it has skyrocketed since, so the \nvalue of N 2 would be far more than 10 20. No one can afford that much time or space, so \na better algorithm is needed.\nFortunately, it is also often the \ncase that the matrix is sparse: a huge \nnumber of its entries are 0. Indeed, \nfor the Google application, the av-\nerage number of nonzero entries \nper row is a small constant: virtual-\nly all web pages have links to only a \nfew others (not all the pages on the \nweb). Accordingly, we can represent \nthe matrix as an array of sparse vec-\ntors, using a SparseVector imple-\nmentation like the HashST client on \nthe facing page. Instead of using the \n  0 .90   0   0   0\n  0   0 .36 .36 .18\n  0   0   0 .90   0\n.90   0   0   0   0\n.47   0 .47   0   0\n.05\n.04\n.36\n.37\n.19\na[][] x[] b[]\n.036\n.297\n.333\n.045\n.1927\n=\nMatrix-vector ", "start": 514, "end": 514}, "752": {"text": ".47   0   0\n.05\n.04\n.36\n.37\n.19\na[][] x[] b[]\n.036\n.297\n.333\n.045\n.1927\n=\nMatrix-vector multiplication\n... \ndouble[][] a = new double[N][N]; \ndouble[] x = new double[N]; \ndouble[] b = new double[N]; \n... \n// Initialize a[][] and x[]. \n... \nfor (int i = 0; i < N; i++) \n{\n   sum = 0.0;\n   for (int j = 0; j < N; j++)\n      sum += a[i][j]*x[j];\n   b[i] = sum; \n}\nStandard implementation of matrix-vector multiplication\n502 CHAPTER 3 \u25a0 Searching\n Sparse vector with dot product\npublic class  SparseVector \n{\n   private HashST<Integer, Double> st;\n   public SparseVector()\n    {  st = new HashST<Integer, Double>();  }\n   public int size()\n   {  return st.size();  }\n   public void put(int i, double x)\n    {  st.put(i, x);  }\n   public double get(int i)\n   {\n      if (!st.contains(i)) return 0.0;\n      else return st.get(i);\n   }\n   public double dot(double[] that)\n   {\n       double sum = 0.0;\n       for (int i : st.keys())\n           sum += that[i]*this.get(i);\n       return sum;\n   }\n}\nThis symbol-table client is a bare-bones sparse vector implementation that illustrates an ef\ufb01cient \ndot product for sparse vectors. We multiply each entry by its counterpart in the other operand and \nadd the result to a running sum. The number of multiplications required is equal to the number of \nnonzero entries in the sparse vector.\n5033.5 \u25a0 Applications   \ncode a[i][j] ", "start": 514, "end": 516}, "753": {"text": "in the other operand and \nadd the result to a running sum. The number of multiplications required is equal to the number of \nnonzero entries in the sparse vector.\n5033.5 \u25a0 Applications   \ncode a[i][j] to refer to the element in row i and column j, we use a[i].put(j, val)\nto set a value in the matrix and a[i].get(j) to retrieve a value. As you can see from the \ncode below, matrix-vector multiplication using this class is even simpler than with the \narray representation (and it more clearly describes the computation). More important, \nit only requires time proportional to N plus the number of nonzero elements in the \nmatrix. \nFor small matrices or matrices that are not sparse, the overhead for maintaining \nsymbol tables can be substantial, but it is worth your while to be sure to understand \nthe rami\ufb01cations of using symbol tables for huge sparse matrices. T o \ufb01x ideas, consider \na huge application (like the one faced by Brin and \nPage) where N is 10 billion or 100 billion, but the \naverage number of nonzero elements per row is \nless than 10 . For such an application, using symbol \ntables speeds up matrix-vector multiplication by a \nfactor of a billion or more . The elementary nature \nof this application should not detract from its im-\nportance: programmers who do not take advan-\ntage of the potential to save time and space in this \nway severely limit their potential to solve practical \nproblems, while programmers who do take factor-\na\n0\n1\n2\n3\n4\n0     1     2     3     4  \n0     1     2     3     4  \n0     1     2     3     4  \n0     1     2     3     4  \n0     1 ", "start": 516, "end": 516}, "754": {"text": "\n0     1     2     3     4  \n0     1     2     3     4  \n0     1     2     3     4  \n0     1     2     3     4  \na\n0\n1\n2\n3\n4\narray of double[]objects array of SparseVector objects\nst\n0.0 .90 0.0 0.0 0.0\n0.0 0.0 .36 .36 .18\n0.0 0.0 0.0 .90 0.0\n.90 0.0 0.0 0.0 0.0\n.45 0.0 .45 0.0 0.0 .452\n.363 .184.362\nst\n.903\nst\n.900\nst\n.450\nst\n.901\nindependent\nsymbol-table\nobjects\nkey value\na[4][2]\nSparse matrix representations\n..\nSparseVector[] a; \na = new SparseVector[N]; \ndouble[] x = new double[N]; \ndouble[] b = new double[N]; \n... \n// Initialize a[] and x[]. \n... \nfor (int i = 0; i < N; i++)\n   b[i] = a[i].dot(x);\nSparse matrix-vector multiplication\n504 CHAPTER 3 \u25a0 Searching\n  \nof-a-billion speedups when they are available are likely to be able to address problems \nthat could not otherwise be contemplated.\nBuilding the matrix for the Google application is a graph-processing application \n(and a symbol-table client!), albeit for a huge sparse matrix. Given the matrix, the Page-\nRank calculation is nothing more than doing a matrix-vector multiplication, replacing \nthe source vector with the result vector, and iterating the process until it converges (as \nguaranteed by fundamental theorems ", "start": 516, "end": 517}, "755": {"text": "matrix, the Page-\nRank calculation is nothing more than doing a matrix-vector multiplication, replacing \nthe source vector with the result vector, and iterating the process until it converges (as \nguaranteed by fundamental theorems in probability theory). Thus, the use of a class \nlike SparseVector can improve the time and space usage for this application by a fac-\ntor of 10 billion or 100 billion or more. \nSimilar savings are possible in many scienti\ufb01c calculations, so sparse vectors and ma-\ntrices are widely used and typically incorporated into specialized systems for scienti\ufb01c \ncomputing. When working with huge vectors and matrices, it is wise to run simple per-\nformance tests to be sure that the kinds of performance gains that we have illustrated \nhere are not being missed. On the other hand, array processing for primitive types of \ndata is built in to most programming languages, so using arrays for vectors that are \nnot sparse, as we did in this example, may offer further speedups. Developing a good \nunderstanding of the underlying costs and making the appropriate implementation \ndecisions is certainly worthwhile for such applications.\nSymbol tables are a primary contribution of algorithmic technology to the \ndevelopment of our modern computational infrastructure because of their ability to \ndeliver savings on a huge scale in a vast array of practical applications, making the dif-\nference between providing solutions to a wide range of  problems and not being able \nto address them at all. Few \ufb01elds of science or engineering involve studying the effects \nof an invention that improves costs by factors of 100 billion\u2014symbol-table applica-\ntions put us in just that position, as we have just seen in several examples, and these \nimprovements have had profound effects. The data structures and algorithms that we \nhave considered are certainly not the \ufb01nal word: they were all developed in just a few \ndecades, and their properties are not fully ", "start": 517, "end": 517}, "756": {"text": "\nimprovements have had profound effects. The data structures and algorithms that we \nhave considered are certainly not the \ufb01nal word: they were all developed in just a few \ndecades, and their properties are not fully understood. Because of their importance, \nsymbol-table implementations continue to be studied intensely by researchers around \nthe world, and we can look forward to new developments on many fronts as the scale \nand scope of the applications they address continue to expand.\n5053.5 \u25a0 Applications\n Q&A\n \nQ.  Can a SET contain null?\nA. No. As with symbol tables, keys are non-null objects.\nQ.  Can a SET be null?\nA. No. A SET can be empty (contain no objects), but not null. As with any Java data \ntype, a variable of type SET can have the value null, but that just indicates that it does \nnot reference any SET. The result of using new to create a SET is always an object that is \nnot null.\nQ.  If all my data is in memory, there is no real reason to use a \ufb01lter, right?\nA. Right. Filtering really shines in the case when you have no idea how much data to \nexpect. Otherwise, it may be a useful way of thinking, but not a cure-all.\nQ.  I have data in a spreadsheet. Can I develop something like LookupCSV to search \nthrough it?\nA.  Yo u r  s p re a d s h e e t  a p p l i c a t i o n  p ro b a b l y  h a s  a n  o p t i o n  t o  e x p o r t  t o  a  .csv \ufb01le, so you \ncan use LookupCSV directly.\nQ.  Why would I need FileIndex? Doesn\u2019t my operating system solve this problem?\nA.  If ", "start": 517, "end": 518}, "757": {"text": "o r t  t o  a  .csv \ufb01le, so you \ncan use LookupCSV directly.\nQ.  Why would I need FileIndex? Doesn\u2019t my operating system solve this problem?\nA.  If you are using an OS that meets your needs, continue to do so, by all means. As \nwith many of our programs, FileIndex is intended to show you the basic underlying \nmechanisms of such applications and to suggest possibilities to you. \nQ.  Why not have the dot() method in SparseVector take a SparseVector object as \nargument and return a SparseVector object?\nA. That is a \ufb01ne alternate design and a nice programming exercise that requires code \nthat is a bit more intricate than for our design (see Exercise 3.5.16). For general matrix \nprocessing, it might be worthwhile to also add a SparseMatrix type. \n506 CHAPTER 3 \u25a0 Searching\n EXERCISES\n3.5.1  Implement SET and HashSET as \u201cwrapper class\u201d clients of ST and HashST, respec-\ntively (provide dummy values and ignore them).\n3.5.2  Develop a SET implementation SequentialSearchSET by starting with the code \nfor SequentialSearchST and eliminating all of the code involving values.\n3.5.3  Develop a SET implementation BinarySearchSET by starting with the code for \nBinarySearchST and eliminating all of the code involving values. \n3.5.4  Develop classes HashSTint and HashSTdouble for maintaining sets of keys of \nprimitive int and double types, respectively. (Convert generics to primitive types in \nthe code of LinearProbingHashST.)\n3.5.5  Develop classes STint and STdouble for maintaining ordered symbol ta-\nbles where keys are primitive int and double types, respectively. (Convert generics \nto primitive types in the code of RedBlackBST.) T est your solution with a version of ", "start": 518, "end": 519}, "758": {"text": "STint and STdouble for maintaining ordered symbol ta-\nbles where keys are primitive int and double types, respectively. (Convert generics \nto primitive types in the code of RedBlackBST.) T est your solution with a version of \nSparseVector as a client.\n3.5.6  Develop classes HashSETint and HashSETdouble for maintaining sets of keys of \nprimitive int and double types, respectively. (Eliminate code involving values in your \nsolution to Exercise 3.5.4.)\n3.5.7 Develop classes SETint and SETdouble for maintaining ordered sets of keys of \nprimitive int and double types, respectively. (Eliminate code involving values in your \nsolution to Exercise 3.5.5.)\n3.5.8 Modify LinearProbingHashST to keep duplicate keys in the table. Return any\nvalue associated with the given key for get(), and remove all items in the table that have \nkeys equal to the given key for delete().\n3.5.9  Modify BST to keep duplicate keys in the tree. Return any value associated with \nthe given key for get(), and remove all nodes in the tree that h ave keys equal to the \ngiven key for delete().\n3.5.10  Modify RedBlackBST to keep duplicate keys in the tree. Return any value associ-\nated with the given key for get(), and remove all nodes in the tree that h ave keys equal \nto the given key for delete().\n5073.5 \u25a0 Applications\n  \n3.5.11 Develop a MultiSET class that is like SET, but allows equal keys and thus imple-\nments a mathematical multiset.\n3.5.12  Modify LookupCSV to associate with each key all values that appear in a key-\nvalue pair with that key in the input (not just the most recent, as in the associative-array \nabstraction). \n3.5.13 Modify LookupCSV to make a program RangeLookupCSV that takes ", "start": 519, "end": 520}, "759": {"text": "in a key-\nvalue pair with that key in the input (not just the most recent, as in the associative-array \nabstraction). \n3.5.13 Modify LookupCSV to make a program RangeLookupCSV that takes two key val-\nues from the standard input and prints all key-value pairs in the .csv \ufb01le such that the \nkey falls within the range speci\ufb01ed.\n3.5.14 Develop and test a static method invert() that takes as argument an \nST<String, Bag<String>> and produces as return value the inverse of the given sym-\nbol table (a symbol table of the same type).\n3.5.15  Write a program that takes a string on standard input and an integer k as com-\nmand-line argument and puts on standard output a sorted list of the k-grams found in \nthe string, each followed by its index in the string.  \n3.5.16  Add a method sum() to SparseVector that takes a SparseVector as argument \nand returns a SparseVector that is the term-by-term sum of this vector and the argu-\nment vector. Note: Y ou need delete() (and special attention to precision) to handle the \ncase where an entry becomes 0.\nEXERCISES  (continued)\n508 CHAPTER 3 \u25a0 Searching\n CREATIVE PROBLEMS\n3.5.17    Mathematical sets. Yo u r  g o a l  i s  t o  d e ve l o p  a n  i m p l e m e n t a t i o n  o f  t h e  f o l l ow i n g   \nAPI MathSET for processing (mutable) mathematical sets: \npublic class  MathSET<Key> \nMathSET(Key[] universe) create a set\nvoid add(Key key) put key into the set\nMathSET<Key> complement() set of keys in the universe that \nare ", "start": 520, "end": 521}, "760": {"text": "sets: \npublic class  MathSET<Key> \nMathSET(Key[] universe) create a set\nvoid add(Key key) put key into the set\nMathSET<Key> complement() set of keys in the universe that \nare not in this set\nvoid union(MathSET<Key> a) put any keys from a into the \nset that are not already there\nvoid intersection(MathSET<Key> a) remove any keys from this set \nthat are not in a\nvoid delete(Key key) remove key from the set\nboolean contains(Key key) is key in the set?\nboolean isEmpty() is the set empty?\nint size() number of keys in the set\nAPI for a basic set data type\n \nUse a symbol table . Extra credit : Represent sets with arrays of boolean values.\n3.5.18   Multisets. After referring to Exercises 3.5.2 and 3.5.3 and the previous exer -\ncise, develop APIs MultiHashSET and MultiSET for multisets (sets that can have equal \nkeys) and implementations SeparateChainingMultiSET and BinarySearchMultiSET\nfor multisets and ordered multisets, respectively.\n3.5.19  Equal keys in symbol tables. Consider the API MultiST (unordered or ordered) \nto be the same as our symbol-table APIs de\ufb01ned on page 363 and page 366, but with equal \nkeys allowed, so that the semantics of get() is to return any value associated with the \ngiven key, and we add a new method\nIterable<Value> getAll(Key key)\n5093.5 \u25a0 Applications\n  \n \nthat returns all values associated with the given key. Using our code for \nSeparateChainingST and BinarySearchST as a starting point, develop implementa-\ntions SeparateChainingMultiST and BinarySearchMultiST for these APIs.\n3.5.20     Concordance. Write an ST client Concordance that puts on ", "start": 521, "end": 522}, "761": {"text": "BinarySearchST as a starting point, develop implementa-\ntions SeparateChainingMultiST and BinarySearchMultiST for these APIs.\n3.5.20     Concordance. Write an ST client Concordance that puts on standard output a \nconcordance of the strings in the standard input stream (see page 498).\n3.5.21  Inverted concordance. Write a program InvertedConcordance that takes a \nconcordance on standard input and puts the original string on standard output stream. \nNote : This computation is associated with a famous story having to do with the Dead \nSea Scrolls. The team that discovered the original tablets enforced a secrecy rule that \nessentially resulted in their making public only a concordance. After a while, other re-\nsearchers \ufb01gured out how to invert the concordance, and the full text was eventually \nmade public.\n3.5.22  Fully indexed CSV. Implement an ST client FullLookupCSV that builds an ar-\nray of ST objects (one for each \ufb01eld), with a test client that allows the user to specify the \nkey and value \ufb01elds in each query.\n3.5.23   Sparse matrices. Develop an API and an implementation for sparse 2D matri-\nces. Support matrix addition and matrix multiplication. Include constructors for row \nand column vectors.\n3.5.24  Non-overlapping interval search. Given a list of non-overlapping intervals of \nitems, write a function that takes an item as argument and determines in which, if \nany, interval that item lies. For example, if the items are integers and the intervals are \n1643-2033, 5532-7643, 8999-10332, 5666653-5669321, then the query point 9122\nlies in the third interval and 8122 lies in no interval.\n3.5.25 ", "start": 522, "end": 522}, "762": {"text": "5532-7643, 8999-10332, 5666653-5669321, then the query point 9122\nlies in the third interval and 8122 lies in no interval.\n3.5.25  Registrar scheduling. The registrar at a prominent northeastern University re-\ncently scheduled an instructor to teach two different classes at the same exact time. Help \nthe registrar prevent future mistakes by describing a method to check for such con\ufb02icts. \nFor simplicity, assume all classes run for 50 minutes starting at 9:00, 10:00, 11:00, 1:00, \n2:00, or 3:00.\n3.5.26   LRU cache. Create a data structure that supports the following operations: ac-\ncess and remove. The access operation inserts the item onto the data structure if it\u2019s \nnot already present. The remove operation deletes and returns the item that was least \nCREATIVE PROBLEMS  (continued)\n510 CHAPTER 3 \u25a0 Searching\n recently accessed. Hint : Maintain the items in order of access in a doubly linked list, \nalong with pointers to the \ufb01rst and last nodes. Use a symbol table with keys = items, \nvalues = location in linked list. When you access an element, delete it from the linked \nlist and reinsert it at the beginning. When you remove an element, delete it from the end \nand remove it from the symbol table.\n3.5.27    List. Develop an implementation of the following API: \npublic class    List<Item> implements Iterable<Item> \nList() create a list\nvoid addFront(Item item) add item to the front\nvoid addBack(Item item) add item to the back\nItem deleteFront() remove from the front\nItem deleteBack() remove from the back\nvoid delete(Item item) remove item from the list\nvoid add(int i, Item item) ", "start": 522, "end": 523}, "763": {"text": "addBack(Item item) add item to the back\nItem deleteFront() remove from the front\nItem deleteBack() remove from the back\nvoid delete(Item item) remove item from the list\nvoid add(int i, Item item) add item as the ith in the list\nItem delete(int i) remove the ith item from the list\nboolean contains(Item item) is key in the list?\nboolean isEmpty() is the list empty?\nint size() number of items in the list\nAPI for a list data type\n \n \nHint : Use two symbol tables, one to \ufb01nd the ith item in the list ef\ufb01ciently, and the other \nto ef\ufb01ciently search by item. (Java\u2019s java.util.List interface contains methods like \nthese but does not supply any implementation that ef\ufb01ciently supports all \noperations.)\n3.5.28  UniQueue. Create a data type that is a queue, except that an element may only \nbe inserted the queue once. Use an existence symbol table to keep track of all elements \nthat have ever been inserted and ignore requests to re-insert such items.\n5113.5 \u25a0 Applications\n 3.5.29  Symbol table with random access. Create a data type that supports inserting a \nkey-value pair, searching for a key and returning the associated value, and deleting and \nreturning a random key. Hint : Combine a symbol table and a randomized queue.\nCREATIVE PROBLEMS  (continued)\n512 CHAPTER 3 \u25a0 Searching\n EXPERIMENTS\n \n3.5.30    Duplicates (revisited). Redo Exercise 2.5.31 using the Dedup \ufb01lter given on \npage 490. Compare the running times of the two approaches. Then use Dedup to run the \nexperiments for N = 10 7, 10 8, and10 9, repeat the experiments for random long values \nand discuss the ", "start": 523, "end": 525}, "764": {"text": "running times of the two approaches. Then use Dedup to run the \nexperiments for N = 10 7, 10 8, and10 9, repeat the experiments for random long values \nand discuss the results.\n3.5.31    Spell checker. With  the \ufb01le dictionary.txt from the booksite as command-\nline argument, the BlackFilter client described on page 491 prints all misspelled words \nin a text \ufb01le taken from standard input. Compare the performance of RedBlackBST, \nSeparateChainingHashST, and LinearProbingHashST for the \ufb01le WarAndPeace.txt\n(available on the booksite) with this client and discuss the results. \n3.5.32    Dictionary. Study the performance of a client like LookupCSV in a scenario \nwhere performance matters. Speci\ufb01cally, design a query-generation scenario instead of \ntaking commands from standard input, and run performance tests for large inputs and \nlarge numbers of queries. \n3.5.33    Indexing. Study a client like LookupIndex in a scenario where performance \nmatters. Speci\ufb01cally, design a query-generation scenario instead of taking commands \nfrom standard input, and run performance tests for large inputs and large numbers of \nqueries.\n3.5.34  Sparse vector. Run experiments to compare the performance of matrix-vector \nmultiplication using  SparseVector to the standard implementation using arrays.  \n3.5.35  Primitive types. Evaluate the utility of using primitive types for Integer and \nDouble values, for LinearProbingHashST and RedBlackBST. How much space and \ntime are saved, for large numbers of searches in large tables?\n5133.5 \u25a0 Applications\n 4.1 Undirected Graphs   .  .  .  .  .  .  .  .  .  .  .  518\n4.2 ", "start": 525, "end": 526}, "765": {"text": "tables?\n5133.5 \u25a0 Applications\n 4.1 Undirected Graphs   .  .  .  .  .  .  .  .  .  .  .  518\n4.2 Directed Graphs  .  .  .  .  .  .  .  .  .  .  .  .  .  566\n4.3 Minimum Spanning Trees  .  .  .  .  .  .  .  604\n4.4 Shortest Paths   .  .  .  .  .  .  .  .  .  .  .  .  .  .  638\nFOUR\n G r a p h s P\n \n \nairwise connections between items play a critical role in a vast array of compu-\ntational applications. The relationships implied by these connections lead im-\nmediately to a host of natural questions: Is there a way to connect one item to \nanother by following the connections? How many other items are connected to a given \nitem? What is the shortest chain of connections between this item and this other item?\nTo  m o d e l  s u c h  s i t u a t i o n s , w e  u s e  a b s t r a c t  m a t h e m a t i c a l  o b j e c t s  c a l l e d  graphs. In this \nchapter, we examine basic properties of graphs in detail, setting the stage for us to study \na variety of algorithms that are useful for answering questions of the type just posed. \nThese algorithms serve as the basis for attacking problems in important applications \nwhose solution we could not even contemplate without good algorithmic technology.\nGraph theory, a major branch of mathematics, has been studied intensively for hun-\ndreds of years. Many important and useful properties of graphs ", "start": 526, "end": 527}, "766": {"text": "in important applications \nwhose solution we could not even contemplate without good algorithmic technology.\nGraph theory, a major branch of mathematics, has been studied intensively for hun-\ndreds of years. Many important and useful properties of graphs have been discovered, \nmany important algorithms have been developed, and many dif\ufb01cult problems are still \nactively being studied. In this chapter, we introduce a variety of fundamental graph \nalgorithms that are important in diverse applications.\nLike so many of the other problem domains that we have studied, the algorithmic in-\nvestigation of graphs is relatively recent. Although a few of the fundamental algorithms \nare centuries old, the majority of the interesting ones have been discovered within the \nlast several decades and have bene\ufb01ted from the emergence of the algorithmic technol-\nogy that we have been studying. Even the simplest graph algorithms lead to useful com-\nputer programs, and the nontrivial algorithms that we examine are among the most \nelegant and interesting algorithms known.\nTo  i l l u s t r a te  t h e  d ive r s i t y  o f  a p p l i c a t i o n s  t h a t  i nvo lve  g r a p h  p ro ce s s i n g , w e  b e g i n  o u r  \nexploration of algorithms in this fertile area by introducing several examples.\n515\n  \nMaps. A person who is planning a trip may need to answer questions such as \u201cWhat is \nthe shortest route from Providence to Princeton?\u201d A seasoned traveler who has experi-\nenced traf\ufb01c delays on the shortest route may ask the question \u201cWhat is the fastest way \nto get from Providence to Princeton?\u201d To answer such questions, we process informa-\ntion about connections (roads) between items (intersections).\nWeb conte nt. When we browse the web, we encounter pages that contain references \n(links) ", "start": 527, "end": 528}, "767": {"text": "\nto get from Providence to Princeton?\u201d To answer such questions, we process informa-\ntion about connections (roads) between items (intersections).\nWeb conte nt. When we browse the web, we encounter pages that contain references \n(links) to other pages and we move from page to page by clicking on the links. The \nentire web is a graph, where the items are pages and the connections are links. Graph-\nprocessing algorithms are essential components of the search engines that help us lo-\ncate information on the web.\nCircuits. An electric circuit comprises devices such as transistors, resistors, and ca -\npacitors that are intricately wired together. We use computers to control machines that \nmake circuits and to check that the circuits perform desired functions. We need to an-\nswer simple questions such as \u201cIs a short-circuit present?\u201d as well as complicated ques-\ntions such as \u201cCan we lay out this circuit on a chip without making any wires cross?\u201d \nThe answer to the \ufb01rst question depends on only the properties of the connections \n(wires), whereas the answer to the second question requires detailed information about \nthe wires, the devices that those wires connect, and the physical constraints of the chip.\nSchedules. A manufacturing process requires a variety of jobs to be performed, under \na set of constraints that specify that certain tasks cannot be started until certain other \ntasks have been completed. How do we schedule the tasks such that we both respect the \ngiven constraints and complete the whole process in the least amount of time?\nCommerce. Retailers and \ufb01nancial instututions track buy/sell orders in a market. A \nconnection in this situation represents the transfer of cash and goods between an in -\nstitution and a customer. Knowledge of the nature of the connection structure in this \ninstance may enhance our understanding of the nature of the market.\nMatching. Students apply for positions in selective institutions such as social clubs, \nuniversities, ", "start": 528, "end": 528}, "768": {"text": "in -\nstitution and a customer. Knowledge of the nature of the connection structure in this \ninstance may enhance our understanding of the nature of the market.\nMatching. Students apply for positions in selective institutions such as social clubs, \nuniversities, or medical schools. Items correspond to the students and the institutions; \nconnections correspond to the applications. We want to discover methods for matching \ninterested students with available positions.\nComputer networks. A computer network consists of interconnected sites that send, \nforward, and receive messages of various types. We are interested in knowing about the \nnature of the interconnection structure because we want to lay wires and build switches \nthat can handle the traf\ufb01c ef\ufb01ciently.  \n516 CHAPTER 4 \u25a0 Graphs\n Software. A compiler builds graphs to represent relationships among modules in a \nlarge software system. The items are the various classes or modules that comprise the \nsystem; connections are associated either with the possibility that a method in one class \nmight call another (static analysis) or with actual calls while the system is in operation \n(dynamic analysis). We need to analyze the graph to determine how best to allocate \nresources to the program most ef\ufb01ciently.\n S o c i a l  n e t w o r k s .  When you use a social network, you build explicit connections with \nyour friends. Items correspond to people; connections are to friends or followers. Un-\nderstanding the properties of these networks is a modern graph-processing applica -\ntions of intense interest not just to compaines that support such networks, but also in \npolitics, diplomacy, entertainment, education, marketing, and many other domains.\nThese examples indicate the range of applications for which graphs are the ap-\npropriate abstraction and also the range of computational problems that we might \nencounter when we work with graphs. Thousands of such problems have been studied, \nbut many problems can be addressed in the context of one of several ", "start": 528, "end": 529}, "769": {"text": "ap-\npropriate abstraction and also the range of computational problems that we might \nencounter when we work with graphs. Thousands of such problems have been studied, \nbut many problems can be addressed in the context of one of several basic graph mod-\nels\u2014we will study the most important \nones in this chapter. In practical appli-\ncations, it is common for the volume of \ndata involved to be truly huge, so that \nef\ufb01cient algorithms make the difference \nbetween whether or not a solution is at \nall feasible.\nTo  o r g a n i z e  t h e  p re s e n t a t i o n , w e  \nprogress through the four most impor -\ntant types of graph models: undirected \ngraphs (with simple connections), di-\ngraphs (where the direction of each con-\nnection is signi\ufb01cant), edge-weighted \ngraphs (where each connection has an \nassociated weight), and edge-weighted \ndigraphs (where each connection has \nboth a direction and a weight).\napplication item connection\nmap intersection road\nweb content page link\ncircuit device wire\nschedule job constraint\ncommerce customer transaction\nmatching student application\ncomputer network site \n \nconnection\nsoftware method call\nsocial network person friendship\nTypical graph applications\n517CHAPTER 4 \u25a0 Graphs\n 4.1 UNDIRECTED GRAPHS\nOur stARting point is the study of graph models where edges are nothing more than \nconnections between vertices. We use the term undirected graph in contexts where we \nneed to distinguish this model from other models (such as the title of this section), but, \nsince this is the simplest model, we start with the following de\ufb01nition: \ngraph is a set of vertices and a collection of  edges that each connect a  D e f i n i t i o n .  A    \npair of vertices.\n \nVer tex  n ", "start": 529, "end": 530}, "770": {"text": "de\ufb01nition: \ngraph is a set of vertices and a collection of  edges that each connect a  D e f i n i t i o n .  A    \npair of vertices.\n \nVer tex  n a m e s  a re  n o t  i m p o r t a n t  to  t h e  d e \ufb01 n i t i o n , b u t  we  n e e d  a  w ay  \nto refer to vertices. By convention, we use the names 0 through V/H110021 \nfor the vertices in a V-vertex graph. The main reason that we choose \nthis system is to make it easy to write code that ef\ufb01ciently accesses in-\nformation corresponding to each vertex, using array indexing. It is not \ndif\ufb01cult to use a symbol table to establish a 1-1 mapping to associate \nV arbitrary vertex names with the V integers between 0 and V/H110021 (see \npage 548), so the convenience of using indices as vertex names comes \nwithout loss of generality (and without much loss of ef\ufb01ciency). W e \nuse the notation v-w to refer to an edge that connects v and w; the nota-\ntion w-v is an alternate way to refer to the same edge. \nWe draw a g raph w ith circles for the ver tices and lines connecting \nthem for the edges. A drawing gives us intuition about the structure of \nthe graph; but this intuition can be misleading, because the graph is \nde\ufb01ned independently of the drawing. For example, the two drawings \nat left represent the same graph, because the graph is nothing more than its (unor -\ndered) set of vertices and its (unordered) collection  of edges (vertex pairs).\nAnomalies. Our de\ufb01nition allows two simple anomalies:\n\u25a0 A ", "start": 530, "end": 530}, "771": {"text": "graph is nothing more than its (unor -\ndered) set of vertices and its (unordered) collection  of edges (vertex pairs).\nAnomalies. Our de\ufb01nition allows two simple anomalies:\n\u25a0 A    self-loop is an edge that connects a vertex to itself. \n\u25a0 Two edges that connect the same pair of  ver tices are    parallel.\nMathematicians sometimes refer to graphs with parallel edges \nas    multigraphs and graphs with no parallel edges or self-loops as \n  simple graphs. Typically, our implementations allow self-loops and \nparallel edges (because they arise in applications), but we do not include them in ex-\namples. Thus, we can refer to every edge just by naming the two vertices it connects.\nTwo drawings of the same graph\nAnomalies\nparallel\nedgesself-loop\n518\n  \n \nGlossary A substantial amount of nomenclature is associated with graphs. Most of \nthe terms have straightforward de\ufb01nitions, and, for reference, we consider them in one \nplace: here.\nadjacent When there is an edge connecting two vertices, we say that the vertices are    \nto one another and that the edge is   incident to both vertices. The    degree of a vertex is the \nnumber of edges incident to it. A  subgraph is a subset of a graph\u2019s edges (and associated \nvertices) that constitutes a graph. Many computational tasks \ninvolve identifying subgraphs of various types. Of particular \ninterest are edges that take us through a sequence of vertices \nin a graph.\n \n D e f i n i t i o n .  A    path in a graph is a sequence of vertices \nconnected by edges. A  simple path is one with no repeated \nvertices. A   cycle is a path with at least one edge whose \ufb01rst \nand last vertices are the same. A  simple cycle is a cycle with \nno repeated ", "start": 530, "end": 531}, "772": {"text": "A  simple path is one with no repeated \nvertices. A   cycle is a path with at least one edge whose \ufb01rst \nand last vertices are the same. A  simple cycle is a cycle with \nno repeated edges or vertices (except the requisite repeti-\ntion of the \ufb01rst and last vertices). The  length of a path or \na cycle is its number of edges.\nMost often, we work with simple cycles and simple paths and \ndrop the simple modifer; when we want to allow repeated ver-\ntices, we refer to general paths and cycles. We say that one vertex is   connected to another \nif there exists a path that contains both of them. We use notation like u-v-w-x to repre-\nsent a path from u to x and u-v-w-x-u to represent a cycle from u to v to w to x and back \nto u again. Several of the algorithms that we consider \ufb01nd paths and cycles. Moreover, \npaths and cycles lead us to consider the structural properties of a graph as a whole:\n D e f i n i t i o n .  A graph is   connected if there is a path from every vertex to every other \nvertex in the graph. A graph that is not connected consists of a set of   connected com-\nponents, which are maximal connected subgraphs. \nIntuitively, if the vertices were physical objects, such as knots or beads, and the edges \nwere physical connections, such as strings or wires, a connected graph would stay in \none piece if picked up by any vertex, and a graph that is not connected comprises two or \nmore such pieces. Generally, processing a graph necessitates processing the connected \ncomponents one at a time.\nAnatomy of a graph\ncycle of\nlength 5\nvertex\nvertex of\ndegree 3\nedge\npath of\nlength 4\nconnected\ncomponents\n5194.1 \u25a0 Undirected ", "start": 531, "end": 531}, "773": {"text": "one at a time.\nAnatomy of a graph\ncycle of\nlength 5\nvertex\nvertex of\ndegree 3\nedge\npath of\nlength 4\nconnected\ncomponents\n5194.1 \u25a0 Undirected Graphs\n An    acyclic graph is a graph with no cycles. Several of \nthe algorithms that we consider are concerned with \ufb01nd -\ning acyclic subgraphs of a given graph that satisfy certain \nproperties. We need additional terminology to refer to \nthese structures:\n D e f i n i t i o n .  A    tree is an acyclic connected graph. A dis-\njoint set of trees is called a    forest. A   spanning tree of a \nconnected graph is a subgraph that contains all of that \ngraph\u2019s vertices and is a single tree. A    spanning forest of \na graph is the union of spanning trees of its connected \ncomponents. \n \nThis de\ufb01nition of tree is quite general: with suitable re\ufb01ne-\nments it embraces the trees that we typically use to model pro-\ngram behavior (function-call hierarchies) and data structures \n(BSTs, 2-3 trees, and so forth). Mathematical properties of \ntrees are well-studied and intuitive, so we state them without \nproof. For example, a graph G with V vertices is a tree if and \nonly if it satis\ufb01es any of the following \ufb01ve conditions: \n\u25a0 G has V/H110021 edges and no cycles. \n\u25a0 G has V/H110021 edges and is connected. \n\u25a0 G is connected, but removing any edge disconnects it.\n\u25a0 G is acyclic, but adding any edge creates a cycle. \n\u25a0 Exactly one simple path connects each pair of vertices in G. \nSeveral of the algorithms that we consider \ufb01nd spanning trees and forests, and these \nproperties play an important role in their analysis and implementation.\nThe density ", "start": 531, "end": 532}, "774": {"text": "\n\u25a0 Exactly one simple path connects each pair of vertices in G. \nSeveral of the algorithms that we consider \ufb01nd spanning trees and forests, and these \nproperties play an important role in their analysis and implementation.\nThe density of a graph is the propor -\ntion of possible pairs of vertices that are \nconnected by edges. A     sparse graph has \nrelatively few of the possible edges pres -\nent; a  dense graph has relatively few of \nthe possible edges missing. Generally, \nwe think of a graph as being sparse if \nits number of different edges is within \na small constant factor of V and as be-\ning dense otherwise. This rule of thumb \nA tree\nacyclic\n19 vertices\n18 edges\nconnected\nA spanning forest\nsparse  (E = 200) dense  (E = 1000)\nTwo graphs (V = 50)\n520 CHAPTER 4 \u25a0 Graphs\n  \nleaves a gray area (when the number of edges is, say, ~ c V3/2) but the distinction be-\ntween sparse and dense is typically very clear in applications. The applications that we \nconsider nearly always involve sparse graphs.\nA  bipartite graph is a graph whose vertices we can divide into two sets \nsuch that all edges connect a vertex in one set with a vertex in the other \nset. The \ufb01gure at right gives an example of a bipartite graph, where one \nset of vertices is colored red and the other set of vertices is colored black. \nBipartite graphs arise in a natural way in many situations, one of which \nwe will consider in detail at the end of this section. \nWith these preparations, we are ready to move on to consider graph-processing \nalgorithms. We begin by considering an API and implementation for a graph data type, \nthen we consider classic algorithms for searching graphs and for identifying connected \ncomponents. To conclude the section, we ", "start": 532, "end": 533}, "775": {"text": "move on to consider graph-processing \nalgorithms. We begin by considering an API and implementation for a graph data type, \nthen we consider classic algorithms for searching graphs and for identifying connected \ncomponents. To conclude the section, we consider real-world applications where vertex \nnames need not be integers and graphs may have huge numbers of vertices and edges.\nA bipartite graph\n5214.1 \u25a0 Undirected Graphs\n Undirected graph data type Our starting point for developing graph-process-\ning algorithms is an API that de\ufb01nes the fundamental graph operations. This scheme \nallows us to address graph-processing tasks ranging from elementary maintenance op-\nerations to sophisticated solutions of dif\ufb01cult problems.\npublic class    Graph \nGraph(int V) create a V-vertex graph with no edges\nGraph(In in) read a graph from input stream in\nint V() number of vertices\nint E() number of edges\nvoid addEdge(int v, int w) add edge v-w to this graph\nIterable<Integer> adj(int v) vertices adjacent to v\nString toString() string representation\n A P I  f o r  a n  u n d i r e c t e d  g r a p h\nThis API contains two constructors, methods to return the number of vertices and \nedges, a method to add an edge, a toString() method, and a method adj() that al-\nlows client code to iterate through the vertices adjacent to a given vertex (the order of \niteration is not speci\ufb01ed). Remarkably, we can build all of the algorithms that we con-\nsider in this section on the basic abstraction embodied in adj().\nThe second constructor assumes an input format consisting of 2E + 2 integer values: \nV, then E, then E pairs of values between 0 and V/H110021, each pair denoting an edge. As \nexamples, we use the two graphs tinyG.txt and ", "start": 533, "end": 534}, "776": {"text": "+ 2 integer values: \nV, then E, then E pairs of values between 0 and V/H110021, each pair denoting an edge. As \nexamples, we use the two graphs tinyG.txt and mediumG.txt that are depicted below. \nSeveral examples of Graph client code are shown in the table on the facing page. \n13\n13\n0 5\n4 3\n0 1\n9 12\n6 4\n5 4\n0 2\n11 12\n9 10\n0 6\n7 8\n9 11\n5 3\ntinyG.txt\nInput format for Graph constructor (two examples)\n250\n1273\n244 246\n239 240\n238 245\n235 238\n233 240\n232 248\n231 248\n229 249\n228 241\n226 231\n...\n(1263 additional lines)\nmediumG.txt\nV\nE\nV\nE\n522 CHAPTER 4 \u25a0 Graphs\n task                     implementation   \ncompute the degree of v\npublic static int degree(Graph G, int v) \n{\n   int degree = 0;\n   for (int w : G.adj(v)) degree++;\n   return degree; \n}\ncompute maximum degree\npublic static int maxDegree(Graph G) \n{\n   int max = 0; \n   for (int v = 0; v < G.V(); v++)\n      if (degree(G, v) > max)\n         max = degree(G, v);\n   return max; \n}\ncompute average degree public static int avgDegree(Graph G) \n{  return 2 * G.E() / G.V();  }\ncount self-loops\npublic static int numberOfSelfLoops(Graph G) \n{\n   int count = 0;\n   for (int v = 0; v < G.V(); v++)\n      for (int w : G.adj(v))\n ", "start": 534, "end": 535}, "777": {"text": "self-loops\npublic static int numberOfSelfLoops(Graph G) \n{\n   int count = 0;\n   for (int v = 0; v < G.V(); v++)\n      for (int w : G.adj(v))\n         if (v == w) count++;\n   return count/2;   // each edge counted twice  \n}\nstring representation of the \ngraph\u2019s adjacency lists\n(instance method in Graph)\npublic String toString() \n{\n   String s = V + \" vertices, \" + E + \" edges\\n\";\n   for (int v = 0; v < V; v++)\n   {\n      s += v + \": \";\n      for (int w : this.adj(v))\n         s += w + \" \";\n      s += \"\\n\";\n   }\n   return s; \n}\n T y p i c a l  g r a p h - p r o c e s s i n g  c o d e  \n5234.1 \u25a0 Undirected Graphs\n  R e p r e s e n t a t i o n  a l t e r n a t i v e s .  The next decision that we face in graph processing is \nwhich graph representation (data structure) to use to implement this API. We have two \nbasic requirements: \n\u25a0 We must have the space to accommodate the types of graphs that we are likely to \nencounter in applications. \n\u25a0  We want to develop time-ef\ufb01cient implementations of Graph instance meth-\nods\u2014the basic methods that we need to develop graph-processing clients.\nThese requirements are a bit vague, but they \nare still helpful in choosing among the three \ndata structures that immediately suggest \nthemselves for representing graphs:\n\u25a0 An     adjacency matrix, where we main-\ntain a V-by-V boolean array, with the \nentry in row v and column w de\ufb01ned \nto be true if there is an edge adjacent \nto ", "start": 535, "end": 536}, "778": {"text": "graphs:\n\u25a0 An     adjacency matrix, where we main-\ntain a V-by-V boolean array, with the \nentry in row v and column w de\ufb01ned \nto be true if there is an edge adjacent \nto both vertex v and vertex w in the \ngraph, and to be false otherwise. This \nrepresentation fails on the \ufb01rst count\u2014\ngraphs with millions of vertices are \ncommon and the space cost for the V 2\nboolean values needed is prohibitive. \n\u25a0 An array of edges, using an Edge class \nwith two instance variables of type int. \nThis direct representation is simple, \nbut it fails on the second count\u2014\nimplementing adj() would involve \nexamining all the edges in the graph.\n\u25a0 \n \nAn array of   adjacency lists, where we \nmaintain a vertex-indexed array of lists \nof the vertices adjacent to each vertex. \nThis data structure satis\ufb01es both re-\nquirements for typical applications and \nis the one that we will use throughout \nthis chapter.\nBeyond these performance objectives, a detailed examination reveals other consider -\nations that can be important in some applications. For example, allowing parallel edges \nprecludes the use of an  adjacency matrix, since the  adjacency matrix has no way to \nrepresent them.\nadj[]\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n5 4\n0 4\n9 12\n11 9\n0\n0\n8\n7\n9\n5 6 3\n3 4 0\n11 10 12\n6 2 1 5\nAdjacency-lists representation (undirected graph)\nBag objects\nrepresentations\nof the same edge\n524 CHAPTER 4 \u25a0 Graphs\n   A d j a c e n c y - l i s t s  d a t a  s t r ", "start": 536, "end": 537}, "779": {"text": "graph)\nBag objects\nrepresentations\nof the same edge\n524 CHAPTER 4 \u25a0 Graphs\n   A d j a c e n c y - l i s t s  d a t a  s t r u c t u r e .  The standard graph representation for graphs that are \nnot dense is called the adjacency-lists data structure , where we keep track of all the \nvertices adjacent to each vertex on a linked list that is associated with that vertex. We \nmaintain an array of lists so that, given a vertex, we can immediately access its list. T o \nimplement lists, we use our Bag ADT from Section 1.3 with a linked-list implementa-\ntion, so that we can add new edges in constant time and iterate through adjacent verti-\nces in constant time per adjacent vertex. The Graph implementation on page 526 is based \non this approach, and the \ufb01gure on the facing page depicts the data structures built by \nthis code for tinyG.txt. To add an edge connecting v and w, we add w to v\u2019s adjacency \nlist and v to w\u2019s adjacency list. Thus, each edge appears twice in the data structure. This \nGraph implementation achieves the following performance characteristics:\n\u25a0 Space usage proportional to V + E \n\u25a0 Constant time to add an edge\n\u25a0 \n \nTime proportional to the degree of v to iterate through vertices adjacent to v\n(constant time per adjacent vertex processed)\nThese characteristics are optimal for this set of operations, which suf\ufb01ce for the graph-\nprocessing applications that we consider. Parallel edges and self-loops are allowed (we \ndo not check for them). Note : It is important to realize that the order in which edges \nare added to the graph determines the order in which vertices appear in the array of \nadjacency lists built by Graph. Many different ar -\nrays of adjacency lists can represent the same graph. \nWhen using the constructor that ", "start": 537, "end": 537}, "780": {"text": "edges \nare added to the graph determines the order in which vertices appear in the array of \nadjacency lists built by Graph. Many different ar -\nrays of adjacency lists can represent the same graph. \nWhen using the constructor that reads edges from \nan input stream, this means that the input format \nand the order in which edges are speci\ufb01ed in the \n\ufb01le determine the order in which vertices appear \nin the array of adjacency lists built by Graph. Since \nour algorithms use adj() and process all adjacent \nvertices without regard to the order in which they \nappear in the lists, this difference does not affect \ntheir correctness, but it is important to bear it in \nmind when debugging or following traces. T o fa-\ncilitate these activities, we assume that Graph has a \ntest client that reads a graph from the input stream \nnamed as command-line argument and then prints \nit (relying on the toString() implementation on \npage 523) to show the order in which vertices appear \nin adjacency lists, which is the order in which algo-\nrithms process them (see Exercise 4.1.7).\n13\n13\n0 5\n4 3\n0 1\n9 12\n6 4\n5 4\n0 2\n11 12\n9 10\n0 6\n7 8\n9 11\n5 3\n% java Graph tinyG.txt\n13 vertices, 13 edges\n0: 6 2 1 5 \n1: 0 \n2: 0 \n3: 5 4 \n4: 5 6 3 \n5: 3 4 0 \n6: 0 4 \n7: 8 \n8: 7 \n9: 11 10 12 \n10: 9\n11: 9 12 \n12: 11 9 \ntinyG.txt\nOutput ", "start": 537, "end": 537}, "781": {"text": "\n6: 0 4 \n7: 8 \n8: 7 \n9: 11 10 12 \n10: 9\n11: 9 12 \n12: 11 9 \ntinyG.txt\nOutput for list-of-edges input\nV\nE\nfirst adjacent\nvertex in input\nis last on list\nsecond\nrepresentation\nof each edge\nappears in red\n5254.1 \u25a0 Undirected Graphs\n  G r a p h  d a t a  t y p e\npublic class  Graph \n{\n   private final int V;          // number of vertices\n   private int E;                // number of edges\n   private Bag<Integer>[] adj;   // adjacency lists\n   public Graph(int V)\n   {\n      this.V = V; this.E = 0;\n      adj = (Bag<Integer>[]) new Bag[V];      // Create array of lists.\n      for (int v = 0; v < V; v++)             // Initialize all lists \n         adj[v] = new Bag<Integer>();         //   to empty. \n   }\n   public Graph(In in)\n   {\n      this(in.readInt());          // Read V and construct this graph.\n      int E = in.readInt();        // Read E.\n      for (int i = 0; i < E; i++)\n      {  // Add an edge.\n         int v = in.readInt();     // Read a vertex,\n         int w = in.readInt();        // read another vertex,\n         addEdge(v, w);               // and add edge connecting them.\n      }\n   }\n   public int V()  {  return V;  }\n   public int E()  {  return E;  }\n   public void addEdge(int v, int w)\n   {\n      adj[v].add(w);                          // Add w to v\u2019s list.\n      adj[w].add(v);                          // ", "start": 537, "end": 538}, "782": {"text": "{  return E;  }\n   public void addEdge(int v, int w)\n   {\n      adj[v].add(w);                          // Add w to v\u2019s list.\n      adj[w].add(v);                          // Add v to w\u2019s list.\n      E++;\n    }\n   public Iterable<Integer> adj(int v)\n   {  return adj[v];  }\n}\nThis Graph implementation maintains a vertex-indexed array of lists of integers. Every edge appears \ntwice: if an edge connects v and w, then w appears in v\u2019s list and v appears in w\u2019s list. The second con-\nstructor reads a graph from an input stream, in the format V followed by E followed by a list of pairs \nof int values between 0 and V/H110021. See page 523 for toString().\n526 CHAPTER 4 \u25a0 Graphs It is certainly reasonable to contemplate other operations that might be useful in \napplications, and to consider methods for\n\u25a0 Adding a vertex\n\u25a0 Deleting a vertex\nOne way to handle such operations is to expand the API to use a symbol table ( ST) \ninstead of a vertex-indexed array (with this change we also do not need our convention \nthat vertex names be integer indices). We might also consider methods for\n\u25a0 Deleting an edge\n\u25a0 Checking whether the graph contains the edge v-w\nTo  i m p l e m e n t  t h e s e  m e t h o d s  ( a n d  d i s a l l ow  p a r a l l e l  e d g e s )  w e  m i g h t  u s e  a  SET instead \nof a Bag for adjacency lists. We refer to this alternative as an    adjacency set representa-\ntion. We do not use these alternatives in this book for several reasons:\n\u25a0 Our clients do not need to add vertices, delete vertices and edges, or check \nwhether ", "start": 538, "end": 539}, "783": {"text": "lists. We refer to this alternative as an    adjacency set representa-\ntion. We do not use these alternatives in this book for several reasons:\n\u25a0 Our clients do not need to add vertices, delete vertices and edges, or check \nwhether an edge exists.\n\u25a0 When clients do need these operations, they typically are invoked infrequently \nor for short adjacency lists, so an easy option is to use a brute-force implementa-\ntion that iterates through an adjacency list.\n\u25a0 The SET and ST representations slightly complicate algorithm implementation \ncode, diverting attention from the algorithms themselves.\n\u25a0 A performance penalty of log V may be involved in some situations.\nIt is not dif\ufb01cult to adapt our algorithms to accommodate other designs (for example \ndisallowing parallel edges or self-loops) without undue performance penalties. The \ntable below summarizes performance characteristics of the alternatives that we have \nmentioned. Typical applications process huge sparse graphs, so we use the adjacency-\nlists representation throughout. \nunderlying\ndata structure space add edge v-w check whether w is \nadjacent to v\niterate through vertices \nadjacent to v\nlist of edges E 1 E E\n a d j a c e n c y  m a t r i x  V 2 1 1 V\nadjacency lists E /H11001 V 1 degree(v) degree(v)\nadjacency sets E /H11001 V log V log V log V  /H11001 degree(v)\nOrder-of-growth performance for typical Graph implementations\n5274.1 \u25a0 Undirected Graphs\n Design pattern for graph processing. Since we consider a large number of graph-pro-\ncessing algorithms, our initial design goal is to decouple our implementations from the \ngraph representation. T o do so, we develop, for each given task, a task-speci\ufb01c class so \nthat clients can create objects to perform the task. Generally, the constructor does some \npreprocessing ", "start": 539, "end": 540}, "784": {"text": "from the \ngraph representation. T o do so, we develop, for each given task, a task-speci\ufb01c class so \nthat clients can create objects to perform the task. Generally, the constructor does some \npreprocessing to build data structures so as to ef\ufb01ciently respond to client queries. A \ntypical client program builds a graph, passes that graph to an algorithm implementa -\ntion class (as argument to a constructor), and then calls client query methods to learn \nvarious properties of the graph. As a warmup, consider this API:\npublic class    Search \nSearch(Graph G, int s) find vertices connected to a source vertex s \nboolean marked(int v) is v connected to s?\nint count() how many vertices are connected to s?\n G r a p h - p r o c e s s i n g  A P I  ( w a r m u p )\nWe use the term source to distinguish the vertex provided as argument to the construc-\ntor from the other vertices in the graph. In this API, the job of the constructor is to \ufb01nd \nthe vertices in the graph that are connected to the source. Then client code calls the in-\nstance methods marked() and count() to learn characteristics of the graph. The name \nmarked() refers to an approach used by the basic algorithms that we consider through-\nout this chapter: they follow paths from the source to other vertices in the graph, mark-\ning each vertex encountered. The example client TestSearch shown on the facing page \ntakes an input stream name and a source vertex number from the command line, reads \na graph from the input stream (using the second Graph constructor), builds a Search\nobject for the given graph and source, and uses marked() to print the vertices in that \ngraph that are connected to the source. It also calls count() and prints whether or not \nthe graph is connected (the graph is connected ", "start": 540, "end": 540}, "785": {"text": "for the given graph and source, and uses marked() to print the vertices in that \ngraph that are connected to the source. It also calls count() and prints whether or not \nthe graph is connected (the graph is connected if and only if the search marked all of \nits vertices).\n528 CHAPTER 4 \u25a0 Graphs\n  \nWe have already seen one way to implement the Search API: the union-\ufb01nd algo-\nrithms of Chapter 1. The constructor can build a UF object, do a union() operation \nfor each of the graph\u2019s edges, and implement marked(v) by calling connected(s, v). \nImplementing count() requires using a weighted UF implementation and extending \nits API to use a count() method that returns wt[find(v)] (see Exercise 4.1.8). This \nimplementation is simple and ef\ufb01cient, but the implementation that we consider next \nis even simpler and more ef\ufb01cient. It is based on depth-\ufb01rst search, a fundamental recur-\nsive method that follows the graph\u2019s edges to \ufb01nd the vertices connected to the source. \nDepth-\ufb01rst search is the basis for several of the graph-processing algorithms that we \nconsider throughout this chapter. \npublic class TestSearch \n{\n   public static void main(String[] args)\n   {\n      Graph G = new Graph(new In(args[0]));\n      int s = Integer.parseInt(args[1]);\n      Search search = new Search(G, s);\n      for (int v = 0; v < G.V(); v++)\n         if (search.marked(v)) \n            StdOut.print(v + \" \");\n      StdOut.println();\n      if (search.count() != G.V())\n         StdOut.print(\"NOT \");\n      StdOut.println(\"connected\");\n   } \n}\nSample graph-processing client (warmup)\n% java TestSearch tinyG.txt 0 \n0 1 2 3 4 ", "start": 540, "end": 541}, "786": {"text": "G.V())\n         StdOut.print(\"NOT \");\n      StdOut.println(\"connected\");\n   } \n}\nSample graph-processing client (warmup)\n% java TestSearch tinyG.txt 0 \n0 1 2 3 4 5 6 \nNOT connected\n% java TestSearch tinyG.txt 9 \n9 10 11 12 \nNOT connected\n13\n13\n0 5\n4 3\n0 1\n9 12\n6 4\n5 4\n0 2\n11 12\n9 10\n0 6\n7 8\n9 11\n5 3\ntinyG.txt\nV\nE\n5294.1 \u25a0 Undirected Graphs\n     D e p t h - \ufb01 r s t  s e a r c h  We often learn proper ties of  a g raph by systematically examin-\ning each of its vertices and each of its edges. Determining some simple graph proper -\nties\u2014for example, computing the degrees of all the vertices\u2014is easy if we just exam-\nine each edge (in any order whatever). But many other graph properties are related to \npaths, so a natural way to learn them is to move from vertex to vertex along the graph\u2019s \nedges. Nearly all of the graph-processing algorithms \nthat we consider use this same basic abstract model, \nalbeit with various different strategies. The simplest \nis a classic method that we now consider. \nSearching in a    maze. It is instructive to think \nabout the process of searching through a graph in \nterms of an equivalent problem that has a long and \ndistinguished history\u2014\ufb01nding our way through a \nmaze that consists of passages connected by inter -\nsections. Some mazes can be handled with a simple \nrule, but most mazes require a more sophisticated \nstrategy. Using the terminology maze instead of \ngraph, passage instead of edge, and ", "start": 541, "end": 542}, "787": {"text": "of passages connected by inter -\nsections. Some mazes can be handled with a simple \nrule, but most mazes require a more sophisticated \nstrategy. Using the terminology maze instead of \ngraph, passage instead of edge, and intersection in-\nstead of vertex is making mere semantic distinc-\ntions, but, for the moment, doing so will help to \ngive us an intuitive feel for the problem. One trick for exploring \na maze without getting lost that has been known since antiquity \n(dating back at least to the legend of  Theseus and the  Minotaur) is \nknown as   Tremaux exploration. To explore all passages in a maze:\n\u25a0 Take any unmar ked passage, unrolling a st r ing behind you.\n\u25a0 Mark all intersections and passages when you \ufb01rst visit \nthem.\n\u25a0 Retrace steps (using the string) when approaching a marked \nintersection. \n\u25a0 Retrace steps when no unvisited options remain at an inter-\nsection encountered while retracing steps.\nThe string guarantees that you can always \ufb01nd a way out and the \nmarks guarantee that you avoid visiting any passage or intersection twice. Knowing \nthat you have explored the whole maze demands a more complicated argument that is \nbetter approached in the context of graph search. Tremaux exploration is an intuitive \nstarting point, but it differs in subtle ways from exploring a graph, so we now move on \nto searching in graphs.\ngraph\nmaze\nEquivalent models of a maze\nvertex edge\nintersection\npassage\nTremaux exploration\n530 CHAPTER 4 \u25a0 Graphs\n War mup. The classic  recursive method for \nsearching in a connected graph (visiting all \nof its vertices and edges) mimics Tremaux \nmaze exploration but is even simpler to de-\nscribe. T o search a graph, invoke a recursive \nmethod that visits vertices. T o visit a vertex:\n\u25a0 Mark it as having been visited.\n\u25a0 ", "start": 542, "end": 543}, "788": {"text": "edges) mimics Tremaux \nmaze exploration but is even simpler to de-\nscribe. T o search a graph, invoke a recursive \nmethod that visits vertices. T o visit a vertex:\n\u25a0 Mark it as having been visited.\n\u25a0 Visit (recursively) all the vertices that \nare adjacent to it and that have not \nyet been marked. \nThis method is called depth-\ufb01rst search\n(DFS). An implementation of our Search\nAPI using this method is shown at right. \nIt maintains an array of boolean val-\nues to mark all of the vertices that are \nconnected to the source. The recursive \nmethod marks the given vertex and calls \nitself for any unmarked vertices on its \nadjacency list. If the graph is connect-\ned, every adjacency-list entry is checked.\nProposition A.   DFS marks all the vertices connected to a \ngiven source in time proportional to the sum of their degrees.\nProof: First, we prove that the algorithm marks all the verti-\nces connected to the source s (and no others). Every marked \nvertex is connected to s, since the algorithm \ufb01nds vertices \nonly by following edges. Now, suppose that some unmarked \nvertex w is connected to s. Since s itself is marked, any path \nfrom s to w must have at least one edge from the set of marked \nvertices to the set of unmarked vertices, say v-x. But the al -\ngorithm would have discovered x after marking v, so no such \nedge can exist, a contradiction. The time bound follows be-\ncause marking ensures that each vertex is visited once (taking \ntime proportional to its degree to check marks).\npublic class  DepthFirstSearch \n{\n   private boolean[] marked;\n   private int count;\n  public DepthFirstSearch(Graph G, int s)\n   {  \n      marked = new boolean[G.V()];\n      dfs(G, s);\n   }\n   private void dfs(Graph G, ", "start": 543, "end": 543}, "789": {"text": "private boolean[] marked;\n   private int count;\n  public DepthFirstSearch(Graph G, int s)\n   {  \n      marked = new boolean[G.V()];\n      dfs(G, s);\n   }\n   private void dfs(Graph G, int v)\n   {\n      marked[v] = true;\n      count++;\n      for (int w : G.adj(v))\n         if (!marked[w]) dfs(G, w);\n   }\n   public boolean marked(int w)\n   {  return marked[w];  }\n   public int count()\n   {  return count;  }\n}\n D e p t h - f i r s t  s e a r c h\nset of\nunmarked\nvertices\nno such edge\ncan exist\nsource\nv\ns\nset of marked\nvertices\nw\nx\n5314.1 \u25a0 Undirected Graphs\n  \n  \n \nOne-way passages. The method call\u2013return mechanism in the program corresponds \nto the string in the maze: when we have processed all the edges incident to a vertex \n(explored all the passages leaving an intersection), we \u201creturn\u201d (in both senses of the \nword). To draw a proper correspondence with Tremaux exploration of a maze, we need \nto imagine a maze constructed entirely of one-way passages (one in each direction).\nIn the same way that we encounter each passage \nin the maze twice (once in each direction), we \nencounter each edge in the graph twice (once at \neach of its vertices). In Tremaux exploration, we \neither explore a passage for the \ufb01rst time or re-\nturn along it from a marked vertex; in DFS of \nan undirected graph, we either do a recursive \ncall when we encounter an edge v-w (if w is not \nmarked) or skip the edge (if w is marked). The \nsecond time that we encounter the edge, in the \nopposite orientation w-v, we always ignore it, ", "start": 543, "end": 544}, "790": {"text": "encounter an edge v-w (if w is not \nmarked) or skip the edge (if w is marked). The \nsecond time that we encounter the edge, in the \nopposite orientation w-v, we always ignore it, \nbecause the destination vertex v has certainly al-\nready been visited (the \ufb01rst time that we encoun-\ntered the edge).\nTrac ing DFS. As usual, one good way to under-\nstand an algorithm is to trace its behavior on a \nsmall example. This is particularly true of depth-\n\ufb01rst search. The \ufb01rst thing to bear in mind when \ndoing a trace is that the order in which edges \nare examined and vertices visited depends upon \nthe representation, not just the graph or the al -\ngorithm. Since DFS only examines vertices con-\nnected to the source, we use the small connected \ngraph depicted at left as an example for traces. \nIn this example, vertex 2 is the \ufb01rst vertex visited \nafter 0 because it happens to be \ufb01rst on 0\u2019s adjacency list. The second thing to bear in \nmind when doing a trace is that, as mentioned above, DFS traverses each edge in the \ngraph twice, always \ufb01nding a marked vertex the second time. One effect of this obser -\nvation is that tracing a DFS takes twice as long as you might think! Our example graph \nhas only eight edges, but we need to trace the action of the algorithm on the 16 entries \non the adjacency lists.\nadj[]\n0\n1\n2\n3\n4\n5\n2 1 5\n0 2\n5 4 2\n3 2\n3 0\n0 1 3 4\n6 \n8\n0 5\n2 4\n2 3\n1 2\n0 1\n3 4\n3 5\n0 2\ntinyCG.txt ", "start": 544, "end": 544}, "791": {"text": "0\n0 1 3 4\n6 \n8\n0 5\n2 4\n2 3\n1 2\n0 1\n3 4\n3 5\n0 2\ntinyCG.txt standard drawing\ndrawing with both edges\nadjacency lists\nA connected undirected graph\nV\nE\n532 CHAPTER 4 \u25a0 Graphs\n   D e t a i l e d  t r a c e  o f  d e p t h - \ufb01 r s t  s e a r c h .  The \ufb01gure at right shows the contents of the data \nstructures just after each vertex is marked for our small example, with source 0. The \nsearch begins when the constructor calls the \nrecursive dfs() to mark and visit vertex 0 \nand proceeds as follows:\n\u25a0 Since 2 is \ufb01rst on 0\u2019s adjacency list \nand is unmarked, dfs() recursively \ncalls itself to mark and visit 2 (in ef-\nfect, the system puts 0 and the current \nposition on 0\u2019s adjacency list on a \nstack). \n\u25a0 Now, 0 is \ufb01rst on 2\u2019s adjacency list \nand is marked, so dfs() skips it. \nThen, since 1 is next on 2\u2019s adjacency \nlist and is unmarked, dfs() recur-\nsively calls itself to mark and visit 1. \n\u25a0 Visiting 1 is different: since both ver-\ntices on its list (0 and 2) are already \nmarked, no recursive calls are needed, \nand dfs() returns from the recursive \ncall  dfs(1). The next edge examined \nis 2-3 (since 3 is the vertex after 1 on \n2\u2019s adjacency list), so dfs() recur-\nsively calls itself to mark and visit 3. \n\u25a0 Ver tex  5 is \ufb01rst ", "start": 544, "end": 545}, "792": {"text": "2-3 (since 3 is the vertex after 1 on \n2\u2019s adjacency list), so dfs() recur-\nsively calls itself to mark and visit 3. \n\u25a0 Ver tex  5 is \ufb01rst on 3\u2019s adjacency list \nand is unmarked, so dfs() recursively \ncalls itself to mark and visit 5. \n\u25a0 Both vertices on 5\u2019s list (3 and 0) are \nalready marked, so no recursive calls \nare needed, \n\u25a0 Ver tex  4 is next on 3\u2019s adjacency list \nand is unmarked, so dfs() recursively \ncalls itself to mark and visit 4, the last \nvertex to be marked.\n\u25a0 After 4 is marked, dfs() needs to \ncheck the vertices on its list, then the \nremaining vertices on 3\u2019s list, then 2\u2019s list, then 0\u2019s list, but no more recursive \ncalls happen because all vertices are marked.\nTrace of depth-first search to find vertices connected to 0\nmarked[]\n  0  T    \n  1   \n  2  \n  3  \n  4  \n  5  \n  0  T\n  1   \n  2  T\n  3  \n  4  \n  5 \n  0  T\n  1  T\n  2  T\n  3  \n  4  \n  5 \n  0  T\n  1  T\n  2  T\n  3  T\n  4  \n  5 \n  0  T\n  1  T\n  2  T\n 3  T\n  4  \n  5  T\n  0  T\n  1  T\n  2  T\n 3  T\n  4  T\n5  T\ndfs(0)\n ", "start": 545, "end": 545}, "793": {"text": "4  \n  5  T\n  0  T\n  1  T\n  2  T\n 3  T\n  4  T\n5  T\ndfs(0)\n dfs(2)\n   check 0\n   dfs(1)\n     check 0\n     check 2\n   1 done\n   dfs(3)\n     dfs(5)\n       check 3\n       check 0\n     5 done\n     dfs(4)\n       check 3\n       check 2\n     4 done\n     check 2\n   3 done\n   check 4\n 2 done\n check 1\n check 5\n0 done \n0  2 1 5 \n1  0 2 \n2  0 1 3 4 \n3  5 4 2 \n4  3 2 \n5  3 0\n0  2 1 5 \n1  0 2 \n2  0 1 3 4 \n3  5 4 2 \n4  3 2 \n5  3 0\n0  2 1 5 \n1  0 2 \n2  0 1 3 4 \n3  5 4 2 \n4  3 2 \n5  3 0\n0  2 1 5 \n1  0 2 \n2  0 1 3 4 \n3  5 4 2 \n4  3 2 \n5  3 0\n0  2 1 5 \n1  0 2 \n2  0 1 3 4 \n3  5 4 2 \n4  3 2 \n5  3 0\n ", "start": 545, "end": 545}, "794": {"text": "1 5 \n1  0 2 \n2  0 1 3 4 \n3  5 4 2 \n4  3 2 \n5  3 0\n adj[]\n0  2 1 5 \n1  0 2 \n2  0 1 3 4 \n3  5 4 2 \n4  3 2 \n5  3 0\n5334.1 \u25a0 Undirected Graphs\n  \n \n \n  \n  \nThis basic recursive scheme is just a start\u2014depth-\ufb01rst search is effective for many \ngraph-processing tasks. For example, in this section, we consider the use of depth-\ufb01rst \nsearch to address a problem that we \ufb01rst posed in Chapter 1:\n   C o n n e c t i v i t y .  Given a graph, support queries of the form Are two given vertices \nconnected ? and How many connected components does the graph have ?\nThis problem is easily solved within our standard graph-processing design pattern, and \nwe will compare and contrast this solution with the union-\ufb01nd algorithms that we \nconsidered in Section 1.5. \nThe question \u201cAre two given vertices connected?\u201d is equivalent to the question \u201cIs \nthere a path connecting two given vertices?\u201d and might be named the path detection \nproblem. However, the union-\ufb01nd data structures that we considered in Section 1.5 do \nnot address the problems of \ufb01nding such a path. Depth-\ufb01rst search is the \ufb01rst of several \napproaches that we consider to solve this problem, as well:\n  S i n g l e - s o u r c e  p a t h s .  Given a graph and a source vertex s, support queries of the \nform Is there a path from s to a given target vertex v? If so, ", "start": 545, "end": 546}, "795": {"text": "e - s o u r c e  p a t h s .  Given a graph and a source vertex s, support queries of the \nform Is there a path from s to a given target vertex v? If so, \ufb01nd such a path.\nDFS is deceptively simple because it is based on a familiar concept and is so easy to \nimplement; in fact, it is a subtle and powerful algorithm that researchers have learned \nto put to use to solve numerous dif\ufb01cult problems. These two are the \ufb01rst of several that \nwe will consider.\n534 CHAPTER 4 \u25a0 Graphs\n  F i n d i n g  p a t h s  The single-source paths problem is fundamental to graph process-\ning. In accordance with our standard design pattern, we use the following API:\npublic class    Paths \nPaths(Graph G, int s) find paths in G from source s \nboolean hasPathTo(int v) is there a path from s to v?\nIterable<Integer> pathTo(int v) path from s to v; null if no such path\n A P I  f o r  p a t h s  i m p l e m e n t a t i o n s\n \n \nThe constructor takes a source vertex s as \nargument and computes paths from s to \neach vertex connected to s. After creating \na Paths object for a source s, the client can \nuse the instance method pathTo() to iter-\nate through the vertices on a path from s to \nany vertex connected to s. For the moment, \nwe accept any path; later, we shall develop \nimplementations that \ufb01nd paths having \ncertain properties. The test client at right \ntakes a graph from the input stream and a \nsource from the command line and prints \na path from the source to each vertex con-\nnected to it.\nImplementation. Algorithm 4.1 on page 536 ", "start": 546, "end": 547}, "796": {"text": "at right \ntakes a graph from the input stream and a \nsource from the command line and prints \na path from the source to each vertex con-\nnected to it.\nImplementation. Algorithm 4.1 on page 536 is a DFS-based implementation of Paths\nthat extends the DepthFirstSearch warmup on page 531 by adding as  an instance vari-\nable an array edgeTo[] of int values that serves the purpose of the ball of string in \nTremaux explor ation: it g ives a way to \ufb01nd a path back to s for every vertex connected \nto s. Instead of just keeping track of the path from the current vertex back to the start, \nwe remember a path from each vertex to the \nstart. T o accomplish this, we remember the edge \nv-w that takes us to each vertex w for the \ufb01rst \ntime, by setting edgeTo[w] to v. In other words, \nv-w is the last edge on the known path from s\nto w. The result of the search is a tree rooted at \nthe source; edgeTo[] is a  parent-link represen-\ntation of that tree. A small example is drawn to \npublic static void main(String[] args) \n{\n   Graph G = new Graph(new In(args[0]));\n   int s = Integer.parseInt(args[1]);\n   Paths search = new Paths(G, s);\n   for (int v = 0; v < G.V(); v++)\n   {\n      StdOut.print(s + \" to \" + v + \": \");\n      if (search.hasPathTo(v))\n         for (int x : search.pathTo(v))\n            if (x == s) StdOut.print(x);\n            else StdOut.print(\"-\" + x);\n      StdOut.println();\n   } \n}\nTest client for paths implementations\n% java Paths tinyCG.txt 0 \n0 to 0: 0 \n0 ", "start": 547, "end": 547}, "797": {"text": "StdOut.print(x);\n            else StdOut.print(\"-\" + x);\n      StdOut.println();\n   } \n}\nTest client for paths implementations\n% java Paths tinyCG.txt 0 \n0 to 0: 0 \n0 to 1: 0-2-1 \n0 to 2: 0-2 \n0 to 3: 0-2-3 \n0 to 4: 0-2-3-4 \n0 to 5: 0-2-3-5\n5354.1 \u25a0 Undirected Graphs\n ALGORITHM 4.1   Depth-first search to find paths in a graph\npublic class  DepthFirstPaths \n{\n   private boolean[] marked; // Has dfs() been called for this vertex?\n   private int[] edgeTo;     // last vertex on known path to this vertex\n   private final int s;      // source\n   public DepthFirstPaths(Graph G, int s)\n   {\n      marked = new boolean[G.V()];\n      edgeTo = new int[G.V()];\n      this.s = s;\n      dfs(G, s);\n   }\n   private void dfs(Graph G, int v)\n   {\n      marked[v] = true;\n      for (int w : G.adj(v))\n         if (!marked[w])\n         {\n            edgeTo[w] = v;\n            dfs(G, w);\n         }\n   }\n   public boolean hasPathTo(int v)\n   {  return marked[v];  }\n   public Iterable<Integer> pathTo(int v)\n   {\n      if (!hasPathTo(v)) return null;\n      Stack<Integer> path = new Stack<Integer>();\n      for (int x = v; x != s; x = edgeTo[x])\n         path.push(x);\n      path.push(s);\n      return path;\n   } \n}\n  This Graph client uses depth-\ufb01rst search to \ufb01nd paths to all the vertices in a graph that are ", "start": 547, "end": 548}, "798": {"text": "edgeTo[x])\n         path.push(x);\n      path.push(s);\n      return path;\n   } \n}\n  This Graph client uses depth-\ufb01rst search to \ufb01nd paths to all the vertices in a graph that are connected \nto a given start vertex s. Code from DepthFirstSearch (page 531) is printed in gray. T o save known \npaths to each vertex, this code maintains a vertex-indexed array edgeTo[] such that edgeTo[w] = v\nmeans that v-w was the edge used to access w for the \ufb01rst time. The edgeTo[] array is a parent-link \nrepresentation of a tree rooted at s that contains all the vertices connected to s.\nTrace of  pathTo(5) computation\nedgeTo[]\n  0    \n  1  2\n  2  0\n  3  2\n  4  3\n  5  3\n  \n5   5\n3   3 5\n2   2 3 5\n0   0 2 3 5\nx  path\n536 CHAPTER 4 \u25a0 Graphs the right of the code  in Algorithm 4.1. To recover \nthe path from s to any vertex v, the pathTo() method \nin Algorithm 4.1 uses a variable x to travel up the \ntree, setting x to edgeTo[x], just as we did for union-\n\ufb01nd in Section 1.5, putting each vertex encountered \nonto a stack until reaching s. Returning the stack to \nthe client as an Iterable enables the client to follow \nthe path from s to v. \nDetailed trace. The \ufb01gure at right shows the con-\ntents of edgeTo[] just after each vertex is marked \nfor our example, with source 0. The contents of \nmarked[] and adj[] are the same as in the ", "start": 548, "end": 549}, "799": {"text": "\ufb01gure at right shows the con-\ntents of edgeTo[] just after each vertex is marked \nfor our example, with source 0. The contents of \nmarked[] and adj[] are the same as in the trace of \nDepthFirstSearch on page 533, as is the detailed de-\nscription of the recursive calls and the edges checked, \nso these aspects of the trace are omitted. The depth-\n\ufb01rst search adds the edges 0-2, 2-1, 2-3, 3-5, and \n3-4 to edgeTo[], in that order. These edges form a \ntree rooted at the source and provide the information \nneeded for pathTo() to provide for the client the path \nfrom 0 to 1, 2, 3, 4, or 5, as just described.\nThe constructor in DepthFirstPaths differs only \nin a few assignment statements from the constructor \nin DepthFirstSearch, so Proposition A on page 531 \napplies. In addition, we have:\n Proposition A (continued).  DFS allows us to pro-\nvide clients with a path from a given source to any \nmarked vertex in time proportional its length. \nProof: By induction on the number of verti -\nces visited, it follows that the edgeTo[] array in \nDepthFirstPaths represents a tree rooted at the \nsource. The pathTo() method builds the path in \ntime proportional to its length. \nTrace of depth-first search to find all paths from 0\nedgeTo[]\n  0      \n  1   \n  2  \n  3  \n  4  \n  5  \n  0  \n  1   \n  2  0\n  3  \n  4  \n  5  \n  0  \n  1  2\n  2  0\n  3  \n  4  \n  5 ", "start": 549, "end": 549}, "800": {"text": "2  0\n  3  \n  4  \n  5  \n  0  \n  1  2\n  2  0\n  3  \n  4  \n  5  \n  0  \n  1  2\n  2  0\n  3  2\n  4  \n  5  \n  0  \n  1  2\n  2  0\n  3  2\n  4  \n  5  3\n  0  \n  1  2\n  2  0\n  3  2\n  4  3\n 5  3\ndfs(0)\n dfs(2)\n   check 0\n   dfs(1)\n     check 0\n     check 2\n   1 done\n   dfs(3)\n     dfs(5)\n       check 3\n       check 0\n     5 done\n     dfs(4)\n       check 3\n       check 2\n     4 done\n     check 2\n   3 done\n   check 4\n 2 done\n check 1\n check 5\n0 done \n  0  \n  1  2\n  2  0\n  3  2\n  4  3\n 5  3\n5374.1 \u25a0 Undirected Graphs\n  \n   B r e a d t h - \ufb01 r s t  s e a r c h  The paths discovered by depth-\ufb01rst search depend not just \non the graph, but also on the representation and the nature of the recursion. Naturally, \nwe are often interested in solving the following problem:\n  S i n g l e - s o u r c e  s h o r t e s t  p ", "start": 549, "end": 550}, "801": {"text": "the representation and the nature of the recursion. Naturally, \nwe are often interested in solving the following problem:\n  S i n g l e - s o u r c e  s h o r t e s t  p a t h s .  Given a graph and a source vertex s, support que-\nries of the form Is there a path from s to a given target vertex v? If so, \ufb01nd a shortest\nsuch path (one with a minimal number of edges).\nThe classical method for accomplishing this task, called breadth-\ufb01rst search (BFS ), is \nalso the basis of numerous algorithms for processing graphs, so we consider it in detail \nin this section. DFS offers us little assistance in solving this problem, because the order \nin which it takes us through the graph has no relationship to the goal of \ufb01nd-\ning shortest paths. In contrast, BFS is based on this goal. T o \ufb01nd a shortest path \nfrom s to v, we start at s and check for v among all the vertices that we can \nreach by following one edge, then we check for v among all the vertices that we \ncan reach from s by following two edges, and so forth. DFS is analogous to one \nperson exploring a maze. BFS is analogous to a group of searchers exploring by \nfanning out in all directions, each unrolling his or her own ball of string. When \nmore than one passage needs to be explored, we imagine that the searchers \nsplit up to expore all of them; when two groups of searchers meet up, they join \nforces (using the ball of string held by the one getting there \ufb01rst).\nIn a program, when we come to a point during a graph search where we \nhave more than one edge to traverse, we choose one and save the others to be \nexplored later. In DFS, we use a pushdown stack (that ", "start": 550, "end": 550}, "802": {"text": "program, when we come to a point during a graph search where we \nhave more than one edge to traverse, we choose one and save the others to be \nexplored later. In DFS, we use a pushdown stack (that is managed by the sys -\ntem to support the recursive search method) for this purpose. Using the LIFO \nrule that characterizes the pushdown stack corresponds to exploring passages \nthat are close by in a maze. We choose, of the passages yet to be explored, \nthe one that was most recently encountered. In BFS, we want to explore the \nvertices in order of their distance from the source. It turns out that this order is easily \narranged: use a (FIFO) queue instead of a (LIFO) stack. We choose, of the passages yet \nto be explored, the one that was least recently encountered.\nImplementation. Algorithm 4.2 on page 540 is an implementation of BFS. It is based \non maintaining a queue of all vertices that have been marked but whose adjacency lists \nhave not been checked. We put the source vertex on the queue, then perform the fol-\nlowing steps until the queue is empty: \n\u25a0 Take the next ver tex v from the queue and mark it. \n\u25a0 Put onto the queue all unmarked vertices that are adjacent to v.\nBreadth-first\nmaze exploration\n538 CHAPTER 4 \u25a0 Graphs\n The bfs() method in Algorithm 4.2 is not re-\ncursive. Instead of the implicit stack provided by \nrecursion, it uses an explicit queue. The product of \nthe search, as for DFS, is an array edgeTo[], a   par-\nent-link representation of a tree rooted at s, which \nde\ufb01nes the shortest paths from s to every \nvertex that is connected to s. The paths \ncan be constructed for the client using the \nsame pathTo() implementation that we \nused for ", "start": 550, "end": 551}, "803": {"text": "at s, which \nde\ufb01nes the shortest paths from s to every \nvertex that is connected to s. The paths \ncan be constructed for the client using the \nsame pathTo() implementation that we \nused for DFS in Algorithm 4.1.\nThe \ufb01gure at right shows the step-by-\nstep development of BFS on our sample \ngraph, showing the contents of the data \nstructures at the beginning of each it-\neration of the loop. V ertex 0 is put on the \nqueue, then the loop completes the search \nas follows:\n\u25a0 Removes 0 from the queue and puts \nits adjacent vertices 2, 1, and 5 on \nthe queue, marking each and setting \nthe edgeTo[] entry for each to 0.\n\u25a0 Removes 2 from the queue, checks \nits adjacent vertices 0 and 1, which \nare marked, and puts its adjacent \nvertices 3 and 4 on the queue, mark-\ning each and setting the edgeTo[]\nentry for each to 2.\n\u25a0 Removes 1 from the queue and \nchecks its adjacent vertices 0 and 2, \nwhich are marked.\n\u25a0 Removes 5 from the queue and \nchecks its adjacent vertices 3 and 0, \nwhich are marked.\n\u25a0 Removes 3 from the queue and \nchecks its adjacent vertices 5, 4, \nand 2, which are marked.\n\u25a0 Removes 4 from the queue and \nchecks its adjacent vertices 3 and 2, which are marked.\nTrace of breadth-first search to find all paths from 0\nmarked[]\n  0  T    \n  1   \n  2  \n  3  \n  4  \n  5  \n  0  T\n  1  T \n  2  T\n  3  \n  4  \n  5 T\n  0  T\n  1  T\n ", "start": 551, "end": 551}, "804": {"text": "\n  5  \n  0  T\n  1  T \n  2  T\n  3  \n  4  \n  5 T\n  0  T\n  1  T\n  2  T\n  3  T\n  4  T\n  5 T\n0    \n  \n1\n5\n3\n4 \n2\n1\n5 \n5\n3\n4 \n3\n4 \n4 \nedgeTo[]\n  0     \n  1   \n  2  \n  3  \n  4  \n  5  \n  0  \n  1  0 \n  2  0\n  3  \n  4  \n  5 0\n  0  \n  1  0\n  2  0\n  3  2\n  4  2\n  5 0\n0  2 1 5 \n1  0 2 \n2  0 1 3 4 \n3  5 4 2 \n4  3 2 \n5  3 0\n0  2 1 5 \n1  0 2 \n2  0 1 3 4 \n3  5 4 2 \n4  3 2 \n5  3 0\n  0  T\n  1  T\n  2  T\n  3  T\n  4  T\n  5 T\n  0  \n  1  0\n  2  0\n  3  2\n  4  2\n  5 0\n0  2 1 5 \n1  0 2 \n2  0 1 3 4 \n3  5 ", "start": 551, "end": 551}, "805": {"text": "2\n  4  2\n  5 0\n0  2 1 5 \n1  0 2 \n2  0 1 3 4 \n3  5 4 2 \n4  3 2 \n5  3 0\n adj[]\n0  2 1 5 \n1  0 2 \n2  0 1 3 4 \n3  5 4 2 \n4  3 2 \n5  3 0\n  0  T\n  1  T\n  2  T\n  3  T\n  4  T\n  5 T\n  0  \n  1  0\n  2  0\n  3  2\n  4  2\n  5 0\n0  2 1 5 \n1  0 2 \n2  0 1 3 4 \n3  5 4 2 \n4  3 2 \n5  3 0\n  0  T\n  1  T\n  2  T\n  3  T\n  4  T\n  5 T\n  0  \n  1  0\n  2  0\n  3  2\n  4  2\n  5 0\n0  2 1 5 \n1  0 2 \n2  0 1 3 4 \n3  5 4 2 \n4  3 2 \n5  3 0\nqueue\nOutcome of breadth-first search to find all paths from 0\nedgeTo[]\n  0    \n  1  0\n  2  0\n ", "start": 551, "end": 551}, "806": {"text": "2 \n5  3 0\nqueue\nOutcome of breadth-first search to find all paths from 0\nedgeTo[]\n  0    \n  1  0\n  2  0\n  3  2\n  4  2\n  5  0\n  \n5394.1 \u25a0 Undirected Graphs\n ALGORITHM 4.2   Breadth-first search to find paths in a graph\npublic class  BreadthFirstPaths\n{\n   private boolean[] marked; // Is a shortest path to this vertex known?\n   private int[] edgeTo;     // last vertex on known path to this vertex\n   private final int s;      // source\n   public BreadthFirstPaths(Graph G, int s)\n   {\n      marked = new boolean[G.V()];\n      edgeTo = new int[G.V()];\n      this.s = s;\n      bfs(G, s);\n   }\n   private void bfs(Graph G, int s)\n   {\n      Queue<Integer> queue = new Queue<Integer>();\n      marked[s] = true;          // Mark the source\n      queue.enqueue(s);          //   and put it on the queue.\n      while (!q.isEmpty())\n      {\n         int v = queue.dequeue(); // Remove next vertex from the queue.\n         for (int w : G.adj(v))\n            if (!marked[w])       // For every unmarked adjacent vertex,\n            {\n               edgeTo[w] = v;     //   save last edge on a shortest path,\n               marked[w] = true;  //   mark it because path is known,\n               queue.enqueue(w);  //   and add it to the queue.\n            }\n      }\n   }\n   public boolean hasPathTo(int v)\n   {  return marked[v];  }\n   public Iterable<Integer> pathTo(int v)\n   // Same as for DFS (see page 536).\n}\nThis Graph client uses breadth-\ufb01rst ", "start": 551, "end": 552}, "807": {"text": "hasPathTo(int v)\n   {  return marked[v];  }\n   public Iterable<Integer> pathTo(int v)\n   // Same as for DFS (see page 536).\n}\nThis Graph client uses breadth-\ufb01rst search to \ufb01nd paths in a graph with the fewest number of edges \nfrom the source s given in the constructor. The bfs() method marks all vertices connected to s, so \nclients can use hasPathTo() to determine whether a given vertex v is connected to s and pathTo() to \nget a path from s to v with the property that no other such path from s to v has fewer edges. \n540 CHAPTER 4 \u25a0 Graphs For this example, the edgeTo[] array is complete after the second step. As with DFS, \nonce all vertices have been marked, the rest of the computation is just checking edges to \nvertices that have already been marked.\nProposition B.  For any vertex v reachable from s, BFS computes a shortest path \nfrom s to v (no path from s to v has fewer edges).\nProof: It is easy to prove by induction that the queue always consists of zero or \nmore vertices of distance k from the source, followed by zero or more vertices of \ndistance k/H110011 from the source, for some integer k, starting with k equal to 0. This \nproperty implies, in particular, that vertices enter and leave the queue in order of \ntheir distance from s. When a vertex v enters the queue, no shorter path to v will \nbe found before it comes off the queue, and no path to v that is discovered after it \ncomes off the queue can be shorter than v\u2019s tree path length. \nProposition B (continued).   BFS takes time proportional to V/H11001E in the worst case.\nProof: As for Proposition A (page 531), BFS marks all the vertices connected to s in \ntime proportional to the ", "start": 552, "end": 553}, "808": {"text": "\nProposition B (continued).   BFS takes time proportional to V/H11001E in the worst case.\nProof: As for Proposition A (page 531), BFS marks all the vertices connected to s in \ntime proportional to the sum of their degrees. If the graph is connected, this sum is \nthe sum of the degrees of all the vertices, or 2E. \nNote that we can also use BFS to implement the Search API that we implemented with \nDFS, since the solution depends on only the ability of the search to examine every ver-\ntex and edge connected to the source. \nAs implied at the outset, DFS and BFS are the \ufb01rst of several instances that we will \nexamine of a general approach to searching graphs. We put the source vertex on the \ndata structure, then perform the following steps until the data structure is empty: \n\u25a0 Take the next ver tex v from the data structure and mark it. \n\u25a0 Put onto the data structure all unmarked vertices that are adjacent to v.\nThe algorithms differ only in the rule used to \ntake the next vertex from the data structure \n(least recently added for BFS, most recently add-\ned for DFS). This difference leads to completely \ndifferent views of the graph, even though all the \nvertices and edges connected to the source are \nexamined no matter what rule is used.\n% java BreadthFirstPaths tinyCG.txt 0 \n0 to 0: 0 \n0 to 1: 0-1 \n0 to 2: 0-2 \n0 to 3: 0-2-3 \n0 to 4: 0-2-4 \n0 to 5: 0-5\n5414.1 \u25a0 Undirected Graphs\n  \nThe diagrams on either side  of \nthis page, which show the progress of \nDFS and BFS for our sample graph \nmediumG.txt, make plain the differ -\nences between ", "start": 553, "end": 554}, "809": {"text": "0-5\n5414.1 \u25a0 Undirected Graphs\n  \nThe diagrams on either side  of \nthis page, which show the progress of \nDFS and BFS for our sample graph \nmediumG.txt, make plain the differ -\nences between the paths that are dis-\ncovered by the two approaches.DFS \nwends its way through the graph, stor-\ning on the stack the points where other \npaths branch off; BFS sweeps through \nthe graph, using a queue to remember \nthe frontier of visited places. DFS ex-\nplores the graph by looking for new \nvertices far away from the start point, \ntaking closer vertices only when dead \nends are encountered; BFS completely \ncovers the area close to the starting \npoint, moving farther away only when \neverything nearby has been examined. \nDFS paths tend to be long and wind-\ning; BFS paths are short and direct. \nDepending upon the application, one \nproperty or the other may be desirable \n(or properties of paths may be imma-\nterial). In Section 4.4, we will be con-\nsidering other implementations of the \nPaths API that \ufb01nd paths having other \nspeci\ufb01ed properties.\nBFS for shortest paths (250 vertices)\n20%\n40%\n60%\n80%\n100%\nDFS for paths (250 vertices)\n20%\n40%\n60%\n80%\n100%\n542 CHAPTER 4 \u25a0 Graphs\n  \n     C o n n e c t e d  c o m p o n e n t s  Our next direct application of depth-\ufb01rst search is to \n\ufb01nd the connected components of a graph. Recall from Section 1.5 (see page 216) that \u201cis \nconnected to\u201d is an  equivalence relation that divides the vertices into equivalence classes \n(the connected components). For this common graph-processing task, we de\ufb01ne the \nfollowing API:\npublic class    CC \nCC(Graph ", "start": 554, "end": 555}, "810": {"text": "\u201cis \nconnected to\u201d is an  equivalence relation that divides the vertices into equivalence classes \n(the connected components). For this common graph-processing task, we de\ufb01ne the \nfollowing API:\npublic class    CC \nCC(Graph G) preprocessing constructor\nboolean connected(int v, int w) are v and w connected?\nint count() number of connected components\nint id(int v) component identifier for v\n( between 0 and count()-1 )\n A P I  f o r  c o n n e c t e d  c o m p o n e n t s\nThe id() method is for client use in indexing an array by component, as in the test cli-\nent below, which reads a graph and then prints its number of connected components \nand then the vertices in each component, one component per line. T o do so, it builds an \narray of Bag objects, then uses each vertex\u2019s component identi\ufb01er as an index into this \narray, to add the vertex to the appropriate Bag. This client is a model for the typical sit-\nuation where we want to independently \nprocess connected components. \nImplementation. The implementation \nCC (Algorithm 4.3 on the next page) \nuses our marked[] array to \ufb01nd a vertex \nto serve as the starting point for a depth-\n\ufb01rst search in each component. The \ufb01rst \ncall to the recursive DFS is for vertex 0\u2014\nit marks all vertices connected to 0. Then \nthe for loop in the constructor looks for \nan unmarked vertex and calls the recur -\nsive dfs() to mark all vertices connected \nto that vertex. Moreover, it maintains a \nvertex-indexed array id[] that associ-\nates the same int value to every vertex \nin each component. This array makes \nthe implementation of connected()\nsimple, in precisely the same manner as \npublic static void main(String[] args) \n{\n ", "start": 555, "end": 555}, "811": {"text": "array id[] that associ-\nates the same int value to every vertex \nin each component. This array makes \nthe implementation of connected()\nsimple, in precisely the same manner as \npublic static void main(String[] args) \n{\n   Graph G = new Graph(new In(args[0]));\n   CC cc = new CC(G);\n   int M = cc.count();\n   StdOut.println(M + \" components\");\n   Bag<Integer>[] components;\n   components = (Bag<Integer>[]) new Bag[M];\n   for (int i = 0; i < M; i++)\n      components[i] = new Bag<Integer>();\n   for (int v = 0; v < G.V(); v++)\n      components[cc.id(v)].add(v);\n   for (int i = 0; i < M; i++)\n   {\n      for (int v: components[i])\n         StdOut.print(v + \" \");\n      StdOut.println();\n   } \n}\n T e s t  c l i e n t  f o r  c o n n e c t e d  c o m p o n e n t s  A P I\n5434.1 \u25a0 Undirected Graphs\n ALGORITHM 4.3   Depth-first search to find connected components in a graph\npublic class  CC \n{\n   private boolean[] marked;\n   private int[] id;\n   private int count;\n   public CC(Graph G)\n   {\n      marked = new boolean[G.V()];\n      id = new int[G.V()];\n      for (int s = 0; s < G.V(); s++)\n         if (!marked[s])\n         {  \n             dfs(G, s);\n             count++;\n         }\n   }\n   private void dfs(Graph G, int v)\n   {\n      marked[v] = true;\n      id[v] = count;\n      for (int w : G.adj(v))\n         if (!marked[w])\n             dfs(G, w);\n   }\n   public ", "start": 555, "end": 556}, "812": {"text": "G, int v)\n   {\n      marked[v] = true;\n      id[v] = count;\n      for (int w : G.adj(v))\n         if (!marked[w])\n             dfs(G, w);\n   }\n   public boolean connected(int v, int w)\n   {  return id[v] == id[w];  }\n   public int id(int v)\n   {  return id[v];  }\n   public int count()\n   {  return count;  }\n}\n \n \nThis Graph client provides its clients with the ability to independently process a graph\u2019s connected \ncomponents.  Code from DepthFirstSearch (page 531) is left in gray.  The computation is based on \na vertex-indexed array id[] such that id[v] is set to i if v is in the ith connected component pro-\ncessed. The constructor \ufb01nds an unmarked vertex and calls the recursive dfs() to mark and identify \nall the vertices connected to it, continuing until all vertices have been marked and identi\ufb01ed.  Imple-\nmentations of the instance methods connected(), id(), and count() are immediate.\n% more tinyG.txt \n13 vertices, 13 edges \n0: 6 2 1 5 \n1: 0 \n2: 0 \n3: 5 4 \n4: 5 6 3 \n5: 3 4 0 \n6: 0 4 \n7: 8 \n8: 7 \n9: 11 10 12 \n10: 9 \n11: 9 12 \n12: 11 9 \n% java CC tinyG.txt \n3 components \n6 5 4 3 2 1 0 \n8 7 \n12 11 10 9\n544 CHAPTER 4 \u25a0 Graphs                 count         marked[]                      id[]\n                      0 1 2 3 4 5 ", "start": 556, "end": 557}, "813": {"text": "2 1 0 \n8 7 \n12 11 10 9\n544 CHAPTER 4 \u25a0 Graphs                 count         marked[]                      id[]\n                      0 1 2 3 4 5 6 7 8 9 10 11 12    0 1 2 3 4 5 6 7 8 9 10 11 12\ndfs(0)             0   T                           0                         \n dfs(6)           0   T           T               0           0             \n   check 0\n   dfs(4)         0   T       T   T               0       0   0             \n     dfs(5)       0   T       T T T               0       0 0 0             \n       dfs(3)     0   T     T T T T               0     0 0 0 0             \n         check 5\n         check 4\n       3 done\n       check 4\n       check 0\n     5 done\n     check 6\n     check 3\n   4 done\n 6 done\n dfs(2)           0   T   T T T T T               0   0 0 0 0 0             \n   check 0\n 2 done\n dfs(1)           0   T T T T T T T               0 0 0 0 0 0 0             \n   check 0\n 1 done\n check 5\n0 done\ndfs(7)             1   T T T T T T T T             0 0 0 0 0 0 0 1           \n dfs(8)           1   T T T T T T T T T           0 0 0 0 0 0 ", "start": 557, "end": 557}, "814": {"text": "0 0 0 0 0 0 0 1           \n dfs(8)           1   T T T T T T T T T           0 0 0 0 0 0 0 1 1         \n   check 7\n 8 done\n7 done\ndfs(9)             2   T T T T T T T T T T         0 0 0 0 0 0 0 1 1 2       \n dfs(11)          2   T T T T T T T T T T   T     0 0 0 0 0 0 0 1 1 2   2   \n   check 9\n   dfs(12)        2   T T T T T T T T T T   T T   0 0 0 0 0 0 0 1 1 2   2 2 \n     check 11\n     check 9\n   12 done\n 11 done\n dfs(10)          2   T T T T T T T T T T T T T   0 0 0 0 0 0 0 1 1 2 2 2 2 \n   check 9\n 10 done\n check 12\n9 done\nTrace of depth-first search to find connected components\ntinyG.txt\n5454.1 \u25a0 Undirected Graphs\n  \n \nconnected() in Section 1.5 (just check if identi\ufb01ers are equal). In this case, the identi-\n\ufb01er 0 is assigned to all the vertices in the \ufb01rst component processed, 1 is assigned to all \nthe vertices in the second component processed, and so forth, so that the identi\ufb01ers are \nall between 0 and count()-1, as ", "start": 557, "end": 558}, "815": {"text": "the \ufb01rst component processed, 1 is assigned to all \nthe vertices in the second component processed, and so forth, so that the identi\ufb01ers are \nall between 0 and count()-1, as speci\ufb01ed in the API. This convention enables the use \nof component-indexed arrays, as in the test client on page 543.\nProposition C.   DFS uses preprocessing time and space proportional to V/H11001E to \nsupport constant-time connectivity queries in a graph.\nProof: Immediate from the code. Each adjacency-list entry is examined exactly \nonce, and there are 2E such entries (two for each edge). Instance methods examine \nor return one or two instance variables.\n \n  U n i o n - \ufb01 n d .  How does the DFS-based solution for graph connectivity in CC compare \nwith the union-\ufb01nd approach of Chapter 1? In theory, DFS is faster than union-\ufb01nd \nbecause it provides a constant-time guarantee, which union-\ufb01nd does not; in practice, \nthis difference is negligible, and union-\ufb01nd is faster because it does not have to build \na full representation of the graph. More important, union-\ufb01nd is an online algorithm \n(we can check whether two vertices are connected in near-constant time at any point, \neven while adding edges), whereas the DFS solution must \ufb01rst preprocess the graph. \nTherefore, for example, we prefer union-\ufb01nd when determining connectivity is our \nonly task or when we have a large number of queries intermixed with edge insertions \nbut may \ufb01nd the DFS solution more appropriate for use in a graph ADT because it \nmakes ef\ufb01cient use of existing infrastructure. \nThe problems that we have solved  with DFS are fundamental. It is a simple ap-\nproach, and recursion provides us a way to reason about the ", "start": 558, "end": 558}, "816": {"text": "ADT because it \nmakes ef\ufb01cient use of existing infrastructure. \nThe problems that we have solved  with DFS are fundamental. It is a simple ap-\nproach, and recursion provides us a way to reason about the computation and develop \ncompact solutions to graph-processing problems. Two additional examples, for solving \nthe following problems, are given in the table on the facing page. \n  C y c l e  d e t e c t i o n .  Support this query: Is a given graph acylic ?\n    T w o - c o l o r a b i l i t y .  Support this query: Can the vertices of a given graph be assigned \none of two colors in such a way that no edge connects vertices of the same color  ? \nwhich is equivalent to this question: Is the graph bipartite ?\nAs usual with DFS, the simple code masks a more sophisticated computation, so study-\ning these examples, tracing their behavior on small sample graphs, and extending them \nto provide a cycle or a coloring, respectively, are worthwhile (and left for exercises).\n546 CHAPTER 4 \u25a0 Graphs\n task                     implementation\nis G  acyclic?\n(assumes\nno self-loops or \nparallel edges)\npublic class    Cycle\n{\n   private boolean[] marked;\n   private boolean hasCycle;\n   public Cycle(Graph G)\n   {\n      marked = new boolean[G.V()];\n      for (int s = 0; s < G.V(); s++)\n         if (!marked[s])\n            dfs(G, s, s);\n   }\n   private void dfs(Graph G, int v, int u)\n   {\n      marked[v] = true;\n      for (int w : G.adj(v))\n         if (!marked[w])\n             dfs(G, w, v);\n         else if (w != u) hasCycle = true;\n   }\n   \n   public boolean hasCycle()\n   {  return hasCycle; ", "start": 558, "end": 559}, "817": {"text": "G.adj(v))\n         if (!marked[w])\n             dfs(G, w, v);\n         else if (w != u) hasCycle = true;\n   }\n   \n   public boolean hasCycle()\n   {  return hasCycle;  } \n}\nis G bipartite?\n(two-colorable)\npublic class    TwoColor\n{\n   private boolean[] marked;\n   private boolean[] color;\n   private boolean isTwoColorable = true;\n   public TwoColor(Graph G)\n   {\n      marked = new boolean[G.V()];\n      color = new boolean[G.V()];\n      for (int s = 0; s < G.V(); s++)\n         if (!marked[s])\n             dfs(G, s);\n   }\n   private void dfs(Graph G, int v)\n   {      \n      marked[v] = true;\n      for (int w : G.adj(v))\n         if (!marked[w])\n         {\n             color[w] = !color[v];\n             dfs(G, w);\n         }\n         else if (color[w] == color[v]) isTwoColorable = false;\n   }\n  public boolean isBipartite()\n   {  return isTwoColorable;  }\n}\n M o r e  e x a m p l e s  o f  g r a p h  p r o c e s s i n g  w i t h  D F S  \n5474.1 \u25a0 Undirected Graphs\n  \n   S y m b o l  g r a p h s  Typical applications involve processing graphs de\ufb01ned in \ufb01les or \non web pages, using strings, not integer indices, to de\ufb01ne and refer to vertices. T o ac-\ncommodate such applications, we de\ufb01ne an input format with the following properties:\n\u25a0 Ver tex  n a m e s  a re  s t r i n g s .\n\u25a0  A speci\ufb01ed delimiter ", "start": 559, "end": 560}, "818": {"text": "such applications, we de\ufb01ne an input format with the following properties:\n\u25a0 Ver tex  n a m e s  a re  s t r i n g s .\n\u25a0  A speci\ufb01ed delimiter separates vertex names (to allow for the possibility of \nspaces in names).\n\u25a0  Each line represents a set of edges, connecting the \ufb01rst vertex name on the line \nto each of the other vertices named on the line.\n\u25a0 \n \nThe number of vertices V and the number of edges E are both implicitly de\ufb01ned.\nShown below is a small example, the \ufb01le routes.txt, which represents a model for a \nsmall transportation system where vertices are U.S. airport codes and edges connecting \nthem are airline routes between the vertices. The \ufb01le is simply a list of edges. Shown \non the facing page is a larger example, taken from the \n\ufb01le movies.txt, from the   Internet Movie Database \n(IMDB), that we introduced in Section 3.5 . Recall \nthat this \ufb01le consists of lines listing a movie name fol-\nlowed by a list of the performers in the movie. In the \ncontext of graph processing, we can view it as de\ufb01ning \na graph with movies and performers as vertices and \neach line de\ufb01ning the adjacency list of edges connect-\ning each movie to its performers. Note that the graph \nis a bipartite graph\u2014there are no edges connecting \nperformers to performers or movies to movies.\nAPI. The following API de\ufb01nes a Graph client that al-\nlows us to immediately use our graph-processing rou-\ntines for graphs de\ufb01ned by such \ufb01les:\npublic class  SymbolGraph\nSymbolGraph(String filename, \n                   String delim)\nbuild graph specified in \nfilename using delim to \nseparate vertex names\nboolean contains(String key) is key a vertex?\nint index(String key) ", "start": 560, "end": 560}, "819": {"text": "\ufb01les:\npublic class  SymbolGraph\nSymbolGraph(String filename, \n                   String delim)\nbuild graph specified in \nfilename using delim to \nseparate vertex names\nboolean contains(String key) is key a vertex?\nint index(String key) index associated with key\nString name(int v) key associated with index v\nGraph G() underlying Graph\n A P I  f o r  g r a p h s  w i t h  s y m b o l i c  v e r t e x  n a m e s\nSymbol graph example (list of edges)\nJFK\nATL\nMCO\nDFW\nHOU\nDEN\nLAS\nPHXLAX\nORD\nJFK MCO\nORD DEN\nORD HOU\nDFW PHX\nJFK ATL\nORD DFW\nORD PHX\nATL HOU\nDEN PHX\nPHX LAX\nJFK ORD\nDEN LAS\nDFW HOU\nORD ATL\nLAS LAX\nATL MCO\nHOU MCO\n  LAS PHX  \nroutes.txt\nV and E \nnot explicitly\nspecified\n\" \"\ndelimiter\n548 CHAPTER 4 \u25a0 Graphs\n Kevin\nBacon\nKathleen\nQuinlan\nMeryl\nStreep\nNicole\nKidman\nJohn\nGielgud\nKate\nWinslet\nBill\nPaxton\nDonald\nSutherland\nThe Stepford\nWives\nPortrait\nof a Lady\nDial M\nfor Murder\nApollo 13\nTo Catch\na Thief\nThe Eagle\nHas Landed\nCold\nMountain\nMurder on the\nOrient Express\nVernon\nDobtcheff\nAn American\nHaunting\nJude\nEnigma\nEternal Sunshine\nof the Spotless\nMind\nThe\nWoodsman\nWild\nThings\nHamlet\nTitanic\nAnimal\nHouse\nGrace\nKellyCaligola\nThe ", "start": 560, "end": 561}, "820": {"text": "Express\nVernon\nDobtcheff\nAn American\nHaunting\nJude\nEnigma\nEternal Sunshine\nof the Spotless\nMind\nThe\nWoodsman\nWild\nThings\nHamlet\nTitanic\nAnimal\nHouse\nGrace\nKellyCaligola\nThe River\nWild\nLloyd\nBridges\nHigh\nNoon\nThe Da\nVinci Code\nJoe Versus\nthe Volcano\nPatrick\nAllen\nTom\nHanks\nSerretta\nWilson\nGlenn\nClose\nJohn\nBelushi\nYves\nAubert Shane\nZaza\nPaul\nHerbert\nperformer\nvertex\nmovie\nvertex\nSymbol graph example (adjacency lists)\n...\nTin Men (1987)/DeBoy, David/Blumenfeld, Alan/... /Geppi, Cindy/Hershey, Barbara...\nTirez sur le pianiste (1960)/Heymann, Claude/.../Berger, Nicole (I)...\nTitanic (1997)/Mazin, Stan/...DiCaprio, Leonardo/.../Winslet, Kate/...\nTitus (1999)/Weisskopf, Hermann/Rhys, Matthew/.../McEwan, Geraldine\nTo Be or Not to Be (1942)/Verebes, Ern\u00f6 (I)/.../Lombard, Carole (I)...\nTo Be or Not to Be (1983)/.../Brooks, Mel (I)/.../Bancroft, Anne/...\nTo Catch a Thief (1955)/Par\u00eds, Manuel/.../Grant, Cary/.../Kelly, Grace/...\nTo Die For (1995)/Smith, Kurtwood/.../Kidman, Nicole/.../ Tucci, Maria...\n...\n  \nmovies.txt\nV and E \nnot explicitly\nspecified\nperformersmovie\n\"/\" delimiter\n5494.1 ", "start": 561, "end": 561}, "821": {"text": "For (1995)/Smith, Kurtwood/.../Kidman, Nicole/.../ Tucci, Maria...\n...\n  \nmovies.txt\nV and E \nnot explicitly\nspecified\nperformersmovie\n\"/\" delimiter\n5494.1 \u25a0 Undirected Graphs\n This API provides a construc-\ntor to read and build the graph \nand client methods name() and \nindex() for translating vertex \nnames between the strings on \nthe input stream and the inte-\nger indices used by our graph-\nprocessing methods. \nTe s t  c l i e n t . The test client at \nleft builds a graph from the \n\ufb01le named as the \ufb01rst com-\nmand-line argument (using \nthe delimiter as speci\ufb01ed by \nthe second command-line ar -\ngument) and then takes queries from standard input. \nThe user speci\ufb01es a vertex name and gets the list of \nvertices adjacent to that vertex. This client immedi -\nately provides the useful inverted index functional-\nity that we considered in Section 3.5. In the case of \nroutes.txt, you can type an airport code to \ufb01nd the \ndirect \ufb02ights from that airport, information that is \nnot directly available in the data \ufb01le. In the case of \nmovies.txt, you can type the name of a performer \nto see the list of the movies in the database in which \nthat performer appeared, or you can type the name \nof a movie to see the list of performers that appear \nin that movie. Typing a movie name and getting its \ncast is not much more than regurgitating the corre-\nsponding line in the input \ufb01le, but typing the name \nof a performer and getting the list of movies in which \nthat performer has appeared is inverting the index. \nEven though the database is built around connect -\ning movies to performers, the bipartite ", "start": 561, "end": 562}, "822": {"text": "but typing the name \nof a performer and getting the list of movies in which \nthat performer has appeared is inverting the index. \nEven though the database is built around connect -\ning movies to performers, the bipartite graph model \nembraces the idea that it also connects performers \nto movies. The bipartite graph model automatically \nserves as an inverted index and also provides the basis \nfor more sophisticated processing, as we will see.\n% java SymbolGraph routes.txt \" \" \nJFK\n   ORD\n   ATL\n   MCO \nLAX\n   LAS\n   PHX\npublic static void main(String[] args) \n{\n   String filename = args[0];\n   String delim = args[1];\n   SymbolGraph sg = new SymbolGraph(filename, delim);\n   Graph G = sg.G();\n   while (StdIn.hasNextLine())\n   {\n      String source = StdIn.readLine();\n      for (int w : G.adj(sg.index(source)))\n         StdOut.println(\"   \" + sg.name(w));\n   } \n}\nTest client for symbol graph API\n% java SymbolGraph movies.txt \"/\" \nTin Men (1987)\n   DeBoy, David\n   Blumenfeld, Alan\n   ... \n   Geppi, Cindy\n   Hershey, Barbara\n   ...\nBacon, Kevin\n   Mystic River (2003)\n   Friday the 13th (1980)\n   Flatliners (1990)\n   Few Good Men, A (1992)\n   ...\n550 CHAPTER 4 \u25a0 Graphs\n This approach is clearly effective for any of the graph-processing methods that \nwe consider: any client can use index() when it wants to convert a vertex name to an \nindex for use in graph processing and name() when it wants to convert an index from \ngraph processing into a name for use in the context of the application. \nImplementation. A full SymbolGraph implementation is given on page 552. ", "start": 562, "end": 563}, "823": {"text": "\nindex for use in graph processing and name() when it wants to convert an index from \ngraph processing into a name for use in the context of the application. \nImplementation. A full SymbolGraph implementation is given on page 552. It builds \nthree data structures:\n\u25a0 A symbol table st with String keys (vertex names) and int values (indices)\n\u25a0 An array keys[] that serves as an inverted index, giving the vertex name associ-\nated with each integer index\n\u25a0 A Graph G built using the indices to refer to vertices\nSymbolGraph uses two passes through the data to build these data structures, pri-\nmarily because the number of vertices V is needed to build the Graph. In typical real-\nworld applications, keeping the value of V and E in the graph de\ufb01nition \ufb01le (as in our \nGraph constructor at the beginning of this section) is somewhat inconvenient\u2014with \nSymbolGraph, we can maintain \ufb01les such as routes.txt or movies.txt by adding or \ndeleting entries without regard to the number of different names involved.\nSymbol graph data structures\nkey value\nundirected graphsymbol table inverted index\nJFK\nMCO\nORD\nDEN\nHOU\nDFW\nPHX\nATL\nLAX\nLAS\nJFK\nMCO\nORD\nDEN\nHOU\nDFW\nPHX\nATL\nLAX\nLAS\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n9 6\n2 7 1\n4 7 3\n9 6 2\n4 2 6\n6 8 3\n9 8 3 2 5\n7 0 6 5 ", "start": 563, "end": 563}, "824": {"text": "7 1\n4 7 3\n9 6 2\n4 2 6\n6 8 3\n9 8 3 2 5\n7 0 6 5 4 3\n1 2 4 0\n1 5 7 2\nST<String, Integer> st String[] keys Graph G\nJFK\nATL\nMCO\nDFW\nHOU\nDEN\nLAS\nPHXLAX\nORD\nBag[] adj\nint V 10\n5514.1 \u25a0 Undirected Graphs\n  S y m b o l  g r a p h  d a t a  t y p e\npublic class  SymbolGraph \n{\n   private ST<String, Integer> st;              // String -> index\n   private String[] keys;                       // index -> String\n   private Graph G;                             // the graph\n   public SymbolGraph(String stream, String sp)\n   {\n      st = new ST<String, Integer>();\n      In in = new In(stream);                   // First pass\n      while (in.hasNextLine())                  //   builds the index\n      {\n         String[] a = in.readLine().split(sp);  //   by reading strings\n         for (int i = 0; i < a.length; i++)     //   to associate each\n            if (!st.contains(a[i]))             //   distinct string\n               st.put(a[i], st.size());         //   with an index.\n      }\n      keys = new String[st.size()];             // Inverted index\n      for (String name : st.keys())             //   to get string keys\n         keys[st.get(name)] = name;             //   is an array.\n      G = new Graph(st.size());\n      in = new In(stream);                      // Second pass\n      while (in.hasNextLine())                  //   builds the graph\n      { ", "start": 563, "end": 564}, "825": {"text": "= name;             //   is an array.\n      G = new Graph(st.size());\n      in = new In(stream);                      // Second pass\n      while (in.hasNextLine())                  //   builds the graph\n      {                                  \n         String[] a = in.readLine().split(sp);  //   by connecting the\n         int v = st.get(a[0]);                  //   first vertex\n         for (int i = 1; i < a.length; i++)     //   on each line\n             G.addEdge(v, st.get(a[i]));        //   to all the others.\n      }\n   }\n   public boolean contains(String s) {  return st.contains(s);  }\n   public int index(String s)        {  return st.get(s);  }\n   public String name(int v)         {  return keys[v];  }\n   public Graph G()                  {  return G;  } \n}\n \n \nThis Graph client allows clients to de\ufb01ne graphs with String vertex names instead of integer indices. \nIt maintains instance variables st (a symbol table that maps names to indices), keys (an array that \nmaps indices to names), and G (a graph, with integer vertex names).  T o build these data structures, \nit makes two passes through the graph de\ufb01nition (each line has a string and a list of adjacent strings, \nseparated by the delimiter sp).\n552 CHAPTER 4 \u25a0 Graphs  \n \n  D e g r e e s  o f  s e p a r a t i o n .  One of the classic applications of graph processing is to \ufb01nd \nthe degree of separation between two individuals in a social network. T o \ufb01x ideas, we \ndiscuss this application in terms of a recently popularized pastime known as the Kevin \nBacon game, which uses the movie-performer graph that we just considered. Kevin Ba-\ncon is ", "start": 564, "end": 565}, "826": {"text": "\ufb01x ideas, we \ndiscuss this application in terms of a recently popularized pastime known as the Kevin \nBacon game, which uses the movie-performer graph that we just considered. Kevin Ba-\ncon is a proli\ufb01c actor who has appeared in many movies. We assign every performer a \nKevin Bacon number as follows: Bacon himself is 0, any performer who has been in the \nsame cast as Bacon has a Kevin Bacon number of 1, any other performer (except Bacon) \nwho has been in the same cast as a performer whose number is 1 has a Kevin Bacon \nnumber of 2, and so forth. For example, Meryl Streep has a Kevin Bacon number of 1 \nbecause she appeared in The River Wild with Kevin Bacon. Nicole Kidman\u2019s number is \n2: although she did not appear in any movie with Kevin Bacon, she was in Days of Thun-\nder with T om Cruise, and Cruise appeared in A Few Good Men with Kevin Bacon. Given \nthe name of a performer, the simplest version of the game is to \ufb01nd some alternating se-\nquence of movies and performers that leads back to Kevin Bacon. For example, a movie \nbuff might know that T om Hanks was in Joe Versus the Volcano with Lloyd Bridges, \nwho was in High Noon with \nGrace Kelly, who was in Dial \nM for Murder  with Patrick \nAllen, who was in The Eagle \nHas Landed  with Donald \nSutherland, who was in \nAnimal House  with Kevin \nBacon. But this knowledge \ndoes not suf\ufb01ce to establish \nTo m  Ha n k s\u2019s  B a co n  n u m b e r  \n(it is actually 1 because he \nwas in Apollo 13 with Kevin \nBacon). Y ou can see that the \nKevin Bacon number has to \nbe de\ufb01ned by counting the ", "start": 565, "end": 565}, "827": {"text": "u m b e r  \n(it is actually 1 because he \nwas in Apollo 13 with Kevin \nBacon). Y ou can see that the \nKevin Bacon number has to \nbe de\ufb01ned by counting the movies in the shortest such sequence, so it is hard to be sure \nwhether someone wins the game without using a computer. Of course, as illustrated in \nthe SymbolGraph client DegreesOfSeparation on page 555, BreadthFirstPaths is the \nprogram we need to \ufb01nd a shortest path that establishes the Kevin Bacon number of \nany performer in movies.txt. This program takes a source vertex from the command \nline, then takes queries from standard input and prints a shortest path from the source \nto the query vertex. Since the graph associated with movies.txt is bipartite, all paths \nalternate between movies and performers, and the printed path is a \u201cproof\u201d that the \npath is valid (but not a proof that it is the shortest such path\u2014you need to educate your \n% java DegreesOfSeparation movies.txt \"/\" \"Bacon, Kevin\" \nKidman, Nicole\n   Bacon, Kevin\n   Few Good Men, A (1992)\n   Cruise, Tom\n   Days of Thunder (1990)\n   Kidman, Nicole \nGrant, Cary\n   Bacon, Kevin\n   Mystic River (2003)\n   Willis, Susan\n   Majestic, The (2001)\n   Landau, Martin\n   North by Northwest (1959)\n   Grant, Cary\n5534.1 \u25a0 Undirected Graphs\n  friends about Proposition B for that). DegreesOfSeparation also \ufb01nds shortest paths \nin graphs that are not bipartite: for example, it \ufb01nds a way to get from one airport to \nanother in routes.txt using the fewest connections.\nYou might enjoy using  DegreesOfSeparation to answer some entertaining ques-\ntions about the movie business. For ", "start": 565, "end": 566}, "828": {"text": "it \ufb01nds a way to get from one airport to \nanother in routes.txt using the fewest connections.\nYou might enjoy using  DegreesOfSeparation to answer some entertaining ques-\ntions about the movie business. For example, you can \ufb01nd separations between mov-\nies, not just performers. More important, the concept of separation has been widely \nstudied in many other contexts. For example, mathematicians play this same game with \nthe graph de\ufb01ned by paper co-authorship and their connection to  P .  Erd\u00f6s, a proli\ufb01c \n20th-century mathematician. Similarly, everyone in New Jersey seems to have a Bruce \nSpringsteen number of 2, because everyone in the state seems to know someone who \nclaims to know Bruce. T o play the Erd\u00f6s game, you would need a database of all math-\nematical papers; playing the Springsteen game is a bit more challenging. On a more \nserious note, degrees of separation play a crucial role in the design of computer and \ncommunications networks, and in our understanding of natural networks in all \ufb01elds \nof science.\n% java DegreesOfSeparation movies.txt \"/\" \"Animal House (1978)\" \nTitanic (1997)\n   Animal House (1978)\n   Allen, Karen (I)\n   Raiders of the Lost Ark (1981)\n   Taylor, Rocky (I)\n   Titanic (1997)\nTo Catch a Thief (1955)\n   Animal House (1978)\n   Vernon, John (I)\n   Topaz (1969)\n   Hitchcock, Alfred (I)\n   To Catch a Thief (1955)\n554 CHAPTER 4 \u25a0 Graphs\n  D e g r e e s  o f  s e p a r a t i o n\npublic class  DegreesOfSeparation \n{\n   public static void main(String[] args)\n   {\n      SymbolGraph sg = new ", "start": 566, "end": 567}, "829": {"text": "g r e e s  o f  s e p a r a t i o n\npublic class  DegreesOfSeparation \n{\n   public static void main(String[] args)\n   {\n      SymbolGraph sg = new SymbolGraph(args[0], args[1]);\n      Graph G = sg.G();\n      String source = args[2];\n      if (!sg.contains(source))\n      {  StdOut.println(source + \" not in database.\"); return;  }\n      int s = sg.index(source);\n      BreadthFirstPaths bfs = new BreadthFirstPaths(G, s);\n      while (!StdIn.isEmpty())\n      {\n         String sink = StdIn.readLine();\n         if (sg.contains(sink))\n         {\n            int t = sg.index(sink);\n            if (bfs.hasPathTo(t))\n               for (int v : bfs.pathTo(t))\n                  StdOut.println(\"   \" + sg.name(v));\n            else StdOut.println(\"Not connected\");\n         }  \n         else StdOut.println(\"Not in database.\");\n      }\n   } \n}\nThis SymbolGraph and BreadthFirstPaths client \ufb01nds shortest paths in graphs. For movies.txt, it \nplays the Kevin Bacon game. \n% java DegreesOfSeparation routes.txt \" \" JFK \nLAS\n   JFK\n   ORD\n   PHX\n   LAS\nDFW\n   JFK\n   ORD\n   DFW\n5554.1 \u25a0 Undirected Graphs Summary In this section, we have introduced several basic concepts that we will \nexpand upon and further develop throughout the rest of this chapter:\n\u25a0 Graph nomenclature\n\u25a0 A graph representation that enables processing of huge sparse graphs\n\u25a0 \n \nA design pattern for graph processing, where we implement algorithms by devel-\noping clients that preprocess the graph in the constructor, building data struc-\ntures that can ef\ufb01ciently support client queries about the graph\n\u25a0  Depth-\ufb01rst search and breadth-\ufb01rst ", "start": 567, "end": 568}, "830": {"text": "by devel-\noping clients that preprocess the graph in the constructor, building data struc-\ntures that can ef\ufb01ciently support client queries about the graph\n\u25a0  Depth-\ufb01rst search and breadth-\ufb01rst search\n\u25a0 A class providing the capability to use symbolic vertex names\nThe table below summarizes the implementations of graph algorithms that we have \nconsidered. These algorithms are a proper introduction to graph processing, since vari-\nants on their code will resurface as we consider more complicated types of graphs and \napplications, and (consequently) more dif\ufb01cult graph-processing problems. The same \nquestions involving connections and paths among vertices become much more dif\ufb01cult \nwhen we add direction and then weights to graph edges, but the same approaches are \neffective in addressing them and serve as a starting point for addressing more dif\ufb01cult \nproblems.\nproblem solution reference\n  s i n g l e - s o u r c e  c o n n e c t i v i t y  DepthFirstSearch page 531\nsingle-source paths DepthFirstPaths page 536\nsingle-source shortest paths BreadthFirstPaths page 540\nconnectivity CC page 544\ncycle detection Cycle page 547\ntwo-colorability (bipartiteness) TwoColor page 547\n(Undirected) graph-processing problems addressed in this section\n556 CHAPTER 4 \u25a0 Graphs\n Q&A\nQ.  Why not jam all of the algorithms into Graph.java?\nA.  Ye s , we  m i g h t  j u s t  a d d  q u e r y  m e t h o d s  ( a n d  w h a te ve r  p r iv a te  \ufb01 e l d s  a n d  m e t h o d s  \neach might need) to the basic Graph ADT de\ufb01nition. While this approach ", "start": 568, "end": 569}, "831": {"text": "r  p r iv a te  \ufb01 e l d s  a n d  m e t h o d s  \neach might need) to the basic Graph ADT de\ufb01nition. While this approach has some of \nthe virtues of data abstraction that we have embraced, it also has some serious draw-\nbacks, because the world of graph processing is signi\ufb01cantly more expansive than the \nkinds of basic data structures treated in Section 1.3. Chief among these drawbacks are \nthe following: \n\u25a0  \n \nThere are many more graph-processing operations to implement than we can \naccurately de\ufb01ne in a single API. \n\u25a0 Simple graph-processing tasks have to use the same API needed by complicated \ntasks. \n\u25a0  \n \nOne method can access a \ufb01eld intended for use by another method, contrary to \nencapsulation principles that we would like to follow. \nThis situation is not unusual: APIs of this kind have come to be known as  wide inter-\nfaces (see page 97). In a chapter \ufb01lled with graph-processing algorithms, an API of this \nsort would be wide indeed.\nQ.  Does SymbolGraph really need two passes?\nA. No. You could pay an extra lg N factor and support adj() directly as an ST instead \nof a Bag. We have an implementation along these lines in our book  An Introduction to \nProgramming in Java: An Interdisciplinary Approach.\n5574.1 \u25a0 Undirected Graphs\n EXERCISES\n \n \n4.1.1 What is the maximum number of edges in a graph with V vertices and no paral-\nlel edges? What is the minimum number of edges in a graph with V vertices, none of \nwhich are isolated?\n4.1.2  Draw, in the style of the \ufb01gure in the text (page 524), the ad-\njacency lists built by Graph\u2019s ", "start": 569, "end": 570}, "832": {"text": "with V vertices, none of \nwhich are isolated?\n4.1.2  Draw, in the style of the \ufb01gure in the text (page 524), the ad-\njacency lists built by Graph\u2019s input stream constructor for the \ufb01le \ntinyGex2.txt depicted at left. \n4.1.3  Create a copy constructor for Graph that takes as input a \ngraph G and creates and initializes a new copy of the graph. Any \nchanges a client makes to G should not affect the newly created \ngraph. \n4.1.4 Add a method hasEdge() to Graph which takes two int\narguments v and w and returns true if the graph has an edge v-w, \nfalse otherwise. \n4.1.5 Modify Graph to disallow parallel edges and self-loops. \n4.1.6 Consider the four-vertex graph with edges 0-1, 1-2, 2-3, \nand 3-0.  Draw an array of adjacency-lists that could not have been \nbuilt calling addEdge() for these edges no matter what order. \n4.1.7  Develop a test client for Graph that reads a graph from the input stream named \nas command-line argument and then prints it, relying on toString().\n4.1.8  Develop an implementation for the Search API on page 528 that uses UF, as de-\nscribed in the text.\n4.1.9 Show, in the style of the \ufb01gure on page 533,  a detailed trace of the call dfs(0) for \nthe graph built by Graph\u2019s input stream constructor for the \ufb01le tinyGex2.txt (see  Ex-\nercise 4.1.2). Also, draw the tree represented by edgeTo[].\n4.1.10 Prove that every connected graph has a vertex whose removal (including all \nadjacent edges) ", "start": 570, "end": 570}, "833": {"text": "Ex-\nercise 4.1.2). Also, draw the tree represented by edgeTo[].\n4.1.10 Prove that every connected graph has a vertex whose removal (including all \nadjacent edges) will not disconnect the graph, and write a DFS method that \ufb01nds such \na vertex. Hint : Consider a vertex whose adjacent vertices are all marked. \n4.1.11 Draw the tree represented by edgeTo[] after the call bfs(G, 0) in Algorithm \n4.2 for the graph built by Graph\u2019s input stream constructor for the \ufb01le tinyGex2.txt\n(see  Exercise 4.1.2).\n12\n16\n 8 4\n 2 3\n 1 11\n 0 6\n 3 6\n10 3\n 7 11\n 7 8\n11 8\n 2 0\n 6 2\n 5 2\n 5 10\n 3 10\n 8 1\n 4 1\ntinyGex2.txt\nV\nE\n558 CHAPTER 4 \u25a0 Graphs\n  \n  \n4.1.12 What does the BFS tree tell us about the \ndistance from v to  w when neither is at the root?\n4.1.13  Add a distTo() method to the \nBreadthFirstPaths API and implementation, \nwhich returns the number of edges on the shortest \npath from the source to a given vertex. A distTo()\nquery should run in constant time.\n4.1.14 Suppose you use a stack instead of a queue \nwhen running breadth-\ufb01rst search. Does it still \ncompute shortest paths?\n4.1.15 Modify the input stream constructor for \nGraph to also allow adjacency lists from standard \ninput (in a manner similar to SymbolGraph), as in \nthe example tinyGadj.txt shown at right. After ", "start": 570, "end": 571}, "834": {"text": "paths?\n4.1.15 Modify the input stream constructor for \nGraph to also allow adjacency lists from standard \ninput (in a manner similar to SymbolGraph), as in \nthe example tinyGadj.txt shown at right. After \nthe number of vertices and edges, each line con-\ntains a vertex and its list of adjacent vertices.\n4.1.16 The  eccentricity of a vertex v is the the length of the shortest path from that ver-\ntex to the furthest vertex from v. The   diameter of a graph is the maximum eccentricity \nof any vertex. The  radius of a graph is the smallest eccentricity of any vertex. A  center is \na vertex whose eccentricity is the radius. Implement the following API:\npublic class  GraphProperties\nGraphProperties(Graph G) constructor (exception if G not connected)\nint eccentricity(int v) eccentricity of v\nint diameter() diameter of G\nint radius() radius of G\nint center() a center of G\n 4.1.18 The  girth of a graph is the length of its shortest cycle. If a graph is acyclic, then its \ngirth is in\ufb01nite. Add a method girth() to GraphProperties that returns the girth of \nthe graph. Hint : Run BFS from each vertex. The shortest cycle containing s is a shortest \npath from s to some vertex v, plus the edge from v back to s.\nsame lists as for \nlist-of-edges input\nbut order within lists\nis different\n13\n13\n0 1 2 5 6\n3 4 5\n4 5 6\n7 8\n9 10 11 12\n11 12\n% java Graph tinyGadj.txt\n13 vertices, 13 edges\n0: 6 5 2 1 \n1: 0 \n2: 0 \n3: 5 ", "start": 571, "end": 571}, "835": {"text": "12\n11 12\n% java Graph tinyGadj.txt\n13 vertices, 13 edges\n0: 6 5 2 1 \n1: 0 \n2: 0 \n3: 5 4 \n4: 6 5 3 \n5: 4 3 0 \n6: 4 0 \n7: 8 \n8: 7 \n9: 12 11 10 \n10: 9\n11: 12 9 \n12: 11 9 \ntinyGadj.txt\nV\nE\nlist order\nis reversed\nfrom input\nsecond\nrepresentation\nof each edge\nappears in red\n5594.1 \u25a0 Undirected Graphs\n  \n \n  \n4.1.19 Show, in the style of the \ufb01gure on page 545, a detailed trace of CC for \ufb01nding the \nconnected components in the graph built by Graph\u2019s input stream constructor for the \n\ufb01le tinyGex2.txt (see Exercise 4.1.2).\n4.1.20 Show, in the style of the \ufb01gures in this section, a detailed trace of Cycle for \n\ufb01nding a cycle in the graph built by Graph\u2019s input stream constructor for the \ufb01le \ntinyGex2.txt (see Exercise 4.1.2). What is the order of growth of the running time \nof the Cycle constructor, in the worst case?\n4.1.21 Show, in the style of the \ufb01gures in this section, a detailed trace of TwoColor for \n\ufb01nding a two-coloring of the graph built by Graph\u2019s input stream constructor for the \n\ufb01le tinyGex2.txt (see Exercise 4.1.2). What is the order of growth of the running \ntime of the TwoColor constructor, in the worst case?\n4.1.22 ", "start": 571, "end": 572}, "836": {"text": "\n\ufb01le tinyGex2.txt (see Exercise 4.1.2). What is the order of growth of the running \ntime of the TwoColor constructor, in the worst case?\n4.1.22 Run SymbolGraph with movies.txt to \ufb01nd the Kevin Bacon number of this \nyear\u2019s Oscar nominees.\n4.1.23  Write a program BaconHistogram that prints a histogram of Kevin Bacon \nnumbers, indicating how many performers from movies.txt have a Bacon number of \n0, 1, 2, 3, ... . Include a category for those who have an in\ufb01nite number (not connected \nto Kevin Bacon).\n4.1.24 Compute the number of connected components in movies.txt, the size of the \nlargest component, and the number of components of size less than 10. Find the eccen-\ntricity, diameter, radius, a center, and the girth of the largest component in the graph. \nDoes it contain Kevin Bacon?\n4.1.25 Modify DegreesOfSeparation to take an int value y as a command-line argu-\nment and ignore movies that are more than y years old.\n4.1.26 Write a SymbolGraph client like DegreesOfSeparation that uses depth-\ufb01rst \nsearch instead of breadth-\ufb01rst search to \ufb01nd paths connecting two performers, produc-\ning output like that shown on the facing page.\nEXERCISES  (continued)\n560 CHAPTER 4 \u25a0 Graphs\n   \n4.1.27 Determine the amount of memory used by Graph to represent a graph with V\nvertices and E edges, using the memory-cost model of Section 1.4.\n4.1.28  Two g r aphs are  isomorphic if there is a way to rename the vertices of one to make \nit identical to the other. Draw all the nonisomorphic graphs with two, three, four, and \n\ufb01ve ", "start": 572, "end": 573}, "837": {"text": "Two g r aphs are  isomorphic if there is a way to rename the vertices of one to make \nit identical to the other. Draw all the nonisomorphic graphs with two, three, four, and \n\ufb01ve vertices.\n4.1.29 Modify Cycle so that it works even if the graph contains self-loops and parallel \nedges.\n% java DegreesOfSeparationDFS movies.txt \nSource: Bacon, Kevin \nQuery:  Kidman, Nicole\n   Bacon, Kevin\n   Mystic River (2003)\n   O\u2019Hara, Jenny\n   Matchstick Men (2003)\n   Grant, Beth\n  ... [123 movies ] (!)\n   Law, Jude\n   Sky Captain... (2004)\n   Jolie, Angelina\n   Playing by Heart (1998)\n   Anderson, Gillian (I)\n   Cock and Bull Story, A (2005)\n   Henderson, Shirley (I)\n   24 Hour Party People (2002)\n   Eccleston, Christopher\n   Gone in Sixty Seconds (2000)\n   Balahoutis, Alexandra\n   Days of Thunder (1990)\n   Kidman, Nicole\n5614.1 \u25a0 Undirected Graphs\n CREATIVE PROBLEMS\n \n \n4.1.30        Eulerian and Hamiltonian cycles. Consider the graphs de\ufb01ned by the following \nfour sets of edges:\n0-1 0-2 0-3 1-3 1-4 2-5 2-9 3-6 4-7 4-8 5-8 5-9 6-7 6-9 7-8   \n0-1 0-2 0-3 1-3 0-3 2-5 5-6 3-6 4-7 4-8 5-8 5-9 6-7 6-9 ", "start": 573, "end": 574}, "838": {"text": "0-2 0-3 1-3 0-3 2-5 5-6 3-6 4-7 4-8 5-8 5-9 6-7 6-9 8-8   \n0-1 1-2 1-3 0-3 0-4 2-5 2-9 3-6 4-7 4-8 5-8 5-9 6-7 6-9 7-8   \n4-1 7-9 6-2 7-3 5-0 0-2 0-8 1-6 3-9 6-3 2-8 1-5 9-8 4-5 4-7 \nWhich of these graphs have Euler cycles (cycles that visit each edge exactly once)? \nWhich of them have Hamilton cycles (cycles that visit each vertex exactly once)?   \n4.1.31  Graph enumeration. How many different undirected graphs are there with V\nvertices and E edges (and no parallel edges)? \n4.1.32  Parallel edge detection. Devise a linear-time algorithm to count the parallel \nedges in a graph.\n4.1.33        Odd cycles. Prove that a graph is two-colorable (bipartite) if and only if it con-\ntains no odd-length cycle. \n4.1.34  Symbol graph. Implement a one-pass SymbolGraph (it need not be a Graph\nclient). Y our implementation may pay an extra log V factor for graph operations, for \nsymbol-table lookups.\n4.1.35     Biconnectedness. A graph is biconnected if every pair of vertices is connected \nby two disjoint paths. An    articulation point in a connected graph is a vertex that would \ndisconnect ", "start": 574, "end": 574}, "839": {"text": "lookups.\n4.1.35     Biconnectedness. A graph is biconnected if every pair of vertices is connected \nby two disjoint paths. An    articulation point in a connected graph is a vertex that would \ndisconnect the graph if it (and its adjacent edges) were removed. Prove that any graph \nwith no articulation points is biconnected. Hint : Given a pair of vertices s and t and a \npath  connecting them, use the fact that none of the vertices on the path are  articulation \npoints to construct two disjoint paths connecting s and t. \n4.1.36    Edge connectivity. A  bridge in a graph is an edge that, if removed, would sepa-\nrate a connected graph into two disjoint subgraphs. A graph that has no bridges is said \nto be    edge connected.  Develop a DFS-based data type for determing whether a given \ngraph is edge connected.\n4.1.37    Euclidean graphs. Design and implement an API EuclideanGraph for graphs \nwhose vertices are points in the plane that include coordinates. Include a method \nshow() that uses StdDraw to draw the graph.\n562 CHAPTER 4 \u25a0 Graphs\n  \n4.1.38  Image processing. Implement the  \ufb02ood \ufb01ll operation on the implicit graph de-\n\ufb01ned by connecting adjacent points that have the same color in an image.\n5634.1 \u25a0 Undirected Graphs\n EXPERIMENTS\n4.1.39    Random graphs. Write a program ErdosRenyiGraph that takes integer values \nV and E from the command line and builds a graph by generating E random pairs of in-\ntegers between 0 and V/H110021. Note: This generator produces self-loops and parallel edges.\n4.1.40  Random simple graphs. Write a program RandomSimpleGraph that takes inte-\nger values V and E ", "start": 574, "end": 576}, "840": {"text": "between 0 and V/H110021. Note: This generator produces self-loops and parallel edges.\n4.1.40  Random simple graphs. Write a program RandomSimpleGraph that takes inte-\nger values V and E from the command line and produces, with equal likelihood, each of \nthe possible simple graphs with V vertices and E edges.  \n4.1.41    Random sparse graphs. Write a program RandomSparseGraph to generate ran-\ndom sparse graphs for a well-chosen set of values of V and E such that you can use it to \nrun meaningful empirical tests on graphs drawn from the Erd\u00f6s-Renyi model.\n4.1.42    Random Euclidean graphs. Write a EuclideanGraph client (see Exercise \n4.1.37) RandomEuclideanGraph that produces random graphs by generating V random \npoints in the plane, then connecting each point with all points that are within a circle \nof radius d centered at that point. Note : The graph will almost certainly be connected if \nd is larger than the threshold value /H20906 lg V//H9266 V and almost certainly disconnected if d is \nsmaller than that value.\n4.1.43    Random grid graphs. Write a EuclideanGraph client RandomGridGraph that \ngenerates random graphs by connecting vertices arranged in a /H20906 V-by-/H20906 V grid to their \nneighbors (see Exercise 1.5.15). Augment your program to add R extra random edges. \nFor large R, shrink the grid so that the total number of edges remains about V. Add \nan option such that an extra edge goes from a vertex s to a vertex t with probability \ninversely proportional to the Euclidean distance between s and t.\n4.1.44  Real-world graphs. Find a large weighted graph on the web\u2014perhaps a map \nwith distances, telephone connections with costs, or an airline rate schedule. ", "start": 576, "end": 576}, "841": {"text": "the Euclidean distance between s and t.\n4.1.44  Real-world graphs. Find a large weighted graph on the web\u2014perhaps a map \nwith distances, telephone connections with costs, or an airline rate schedule. Write a \nprogram RandomRealGraph that builds a graph by choosing V vertices at random and \nE edges at random from the subgraph induced by those vertices.\n4.1.45  Random   interval graphs. Consider a collection of V intervals on the real line \n(pairs of real numbers). Such a collection de\ufb01nes an interval graph with one vertex cor-\nresponding to each interval, with edges between vertices if the corresponding intervals \nintersect (have any points in common). Write a program that generates V random in-\ntervals in the unit interval, all of length d, then builds the corresponding interval graph. \nHint: Use a BST.\n564 CHAPTER 4 \u25a0 Graphs\n 4.1.46  Random transportation graphs. One way to de\ufb01ne a transportation system is \nwith a set of sequences of vertices, each sequence de\ufb01ning a path connecting the ver -\ntices. For example, the sequence 0-9-3-2 de\ufb01nes the edges 0-9, 9-3, and 3-2. Write a \nEuclideanGraph client RandomTransportation that builds a graph from an input \ufb01le \nconsisting of one sequence per line, using symbolic names. Develop input suitable to al-\nlow you to use your program to build a graph corresponding to the Paris M\u00e9tro system.\nTesting all algorithms and study ing all parameters against all graph models is unrealistic. \nFor each problem listed below, write a client that addresses the problem for any given input \ngraph, then choose among the generators above to run experiments for that graph model. \nUse your judgment in selecting experiments, perhaps in response to results of previous \nexperiments. Write a narrative explaining your ", "start": 576, "end": 577}, "842": {"text": "problem for any given input \ngraph, then choose among the generators above to run experiments for that graph model. \nUse your judgment in selecting experiments, perhaps in response to results of previous \nexperiments. Write a narrative explaining your results and any conclusions that might be \ndrawn.\n4.1.47  Path lengths in DFS. Run experiments to determine empirically the probability \nthat DepthFirstPaths \ufb01nds a path between two randomly chosen vertices and to cal-\nculate the average length of the paths found, for various graph models.\n4.1.48  Path lengths in BFS. Run experiments to determine empirically the probability \nthat BreadthFirstPaths \ufb01nds a path between two randomly chosen vertices and to \ncalculate the average length of the paths found, for various graph models.\n4.1.49  Connected components. Run experiments to determine empirically the distri -\nbution of the number of components in random graphs of various types, by generating \nlarge numbers of graphs and drawing a histogram.\n4.1.50  Two-colorable. Most graphs are not two-colorable, and DFS tends to discov -\ner that fact quickly. Run empirical tests to study the number of edges examined by \nTwoColor, for various graph models.\n5654.1 \u25a0 Undirected Graphs\n 4.2  DIRECTED GRAPHS\n \nIn directed graphs , edges are one-way: the pair of vertices that de\ufb01nes each edge is \nan ordered pair that speci\ufb01es a one-way adjacency. Many applications (for example, \ngraphs that represent the web, scheduling constraints, or telephone calls) are naturally \nexpressed in terms of directed graphs. The \none-way restriction is natural, easy to en-\nforce in our implementations, and seems in-\nnocuous; but it implies added combinatori-\nal structure that has profound implications \nfor our algorithms and makes working with \ndirected graphs quite different from work-\ning ", "start": 577, "end": 578}, "843": {"text": "to en-\nforce in our implementations, and seems in-\nnocuous; but it implies added combinatori-\nal structure that has profound implications \nfor our algorithms and makes working with \ndirected graphs quite different from work-\ning with undirected graphs. In this section, \nwe consider classic algorithms for exploring \nand processing directed graphs. \nGlossary Our de\ufb01nitions for directed \ngraphs are nearly identical to those for un -\ndirected graphs (as are some of the algo-\nrithms and programs that we use), but they \nare worth restating. The slight differences in the wording to account for edge directions \nimply structural properties that will be the focus of this section.\n D e f i n i t i o n .  A  directed graph (or digraph) is a set of vertices and a collection of    di-\nrected edges. Each directed edge connects an ordered pair of vertices.\n  \nWe say that a directed edge points from the \ufb01rst vertex in the pair and points to the \nsecond vertex in the pair. The  outdegree of a vertex in a digraph is the number of edges \ngoing from it; the    indegree of a vertex is the number of edges going to it. We drop the \nmodi\ufb01er directed when referring to edges in digraphs when the distinction is obvious in \ncontext. The \ufb01rst vertex in a directed edge is called its  head ; the second vertex is called \nits tail. We draw directed edges as arrows pointing from head to tail. We use the nota-\ntion v->w to refer to an edge that points from v to w in a digraph. As with undirected \ngraphs, our code handles  parallel edges and  self-loops, but they are not present in ex-\namples and we generally ignore them in the text. Ignoring anomalies, there are four \napplication vertex edge\nfood web species predator-prey\ninternet ", "start": 578, "end": 578}, "844": {"text": "handles  parallel edges and  self-loops, but they are not present in ex-\namples and we generally ignore them in the text. Ignoring anomalies, there are four \napplication vertex edge\nfood web species predator-prey\ninternet content page hyperlink\nprogram module external reference\ncellphone phone call\nscholarship paper citation\nfinancial stock transaction\ninternet machine connection\nTypical digraph applications\n566\n different ways in which two vertices might be related in a digraph: no edge; an edge \nv->w from v to w; an edge w->v from w to v; or two edges v->w and w->v, which indicate \nconnections in both directions. \n \n \n  D e f i n i t i o n .  A    directed path in a digraph is a sequence of vertices in which there is \na (directed) edge pointing from each vertex in the sequence to its successor in the \nsequence. A    directed  cycle is a directed path with at least one edge whose \ufb01rst and \nlast vertices are the same. A  simple cycle is a cycle with no repeated edges or vertices \n(except the requisite repetition of the \ufb01rst and last vertices). The  length of a path or \na cycle is its number of edges.\n  \n \nAs for undirected graphs, we assume that directed paths \nare  simple unless we speci\ufb01cally relax this assumption by \nreferring to speci\ufb01c repeated vertices (as in our de\ufb01nition \nof directed cycle) or to general directed paths. We say that \na vertex w is    reachable from a vertex v if there is a directed \npath from v to w. Also, we adopt the convention that each \nvertex is reachable from itself. Except for this case, the fact \nthat w is reachable from v in a digraph indicates nothing \nabout whether v is reachable from w. This distinction is \nobvious, ", "start": 578, "end": 579}, "845": {"text": "we adopt the convention that each \nvertex is reachable from itself. Except for this case, the fact \nthat w is reachable from v in a digraph indicates nothing \nabout whether v is reachable from w. This distinction is \nobvious, but critical, as we shall see.\nUnderstanding the algorithms in this section requires an appreciation of the dis -\ntinction between reachability in digraphs and connectivity in undirected graphs. De-\nveloping such an appreciation is more complicated than you might think. For example, \nalthough you are likely to be able to tell at a glance \nwhether two vertices in a small undirected graph are \nconnected, a directed path in a digraph is not so easy \nto spot, as indicated in the example at left. Processing \ndigraphs is akin to traveling around in a city where \nall the streets are one-way, with the directions not \nnecessarily assigned in any uniform pattern. Getting \nfrom one point to another in such a situation could \nbe a challenge indeed. Counter to this intuition is \nthe fact that the standard data structure that we use \nfor representing digraphs is simpler than the corre-\nsponding representation for undirected graphs!Is w reachable from v in this digraph?\nv\nw\nAnatomy of a digraph\ndirected\ncycle of\nlength 3\nvertex\nvertex of\nindegree 3 and \noutdegree 2\ndirected\nedge\ndirected\npath of\nlength 4\n5674.2 \u25a0 Directed Graphs\n Digraph data type The API below and the class Digraph shown on the facing \npage are virtually identical to those for Graph (page 526).\npublic class  Digraph \nDigraph(int V) create a V-vertex digraph with no edges\nDigraph(In in) read a digraph from input stream in\nint V() number of vertices\nint E() number of edges\nvoid addEdge(int ", "start": 579, "end": 580}, "846": {"text": "\nDigraph(int V) create a V-vertex digraph with no edges\nDigraph(In in) read a digraph from input stream in\nint V() number of vertices\nint E() number of edges\nvoid addEdge(int v, int w) add edge v->w to this digraph\nIterable<Integer> adj(int v) vertices connected to v by edges \npointing from v\nDigraph reverse() reverse of this digraph\nString toString() string representation\n A P I  f o r  a  d i g r a p h\n   Representation. We use the  adjacency-lists representation, where an edge v->w is rep-\nresented as a list node containing w in the linked list corresponding to v. This represen-\ntation is essentially the same as for undirected graphs but is even more straightforward \nbecause each edge occurs just once, as shown on the facing page.\nInput format. The code for the constructor that takes a digraph from an input stream \nis identical to the corresponding constructor in Graph\u2014the input format is the same, \nbut all edges are interpreted to be directed edges. In the list-of-edges format, a pair v w\nis interpreted as an edge v->w. \nReversing a digraph. Digraph also adds to the API a method reverse() which re-\nturns a copy of the digraph, with all edges reversed. This method is sometimes needed \nin digraph processing because it allows clients to \ufb01nd the edges that point to each ver-\ntex, while adj() gives just vertices connected by edges that point from each vertex.\nSymbolic names. It is also a simple matter to allow clients to use symbolic names in \ndigraph applications. T o implement a class SymbolDigraph like SymbolGraph on page \n552, replace Graph by Digraph everywhere.\nIt is worthwhile to take the time to consider carefully the difference, by comparing \ncode and ", "start": 580, "end": 580}, "847": {"text": "\ndigraph applications. T o implement a class SymbolDigraph like SymbolGraph on page \n552, replace Graph by Digraph everywhere.\nIt is worthwhile to take the time to consider carefully the difference, by comparing \ncode and the \ufb01gure at right with their counterparts for undirected graphs on page 524\nand page 526. In the adjacency-lists representation of an undirected graph, we know that \nif v is on w\u2019s list, then w will be on v\u2019s list; the adjacency-lists representation of a di -\ngraph has no such symmetry. This difference has profound implications in processing \ndigraphs.\n568 CHAPTER 4 \u25a0 Graphs\n Directed graph (digraph) data type\npublic class  Digraph \n{\n   private final int V;\n   private int E;\n   private Bag<Integer>[] adj;\n   public Digraph(int V)\n   {\n      this.V = V;\n      this.E = 0;\n      adj = (Bag<Integer>[]) new Bag[V];\n      for (int v = 0; v < V; v++) \n         adj[v] = new Bag<Integer>();\n   }\n   public int V()  {  return V;  }\n   public int E()  {  return E;  }\n   public void addEdge(int v, int w)\n   {\n      adj[v].add(w);\n      E++;\n   }\n   public Iterable<Integer> adj(int v)\n   {  return adj[v];  }\n   public Digraph reverse()\n   {\n      Digraph R = new Digraph(V);\n      for (int v = 0; v < V; v++)\n         for (int w : adj(v))\n            R.addEdge(w, v);\n      return R;\n   } \n}\nThis Digraph data type is identical to Graph (page 526) ex-\ncept that addEdge() only calls add() once, and it has an \ninstance method reverse() that returns a copy ", "start": 580, "end": 581}, "848": {"text": "R;\n   } \n}\nThis Digraph data type is identical to Graph (page 526) ex-\ncept that addEdge() only calls add() once, and it has an \ninstance method reverse() that returns a copy with all \nits edges reversed. Since the code is easily derived from the \ncorresponding code for Graph, we omit the toString()\nmethod (see the table on page 523) and the input stream con-\nstructor from (see page 526).\nadj[]\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n5 2\n3 2\n0 3\n5 1\n7 9\n11 10\n6 8\n4 12\n4\n12\n9\n9 4 0\nDigraph input format and \nadjacency-lists representation\n13\n22\n 4  2\n 2  3\n 3  2\n 6  0\n 0  1\n 2  0\n11 12\n12  9\n 9 10\n 9 11\n 8  9\n10 12\n11  4\n 4  3\n 3  5\n 7  8\n 8  7\n 5  4\n 0  5\n 6  4\n 6  9\n 7  6\ntinyDG.txt\nV\nE\n5694.2 \u25a0 Directed Graphs     R e a c h a b i l i t y  i n  d i g r a p h s  Our \ufb01rst graph-processing algorithm for undirected \ngraphs was DepthFirstSearch on page 531, which solves the single-source connectivity \nproblem, allowing clients to determine which vertices are connected to a given source. \nThe identical code ", "start": 581, "end": 582}, "849": {"text": "\ufb01rst graph-processing algorithm for undirected \ngraphs was DepthFirstSearch on page 531, which solves the single-source connectivity \nproblem, allowing clients to determine which vertices are connected to a given source. \nThe identical code with Graph changed to Digraph solves the analogous problem for \ndigraphs:\n S i n g l e - s o u r c e  r e a c h a b i l i t y .  Given a digraph and a source vertex s, support que-\nries of the form Is there a directed path from s to a given target vertex v? \nDirectedDFS on the facing page is a slight embellishment of DepthFirstSearch that \nimplements the following API:\npublic class  DirectedDFS\nDirectedDFS(Digraph G, int s) find vertices in G that are\nreachable from s\nDirectedDFS(Digraph G,\n        Iterable<Integer> sources)\nfind vertices in G that are\nreachable from sources\nboolean marked(int v) is v reachable?\n A P I  f o r  r e a c h a b i l i t y  i n  d i g r a p h s\nBy adding a second constructor that takes a list of vertices, this API supports for clients \nthe following generalization of the problem:\n  M u l t i p l e - s o u r c e  r e a c h a b i l i t y .  Given a digraph and a set of source vertices, sup-\nport queries of the form Is there a directed path from any vertex in the set to a given \ntarget vertex v?\nThis problem arises in the solution of a classic string-processing problem that we con-\nsider in Section 5.4.\nDirectedDFS uses our standard graph-processing paradigm and a standard recur -\nsive depth-\ufb01rst search to solve these problems. It calls the recursive dfs() for each \nsource, which marks every vertex encountered.\nProposition ", "start": 582, "end": 582}, "850": {"text": "5.4.\nDirectedDFS uses our standard graph-processing paradigm and a standard recur -\nsive depth-\ufb01rst search to solve these problems. It calls the recursive dfs() for each \nsource, which marks every vertex encountered.\nProposition D.   DFS marks all the vertices in a digraph reachable from a given set of \nsources in time proportional to the sum of the outdegrees of the vertices marked.\nProof: Same as Proposition A on page 531. \nA trace of the operation of this algorithm for our sample digraph appears on page 572. \nThis trace is somewhat simpler than the corresponding trace for undirected graphs, \n570 CHAPTER 4 \u25a0 Graphs\n ALGORITHM 4.4   Reachability in digraphs\npublic class  DirectedDFS \n{\n   private boolean[] marked;\n   public DirectedDFS(Digraph G, int s)\n   {\n      marked = new boolean[G.V()];\n      dfs(G, s);\n   }\n   public DirectedDFS(Digraph G, Iterable<Integer> sources)\n   {\n      marked = new boolean[G.V()];\n      for (int s : sources)\n         if (!marked[s]) dfs(G, s);\n   }\n   private void dfs(Digraph G, int v)\n   {\n      marked[v] = true;\n      for (int w : G.adj(v))\n         if (!marked[w]) dfs(G, w);\n   }\n   public boolean marked(int v)\n   {  return marked[v];  }\n   public static void main(String[] args)\n   {\n      Digraph G = new Digraph(new In(args[0]));\n      Bag<Integer> sources = new Bag<Integer>();\n      for (int i = 1; i < args.length; i++)\n          sources.add(Integer.parseInt(args[i]));\n      DirectedDFS reachable = new DirectedDFS(G, sources);\n      for (int v = 0; v < G.V(); v++)\n         if (reachable.marked(v)) StdOut.print(v + \" ", "start": 582, "end": 583}, "851": {"text": "sources.add(Integer.parseInt(args[i]));\n      DirectedDFS reachable = new DirectedDFS(G, sources);\n      for (int v = 0; v < G.V(); v++)\n         if (reachable.marked(v)) StdOut.print(v + \" \");\n      StdOut.println();\n   }\n}\n This implementation of depth-\ufb01rst search provides clients the ability to test which vertices are reach-\nable from a given vertex or a given set of vertices. \n% java DirectedDFS tinyDG.txt 1 \n1 \n% java DirectedDFS tinyDG.txt 2 \n0 1 2 3 4 5 \n% java DirectedDFS tinyDG.txt 1 2 6 \n0 1 2 3 4 5 6 9 10 11 12\n5714.2 \u25a0 Directed Graphs Trace of depth-first search to find vertices reachable from vertex 0 in a digraph\nmarked[]\n  0  T    \n  1   \n  2  \n  3  \n  4  \n  5  \n  .\n  .\n  .  \n  0  T    \n  1   \n  2  \n  3  \n  4  \n  5  T  \n  .\n  .\n  .  \n  0  T     \n  1   \n  2  \n  3  \n  4  T  \n  5  T  \n  .\n  .\n  .  \n  0  T    \n  1   \n  2  \n  3  T  \n  4  T  \n  5  T  \n  .\n  .\n  .  \n  0  T    \n  1   \n  2  T\n  3  T  \n  4  T  \n  5  T  \n  .\n  .\n  .  \n  0  T    \n  1  T\n  2  T\n  3  T  \n  4 ", "start": 583, "end": 584}, "852": {"text": "T  \n  4  T  \n  5  T  \n  .\n  .\n  .  \n  0  T    \n  1  T\n  2  T\n  3  T  \n  4  T  \n  5  T  \n  .\n  .\n  .  \nadj[]\n  0  5 1 \n  1   \n  2  0 3  \n  3  5 2\n  4  3 2\n  5  4\n .\n  .\n  .  \n  0  5 1 \n  1   \n  2  0 3  \n  3  5 2\n  4  3 2\n  5  4\n .\n  .\n  .  \n  0  5 1 \n  1   \n  2  0 3  \n  3  5 2\n  4  3 2\n  5  4\n .\n  .\n  .  \n  0  5 1 \n  1   \n  2  0 3  \n  3  5 2\n  4  3 2\n  5  4\n .\n  .\n  .  \n  0  5 1 \n  1   \n  2  0 3  \n  3  5 2\n  4  3 2\n  5  4\n .\n  .\n  .  \n  0  5 1 \n  1   \n  2  0 3  \n  3  5 2\n  4  3     \n  5  4\n .\n  .\n  .  \ndfs(0)\n  dfs(5)\n    dfs(4)\n      dfs(3)\n ", "start": 584, "end": 584}, "853": {"text": "3  5 2\n  4  3     \n  5  4\n .\n  .\n  .  \ndfs(0)\n  dfs(5)\n    dfs(4)\n      dfs(3)\n        check 5\n        dfs(2)\n          check 0\n          check 3\n        2 done\n      3 done\n      check 2\n    4 done\n  5 done\n  dfs(1)\n  1 done\n0 done\n572 CHAPTER 4 \u25a0 Graphs\n  \n \nbecause DFS is fundamentally a digraph-processing algorithm, with one representation \nof each edge. Following this trace is a worthwhile way to help cement your understand-\ning of depth-\ufb01rst search in digraphs.\n  M a r k - a n d - s w e e p  g a r b a g e  c o l l e c t i o n .  An im -\nportant application of multiple-source reach-\nability is found in typical memory-management \nsystems, including many implementations of \nJava. A digraph where each vertex represents \nan object and each edge represents a reference \nto an object is an appropriate model for the \nmemory usage of a running Java program. At \nany point in the execution of a program, certain \nobjects are known to be directly accessible, and \nany object not reachable from that set of objects \ncan be returned to available memory. A mark-\nand-sweep garbage collection strategy reserves \none bit per object for the purpose of garbage \ncollection, then periodically marks the set of \npotentially accessible objects by running a di-\ngraph reachability algorithm like DirectedDFS\nand sweeps through all objects, collecting the \nunmarked ones for use for new objects.\nFinding paths in digraphs. DepthFirstPaths (Algorithm 4.1  on page 536) and \nBreadthFirstPaths (Algorithm 4.2 on page 540) ", "start": 584, "end": 585}, "854": {"text": "\nunmarked ones for use for new objects.\nFinding paths in digraphs. DepthFirstPaths (Algorithm 4.1  on page 536) and \nBreadthFirstPaths (Algorithm 4.2 on page 540) are also fundamentally digraph-\nprocessing algorithms. Again, the identical APIs and code (with Graph changed to \nDigraph) effectively solve the following problems:\n   S i n g l e - s o u r c e  d i r e c t e d  p a t h s .  Given a digraph and a source vertex s, support \nqueries of the form Is there a directed path from s to a given target vertex  v? If so, \n\ufb01nd such a path. \nSingle-source shortest directed paths. Given a digraph and a source vertex s, \nsupport queries of the form Is there a directed path from s to a given target vertex \nv? If so, \ufb01nd a shortest such path (one with a minimal number of edges).\nOn the booksite and in the exercises at the end of this section, we refer to these solu-\ntions as  DepthFirstDirectedPaths and  BreadthFirstDirectedPaths, respectively. \nGarbage collection scenario\ndirectly\naccessible\nobjects\npotentially\n accessible\nobjects\nobjects\n available\nfor collection\n5734.2 \u25a0 Directed Graphs\n  \n    C y c l e s  a n d   D A G s  Directed cycles are of partic-\nular importance in applications that involve process-\ning digraphs. Identifying directed cycles in a typical \ndigraph can be a challenge without the help of a \ncomputer, as shown at right. In principle, a digraph \nmight have a huge number of cycles; in practice, we \ntypically focus on a small number of them, or simply \nare interested in knowing that none are present.\nTo  m o t iv a te ", "start": 585, "end": 586}, "855": {"text": "a digraph \nmight have a huge number of cycles; in practice, we \ntypically focus on a small number of them, or simply \nare interested in knowing that none are present.\nTo  m o t iv a te  t h e  s t u d y  o f  t h e  ro l e  o f  d i re c te d  c yc l e s  \nin digraph processing we consider, as a running ex-\nample, the following prototypical application where \ndigraph models arise directly: \n   S c h e d u l i n g  p r o b l e m s .  A widely applicable problem-solving model has to do with ar -\nranging for the completion of a set of jobs, under a set of constraints, by specifying \nwhen and how the jobs are to be performed. Constraints might involve functions of \nthe time taken or other resources consumed by the jobs. The most important type \nof constraints is precedence constraints, which specify that certain tasks must be per -\nformed before certain others. Different types of additional constraints lead to many \ndifferent types of scheduling problems, of varying dif\ufb01culty. Literally thousands of dif-\nferent problems have been studied, and researchers still seek better algorithms for many \nof them. As an example, consider a college student planning a course schedule, under \nthe constraint that certain courses are prerequisite for certain other courses, as in the \nexample below. \nDoes this digraph have a directed cycle?\nA precedence-constrained scheduling problem\nAlgorithms\nDatabases\nScientific Computing\nTheoretical CS\nIntroduction to CS\nAdvanced Programming\nComputational Biology\nArtificial Intelligence\nCalculusLinear Algebra\nRobotics\nMachine Learning Neural Networks\n574 CHAPTER 4 \u25a0 Graphs\n If we further assume that the student can take only one course at a time, we have an \ninstance of the following problem:\n P r e c e d ", "start": 586, "end": 587}, "856": {"text": "Algebra\nRobotics\nMachine Learning Neural Networks\n574 CHAPTER 4 \u25a0 Graphs\n If we further assume that the student can take only one course at a time, we have an \ninstance of the following problem:\n P r e c e d e n c e - c o n s t r a i n e d  s c h e d u l i n g .  Given a set of jobs to be completed, with \nprecedence constraints that specify that certain jobs have to be completed before \ncertain other jobs are begun, how can we schedule the jobs such that they are all \ncompleted while still respecting the constraints?\nFor any such problem, a digraph model is immediate, with \nvertices corresponding to jobs and directed edges cor-\nresponding to precedence constraints. For economy, we \nswitch the example to our standard model with vertices \nlabeled as integers, as shown at left. In digraphs, prece-\ndence-constrained schedul-\ning amounts to the following \nfundamental problem: \nTopolog ical sor t. Given a digraph, put the vertices in \norder such that all its directed edges point from a ver-\ntex earlier in the order to a vertex later in the order (or \nreport that doing so is not possible). \nA topological order for our example model is shown at \nright. All edges point down, so it clearly represents a so -\nlution to the precedence-constrained scheduling prob -\nlem that this digraph models: the student can satisfy all \ncourse prerequisites by taking the courses in this order. \nThis application is typical\u2014some other representative \napplications are listed in the table below.\nStandard digraph model\nTopological sor t\nedges all\npoint down \nprerequisites\nall satisfied\nCalculus\nbraLinear Algeb\n to CSIntro\nrogrammingAdvanced Pr\nAlgorith\nCTheoretical C\nelligenceArtificial Inte\nRobotics\nrningMachine ", "start": 587, "end": 587}, "857": {"text": "all\npoint down \nprerequisites\nall satisfied\nCalculus\nbraLinear Algeb\n to CSIntro\nrogrammingAdvanced Pr\nAlgorith\nCTheoretical C\nelligenceArtificial Inte\nRobotics\nrningMachine Lea\norksNeural Netw\nDatabases\nmputingScienti  m\nnal BiologyComputation\napplication vertex edge\njob schedule job precedence constraint\ncourse schedule course prerequisite\ninheritance Java class extends\nspreadsheet cell formula\nsymbolic links file name link\nTypical topological-sort applications\n5754.2 \u25a0 Directed Graphs\n Cycles in digraphs. If job x must be completed before job y, job y before job z, and \njob z before job x, then someone has made a mistake, because those three constraints \ncannot all be satis\ufb01ed. In general, if a precedence-constrained scheduling problem has \na directed cycle, then there is no feasible solution. T o check for such errors, we need to \nbe able to solve the following problem:\n  D i r e c t e d  c y c l e  d e t e c t i o n .  Does a given digraph have a directed cycle? If so, \ufb01nd \nthe vertices on some such cycle, in order from some vertex back to itself.\nA graph may have an exponential number of cycles (see Exercise 4.2.11) so we only ask \nfor one cycle, not all of them. For job scheduling and many other applications it is re-\nquired that no directed cycle exists, so digraphs where they are absent play a special role:\n D e f i n i t i o n .  A directed  acyclic graph (DAG) is a digraph with no directed cycles.\n \nSolving the directed cycle detection problem thus answers the following question: Is a \ngiven digraph a DAG ? Developing a depth-\ufb01rst-search-based ", "start": 587, "end": 588}, "858": {"text": "directed  acyclic graph (DAG) is a digraph with no directed cycles.\n \nSolving the directed cycle detection problem thus answers the following question: Is a \ngiven digraph a DAG ? Developing a depth-\ufb01rst-search-based solution to this problem \nis not dif\ufb01cult, based on the fact that the recursive call stack maintained by the system \nrepresents the \u201ccurrent\u201d directed path under consideration (like the string back to the \nentrance in Tremaux maze exporation). If we ever \ufb01nd a directed edge v->w to a vertex \nw that is on that stack, we have found a cycle, since the stack is evidence of a directed \npath from w to v, and the edge v->w completes the cycle. Moreover, the absence of any \nsuch back edges implies that the graph is acyclic. DirectedCycle on the facing page \nuses this idea to implement the following API:\npublic class  DirectedCycle\nDirectedCycle(Digraph G) cycle-finding constructor\nboolean hasCycle() does G have a directed cycle?\nIterable<Integer> cycle() vertices on a cycle (if one exists)\n A P I  f o r  r e a c h a b i l i t y  i n  d i g r a p h s\nFinding a directed cycle in a digraph\ndfs(0)\n  dfs(5)\n    dfs(4)\n      dfs(3)\n        check 5\n  marked[]        edgeTo[]            onStack[]\n0 1 2 3 4 5 ...   0 1 2 3 4 5 ...   0 1 2 3 4 5 ... \n1 0 0 0 0 0       - - - - - 0       1 0 0 0 0 0 \n1 0 0 0 0 1       - ", "start": 588, "end": 588}, "859": {"text": "\n1 0 0 0 0 0       - - - - - 0       1 0 0 0 0 0 \n1 0 0 0 0 1       - - - - 5 0       1 0 0 0 0 1 \n1 0 0 0 1 1       - - - 4 5 0       1 0 0 0 1 1 \n1 0 0 1 1 1       - - - 4 5 0       1 0 0 1 1 1 \n576 CHAPTER 4 \u25a0 Graphs\n  F i n d i n g  a  d i r e c t e d  c y c l e\npublic class  DirectedCycle \n{\n   private boolean[] marked;\n   private int[] edgeTo;\n   private Stack<Integer> cycle;   // vertices on a cycle (if one exists)\n   private boolean[] onStack;      // vertices on recursive call stack \n   public DirectedCycle(Digraph G)\n   {\n      onStack = new boolean[G.V()];\n      edgeTo  = new int[G.V()];\n      marked  = new boolean[G.V()];\n      for (int v = 0; v < G.V(); v++)\n          if (!marked[v]) dfs(G, v);\n   }\n   private void dfs(Digraph G, int v)\n   {\n      onStack[v] = true;\n      marked[v] = true;\n      for (int w : G.adj(v))\n         if (this.hasCycle()) return;\n         else if (!marked[w])\n         {  edgeTo[w] = v; dfs(G, w);  }\n         else if (onStack[w])\n         {\n            cycle = new Stack<Integer>();\n            for (int x = v; x != w; x = edgeTo[x])\n               cycle.push(x);\n ", "start": 588, "end": 589}, "860": {"text": "= v; dfs(G, w);  }\n         else if (onStack[w])\n         {\n            cycle = new Stack<Integer>();\n            for (int x = v; x != w; x = edgeTo[x])\n               cycle.push(x);\n            cycle.push(w); \n            cycle.push(v);\n         }\n      onStack[v] = false;\n   }\n   public boolean hasCycle()\n   {  return cycle != null;  }\n   public Iterable<Integer> cycle()\n   {  return cycle;  } \n}\n \nThis class adds to our standard recursive dfs() a boolean array onStack[] to keep track of the verti-\nces for which the recursive call has not completed. When it \ufb01nds an edge v->w to a vertex w that is on \nthe stack, it has discovered a directed cycle, which it can recover by following edgeTo[] links. \nTrace of cycle computation\nedgeTo[]\n  0    \n  1  \n  2  \n  3  4\n  4  5\n  5  0\n  \n3  5  3   3\n3  5  4   4 3\n3  5  4   5 4 3\n3  5  4   3 5 4 3\nv  w  x  cycle\n5774.2 \u25a0 Directed Graphs  \nWhen executing dfs(G, v), we have followed a directed path from the source to v. To \nkeep track of this path, DirectedCycle maintains a vertex-indexed array onStack[]\nthat marks the vertices on the recursive call stack (by setting onStack[v] to true\non entry to dfs(G, v)  and to false on exit). DirectedCycle also maintains an \nedgeTo[] array so that it can return the cycle when it is detected, in the same way as \nDepthFirstPaths (page ", "start": 589, "end": 590}, "861": {"text": "to dfs(G, v)  and to false on exit). DirectedCycle also maintains an \nedgeTo[] array so that it can return the cycle when it is detected, in the same way as \nDepthFirstPaths (page 536) and BreadthFirstPaths (page 540) return paths.\n D e p t h - \ufb01 r s t  o r d e r s  a n d   t o p o l o g i c a l  s o r t .  Precedence-constrained scheduling amounts \nto computing a topological order for the vertices of a DAG, as in this API:\npublic class    Topological \nTopological(Digraph G) topological-sorting constructor\nboolean isDAG() is G a DAG?\nIterable<Integer> order() vertices in topological order\n A P I  f o r  t o p o l o g i c a l  s o r t i n g\nProposition E.  A digraph has a  topological order if and only if it is a DAG.\nProof: If the digraph has a directed cycle, it has no topological order. Conversely, \nthe algorithm that we are about to examine computes a topological order for any \ngiven DAG.\n \nRemarkably, it turns out that we have already seen an algorithm for topological sort: a \none-line addition to our standard recursive DFS does the job! T o convince you of this \nfact, we begin with the class DepthFirstOrder on page 580. It is based on the idea that \ndepth-\ufb01rst search visits each vertex exactly once. If we save the vertex given as argument \nto the recursive dfs() in a data structure, then iterate through that data structure, we \nsee all the graph vertices, in order determined by the nature of the data structure and \nby whether we do the save before or after the recursive calls. Three vertex orderings are \nof ", "start": 590, "end": 590}, "862": {"text": "then iterate through that data structure, we \nsee all the graph vertices, in order determined by the nature of the data structure and \nby whether we do the save before or after the recursive calls. Three vertex orderings are \nof interest in typical applications:\n\u25a0       Preorder : Put the vertex on a queue before the recursive calls.\n\u25a0 Postorder : Put the vertex on a queue after the recursive calls.\n\u25a0    Reverse postorder : Put the vertex on a stack after the recursive calls.\nA trace of DepthFirstOrder for our sample DAG is given on the facing page. It is simple \nto implement and supports pre(), post(), and reversePost() methods that are use-\nful for advanced graph-processing algorithms. For example, order() in Topological\nconsists of a call on reversePost().\n578 CHAPTER 4 \u25a0 Graphs\n dfs(0)\n  dfs(5)\n    dfs(4)\n    4 done\n  5 done\n  dfs(1)\n  1 done\n  dfs(6)\n    dfs(9)\n      dfs(11)\n        dfs(12)\n        12 done\n      11 done\n      dfs(10)\n      10 done\n      check 12\n    9 done\n    check 4\n  6 done\n0 done\ncheck 1\ndfs(2)\n  check 0\n  dfs(3)\n    check 5\n  3 done\n2 done\ncheck 3\ncheck 4\ncheck 5\ncheck 6\ndfs(7)\n  check 6\n7 done\ndfs(8)\n  check 7\n8 done\ncheck 9\ncheck 10\ncheck 11\ncheck 12\nComputing depth-first orders in a digraph (preorder, postorder, and reverse postorder)\n0\n0 5\n0 5 4\n0 5 4 1\n0 5 ", "start": 590, "end": 591}, "863": {"text": "12\nComputing depth-first orders in a digraph (preorder, postorder, and reverse postorder)\n0\n0 5\n0 5 4\n0 5 4 1\n0 5 4 1 6\n0 5 4 1 6 9\n0 5 4 1 6 9 11\n0 5 4 1 6 9 11 12\n0 5 4 1 6 9 11 12 10\n0 5 4 1 6 9 11 12 10 2\n0 5 4 1 6 9 11 12 10 2 3\n0 5 4 1 6 9 11 12 10 2 3 7\n0 5 4 1 6 9 11 12 10 2 3 7 8\n4\n4 5\n4 5 1\n4 5 1 12\n4 5 1 12 11\n4 5 1 12 11 10\n4 5 1 12 11 10 9\n4 5 1 12 11 10 9 6\n4 5 1 12 11 10 9 6 0\n4 5 1 12 11 10 9 6 0 3\n4 5 1 12 11 10 9 6 0 3 2\n4 5 1 12 11 10 9 6 0 3 2 7\n4 5 1 12 11 10 9 6 0 3 2 7 8\n4\n5 4\n1 5 4\n12 ", "start": 591, "end": 591}, "864": {"text": "0 3 2 7\n4 5 1 12 11 10 9 6 0 3 2 7 8\n4\n5 4\n1 5 4\n12 1 5 4\n11 12 1 5 4\n10 11 12 1 5 4\n9 10 11 12 1 5 4\n6 9 10 11 12 1 5 4\n0 6 9 10 11 12 1 5 4\n3 0 6 9 10 11 12 1 5 4\n2 3 0 6 9 10 11 12 1 5 4\n7 2 3 0 6 9 10 11 12 1 5 4\n8 7 2 3 0 6 9 10 11 12 1 5 4\npre post reversePost\nreverse\npostorder\npostorder\nis order\nin which\nvertices\nare done\npreorder\nis order of\ndfs() calls\nqueue queue stack\n5794.2 \u25a0 Directed Graphs\n  D e p t h - f i r s t  s e a r c h  v e r t e x  o r d e r i n g  i n  a  d i g r a p h\npublic class  DepthFirstOrder \n{\n   private boolean[] marked;     \n   private Queue<Integer> pre;          // vertices in preorder\n   private Queue<Integer> post;         // vertices in postorder\n   private Stack<Integer> reversePost;  // vertices in reverse postorder\n   public DepthFirstOrder(Digraph G)\n   {\n      pre           = new Queue<Integer>();\n      post          = new Queue<Integer>();\n      reversePost ", "start": 591, "end": 592}, "865": {"text": "postorder\n   private Stack<Integer> reversePost;  // vertices in reverse postorder\n   public DepthFirstOrder(Digraph G)\n   {\n      pre           = new Queue<Integer>();\n      post          = new Queue<Integer>();\n      reversePost   = new Stack<Integer>();\n      marked  = new boolean[G.V()];\n      for (int v = 0; v < G.V(); v++)\n          if (!marked[v]) dfs(G, v);\n   }\n   private void dfs(Digraph G, int v)\n   {\n      pre.enqueue(v);\n      marked[v] = true;\n      for (int w : G.adj(v))\n         if (!marked[w])\n            dfs(G, w);\n      post.enqueue(v);\n      reversePost.push(v);\n   }\n   public Iterable<Integer> pre() \n   {  return pre;  }\n   public Iterable<Integer> post()\n   {  return post;  }\n   public Iterable<Integer> reversePost()\n   {  return reversePost;  }\n}\n This class enables clients to iterate through the vertices in various orders de\ufb01ned by depth-\ufb01rst search. \nThis ability is very useful in the development of advanced digraph-processing algorithms, because the \nrecursive nature of the search enables us to prove properties of the computation (see, for example, \nProposition F). \n580 CHAPTER 4 \u25a0 Graphs ALGORITHM 4.5    Topological sort\npublic class Topological \n{\n   private Iterable<Integer> order;       // topological order\n   public Topological(Digraph G)\n   {\n      DirectedCycle cyclefinder = new DirectedCycle(G);\n      if (!cyclefinder.hasCycle())\n      {\n         DepthFirstOrder dfs = new DepthFirstOrder(G);\n         order = dfs.reversePost();\n      }\n   }\n   public Iterable<Integer> order() \n   {  return order;  }\n   public boolean isDAG() \n   {  return order == null;  }\n   public static void ", "start": 592, "end": 593}, "866": {"text": "dfs.reversePost();\n      }\n   }\n   public Iterable<Integer> order() \n   {  return order;  }\n   public boolean isDAG() \n   {  return order == null;  }\n   public static void main(String[] args)\n   {\n      String filename = args[0];\n      String separator = args[1];\n      SymbolDigraph sg = new SymbolDigraph(filename, separator);\n      Topological top = new Topological(sg.G());\n      for (int v : top.order())\n         StdOut.println(sg.name(v));\n   }\n}\nThis DepthFirstOrder and DirectedCycle client returns a topological order for a DAG. The test \nclient solves the precedence-constrained scheduling problem for a SymbolDigraph. The instance \nmethod order() returns null if the given digraph is not a DAG and an iterator giving the vertices in \ntopological order otherwise. The code for   SymbolDigraph is omitted because it is precisely the same \nas for SymbolGraph (page 552), with Digraph replacing Graph everywhere.\n5814.2 \u25a0 Directed Graphs Proposition F.  Reverse postorder in a DAG is a  topological sort.\nProof: Consider any edge v->w. One of the following three cases must hold when \ndfs(v) is called (see the diagram on page 583):\n\u25a0 dfs(w) has already been called and has returned (w is marked).\n\u25a0 dfs(w) has not yet been called (w is unmarked), so v->w will cause dfs(w) to \nbe called (and return), either directly or indirectly,  before dfs(v) returns.\n\u25a0 dfs(w) has been called and has not yet returned when dfs(v) is called. The \nkey to the proof is that this case is impossible in a DAG, because the recursive \ncall chain implies a path from w to v and v->w would complete a directed \ncycle.\nIn the two possible cases, dfs(w) is ", "start": 593, "end": 594}, "867": {"text": "the proof is that this case is impossible in a DAG, because the recursive \ncall chain implies a path from w to v and v->w would complete a directed \ncycle.\nIn the two possible cases, dfs(w) is done before dfs(v), so w appears before v in \npostorder and after v in reverse postorder. Thus, each edge v->w points from a ver-\ntex earlier in the order to a vertex later in the order, as desired. \n% more jobs.txt\nAlgorithms/Theoretical CS/Databases/Scientific Computing \nIntroduction to CS/Advanced Programming/Algorithms \nAdvanced Programming/Scientific Computing \nScientific Computing/Computational Biology \nTheoretical CS/Computational Biology/Artificial Intelligence \nLinear Algebra/Theoretical CS \nCalculus/Linear Algebra \nArtificial Intelligence/Neural Networks/Robotics/Machine Learning \nMachine Learning/Neural Networks\n% java Topological jobs.txt \"/\"\nCalculus \nLinear Algebra \nIntroduction to CS \nAdvanced Programming \nAlgorithms \nTheoretical CS \nArtificial Intelligence \nRobotics \nMachine Learning \nNeural Networks \nDatabases \nScientific Computing \nComputational Biology\n582 CHAPTER 4 \u25a0 Graphs\n Topological (Algorithm 4.5 on page 581) is\nan  implementation that uses depth-\ufb01rst search to \ntopologically sort a DAG. A trace is given at right.\nProposition G.  With DFS, we can  topologically \nsort a DAG in time proportional to V/H11001E.\nProof: Immediate from the code. It uses one \ndepth-\ufb01rst search to ensure that the graph has \nno directed cycles, and another to do the re-\nverse postorder ordering. Both involve examin-\ning all the edges and all the vertices, and thus \ntake time proportional to V/H11001E.\nDespite the simplicity of this algorithm, it escaped \nattention for many years, in favor of a more ", "start": 594, "end": 595}, "868": {"text": "involve examin-\ning all the edges and all the vertices, and thus \ntake time proportional to V/H11001E.\nDespite the simplicity of this algorithm, it escaped \nattention for many years, in favor of a more intui-\ntive algorithm based on maintaining a queue of \nsources (see Exercise 4.2.30).\nIn practice, topological sorting and cycle detec -\ntion go hand in hand, with cycle detection play-\ning the role of a debugging tool. For example, in \na job-scheduling application, a directed cycle in \nthe underlying digraph represents a mistake that \nmust be corrected, no matter how the schedule was \nformulated. Thus, a job-scheduling application is \ntypically a three-step process:\n\u25a0 Specify the tasks and precedence constraints.\n\u25a0 Make sure that a feasible solution exists, by \ndetecting and removing cycles in the under-\nlying digraph until none exist.\n\u25a0 Solve the scheduling problem, using topo-\nlogical sort. \nSimilarly, any changes in the schedule can be \nchecked for cycles (using DirectedCycle), then a \nnew schedule computed (using Topological). \ndfs(0)\n dfs(5)\n   dfs(4)\n   4 done\n5 done\n dfs(1)\n1 done\n dfs(6)\n   dfs(9)\n     dfs(11)\n       dfs(12)\n       12 done\n     11 done\n     dfs(10)\n     10 done\n     check 12\n   9 done\n   check 4\n 6 done\n0 done\ncheck 1\ndfs(2)\n check 0\n dfs(3)\n   check 5\n3 done\n2 done\ncheck 3\ncheck 4\ncheck 5\ncheck 6\ndfs(7)\n check 6\n7 done\ndfs(8)\n check 7\n8 done\ncheck 9\ncheck 10\ncheck 11\ncheck ", "start": 595, "end": 595}, "869": {"text": "3\ncheck 4\ncheck 5\ncheck 6\ndfs(7)\n check 6\n7 done\ndfs(8)\n check 7\n8 done\ncheck 9\ncheck 10\ncheck 11\ncheck 12\nReverse postorder in a DAG is a topological sort\nall edges point up ;\nturn upside down\nfor a topological sort\nreverse postorder\nis reverse of order\nin which vertices\nare done (read up)\ndfs(6) for 7\u2019s marked\nneighbor 6 was done\nbefore dfs(7) is done\nso 7->6 points up \ndfs(5) for 0\u2019s unmarked\nneighbor 5 is done\nbefore dfs(0) is done\nso 0->5 points up \n5834.2 \u25a0 Directed Graphs\n      S t r o n g  c o n n e c t i v i t y  i n  d i g r a p h s  We have been careful to maintain a distinction \nbetween reachability in digraphs and connectivity in undirected graphs. In an undirect-\ned graph, two vertices v and w are connected if there is a path connecting \nthem\u2014we can use that path to get from v to w or to get from w to v. In \na digraph, by contrast, a vertex w is reachable from a vertex v if there is \na directed path from v to w, but there may or may not be a directed path \nback to v from w. To complete our study of digraphs, we consider the \nnatural analog of connectivity in undirected graphs.\n \nDefinition. Two ver tices v and w are  strongly connected if they are \nmutually reachable: that is, if there is a directed path from v to w\nand a directed path from w to v. A digraph is strongly connected if all \nits vertices are strongly connected to ", "start": 595, "end": 596}, "870": {"text": "if they are \nmutually reachable: that is, if there is a directed path from v to w\nand a directed path from w to v. A digraph is strongly connected if all \nits vertices are strongly connected to one another.\n \nSeveral examples of strongly connected graphs are given in the \ufb01gure \nat left. As you can see from the examples, cycles play an important role \nin understanding strong connectivity. Indeed, recalling that a general \ndirected cycle is a directed cycle that may have repeated vertices, it is easy \nto see that two vertices are strongly connected if and only if there exists a \ngeneral directed cycle that contains them both. (Proof : compose the paths \nfrom v to w and from w to v.) \n  S t r o n g  c o m p o n e n t s .  Like connectivity in undirected graphs, strong connectivity in di-\ngraphs is an  equivalence relation on the set of vertices, as it has the following properties:\n\u25a0 Re\ufb02exive : Every vertex v is strongly connected to itself.\n\u25a0 Symmetric : If v is strongly connected to w, then w is strongly connected to v.\n\u25a0     Transitive : If v is strongly connected to w and w is strongly connected to x, then v\nis also strongly connected to x.\nAs an equivalence relation, strong connectivity partitions the vertices into equivalence \nclasees. The equivalence classes are maximal subsets \nof vertices that are strongly connected to one anoth-\ner, with each vertex in exactly one subset. We refer \nto these subsets as strongly connected components , \nor strong components for short. Our sample digraph \ntinyDG.txt has \ufb01ve strong components, as shown \nin the diagram at right. A digraph with V vertices \nhas between 1 and V strong components\u2014a strongly \nStrongly connected digraphs\nA digraph and its strong components\n584 CHAPTER 4 \u25a0 Graphs\n ", "start": 596, "end": 596}, "871": {"text": "the diagram at right. A digraph with V vertices \nhas between 1 and V strong components\u2014a strongly \nStrongly connected digraphs\nA digraph and its strong components\n584 CHAPTER 4 \u25a0 Graphs\n connected digraph has 1 strong component and a DAG has V strong components. \nNote that the strong components are de\ufb01ned in terms of the vertices, not the edges. \nSome edges connect two vertices in the same strong component; some other edges con-\nnect vertices in different strong components. The latter are not found on any directed \ncycle. Just as identifying connected components is typically important in processing \nundirected graphs, identifying strong components is typically important in processing \ndigraphs. \nExamples of applications. Strong connectivity is  a useful abstraction in understand-\ning the structure of a digraph, highlighting inter -\nrelated sets of vertices (strong components). For \nexample, strong components can help textbook \nauthors decide which topics should be grouped to-\ngether and software developers decide how to orga-\nnize program modules. The \ufb01gure below shows an \nexample from ecology. It illustrates a digraph that \nmodels the food web connecting living organisms, \nwhere vertices represent species and an edge from \none vertex to another indicates that an organism of \nthe species indicated by the point from vertex con-\nsumes organisms of the species indicated by the \npoint to vertex for food. Scienti\ufb01c studies on such digraphs (with carefully chosen sets \nof species and carefully documented relationships) play an important role in helping \necologists answer basic questions about ecological systems. Strong components in such \ndigraphs can help ecologists understand en-\nergy \ufb02ow in the food web. The \ufb01gure on page \n591 shows a digraph model of web content, \nwhere vertices represent pages and edges rep-\nresent hyperlinks from one page to another. \nStrong components in such a digraph can help ", "start": 596, "end": 597}, "872": {"text": "The \ufb01gure on page \n591 shows a digraph model of web content, \nwhere vertices represent pages and edges rep-\nresent hyperlinks from one page to another. \nStrong components in such a digraph can help \nnetwork engineers partition the huge number \nof pages on the web into more manageable siz-\nes for processing. Further properties of these \napplications and other examples are addressed \nin the exercises and on the booksite. \nSmall subset of food web digraph\ngrass\nalgae\nmosquito\nslug\nant\nfox\nworm\nfrog salamander\nshrew\nsnake fish\negret\napplication vertex edge\nweb page hyperlink\ntextbook topic reference\n software module call\nfood web organism predator-prey\nrelationship\nTypical strong-component applications\n5854.2 \u25a0 Directed Graphs\n Accordingly, we need the following API, the analog for digraphs of CC (page 543):\npublic class    SCC \nSCC(Digraph G) preprocessing constructor\nboolean stronglyConnected(int v, int w) are v and w strongly connected?\nint count() number of strong components\nint id(int v) component identifier for v\n( between 0 and count()-1 )\n A P I  f o r  s t r o n g  c o m p o n e n t s\nA quadratic algorithm to compute strong components is not dif\ufb01cult to develop (see \nExercise 4.2.23), but (as usual) quadratic time and space requirements are prohibitive  \nfor huge digraphs that arise in practical applications like the ones just described.\n    K o s a r a j u \u2019 s  a l g o r i t h m .  We saw in CC (Algorithm 4.3 on page 544) that computing con-\nnected components in undirected graphs is a simple application of depth-\ufb01rst search. \nHow can we ef\ufb01ciently ", "start": 597, "end": 598}, "873": {"text": "saw in CC (Algorithm 4.3 on page 544) that computing con-\nnected components in undirected graphs is a simple application of depth-\ufb01rst search. \nHow can we ef\ufb01ciently compute strong components in digraphs? Remarkably, the im-\nplementation KosarajuSCC on the facing page does the job with just a few lines of code \nadded to CC, as follows:\n\u25a0 Given a digraph G, use DepthFirstOrder to compute the reverse postorder of \nits  reverse, G R.\n\u25a0 Run standard DFS on G, but consider the unmarked vertices in the order just \ncomputed instead of the standard numerical order. \n\u25a0 All vertices reached on a call to the recursive dfs() from the constructor are in a \nstrong component (!), so identify them as in CC.\n..\n.\n  dfs(v)\n  .  .  .\n  v done.\n.\n.\n     dfs(s)\n     .     .     .\n     s done.\n.\n.\n \n.\n.\n.\n  dfs(s)\n  .  .  .     dfs(v)     .     .     .\n     v done  .  .  .\n  s done.\n.\n.\n \nimpossible since GR \nhas a path from v to s\n DFS in GR (DepthFirstOrder)\n \n.\n.\n. dfs(s)\n  .  .  .     dfs(v)     .     .     .\n     v done  .  .  .\n  s done.\n.\n.\n \npremise is that \nv is reachable from s\nso G must have\na path\nfrom s to v\n GR must have\na path\nfrom s to v\nv must be done\nbefore s\nelse dfs(v)\nwould be called\nbefore dfs(s)\nin  G\nDFS in G (KosarajuSCC)\nProof of correctness for Kosaraju\u2019s algorithm   \n586 CHAPTER 4 \u25a0 Graphs\n ALGORITHM 4.6 ", "start": 598, "end": 599}, "874": {"text": "dfs(v)\nwould be called\nbefore dfs(s)\nin  G\nDFS in G (KosarajuSCC)\nProof of correctness for Kosaraju\u2019s algorithm   \n586 CHAPTER 4 \u25a0 Graphs\n ALGORITHM 4.6   Kosaraju\u2019s algorithm for computing strong components\npublic class  KosarajuSCC \n{\n   private boolean[] marked;   // reached vertices\n   private int[] id;           // component identifiers\n   private int count;          // number of strong components\n   public KosarajuSCC(Digraph G)\n   {\n      marked = new boolean[G.V()];\n      id = new int[G.V()];\n      DepthFirstOrder order = new DepthFirstOrder(G.reverse());\n      for (int s : order.reversePost()) \n        if (!marked[s])\n         {  dfs(G, s); count++;  }\n    }\n   private void dfs(Digraph G, int v)\n   {\n      marked[v] = true;\n      id[v] = count;\n      for (int w : G.adj(v))\n         if (!marked[w])\n             dfs(G, w);\n   }\n   public boolean stronglyConnected(int v, int w)\n   {  return id[v] == id[w];  }\n   public int id(int v)\n   {  return id[v];  }\n   public int count()\n   {  return count;  }\n}\n \nThis implementation differs from CC (Algorithm 4.3) only in the highlighted code (and in the im-\nplementation of main() where we use the code on page 543, with Graph changed to Digraph,, and CC \nchanged to KosarajuSCC). T o \ufb01nd strong components, it does a depth-\ufb01rst search in the reverse \ndigraph to produce a vertex order (reverse postorder of that search) for use in a depth-\ufb01rst search of \nthe given digraph.  \n% java KosarajuSCC ", "start": 599, "end": 599}, "875": {"text": "depth-\ufb01rst search in the reverse \ndigraph to produce a vertex order (reverse postorder of that search) for use in a depth-\ufb01rst search of \nthe given digraph.  \n% java KosarajuSCC tinyDG.txt \n5 components \n1 \n0 5 4 3 2 \n11 12 9 10 \n6 \n8 7\n5874.2 \u25a0 Directed Graphs Kosaraju\u2019s algorithm is an extreme example of a method that is easy to code but dif -\n\ufb01cult to understand. Despite its mysterious nature, if you follow the proof of the fol-\nlowing proposition step by step, with reference to the diagram on page 586, you can be \nconvinced that the algorithm is correct: \nProposition H.  In a  DFS of a digraph G where marked vertices are considered in \nreverse postorder given by a DFS of the digraph\u2019s reverse G R (Kosaraju\u2019s algorithm), \nthe vertices reached in each call of the recursive method from the constructor are \nin a strong component.\nProof: First, we prove by contradiction that every vertex v that is strongly connected \nto s is reached by the call  dfs(G, s) in the constructor. Suppose a vertex v that is \nstrongly connected to s is not reached by dfs(G, s). Since there is a path from s to \nv, v must have been previously marked. But then, since there is a path from v to s, s\nwould have been marked during the call  dfs(G, v) and the constructor would not \ncall dfs(G, s), a contradiction.\nSecond, we prove that every vertex v reached by the call dfs(G, s) in the construc-\ntor is strongly connected to s. Let v be a vertex reached by the call dfs(G, s). Then, \nthere is a path from s to v in G, ", "start": 599, "end": 600}, "876": {"text": "the call dfs(G, s) in the construc-\ntor is strongly connected to s. Let v be a vertex reached by the call dfs(G, s). Then, \nthere is a path from s to v in G, so it remains only to prove that there is a path from \nv to s in G. This statement is equivalent to saying that there is a path from s to v in \nGR, so it remains only to prove that there is a path from s to v in GR.\nThe crux of the proof is that the reverse postorder construction implies that \ndfs(G, v) must have been done before dfs(G, s) during the DFS of GR, leaving \njust two cases to consider for dfs(G, v): it might have been called\n\u25a0 before dfs(G, s) was called (and also done before dfs(G, s) was called)\n\u25a0 \n \nafter dfs(G, s) was called (and done before dfs(G, s) was done)\nThe \ufb01rst of these is not possible because there is a path from v to s in GR; the second \nimplies that there is a path from s to v in GR, completing the proof.\nA trace of Kosaraju\u2019s algorithm for tinyDG.txt is shown on the facing page. T o the \nright of each DFS trace appears a drawing of the digraph, with vertices appearing in \nthe order they are done. Thus reading up the reverse digraph drawing on the left gives \nreverse postorder, the order in which unmarked vertices are checked in the DFS of the \noriginal digraph. As you can see from the diagram, the second DFS calls dfs(1) (which \nmarks vertex 1)  then calls dfs(0) (which marks 5, 4, 3, and 2), then checks 2, 4, 5, and \n3, then calls dfs(11) (which marks ", "start": 600, "end": 600}, "877": {"text": "then calls dfs(0) (which marks 5, 4, 3, and 2), then checks 2, 4, 5, and \n3, then calls dfs(11) (which marks 11, 12, 9, and 10), then checks 9, 12, and 10, then \ncalls dfs(6) (which marks 6), and \ufb01nally dfs(7), which marks 7 and 8. \n588 CHAPTER 4 \u25a0 Graphs\n dfs(1)\n1 done\ndfs(0)\n  dfs(5)\n    dfs(4)\n      dfs(3)\n        check 5\n        dfs(2)\n          check 0\n          check 3\n        2 done\n      3 done\n      check 2\n    4 done\n  5 done\n  check 1\n0 done\ncheck 2\ncheck 4\ncheck 5\ncheck 3\ndfs(11)\n  check 4\n  dfs(12)\n    dfs(9)\n      check 11\n      dfs(10)\n        check 12\n      10 done\n    9 done\n  12 done\n11 done\ncheck 9\ncheck 12\ncheck 10\ndfs(6)\n  check 9\n  check 4\n  check 0\n6 done\ndfs(7)\n  check 6\n  dfs(8)\n    check 7\n    check 9\n  8 done\n7 done\ncheck 8\n    \nKosaraju\u2019s algorithm for finding strong components in digraphs\ncheck unmarked vertices in the order\n0 1 2 3 4 5 6 7 8 9 10 11 12\ndfs(0)\n  dfs(6)\n    dfs(7)\n      dfs(8)\n        check 7\n ", "start": 600, "end": 601}, "878": {"text": "2 3 4 5 6 7 8 9 10 11 12\ndfs(0)\n  dfs(6)\n    dfs(7)\n      dfs(8)\n        check 7\n      8 done\n    7 done\n  6 done\n  dfs(2)\n    dfs(4)\n      dfs(11)\n        dfs(9)\n          dfs(12)\n            check 11\n            dfs(10)\n              check 9\n            10 done\n          12 done\n          check 8\n          check 6\n        9 done\n      11 done\n      check 6\n      dfs(5)\n        dfs(3)\n          check 4\n          check 2\n        3 done\n        check 0\n      5 done\n    4 done\n    check 3\n  2 done\n0 done\ndfs(1)\n  check 0\n1 done\ncheck 2\ncheck 3\ncheck 4\ncheck 5\ncheck 6\ncheck 7\ncheck 8\ncheck 9\ncheck 10\ncheck 11\ncheck 12\nstrong \ncomponents\nreverse\npostorder\nfor use\nin second\ndfs()\n(read up )\nDFS in reverse digraph (ReversePost)\ncheck unmarked vertices in the order\n1 0 2 4 5 3 11 9 12 10 6 7 8\n DFS in original digraph\n5894.2 \u25a0 Directed Graphs\n A larger example, a very small subset of a digraph model of the web, is shown on the \nfacing page.\nKosaraju\u2019s algorithm solves the following analog of the connectivity problem for \nundirected graphs that we \ufb01rst posed in Chapter 1 and reintroduced in Section 4.1 \n(page 534):\n S t r o n g  c ", "start": 601, "end": 602}, "879": {"text": "page.\nKosaraju\u2019s algorithm solves the following analog of the connectivity problem for \nundirected graphs that we \ufb01rst posed in Chapter 1 and reintroduced in Section 4.1 \n(page 534):\n S t r o n g  c o n n e c t i v i t y .  Given a digraph, support queries of the form: Are two given \nvertices strongly connected ? and How many strong components does the digraph \nhave ?\nThat we can solve this problem in digraphs as ef\ufb01ciently as the corresponding con-\nnectivity problem in undirected graphs was an open research problem for some time \n(resolved by  R. E. Tarjan in the late 1970s). That such a simple solution is now available \nis quite surprising.\nProposition I.   Kosaraju\u2019s algorithm  uses preprocessing time and space proportion-\nal to V/H11001E to support constant-time strong connectivity queries in a digraph.\nProof: The algorithm computes the reverse of the digraph and does two depth-\ufb01rst \nsearches. Each of these three steps takes time proportional to V/H11001E. The reverse copy \nof the digraph uses space proportional to V/H11001E. \n R e a c h a b i l i t y  r e v i s i t e d .  With CC for undirected graphs, we can infer from the fact that \ntwo vertices v and w are connected that there is a path from v to w and a path (the same \none) from w to v. With KosarajuCC, we can infer from the fact that v and w are strongly \nconnected that there is a path from v to w and a path (a different one) from w to v. But \nwhat about pairs of vertices that are not strongly connected? There may be a path from \nv to w or a path from w to ", "start": 602, "end": 602}, "880": {"text": "from v to w and a path (a different one) from w to v. But \nwhat about pairs of vertices that are not strongly connected? There may be a path from \nv to w or a path from w to v or neither, but not both. \n  A l l - p a i r s  r e a c h a b i l i t y .  Given a digraph, support queries of the form Is there a \ndirected path from a given vertex v to another given vertex w?\nFor undirected graphs, the corresponding problem is equivalent to the connectivity \nproblem; for digraphs, it is quite different from the strong connectivity problem. Our \nCC implementation uses linear preprocessing time to support constant-time answers \nto such queries for undirected graphs. Can we achieve this performance for digraphs? \nThis seemingly innocuous question has confounded experts for decades. T o better \n590 CHAPTER 4 \u25a0 Graphs\n 18\n31\n6\n42 13\n28\n32\n49\n22\n45\n1 14\n40\n48\n7\n44\n10\n41 29\n0\n39\n11\n9\n12\n30\n26\n21\n46\n5\n24\n37\n43\n35\n47\n38\n23\n16\n36\n4\n3 17\n27\n20\n34\n15\n2\n19 33\n25\n8\nHow many strong components are there in this digraph?\n5914.2 \u25a0 Directed Graphs\n understand the challenge, consider the diagram at \nleft, which illustrates the following fundamental \nconcept:\n D e f i n i t i o n .  The     transitive closure of a digraph \nG is another digraph with the same set of ver-\ntices, but with an edge from v to w in the tran-\nsitive closure if and only if w is reachable from \nv in ", "start": 602, "end": 604}, "881": {"text": "of a digraph \nG is another digraph with the same set of ver-\ntices, but with an edge from v to w in the tran-\nsitive closure if and only if w is reachable from \nv in G. \n \nBy convention, every vertex is reachable from itself, \nso the transitive closure has V self-loops. Our sam-\nple digraph has just 13 directed edges, but its transi-\ntive closure has 102 out of a possible 169 directed \nedges. Generally, the transitive closure of a digraph \nhas many more edges than the digraph itself, and \nit is not at all unusual for a sparse graph to have a \ndense transitive closure. For example, the transitive \nclosure of a V-vertex directed cycle, which has V di-\nrected edges, is a complete digraph with V2 directed \nedges. Since transitive closures are typically dense, \nwe normally represent them with a matrix of boolean values, where the entry in row v\nand column w is true if and only if w is reachable from v. Instead of explicitly comput-\ning the transitive closure, we use depth-\ufb01rst search to implement the following API:\npublic class  TransitiveClosure \nTransitiveClosure(Digraph G) preprocessing constructor\nboolean reachable(int v, int w) is w reachable from v?\n A P I  f o r  a l l - p a i r s  r e a c h a b i l i t y\nThe code at the top of the next page is a straightforward implementation that uses \nDirectedDFS (Algorithm 4.4). This solution is ideal for small or dense digraphs, but \nit is not a solution for the large digraphs we might encounter in practice because the \nconstructor uses space proportional to  V2 and time proportional to V (V/H11001E): each of the \nV DirectedDFS objects ", "start": 604, "end": 604}, "882": {"text": "is not a solution for the large digraphs we might encounter in practice because the \nconstructor uses space proportional to  V2 and time proportional to V (V/H11001E): each of the \nV DirectedDFS objects takes space proportional to V (they all have marked[] arrays of \nsize V and examine E edges to compute the marks). Essentially, TransitiveClosure \nTransitive closure\n   0  1  2  3  4  5  6  7  8  9 10 11 12\n0  T  T  T  T  T  T  \n1     T\n2  T  T  T  T  T  T\n3  T  T  T  T  T  T\n4  T  T  T  T  T  T\n5  T  T  T  T  T  T\n6  T  T  T  T  T  T  T        T  T  T  T\n7  T  T  T  T  T  T  T  T  T  T  T  T  T\n8  T  T  T  T  T  T  T  T  T  T  T  T  T\n9  T  T  T  T  T  T           T  T  T  T\n10 T  T  T  T  T  T           T  T  T  T\n11 T  T  T  T  T  T           T  T  T  T\n12 T  T  T  T  T  T           T  T  T  T \n self-loop\n(gray )\n12 is\n reachable\nfrom  6\noriginal edge\n(red )\n592 CHAPTER 4 \u25a0 Graphs\n computes and stores the transitive closure of G, to support constant-time ", "start": 604, "end": 605}, "883": {"text": "T  T \n self-loop\n(gray )\n12 is\n reachable\nfrom  6\noriginal edge\n(red )\n592 CHAPTER 4 \u25a0 Graphs\n computes and stores the transitive closure of G, to support constant-time queries\u2014\nrow v in the transitive closure matrix is the marked[] array for the vth entry in the \nDirectedDFS[] in TransitiveClosure. Can we support constant-time queries with \nsubstantially less preprocessing time and substantially less space? A general solution \nthat achieves constant-time queries with \nsubstantially less than quadratic space is \nan unsolved research problem, with im-\nportant practical implications: for exam-\nple, until it is solved, we cannot hope to \nhave a practical solution to the all-pairs \nreachability problem for a giant digraph \nsuch as the web graph.\npublic class  TransitiveClosure \n{\n   private DirectedDFS[] all;\n   TransitiveClosure(Digraph G)\n   {\n      all = new DirectedDFS[G.V()];\n      for (int v = 0; v < G.V(); v++)\n         all[v] = new DirectedDFS(G, v);\n   }\n   boolean reachable(int v, int w)\n   {  return all[v].marked(w);  }\n}\n A l l - p a i r s  r e a c h a b i l i t y\n5934.2 \u25a0 Directed Graphs\n Summary In this section, we have introduced directed edges and digraphs, empha-\nsizing the relationship between digraph processing and corresponding problems for \nundirected graphs, as summarized in the following list of topics:\n\u25a0 Digraph nomenclature\n\u25a0 The idea that the representation and approach are essentially the same as for \nundirected graphs, but some digraph problems are more complicated\n\u25a0 Cycles, DAGs, topological sort, and precedence-constrainted scheduling\n\u25a0 Reachability, paths, and strong connectivity in digraphs\nThe ", "start": 605, "end": 606}, "884": {"text": "for \nundirected graphs, but some digraph problems are more complicated\n\u25a0 Cycles, DAGs, topological sort, and precedence-constrainted scheduling\n\u25a0 Reachability, paths, and strong connectivity in digraphs\nThe table below summarizes the implementations of digraph algorithms that we have \nconsidered (all but one of the algorithms are based on depth-\ufb01rst search). The prob -\nlems addressed are all simply stated, but the solutions that we have considered range \nfrom easy adaptations of corresponding algorithms for undirected graphs to an inge-\nnious and surprising solution. These algorithms are a starting point for several of the \nmore complicated algorithms that we consider in Section 4.4, when we consider edge-\nweighted digraphs. \nproblem solution reference\nsingle- and multiple-source reachability DirectedDFS page 571\nsingle-source directed paths DepthFirstDirectedPaths page 573\nsingle-source shortest directed paths BreadthFirstDirectedPaths page 573\ndirected cycle detection DirectedCycle page 577\ndepth-first vertex orders DepthFirstOrder page 580\nprecedence-constrained scheduling Topological page 581\ntopological sort Topological page 581\nstrong connectivity KosarajuSCC page 587\nall-pairs reachability TransitiveClosure page 593\nDigraph-processing problems addressed in this section\n594 CHAPTER 4 \u25a0 Graphs\n Q&A\nQ. Is a self-loop a cycle?\nA. Ye s , b u t  n o  s e l f - l o o p  i s  n e e d e d  f o r  a  ve r tex  to  b e  re a ch a b l e  f ro m  i t s e l f .\n5954.2 \u25a0 Directed Graphs\n EXERCISES\n \n4.2.1 What is the maximum number of edges in a digraph with V vertices and no paral-\nlel edges? ", "start": 606, "end": 608}, "885": {"text": "e l f .\n5954.2 \u25a0 Directed Graphs\n EXERCISES\n \n4.2.1 What is the maximum number of edges in a digraph with V vertices and no paral-\nlel edges? What is the minimum number of edges in a digraph with V vertices, none of \nwhich are isolated?\n4.2.2 Draw,  in the style of the \ufb01gure in the text (page 524), the adja-\ncency lists built by Digraph\u2019s input stream constructor for the \ufb01le \ntinyDGex2.txt depicted at left. \n4.2.3  Create a copy constructor for Digraph that takes as input \na digraph G and creates and initializes a new copy of the digraph. \nAny changes a client makes to G should not affect the newly created \ndigraph. \n4.2.4 Add a method hasEdge() to Digraph which takes two int\narguments v and w and returns true if the graph has an edge v->w, \nfalse otherwise. \n4.2.5 Modify Digraph to disallow parallel edges and self-loops. \n4.2.6 Develop a test client for Digraph.\n4.2.7  The indegree of a vertex in a digraph is the number of directed edges that point to \nthat vertex. The outdegree of a vertex in a digraph is the number of directed edges that \nemanate from that vertex. No vertex is reachable from a vertex of outdegree 0, which is \ncalled a sink; a vertex of indegree 0, which is called a source, is not reachable from any \nother vertex. A digraph where self-loops are allowed and every vertex has outdegree 1 \nis called a map (a function from the set of integers from 0 to V\u20131 onto itself). Write a \nprogram Degrees.java that implements the following API:\npublic ", "start": 608, "end": 608}, "886": {"text": "are allowed and every vertex has outdegree 1 \nis called a map (a function from the set of integers from 0 to V\u20131 onto itself). Write a \nprogram Degrees.java that implements the following API:\npublic class Degrees\nDegrees(Digraph G) constructor\nint indegree(int v) indegree of v \nint outdegree(int v) outdegree of v\nIterable<Integer> sources() sources\nIterable<Integer> sinks() sinks\nboolean isMap() is G a map?\n12\n16\n 8 4\n 2 3\n 1 11\n 0 6\n 3 6\n10 3\n 7 11\n 7 8\n11 8\n 2 0\n 6 2\n 5 2\n 5 10\n 3 10\n 8 1\n 4 1\ntinyDGex2.txt\nV\nE\n596 CHAPTER 4 \u25a0 Graphs\n 4.2.8 Draw all the nonisomorphic DAGs with two, three, four, and \ufb01ve vertices (see \nExercise 4.1.28).\n4.2.9 Write a method that checks whether or not a given permutation of a DAG\u2019s ver-\ntices is a topological order of that DAG.\n4.2.10 Given a DAG, does there exist a topological order that cannot result  from ap-\nplying a DFS-based algorithm, no matter in what order the vertices adjacent to each \nvertex are chosen? Prove your answer.\n4.2.11  Describe a family of sparse digraphs whose number of directed cycles grows \nexponentially in the number of vertices.\n4.2.12 How many edges are there in the transitive closure of a digraph that is a simple \ndirected path with V vertices and V\u20131 edges?\n4.2.13 Give the transitive closure of the ", "start": 608, "end": 609}, "887": {"text": "vertices.\n4.2.12 How many edges are there in the transitive closure of a digraph that is a simple \ndirected path with V vertices and V\u20131 edges?\n4.2.13 Give the transitive closure of the digraph with ten vertices and these edges:\n3->7 1->4 7->8 0->5 5->2 3->8 2->9 0->6 4->9 2->6 6->4\n4.2.14 Prove that the strong components in GR are the same as in G.\n4.2.15 What are the strong components of a DAG?\n4.2.16 What happens if you run Kosaraju\u2019s algorithm on a DAG?\n4.2.17 True or false: The reverse postorder of  a g raph\u2019s reverse is the same as the post-\norder of the graph.\n4.2.18  Compute the memory usage of a Digraph with V vertices and E edges, under \nthe memory cost model of Section 1.4. \n5974.2 \u25a0 Directed Graphs\n CREATIVE PROBLEMS\n \n  \n4.2.19  Topolog ical sort and BFS. Explain why the following algorithm does not neces-\nsarily produce a topological order: Run BFS, and label the vertices by increasing dis-\ntance to their respective source.\n4.2.20  Directed    Eulerian cycle. An  Eulerian cycle is a directed cycle that contains each \nedge exactly once. Write a graph client Euler that \ufb01nds an  Eulerian cycle or reports that \nno such tour exists. Hint : Prove that a digraph G has a directed  Eulerian cycle if and \nonly if G is connected and each vertex has its indegree equal to its outdegree.\n4.2.21  LCA of a DAG. Given a DAG and two vertices v and w, ", "start": 609, "end": 610}, "888": {"text": "Eulerian cycle if and \nonly if G is connected and each vertex has its indegree equal to its outdegree.\n4.2.21  LCA of a DAG. Given a DAG and two vertices v and w, \ufb01nd the  lowest common \nancestor (LCA) of v and w. The LCA of v and w is an ancestor of v and w that has no \ndescendants that are also ancestors of v and w. Computing the LCA is useful in multiple \ninheritance in programming languages, analysis of genealogical data (\ufb01nd degree of \ninbreeding in a pedigree graph), and other applications. Hint : De\ufb01ne the height of a \nvertex v in a DAG to be the length of the longest path from a root to v. Among vertices \nthat are ancestors of both v and w, the one with the greatest height is an LCA of v and w.\n4.2.22      Shortest ancestral path. Given a DAG and two vertices v and w, \ufb01nd the shortest \nancestral path between v and w. An ancestral path between v and w is a common ancestor \nx along with a shortest path from v to x and a shortest path from w to x. The shortest \nancestral path is the ancestral path whose total length is minimized. Warmup: Find a \nDAG where the shortest ancestral path goes to a common ancestor x that is not an LCA. \nHint: Run BFS twice, once from v and once from w.\n4.2.23    Strong component. Describe a linear-time algorithm for computing the strong \nconnected component containing a given vertex v. On the basis of that algorithm, de -\nscribe a simple quadratic algorithm for computing the strong components of a digraph. \n4.2.24     Hamiltonian path in DAGs. Given a DAG, design a linear-time algorithm to \ndetermine whether there is a directed path that ", "start": 610, "end": 610}, "889": {"text": "quadratic algorithm for computing the strong components of a digraph. \n4.2.24     Hamiltonian path in DAGs. Given a DAG, design a linear-time algorithm to \ndetermine whether there is a directed path that visits each vertex exactly once. \nAnswer : Compute a topological sort and check if there is an edge between each con -\nsecutive pair of vertices in the topological order.\n4.2.25  Unique topological ordering. Design an algorithm to determine whether a di-\ngraph has a unique topological ordering. Hint : A digraph has a unique topological \nordering if and only if there is a directed edge between each pair of consecutive vertices \nin the topological order (i.e., the digraph has a  Hamiltonian path). If the digraph has \n598 CHAPTER 4 \u25a0 Graphs\n multiple topological orderings, then a second topological order can be obtained by \nswapping a pair of consecutive vertices.\n4.2.26   2-satis\ufb01ability. Given boolean formula in conjunctive normal form with M\nclauses and N literals such that each clause has exactly two literals, \ufb01nd a satisfying as-\nsignment (if one exists). Hint : Form the implication digraph with 2N vertices (one per \nliteral and its negation). For each clause x + y, include edges from y' to x and from x' \nto y. To satisfy the clause  x + y, (i) if y is false, then x is true and (ii) if x is false, then y is \ntrue. Claim: The formula is satis\ufb01able if and only if no variable x is in the same strong \ncomponent as its negation x'. Moreover, a topological sort of the kernel DAG (contract \neach strong component to a single vertex) yields a satisfying assignment. \n4.2.27  Digraph enumeration. Show that ", "start": 610, "end": 611}, "890": {"text": "\ncomponent as its negation x'. Moreover, a topological sort of the kernel DAG (contract \neach strong component to a single vertex) yields a satisfying assignment. \n4.2.27  Digraph enumeration. Show that the number of different V-vertex digraphs \nwith no parallel edges is 2V 2\n.  (How many digraphs are there that contain V vertices and \nE  edges?) Then compute an upper bound on the percentage of 20-vertex digraphs that \ncould ever be examined by any computer, under the  assumptions that every electron \nin the universe examines a digraph every nanosecond, that the universe has fewer than \n1080 electrons, and that the age of the universe will be less than 1020 years.\n4.2.28  DAG enumeration. Give a formula for the number of V-vertex DAGs with E  \nedges.\n4.2.29  Arithmetic expressions. Write a class that evaluates DAGs that represent arith-\nmetic expressions. Use a vertex-indexed array to hold values corresponding to each \nvertex. Assume that values corresponding to leaves have been established. Describe a \nfamily of arithmetic expressions with the property that the size of the expression tree \nis exponentially larger than the size of the corresponding DAG (so the running time of \nyour program for the DAG is proportional to the logarithm of the running time for the \ntree).\n4.2.30    Queue-based  topological sort. Develop a topological sort implementation that \nmaintains a vertex-indexed array that keeps track of the indegree of each vertex. Initial-\nize the array and a queue of sources in a single pass through all the edges, as in Exercise \n4.2.7. Then, perform the following operations until the source queue is empty: \n\u25a0 Remove a source from the queue and label it. \n\u25a0 Decrement the entries in the indegree array corresponding to the ", "start": 611, "end": 611}, "891": {"text": "\n4.2.7. Then, perform the following operations until the source queue is empty: \n\u25a0 Remove a source from the queue and label it. \n\u25a0 Decrement the entries in the indegree array corresponding to the destination \nvertex of each of the removed vertex\u2019s edges. \n5994.2 \u25a0 Directed Graphs\n \u25a0  If decrementing any entry causes it to become 0, insert the corresponding vertex \nonto the source queue.\n4.2.31    Euclidean digraphs. Modify your solution to Exercise 4.1.37 to create an API \nEuclideanDigraph for graphs whose vertices are points in the plane, so that you can \nwork with graphical representations.\nCREATIVE PROBLEMS  (continued)\n600 CHAPTER 4 \u25a0 Graphs\n EXPERIMENTS\n4.2.32    Random digraphs. Write a program ErdosRenyiDigraph that takes integer \nvalues V and E from the command line and builds a digraph by generating E random \npairs of integers between 0 andV/H110021. Note: This generator produces self-loops and par-\nallel edges.\n4.2.33  Random simple digraphs. Write a program RandomDigraph that takes integer \nvalues V and E from the command line and produces, with equal likelihood, each of the \npossible simple digraphs with V vertices and E edges.  \n4.2.34  Random sparse digraphs. Modify your solution to Exercise 4.1.41 to create \na program RandomSparseDigraph that generates random sparse digraphs for a well-\nchosen set of values  of V and E that you can use it to run meaningful empirical tests.\n4.2.35  Random Euclidean digraphs. Modify your solution to Exercise 4.1.42 to create \na EuclideanDigraph client RandomEuclideanDigraph that assigns a random direc-\ntion to each ", "start": 611, "end": 613}, "892": {"text": "tests.\n4.2.35  Random Euclidean digraphs. Modify your solution to Exercise 4.1.42 to create \na EuclideanDigraph client RandomEuclideanDigraph that assigns a random direc-\ntion to each edge.\n4.2.36  Random grid digraphs. Modify your solution to Exercise 4.1.43 to create a \nEuclideanDiGraph client RandomGridDigraph that assigns a random direction to each \nedge.\n4.2.37  Real-world digraphs. Find a large digraph somewhere online\u2014perhaps a   \ntransaction graph in some online system, or a digraph de\ufb01ned by links on web pages. \nWrite a program RandomRealDigraph that builds a graph by choosing V vertices at \nrandom and E directed edges at random from the subgraph induced by those vertices.\n4.2.38  Real-world DAG. Find a large DAG somewhere online\u2014perhaps one de\ufb01ned \nby class-de\ufb01nition dependencies in a large software system, or by directory links in a \nlarge \ufb01le system. Write a program RandomRealDAG that builds a graph by choosing V\nvertices at random and E directed edges at random from the subgraph induced by those \nvertices.\n6014.2 \u25a0 Directed Graphs\n Testing all algorithms and study ing all parameters against all graph models is unrealistic. \nFor each problem listed below, write a client that addresses the problem for any given input \ngraph, then choose among the generators above to run experiments for that graph model. \nUse your judgment in selecting experiments, perhaps in response to results of previous \nexperiments. Write a narrative explaining your results and any conclusions that might be \ndrawn.\n4.2.39  Reachability. Run experiments to determine empirically the average number of \nvertices that are reachable from a randomly chosen vertex, for various digraph models.\n4.2.40  Path lengths ", "start": 613, "end": 614}, "893": {"text": "\ndrawn.\n4.2.39  Reachability. Run experiments to determine empirically the average number of \nvertices that are reachable from a randomly chosen vertex, for various digraph models.\n4.2.40  Path lengths in DFS. Run experiments to determine empirically the probability \nthat  DepthFirstDirectedPaths \ufb01nds a path between two randomly chosen vertices \nand to calculate the average length of the paths found, for various random digraph \nmodels.\n4.2.41  Path lengths in BFS. Run experiments to determine empirically the probability \nthat  BreadthFirstDirectedPaths \ufb01nds a path between two randomly chosen verti-\nces and to calculate the average length of the paths found, for various random digraph \nmodels.\n4.2.42  Strong components. Run experiments to determine empirically the distribu -\ntion of the number of strong components in random digraphs of various types, by \ngenerating large numbers of digraphs and drawing a histogram.\nEXPERIMENTS  (continued)\n602 CHAPTER 4 \u25a0 Graphs\n This page intentionally left blank \n 4.3     MINIMUM SPANNING TREES\n \nAn edge-weighted graph is a graph model where we associate  weights or costs with each \nedge. Such graphs are natural models for many applications. In an airline map where \nedges represent \ufb02ight routes, these weights might represent distances or fares. In an \nelectric circuit where edges represent wires, the weights might represent the length of \nthe wire, its cost, or the time that it takes a signal to propagate through it. Minimizing \ncost is naturally of interest in such situations. In this section, we consider undirected\nedge-weighted graph models and examine algorithms for one such problem:\nMinimum spanning tree. Given an undirected edge-\nweighted graph, \ufb01nd an MST.\nDefinition. Recall that a  spanning tree of a graph is a \nconnected subgraph ", "start": 614, "end": 616}, "894": {"text": "undirected\nedge-weighted graph models and examine algorithms for one such problem:\nMinimum spanning tree. Given an undirected edge-\nweighted graph, \ufb01nd an MST.\nDefinition. Recall that a  spanning tree of a graph is a \nconnected subgraph with no cycles that includes all \nthe vertices. A  minimum spanning tree  (MST  ) of an \nedge-weighted graph is a spanning tree whose weight \n(the sum of the weights of its edges) is no larger than \nthe weight of any other spanning tree.\nIn this section, we examine two classical algorithms for \ncomputing MSTs: Prim\u2019s algorithm and Kruskal\u2019s algorithm. \nThese algorithms are easy to understand and not dif\ufb01cult \nto implement. They are among the oldest and most well-\nknown algorithms in this book, and they also take \ngood advantage of modern data structures. Since \nMSTs have numerous important applications, al-\ngorithms to solve the problem have been studied at \nleast since the 1920s, at \ufb01rst in the context of power \ndistribution networks, later in the context of tele-\nphone networks. MST algorithms are now impor -\ntant in the design of many types of networks (com-\nmunication, electrical, hydraulic, computer, road, \nrail, air, and many others) and also in the study of \nbiological, chemical, and physical networks that are \nfound in nature. \n8\n16\n4 5  0.35 \n4 7  0.37 \n5 7  0.28 \n0 7  0.16\n1 5  0.32 \n0 4  0.38\n2 3  0.17\n1 7  0.19 \n0 2  0.26 \n1 2  0.36 \n1 3  0.29 \n2 7 ", "start": 616, "end": 616}, "895": {"text": "3  0.17\n1 7  0.19 \n0 2  0.26 \n1 2  0.36 \n1 3  0.29 \n2 7  0.34\n6 2  0.40 \n3 6  0.52\n6 0  0.58\n6 4  0.93 \nnon-MST edge\n(gray)\nMST edge\n(black)\nAn edge-weighted graph and its MST\ntinyEWG.txt\nV\nE\napplication vertex edge\ncircuit component wire\nairline airport flight route\npower \ndistribution power plant transmission \nlines\nimage \nanalysis feature proximity \nrelationship\nTypical MST applications\n604\n Assumptions. Var ious anomalous situations, which are generally easy to handle, can \narise when computing minimum spanning trees. T o streamline the presentation, we \nadopt the following conventions:\n\u25a0 \n \n \nThe graph is connected. The spanning-tree condition in \nour de\ufb01nition implies that the graph must be connected \nfor an MST to exist. Another way to pose the problem, \nrecalling basic properties of trees from Section 4.1, is \nto \ufb01nd a minimal-weight set of V/H110021 edges that connect \nthe graph. If a graph is not connected, we can adapt our \nalgorithms to compute the MSTs of each of its con-\nnected components, collectively known as a   minimum \nspanning forest (see Exercise 4.3.22).\n\u25a0 \n \nThe edge weights are not necessarily distances. Geometric \nintuition is sometimes bene\ufb01cial in understanding al-\ngorithms, so we use examples where vertices are points \nin the plane and weights are distances, such as the graph \non the facing page. But it is important to remember that \nthe weights might represent time or cost or an entirely \ndifferent ", "start": 616, "end": 617}, "896": {"text": "so we use examples where vertices are points \nin the plane and weights are distances, such as the graph \non the facing page. But it is important to remember that \nthe weights might represent time or cost or an entirely \ndifferent variable and do not need to be proportional to \na distance at all. \n\u25a0 \n  \nThe edge weights may be zero or negative. If the edge \nweights are all positive, it suf\ufb01ces to de\ufb01ne the MST as \nthe subgraph with minimal total weight that connects \nall the vertices, as such a subgraph must form a span-\nning tree. The spanning-tree condition in the de\ufb01nition \nis included so that it applies for graphs that may have \nzero negative edge weights.\n\u25a0 The edge weights are all different. If edges can have equal \nweights, the minimum spanning tree may not be unique \n(see Exercise 4.3.2). The possibility of multiple MSTs \ncomplicates the correctness proofs of some of our algo-\nrithms, so we rule out that possibility in the presenta-\ntion. It turns out that this assumption is not restrictive \nbecause our algorithms work without modi\ufb01cation in \nthe presence of equal weights.\nIn summary, we assume throughout the presentation that our \njob is to \ufb01nd the MST of a connected edge-weighted graph \nwith arbitrary (but distinct) weights.\nweights need not be \nproportional to distance\n4 6  0.62\n5 6  0.88\n1 5  0.02\n0 4  0.64\n1 6  0.90\n0 2  0.22\n1 2  0.50\n1 3  0.97\n2 6  0.17\nno MST if graph is not connected\n4 5  0.61\n4 6  0.62\n5 ", "start": 617, "end": 617}, "897": {"text": "2  0.50\n1 3  0.97\n2 6  0.17\nno MST if graph is not connected\n4 5  0.61\n4 6  0.62\n5 6  0.88\n1 5  0.11\n2 3  0.35\n0 3  0.6\n1 6  0.10\n0 2  0.22\ncan independently compute \nMSTs of components\nVarious MST anomalies\nweights can be 0 or negative\n4 6  0.62\n5 6  0.88\n1 5  0.02\n0 4 -0.99\n1 6  0\n0 2  0.22   \n1 2  0.50\n1 3  0.97\n2 6  0.17\nMST may not be unique\nwhen weights have equal values\n1 2  1.00\n1 3  0.50\n2 4  1.00\n3 4  0.50\n1 2  1.00\n1 3  0.50\n2 4  1.00\n3 4  0.50\n6054.3 \u25a0 Minimum Spanning Trees\n  \nUnderlying principles To  b e g i n , w e  re c a l l  f ro m  Sec-\ntion 4.1 two of the de\ufb01ning properties of a tree: \n\u25a0 Adding an edge that connects two vertices in a tree \ncreates a unique cycle.\n\u25a0 Removing an edge from a tree breaks it into two \nseparate subtrees.\nThese properties are the basis for proving a fundamental \nproperty of MSTs that leads to the MST algorithms that ", "start": 617, "end": 618}, "898": {"text": "a tree \ncreates a unique cycle.\n\u25a0 Removing an edge from a tree breaks it into two \nseparate subtrees.\nThese properties are the basis for proving a fundamental \nproperty of MSTs that leads to the MST algorithms that we \nconsider in this section.\n  C u t  p r o p e r t y .  This property, which we refer to as the cut \nproperty, has to do with identifying edges that must be in the \nMST of a given edge-weighted graph, by dividing vertices \ninto two sets and examining edges that cross the division.\n Definition. A  cut of a graph is a partition of its vertices into two nonempty disjoint \nsets. A    crossing edge of a cut is an edge that connects a vertex in one set with a vertex \nin the other.\n \nTypically, we specify a cut by specifying a set of  vertices, leaving implicit the assumption \nthat the cut comprises the given vertex set and its complement, so that a crossing edge \nis an edge from a vertex in the set to a vertex not in the set. In \ufb01gures, we draw vertices \non one side of the cut in gray and vertices on the other side in white.\nProposition J.  ( Cut property ) Given any cut in an edge- \nweighted graph, the crossing edge of minimum weight is in \nthe MST of the graph.\nProof: Let e be the crossing edge of minimum weight and \nlet T be the MST. The proof is by contradiction: Suppose \nthat T does not contain e. Now consider the graph formed \nby adding e to T. This graph has a cycle that contains e, and \nthat cycle must contain at least one other crossing edge\u2014\nsay, f, which has higher weight than e (since e is minimal and \nall edge weights are different). We can get a spanning tree of \nstrictly lower weight by deleting f and adding e, contradict-\ning the assumed ", "start": 618, "end": 618}, "899": {"text": "edge\u2014\nsay, f, which has higher weight than e (since e is minimal and \nall edge weights are different). We can get a spanning tree of \nstrictly lower weight by deleting f and adding e, contradict-\ning the assumed minimality of T.\nCut property\nminimum-weight crossing edge\n must be in the MST\ncrossing edges separating\ngray from white vertices\nare drawn in red\ne\nf\nBasic properties of a tree\nadding an edge\ncreates a cycle\nremoving an edge\nbreaks tree into two parts\n606 CHAPTER 4 \u25a0 Graphs\n Under our assumption that edge weights \nare distinct, every connected graph has a \nunique MST (see Exercise 4.3.3); and the \ncut property says that the shortest crossing \nedge for every cut must be in the MST. \nThe \ufb01gure to the left of Proposition J il-\nlustrates the cut property. Note that there is \nno requirement that the minimal edge be the only MST edge connect-\ning the two sets; indeed, for typical cuts there are several MST edges \nthat connect a vertex in one set with a vertex in the other, as illustrated \nin the \ufb01gure above. \n  G r e e d y  a l g o r i t h m .  The cut property is the basis for the algorithms \nthat we consider for the MST problem. Speci\ufb01cally, they are special \ncases of a general paradigm known as the greedy algorithm: apply the \ncut property to accept an edge as an MST edge, continuing until \ufb01nd-\ning all of the MST edges. Our algorithms differ in their approaches \nto maintaining cuts and identifying the crossing edge of minimum \nweight, but are special cases of the following:\nProposition K.  \u00a0 ( Greedy MST algorithm) The following method \ncolors black all edges in the the MST of any connected edge-\nweighted graph with V ", "start": 618, "end": 619}, "900": {"text": "of minimum \nweight, but are special cases of the following:\nProposition K.  \u00a0 ( Greedy MST algorithm) The following method \ncolors black all edges in the the MST of any connected edge-\nweighted graph with V vertices: starting with all edges colored \ngray, \ufb01nd a cut with no black edges, color its minimum-weight \nedge black, and continue until V/H110021 edges have been colored black. \nProof: For simplicity, we assume in the discussion that the edge \nweights are all different, though the proposition is still true when \nthat is not the case (see Exercise 4.3.5). By the cut property, any \nedge that is colored black is in the MST. If fewer than V/H110021 edges \nare black, a cut with no black edges exists (recall that we assume \nthe graph to be connected). Once V/H110021 edges are black, the black \nedges form a spanning tree. \nThe diagram at right is a typical trace of the greedy algorithm. Each \ndrawing depicts a cut and identi\ufb01es the minimum-weight edge in the \ncut (thick red) that is added to the MST by the algorithm.\nA cut with two MST edges\nGreedy MST algorithm\nin MST\nminimum\nedge in cut\n6074.3 \u25a0 Minimum Spanning Trees\n   \nEdge-weighted graph data type How should we represent edge-weighted \ngraphs? Perhaps the simplest way to proceed is to extend the basic graph representa -\ntions from Section 4.1: in the adjacency-matrix representation, the matrix can contain \nedge weights rather than boolean values; in the adjacency-lists representation, we can \nde\ufb01ne a node that contains both a vertex and a weight \ufb01eld to put in the adjacency lists. \n(As usual, we focus on sparse graphs and leave the adjacency-matrix representation for \nexercises.) This classic approach is appealing, but we will ", "start": 619, "end": 620}, "901": {"text": "a vertex and a weight \ufb01eld to put in the adjacency lists. \n(As usual, we focus on sparse graphs and leave the adjacency-matrix representation for \nexercises.) This classic approach is appealing, but we will use a different method that is \nnot much more complicated, will make our programs useful in more general settings, \nand needs a slightly more general API, which allows us to process Edge objects:\npublic class    Edge implements Comparable<Edge>\nEdge(int v, int w, double weight) initializing constructor\ndouble weight() weight of this edge\nint either() either of this edge\u2019s vertices\nint other(int v) the other vertex\nint compareTo(Edge that) compare this edge to e \nString toString() string representation\nAPI for a weighted edge\n \n \nThe either() and other() methods for accessing the edge\u2019s vertices may be a bit puz-\nzling at \ufb01rst\u2014the need for them will become plain when we examine client code. Y ou can \n\ufb01nd an implementation of Edge on page 610. It is the basis for this EdgeWeightedGraph\nAPI, which refers to Edge objects in a natural manner:\npublic class    EdgeWeightedGraph \nEdgeWeightedGraph(int V) create an empty V-vertex graph\nEdgeWeightedGraph(In in) read graph from input stream\nint V() number of vertices\nint E() number of edges\nvoid addEdge(Edge e) add edge e to this graph\nIterable<Edge> adj(int v) edges incident to v\nIterable<Edge> edges() all of this graph\u2019s edges\nString toString() string representation\nAPI for an edge-weighted graph\n608 CHAPTER 4 \u25a0 Graphs\n  \n \nThis API is very similar to the API for Graph (page 522). The two \nimportant differences are that it is based on Edge and that it adds the edges() method \nat right, which provides clients with the ability to iterate through to all the graph\u2019s ", "start": 620, "end": 621}, "902": {"text": "the API for Graph (page 522). The two \nimportant differences are that it is based on Edge and that it adds the edges() method \nat right, which provides clients with the ability to iterate through to all the graph\u2019s edges \n(ignoring any self-loops). The rest of the implementation of EdgeWeightedGraph on \npage 611 is quite similar to the unweighted undi-\nrected graph implementation of Section 4.1,  \nbut instead of the    adjacency lists of integers \nused in Graph, it uses adjacency lists of Edge\nobjects.\nThe \ufb01gure at the bottom of this page shows \nthe edge-weighted graph representation that \nEdgeWeightedGraph builds from the sample \n\ufb01le tinyEWG.txt, showing the contents of each \nBag as a linked list to re\ufb02ect the standard imple-\nmentation of Section 1.3. To reduce clutter in the \ufb01gure, we show each Edge as a pair \nof int values and a double value. The actual data structure is a linked list of links to \nobjects containing those values. In particular, although there are two  references to each \nEdge (one in the list for each vertex), there is only one Edge object corresponding to \neach graph edge. In the \ufb01gure, the edges appear in each list in reverse order of the order \nthey are processed, because of the stack-like nature of the standard linked-list imple-\nmentation. As in Graph, by using a Bag we are making clear that our client code makes \nno assumptions about the order of objects in the lists.\nEdge-weighted graph representation\nadj[]\n0\n1\n2\n3\n4\n5\n6\n7\n6 0 .58 0 2 .26 0 4 .38 0 7 .16 Bag\nobjects\n8\n16\n4 5  0.35 \n4 7 ", "start": 621, "end": 621}, "903": {"text": "representation\nadj[]\n0\n1\n2\n3\n4\n5\n6\n7\n6 0 .58 0 2 .26 0 4 .38 0 7 .16 Bag\nobjects\n8\n16\n4 5  0.35 \n4 7  0.37 \n5 7  0.28 \n0 7  0.16\n1 5  0.32 \n0 4  0.38\n2 3  0.17\n1 7  0.19 \n0 2  0.26 \n1 2  0.36 \n1 3  0.29 \n2 7  0.34\n6 2  0.40 \n3 6  0.52\n6 0  0.58\n6 4  0.93 \n1 3 .29 1 2 .36 1 7 .19 1 5 .32\n6 2 .40 2 7 .34 1 2 .36 0 2 .26 2 3 .17\n3 6 .52 1 3 .29 2 3 .17\n6 4 .93 0 4 .38 4 7 .37 4 5 .35\n1 5 .32 5 7 .28 4 5 .35\n6 4 .93 6 0 .58 3 6 .52 6 2 .40\n2 7 .34 1 7 .19 0 7 .16 5 7 .28 5 7 .28\nreferences to the \nsame Edge object\ntinyEWG.txt\nV\nE\npublic Iterable<Edge> edges() \n{\n   Bag<Edge> b = new Bag<Edge>();\n   for ", "start": 621, "end": 621}, "904": {"text": "7 .28 5 7 .28\nreferences to the \nsame Edge object\ntinyEWG.txt\nV\nE\npublic Iterable<Edge> edges() \n{\n   Bag<Edge> b = new Bag<Edge>();\n   for (int v = 0; v < V; v++)\n      for (Edge e : adj[v])\n         if (e.other(v) > v) b.add(e);\n   return b; \n}\nGathering all the edges in an edge-weighted graph\n6094.3 \u25a0 Minimum Spanning Trees\n  W e i g h t e d  e d g e  d a t a  t y p e\npublic class  Edge implements Comparable<Edge> \n{\n   private final int v;                       // one vertex\n   private final int w;                       // the other vertex\n   private final double weight;               // edge weight\n   public Edge(int v, int w, double weight)\n   {\n      this.v = v;\n      this.w = w;\n      this.weight = weight;\n   }\n   public double weight()\n   {  return weight;  }\n   public int either()\n   {  return v;  }\n   public int other(int vertex)\n   {\n      if      (vertex == v) return w;\n      else if (vertex == w) return v;\n      else throw new RuntimeException(\"Inconsistent edge\");\n   }\n   public int compareTo(Edge that)\n   {\n      if      (this.weight() < that.weight()) return -1;\n      else if (this.weight() > that.weight()) return +1;\n      else                                    return  0;\n   }\n   public String toString()\n   {  return String.format(\"%d-%d %.2f\", v, w, weight);  }\n}\nThis data type provides the methods either() and other() so that such clients can use other(v) to \n\ufb01nd the other vertex when it knows v. When neither vertex is known, ", "start": 621, "end": 622}, "905": {"text": "v, w, weight);  }\n}\nThis data type provides the methods either() and other() so that such clients can use other(v) to \n\ufb01nd the other vertex when it knows v. When neither vertex is known, our clients use the idiomatic \ncode int v = e.either(), w = e.other(v); to access an Edge e\u2019s two vertices.\n610 CHAPTER 4 \u25a0 Graphs  E d g e - w e i g h t e d  g r a p h  d a t a  t y p e\npublic class  EdgeWeightedGraph \n{\n   private final int V;               // number of vertices\n   private int E;                     // number of edges\n   private Bag<Edge>[] adj;           // adjacency lists\n   public EdgeWeightedGraph(int V)\n   {\n      this.V = V;\n      this.E = 0;\n      adj = (Bag<Edge>[]) new Bag[V];\n      for (int v = 0; v < V; v++) \n         adj[v] = new Bag<Edge>();\n   }\n   public EdgeWeightedGraph(In in)\n   // See Exercise 4.3.9.\n   public int V() {  return V;  }\n   public int E() {  return E;  }\n   public void addEdge(Edge e)\n   {\n      int v = e.either(), w = e.other(v);\n      adj[v].add(e);\n      adj[w].add(e);\n      E++;\n    }\n   public Iterable<Edge> adj(int v)\n   {  return adj[v];  }\n   public Iterable<Edge> edges()\n  // See page 609.\n}\nThis implementation maintains a vertex-indexed array of lists of edges. As with Graph (see page 526), \nevery edge appears twice: if an edge connects v and w, it appears both in v\u2019s list and in w\u2019s list. The \nedges() method puts ", "start": 622, "end": 623}, "906": {"text": "lists of edges. As with Graph (see page 526), \nevery edge appears twice: if an edge connects v and w, it appears both in v\u2019s list and in w\u2019s list. The \nedges() method puts all the edges in a Bag (see page 609). The toString() implementation is left as \nan exercise. \n6114.3 \u25a0 Minimum Spanning Trees   C o m p a r i n g  e d g e s  b y  w e i g h t .  The API speci\ufb01es that the Edge class must implement the \nComparable interface and include a compareTo() implementation. The natural order-\ning for edges in an edge-weighted graph is by weight. Accordingly, the implementation \nof compareTo() is straightforward. \n P a r a l l e l  e d g e s .  As with our undirected-graph implementations, we allow paral -\nlel edges. Alternatively, we could develop a more complicated implementation of \nEdgeWeightedGraph that disallows them, perhaps keeping the minimum-weight edge \nfrom a set of parallel edges. \n S e l f - l o o p s .  We allow self-loops. However, our edges() implementation in \nEdgeWeightedGraph does not include self-loops even though they might be present \nin the input or in the data structure. This omission has no effect on our MST algo-\nrithms because no MST contains a self-loop. When working with an application where \nself-loops are signi\ufb01cant, you may need to modify our code as appropriate for the \napplication.\nOur choice to use explicit Edge objects leads to clear and compact client code, as you \nwill see. It carries a small price: each adjacency-list node has a reference to an Edge ob-\nject, with redundant information (all the nodes on v\u2019s adjacency list have a v). We also \npay object overhead cost. Although we ", "start": 623, "end": 624}, "907": {"text": "It carries a small price: each adjacency-list node has a reference to an Edge ob-\nject, with redundant information (all the nodes on v\u2019s adjacency list have a v). We also \npay object overhead cost. Although we have only one copy of each Edge, we do have two \nreferences to each Edge object. An alternative and widely used approach is to keep two \nlist nodes corresponding to each edge, just as in Graph, each with a vertex and the edge \nweight in each list node. This alternative also carries a price\u2014two nodes, including two \ncopies of the weight for each edge.\n612 CHAPTER 4 \u25a0 Graphs\n MST API and test client As usual, for graph processing, we de\ufb01ne an API where \nthe constructor takes an edge-weighted graph as argument and supports client query \nmethods that return the MST and its weight. How should we represent the MST itself? \nThe MST of a graph G is a subgraph of G that is also a tree, so we have numerous op-\ntions. Chief among them are \n\u25a0 A list of edges\n\u25a0 An edge-weighted graph\n\u25a0 A vertex-indexed array with parent links\nTo  g ive  c l i e n t s  a n d  o u r  i m p l e m e n t a t i o n s  a s  m u c h  \ufb02 e x i b i l i t y  a s  p o s s i b l e  i n  c h o o s i n g  \namong these alternatives for various applications, we adopt the following API:\n public class  MST\nMST(EdgeWeightedGraph G) constructor\nIterable<Edge> edges() all of the MST edges\ndouble weight() weight of MST\n A P I  f o r  M S T  i m p l e m e n t a t i o n s\nTe s ", "start": 624, "end": 625}, "908": {"text": "constructor\nIterable<Edge> edges() all of the MST edges\ndouble weight() weight of MST\n A P I  f o r  M S T  i m p l e m e n t a t i o n s\nTe s t  c l i e n t . As usual, we create sample graphs and develop a test client for use in test -\ning our implementations. A sample client is shown below. It reads edges from the input \nstream, builds an edge-weighted graph, computes the MST of that graph, prints the \nMST edges, and prints the total weight of the MST. \npublic static void main(String[] args) \n{\n   In in = new In(args[0]);\n   EdgeWeightedGraph G;\n   G = new EdgeWeightedGraph(in);\n   MST mst = new MST(G);\n   for (Edge e : mst.edges())\n      StdOut.println(e);\n   StdOut.println(mst.weight()); \n}\nMST test client\n6134.3 \u25a0 Minimum Spanning Trees\n  Te s t  d a t a . Yo u  c a n  \ufb01 n d  t h e  \ufb01 l e  tinyEWG.txt on the booksite, which de-\n\ufb01nes the small sample graph on page 604 that we use for detailed traces of MST\nalgorithms. Y ou can also \ufb01nd on the booksite the \ufb01le mediumEWG.txt, which de\ufb01nes \nthe weighted graph with 250 vertices that is drawn on bottom of the the facing page. \nIt is an example of a Euclidean graph, whose vertices are points in the plane and whose \nedges are lines connecting them with weights equal to their Euclidean distances. Such \ngraphs are useful for gaining insight into the behavior of MST algorithms, and they \nalso model many of the typical practical problems we \nhave mentioned, such as road maps or electric circuits. \nYo ", "start": 625, "end": 626}, "909": {"text": "to their Euclidean distances. Such \ngraphs are useful for gaining insight into the behavior of MST algorithms, and they \nalso model many of the typical practical problems we \nhave mentioned, such as road maps or electric circuits. \nYo u  c a n  a l s o  \ufb01 n d  o n  t h e  b o o k s i t e  i s  a  l a r g e r  e x a m p l e  \nlargeEWG.txt that de\ufb01nes a Euclidean graph with \n1 million vertices. Our goal is to be able to \ufb01nd the \nMST of such a graph in a reasonable amount of time. \n% more tinyEWG.txt \n8 16 \n4 5 .35 \n4 7 .37 \n5 7 .28 \n0 7 .16 \n1 5 .32 \n0 4 .38 \n2 3 .17 \n1 7 .19 \n0 2 .26 \n1 2 .36 \n1 3 .29 \n2 7 .34 \n6 2 .40 \n3 6 .52 \n6 0 .58 \n6 4 .93\n% java MST tinyEWG.txt \n0-7 0.16 \n1-7 0.19 \n0-2 0.26 \n2-3 0.17 \n5-7 0.28 \n4-5 0.35 \n6-2 0.40\n1.81\n614 CHAPTER 4 \u25a0 Graphs\n A 250-node Euclidean graph (with 1,273 edges) and its MST\nMSTgraph\n% more mediumEWG.txt \n250 1273 \n244 246 0.11712 \n239 240 0.10616 \n238 245 0.06142 \n235 238 0.07048 \n233 ", "start": 626, "end": 627}, "910": {"text": "mediumEWG.txt \n250 1273 \n244 246 0.11712 \n239 240 0.10616 \n238 245 0.06142 \n235 238 0.07048 \n233 240 0.07634 \n232 248 0.10223 \n231 248 0.10699 \n229 249 0.10098 \n228 241 0.01473 \n226 231 0.07638 \n... [1263 more edges ] \n% java MST mediumEWG.txt\n  0 225 0.02383\n 49 225 0.03314\n 44  49 0.02107\n 44 204 0.01774\n 49  97 0.03121 \n202 204 0.04207 \n176 202 0.04299 \n176 191 0.02089\n 68 176 0.04396\n 58  68 0.04795 \n... [239 more edges ] \n10.46351\n6154.3 \u25a0 Minimum Spanning Trees\n  \n    P r i m \u2019 s  a l g o r i t h m  Our \ufb01rst MST method, known as Prim\u2019s algorithm, is to attach \na new edge to a single growing tree at each step. Start with any vertex as a single-ver -\ntex tree; then add V/H110021 edges to it, always taking next (coloring black) the minimum-\nweight edge that connects a vertex on the tree to a vertex not yet \non the tree (a crossing edge for the cut de\ufb01ned by tree vertices). \nProposition L.   Prim\u2019s algorithm computes the MST of any \nconnected edge-weighted graph.\nProof: Immediate from Proposition K . The growing tree \nde\ufb01nes a cut with no black edges; the algorithm takes the ", "start": 627, "end": 628}, "911": {"text": "\nProposition L.   Prim\u2019s algorithm computes the MST of any \nconnected edge-weighted graph.\nProof: Immediate from Proposition K . The growing tree \nde\ufb01nes a cut with no black edges; the algorithm takes the \ncrossing edge of minimal weight, so it is successively coloring \nedges black in accordance with the greedy algorithm.\nThe one-sentence description of Prim\u2019s algorithm just given \nleaves unanswered a key question: How do we (ef\ufb01ciently) \ufb01nd \nthe crossing edge of minimal weight? Several methods have been proposed\u2014we will \ndiscuss some of them after we have developed a full solution based on a particularly \nsimple approach.\nData structures. We implement Pr im\u2019s algor ithm w ith the aid of  a few simple and \nfamiliar data structures. In particular, we represent the vertices on the tree, the edges on \nthe tree, and the crossing edges, as follows:\n\u25a0 Ver t ices on the t ree : We use a vertex-indexed boolean array marked[], where \nmarked[v] is true if v is on the tree.\n\u25a0 Edges on the tree : We use one of two data structures: a queue mst to collect MST \nedges or a vertex-indexed array edgeTo[] of Edge objects, where edgeTo[v] is \nthe Edge that connects v to the tree. \n\u25a0 Crossing edges : We use a MinPQ<Edge>  priority queue that compares edges by \nweight (see page 610).\nThese data structures allow us to directly answer the basic question \u201cWhich is the min-\nimal-weight crossing edge?\u201d\nMaintaining the set of crossing edges. Each time that we add an edge to the tree, we \nalso add a vertex to the tree. T o maintain the set of crossing edges, we need to add to \nthe priority queue all edges from that vertex to any non-tree vertex (using marked[] to \nidentify such edges). ", "start": 628, "end": 628}, "912": {"text": "\nalso add a vertex to the tree. T o maintain the set of crossing edges, we need to add to \nthe priority queue all edges from that vertex to any non-tree vertex (using marked[] to \nidentify such edges). But we must do more: any edge connecting the vertex just added \nto a tree vertex that is already on the priority queue now becomes    ineligible (it is no \nlonger a crossing edge because it connects two tree vertices). An eager implementation \nPrim\u2019s MST algorithm\nminimum-weight\ncrossing edge\nmust be on MSTtree edge\n(thick black)\nineligible crossing edge (red)edge (gray)\n616 CHAPTER 4 \u25a0 Graphs\n of Prim\u2019s algorithm would remove such edges from \nthe priority queue; we \ufb01rst consider a simpler lazy\nimplementation of the algorithm where we leave \nsuch edges on the priority queue, deferring the eli-\ngibility test to when we remove them.\nThe \ufb01gure at right is a trace for our small sam -\nple graph tinyEWG.txt. Each drawing depicts the \ngraph and the priority queue just after a vertex is \nvisited (added to the tree and the edges in its ad -\njacency list processed). The contents of the prior -\nity queue are shown in order on the side, with new \nedges marked with asterisks. The algorithm builds \nthe MST as follows:\n\u25a0 Adds 0 to the MST and all edges in its adja-\ncency list to the priority queue.\n\u25a0 Adds 7 and 0-7 to the MST and all edges in \nits adjacency list to the priority queue.\n\u25a0 Adds 1 and 1-7 to the MST and all edges in \nits adjacency list to the priority queue.\n\u25a0 Adds 2 and 0-2 to the MST and edges 2-3\nand 6-2 to the priority queue. Edges 2-7 and \n1-2 become ineligible. ", "start": 628, "end": 629}, "913": {"text": "priority queue.\n\u25a0 Adds 2 and 0-2 to the MST and edges 2-3\nand 6-2 to the priority queue. Edges 2-7 and \n1-2 become ineligible. \n\u25a0 Adds 3 and 2-3 to the MST and edge 3-6 to \nthe priority queue. Edge 1-3 becomes ineli-\ngible. \n\u25a0 Removes ineligible edges 1-3, 1-5, and 2-7\nfrom the priority queue.\n\u25a0 Adds 5 and 5-7 to the MST and edge 4-5 to \nthe priority queue. Edge 1-5 becomes ineli-\ngible.\n\u25a0 Adds 4 and 4-5 to the MST and edge 6-4\nto the priority queue. Edges 4-7 and 0-4\nbecome ineligible.\n\u25a0 Removes ineligible edges 1-2, 4-7, and 0-4\nfrom the priority queue.\n\u25a0 Adds 6 and 6-2 to the MST. The other edges \nincident to 6 become ineligible.\nTrace of Prim\u2019s algorithm (lazy version)\n  3-6 0.52 \n  6-0 0.58 \n  6-4 0.93 \n* 0-7 0.16\n* 0-2 0.26 \n* 0-4 0.38 \n* 6-0 0.58 \n* 1-7 0.19 \n  0-2 0.26 \n* 5-7 0.28 \n* 2-7 0.34 \n* 4-7 0.37 \n  0-4 0.38 \n  6-0 0.58 \n  0-2 0.26 \n  5-7 0.28 ", "start": 629, "end": 629}, "914": {"text": "\n* 4-7 0.37 \n  0-4 0.38 \n  6-0 0.58 \n  0-2 0.26 \n  5-7 0.28 \n* 1-3 0.29 \n* 1-5 0.32 \n  2-7 0.34 \n* 1-2 0.36 \n  4-7 0.37 \n  0-4 0.38 \n  0-6 0.58 \n* 2-3 0.17 \n  5-7 0.28 \n  1-3 0.29 \n  1-5 0.32 \n  2-7 0.34\n  1-2 0.36\n  4-7 0.37 \n  0-4 0.38 \n* 6-2 0.40 \n  6-0 0.58 \n  5-7 0.28 \n  1-3 0.29\n  1-5 0.32 \n  2-7 0.34\n  1-2 0.36\n  4-7 0.37 \n  0-4 0.38 \n  6-2 0.40 \n* 3-6 0.52 \n  6-0 0.58 \n  1-3 0.29\n  1-5 0.32 \n  2-7 0.34\n* 4-5 0.35  \n  1-2 0.36\n  4-7 0.37 \n  0-4 0.38 \n  6-2 0.40 ", "start": 629, "end": 629}, "915": {"text": "4-5 0.35  \n  1-2 0.36\n  4-7 0.37 \n  0-4 0.38 \n  6-2 0.40 \n  3-6 0.52 \n  6-0 0.58 \n  1-2 0.36 \n  4-7 0.37 \n  0-4 0.38 \n  6-2 0.40 \n  3-6 0.52 \n  6-0 0.58 \n* 6-4 0.93 \nineligible\nedges\n(gray)\ncrossing edges\n(ordered by weight)\n* marks new\nentries\n6174.3 \u25a0 Minimum Spanning Trees\n After having added V vertices (and V/H110021 edges), the MST is complete. The remaining \nedges on the priority queue are ineligible, so we need not examine them again.\nImplementation. With these preparations, implementing Prim\u2019s algorithm is straight-\nforward, as shown in the implementation LazyPrimMST on the facing page. As with our \ndepth-\ufb01rst search and breadth-\ufb01rst search implementations in the previous two sec-\ntions, it computes the MST in the constructor so that client methods can learn proper-\nties of the MST with query methods. We use a private method visit() that puts a ver-\ntex on the tree, by marking it as visited and then putting all of its incident edges that are \nnot ineligible onto the priority queue, thus ensuring that the priority queue contains \nthe crossing edges from tree vertices to non-tree vertices (perhaps also some ineligible \nedges). The inner loop is a rendition in code of the one-sentence description of the al-\ngorithm: we take an edge from the priority queue and (if it is not ineligible) add it to the \ntree, and also add ", "start": 629, "end": 630}, "916": {"text": "The inner loop is a rendition in code of the one-sentence description of the al-\ngorithm: we take an edge from the priority queue and (if it is not ineligible) add it to the \ntree, and also add to the tree the new vertex that it leads to, updating the set of crossing \nedges by calling visit() with that vertex as argument. The weight() method requires \niterating through the tree edges to add up the edge weights (lazy approach) or keeping \na running total in an instance variable (eager approach) and is left as Exercise 4.3.31.\nRunning time. How fast is Prim\u2019s algorithm? This question is not dif\ufb01cult to answer, \ngiven our knowledge of the behavior characteristics of priority queues:\nProposition M.  The lazy version of  Prim\u2019s algorithm uses space proportional to E\nand time proportional to E log E (in the worst case) to compute the MST of a con-\nnected edge-weighted graph with E edges and V vertices.\nProof: The bottleneck in the algorithm is the number of edge-weight comparisons \nin the priority-queue methods insert() and delMin(). The number of edges on \nthe priority queue is at most E, which gives the space bound. In the worst case, \nthe cost of an insertion is ~lg E and the cost to delete the minimum is ~2 lg E (see \nProposition O in Chapter 2). Since at most E edges are inserted and at most E are \ndeleted, the time bound follows.\n \nIn practice, the upper bound on the running time is a bit conservative because the \nnumber of edges on the priority queue is typically much less than E. The existence of \nsuch a simple, ef\ufb01cient, and useful algorithm for such a challenging task is quite re-\nmarkable. Next, we brie\ufb02y discuss some improvements. As usual, detailed evaluation of \nsuch improvements in performance-critical ", "start": 630, "end": 630}, "917": {"text": "simple, ef\ufb01cient, and useful algorithm for such a challenging task is quite re-\nmarkable. Next, we brie\ufb02y discuss some improvements. As usual, detailed evaluation of \nsuch improvements in performance-critical applications is a job for experts.\n618 CHAPTER 4 \u25a0 Graphs\n  L a z y  v e r s i o n  o f  P r i m \u2019 s  M S T  a l g o r i t h m\npublic class  LazyPrimMST \n{\n   private boolean[] marked;          // MST vertices\n   private Queue<Edge> mst;           // MST edges \n  private MinPQ<Edge> pq;            // crossing (and ineligible) edges\n   public LazyPrimMST(EdgeWeightedGraph G)\n   {  \n      pq = new MinPQ<Edge>();\n      marked = new boolean[G.V()];\n      mst = new Queue<Edge>();\n      visit(G, 0);   // assumes G is connected (see Exercise 4.3.22)\n      while (!pq.isEmpty())\n      { \n         Edge e = pq.delMin();                  // Get lowest-weight\n         int v = e.either(), w = e.other(v);    //    edge from pq.\n         if (marked[v] && marked[w]) continue;  // Skip if ineligible.\n         mst.enqueue(e);                        // Add edge to tree.\n         if (!marked[v]) visit(G, v);           // Add vertex to tree\n         if (!marked[w]) visit(G, w);           //   (either v or w).\n      }\n   }\n   private void visit(EdgeWeightedGraph G, int v)\n   {  // Mark v and add to pq all edges from v to unmarked vertices.\n      marked[v] = true;\n      for (Edge e : G.adj(v))\n         if (!marked[e.other(v)]) pq.insert(e);\n   }\n   public ", "start": 630, "end": 631}, "918": {"text": "and add to pq all edges from v to unmarked vertices.\n      marked[v] = true;\n      for (Edge e : G.adj(v))\n         if (!marked[e.other(v)]) pq.insert(e);\n   }\n   public Iterable<Edge> edges()\n   {  return mst;  }\n   public double weight()   // See Exercise 4.3.31.\n}\nThis implementation of Prim\u2019s algorithm uses a priority queue to hold crossing edges, a vertex-in -\ndexed arrays to mark tree vertices, and a queue to hold MST edges. This implementation is a lazy \napproach where we leave ineligible edges in the priority queue. \n6194.3 \u25a0 Minimum Spanning Trees  E a g e r  v e r s i o n  o f  P r i m \u2019 s  a l g o r i t h m  To  i m p rove  t h e  LazyPrimMST, we might try \nto delete ineligible edges from the priority queue, so that the priority queue contains \nonly the crossing edges between tree vertices and non-tree vertices. But we can elimi-\nnate even more edges. The key is to note that our only interest is in the minimal edge \nfrom each non-tree vertex to a tree vertex. When we add a vertex \nv to the tree, the only possible change with respect to each non-\ntree vertex w is that adding v brings w closer than before to the \ntree. In short, we do not need to keep on the priority queue all\nof the edges from w to tree vertices\u2014we just need to keep track \nof the minimum-weight edge and check whether the addition \nof v to the tree necessitates that we update that minimum (be-\ncause of an edge v-w that has lower weight), which we can do \nas we process each edge in v\u2019s adjacency list. In other words, we \nmaintain on the priority queue just one edge for each ", "start": 631, "end": 632}, "919": {"text": "(be-\ncause of an edge v-w that has lower weight), which we can do \nas we process each edge in v\u2019s adjacency list. In other words, we \nmaintain on the priority queue just one edge for each non-tree vertex w : the shortest \nedge that connects it to the tree. Any longer edge connecting w to the tree will become \nineligible at some point, so there is no need to keep it on the priority queue.\nPrimMST (Algorithm 4.7 on page 622) implements Prim\u2019s algorithm using our in-\ndex priority queue data type from Section 2.4  (see page 320). It replaces the data\nstructures marked[] and mst[] in LazyPrimMST by two vertex-indexed arrays edgeTo[] \nand distTo[], which have the following properties:\n\u25a0 If v is not on the tree but has at least one edge connecting it to the tree, then \nedgeTo[v] is the shortest edge connecting v to the tree, and distTo[v] is the \nweight of that edge.\n\u25a0 \n \nAll such vertices v are maintained on the  index priority queue, as an index v as-\nsociated with the weight of edgeTo[v].\nThe key implications of these properties is that the minimum key on the priority queue \nis the weight of the minimal-weight crossing edge, and its associated vertex  v is the next \nto add to the tree . The marked[] array is not needed, since the condition !marked[w]\nis equivalent to the condition that distTo[w] is in\ufb01nite (and that edgeTo[w] is null). \nTo  m a i n t a i n  t h e  d a t a  s t r u c t u re s , PrimMST takes an edge v from the priority queue, then \nchecks each edge v-w on its adjacency list. If w is marked, the edge is ineligible; if ", "start": 632, "end": 632}, "920": {"text": "s t r u c t u re s , PrimMST takes an edge v from the priority queue, then \nchecks each edge v-w on its adjacency list. If w is marked, the edge is ineligible; if it is not \non the priority queue or its weight is lower than the current best-known edgeTo[w], \nthe code updates the data structures to establish v-w as the best-known way to connect \nv to the tree. \nThe \ufb01gure on the facing page is a trace of PrimMST for our small sample graph \ntinyEWG.txt. The contents of the edgeTo[] and distTo[] arrays are depicted after \neach vertex is added to the MST, color-coded to depict the MST vertices (index in black), \nthe non-MST vertices (index in gray), the MST edges (in black), and the priority-queue \nw\nv\nconnecting v\nto the tree\nbrings w\ncloser to the tree\n620 CHAPTER 4 \u25a0 Graphs\n index/value pairs (in red). In the drawings, \nthe shortest edge connecting each non-MST \nvertex to an MST vertex is drawn in red. The \nalgorithm adds edges to the MST in the same \norder as the lazy version; the difference is in \nthe priority-queue operations. It builds the \nMST as follows:\n\u25a0 Adds 0 to the MST and all edges in its \nadjacency list to the priority queue, \nsince each such edge is the best (only) \nknown connection between a tree ver-\ntex and a non-tree vertex.\n\u25a0 Adds 7 and 0-7 to the MST and 1-7\nand 5-7 to the priority queue. Edges \n4-7 and 2-7 do not affect the priority \nqueue because their weights are not less \nthan the weights of the known connec-\ntions from the MST to 4 and 2, respec-\ntively.\n\u25a0 ", "start": 632, "end": 633}, "921": {"text": "Edges \n4-7 and 2-7 do not affect the priority \nqueue because their weights are not less \nthan the weights of the known connec-\ntions from the MST to 4 and 2, respec-\ntively.\n\u25a0 Adds 1 and 1-7 to the MST and 1-3 to \nthe priority queue.\n\u25a0 Adds 2 and 0-2 to the MST, replaces \n0-6 with 2-6 as the shortest edge from \na tree vertex to 6, and replaces 1-3 with \n2-3 as the shortest edge from a tree \nvertex to 3. \n\u25a0 Adds 3 and 2-3 to the MST. \n\u25a0 Adds 5 and 5-7 to the MST and re-\nplaces 0-4 with 4-5 as the shortest edge \nfrom a tree vertex to 4.\n\u25a0 Adds 4 and 4-5 to the MST.\n\u25a0 Adds 6 and 6-2 to the MST.\nAfter having added V/H110021 edges, the MST is \ncomplete and the priority queue is empty.\nan essentially identical argument as in \nthe proof of Proposition M proves that the \neager version of Prim\u2019s algorithm \ufb01nds the Trace of Prim\u2019s algorithm (eager version)\nred: on pq\nthick red:\nsmallest on pq, \nnext to add \nto MST\nblack: on MST\ngray: not\non MST\n0\n1 \n2 0-2 0.26 \n3\n4 0-4 0.38\n5\n6 6-0 0.58\n7 0-7 0.16 \n0\n1 1-7 0.19 \n2 0-2 0.26 \n3\n4 0-4 0.38\n5 5-7 0.28\n6 6-0 ", "start": 633, "end": 633}, "922": {"text": "\n0\n1 1-7 0.19 \n2 0-2 0.26 \n3\n4 0-4 0.38\n5 5-7 0.28\n6 6-0 0.58\n7 0-7 0.16\n0\n1 1-7 0.19 \n2 0-2 0.26 \n3 1-3 0.29\n4 0-4 0.38\n5 5-7 0.28\n6 6-0 0.58\n7 0-7 0.16 \n0\n1 1-7 0.19  \n2 0-2 0.26 \n3 2-3 0.17\n4 0-4 0.38\n5 5-7 0.28\n6 6-2 0.40\n7 0-7 0.16 \n0\n1 1-7 0.19  \n2 0-2 0.26 \n3 2-3 0.17\n4 0-4 0.38\n5 5-7 0.28\n6 6-2 0.40\n7 0-7 0.16 \n0\n1 1-7 0.19  \n2 0-2 0.26 \n3 2-3 0.17\n4 4-5 0.35\n5 5-7 0.28\n6 6-2 0.40\n7 0-7 0.16 \n0\n1 1-7 0.19  \n2 0-2 0.26 \n3 2-3 0.17\n4 4-5 0.35\n5 5-7 ", "start": 633, "end": 633}, "923": {"text": "\n0\n1 1-7 0.19  \n2 0-2 0.26 \n3 2-3 0.17\n4 4-5 0.35\n5 5-7 0.28\n6 6-2 0.40\n7 0-7 0.16 \n0\n1 1-7 0.19  \n2 0-2 0.26 \n3 2-3 0.17\n4 4-5 0.35\n5 5-7 0.28\n6 6-2 0.40\n7 0-7 0.16 \nedgeTo[] distTo[]\n6214.3 \u25a0 Minimum Spanning Trees\n ALGORITHM 4.7   Prim\u2019s MST algorithm (eager version)\npublic class  PrimMST \n{\n   private Edge[] edgeTo;          // shortest edge from tree vertex\n   private double[] distTo;        // distTo[w] = edgeTo[w].weight()\n   private boolean[] marked;       // true if v on tree\n   private IndexMinPQ<Double> pq;  // eligible crossing edges\n   public PrimMST(EdgeWeightedGraph G)\n   {\n      edgeTo = new Edge[G.V()];\n      distTo = new double[G.V()];\n      marked = new boolean[G.V()];\n      for (int v = 0; v < G.V(); v++)\n         distTo[v] = Double.POSITIVE_INFINITY;\n      pq = new IndexMinPQ<Double>(G.V());\n      distTo[0] = 0.0;\n      pq.insert(0, 0.0);              // Initialize pq with 0, weight 0.\n      while (!pq.isEmpty())\n         visit(G, pq.delMin());       // Add closest vertex to tree.\n   }\n   private void visit(EdgeWeightedGraph G, ", "start": 633, "end": 634}, "924": {"text": "// Initialize pq with 0, weight 0.\n      while (!pq.isEmpty())\n         visit(G, pq.delMin());       // Add closest vertex to tree.\n   }\n   private void visit(EdgeWeightedGraph G, int v)\n   {  // Add v to tree; update data structures.\n      marked[v] = true;\n      for (Edge e : G.adj(v))\n      {\n         int w = e.other(v);\n         if (marked[w]) continue;     // v-w is ineligible.\n         if (e.weight() < distTo[w])\n         {  // Edge e is new best connection from tree to w.\n            edgeTo[w] = e; \n            distTo[w] = e.weight(); \n            if (pq.contains(w)) pq.change(w, distTo[w]);\n            else                pq.insert(w, distTo[w]);\n         }\n      }\n   }\n   public Iterable<Edge> edges()    // See Exercise 4.3.21.\n   public double weight()           // See Exercise 4.3.31. \n}\nThis implementation of Prim\u2019s algorithm keeps eligible crossing edges on an index priority queue. \n622 CHAPTER 4 \u25a0 Graphs Prim\u2019s algorithm (250 vertices)\n20%\n40%\n60%\n80%\nMST\nMST of a connected edge-weighted graph in time proportional \nto E log V and extra space proportional to V (see page 623). For the \nhuge sparse graphs that are typical in practice, there is no asymp -\ntotic difference in the time bound (because lg E ~ lg V for sparse \ngraphs); the space bound is a constant-factor (but signi\ufb01cant) \nimprovement. Further analysis and experimentation are best left \nfor experts facing performance-critical applications, where many \nfactors come into play, including the implementations of MinPQ\nand IndexMinPQ, the graph representation, properties of the ap -\nplication\u2019s graph model, ", "start": 634, "end": 635}, "925": {"text": "\nfor experts facing performance-critical applications, where many \nfactors come into play, including the implementations of MinPQ\nand IndexMinPQ, the graph representation, properties of the ap -\nplication\u2019s graph model, and so forth. As usual, such improvements \nneed to be carefully considered, as the increased code complexity is \nonly justi\ufb01ed for applications where constant-factor performance \ngains are important, and might even be counterproductive on \ncomplex modern systems.\n Proposition N. The eager version of  Prim\u2019s algorithm uses ex-\ntra space proportional to V and time proportional to E log V\n(in the worst case) to compute the MST of a connected edge-\nweighted graph with E edges and V vertices.\nProof: The number of edges on the priority queue is at most \nV, and there are three vertex-indexed arrays, which implies the \nspace bound. The algorithm uses V insert operations, V delete \nthe minimum operations, and (in the worst case) E change pri-\nority operations. These counts, coupled with the fact that our \nheap-based implementation of the index priority queue imple-\nments all these operations in time proportional to log V (see \npage 321), imply the time bound.\nThe diagram at right shows Prim\u2019s algorithm in operation on our \n250-vertex Euclidean graph mediumEWG.txt. It is a fascinating dy-\nnamic process (see also Exercise 4.3.27). Most often the tree grows \nby connecting a new vertex to the vertex just added. When reaching \nan area with no nearby non-tree vertices, the growth starts from \nanother part of the tree. \n6234.3 \u25a0 Minimum Spanning Trees\n    K r u s k a l \u2019 s  a l g o r i t h m  The second MST algorithm \nthat we consider in detail is to process the edges in order of \ntheir weight values (smallest to largest), taking ", "start": 635, "end": 636}, "926": {"text": "s k a l \u2019 s  a l g o r i t h m  The second MST algorithm \nthat we consider in detail is to process the edges in order of \ntheir weight values (smallest to largest), taking for the MST \n(coloring black) each edge that does not form a cycle with \nedges previously added, stopping after adding V/H110021 edges \nhave been taken. The black edges form a forest of trees that \nevolves gradually into a single tree, the MST. This method \nis known as Kruskal\u2019s algorithm:\nProposition O.   Kruskal\u2019s algorithm computes the \nMST of any edge-weighted connected graph.\nProof: Immediate from Proposition K . If the next \nedge to be considered does not form a cycle with black \nedges, it crosses a cut de\ufb01ned by the set of vertices \nconnected to one of the edge\u2019s vertices by tree edges \n(and its complement). Since the edge does not create a \ncycle, it is the only crossing edge seen so far, and since \nwe consider the edges in sorted order, it is a crossing \nedge of minimum weight. Thus, the algorithm is suc-\ncessively taking a minimal-weight crossing edge, in ac-\ncordance with the greedy algorithm. \n \nPrim\u2019s algorithm builds the MST one edge at a time, \ufb01nding \na new edge to attach to a single growing tree at each step. \nKruskal\u2019s algorithm also builds the MST one edge at a time; \nbut, by contrast, it \ufb01nds an edge that connects two trees in a \nforest of growing trees. We start with a degenerate forest of \nV single-vertex trees and perform the operation of combin-\ning two trees (using the shortest edge possible) until there is \njust one tree left: the MST.\nThe \ufb01gure at left shows a step-by-step example of the \noperation of Kruskal\u2019s algorithm on tinyEWG.txt. ", "start": 636, "end": 636}, "927": {"text": "two trees (using the shortest edge possible) until there is \njust one tree left: the MST.\nThe \ufb01gure at left shows a step-by-step example of the \noperation of Kruskal\u2019s algorithm on tinyEWG.txt. The \ufb01ve \nlowest-weight edges in the graph are taken for the MST, \nthen 1-3, 1-5, and 2-7 are determined to be ineligible be-\nfore 4-5 is taken for the MST, and \ufb01nally 1-2, 4-7, and 0-4\nare determined to be ineligible and 6-2 is taken for the MST. Trace of Kruskal\u2019s algorithm\n0-7 0.16\n2-3 0.17\n1-7 0.19 \n0-2 0.26 \n5-7 0.28 \n1-3 0.29 \n1-5 0.32 \n2-7 0.34\n4-5 0.35 \n1-2 0.36 \n4-7 0.37 \n0-4 0.38\n6-2 0.40 \n3-6 0.52\n6-0 0.58\n6-4 0.93 \nobsolete\nedge\n(gray)\ngrey vertices are a cut\ndefined by the vertices\nconnected to one of\nthe red edge\u2019s vertices\nnext MST edge is red\nMST edge\n(black)\ngraph edges\nsorted\nby weight\n624 CHAPTER 4 \u25a0 Graphs\n Kruskal\u2019s algorithm is also not dif\ufb01cult to implement, given the basic algorithmic \ntools that we have considered in this book: we use a priority queue ( Section 2.4) to \nconsider the edges in order by weight, a  union-\ufb01nd data structure ( Section 1.5) ", "start": 636, "end": 637}, "928": {"text": "\ntools that we have considered in this book: we use a priority queue ( Section 2.4) to \nconsider the edges in order by weight, a  union-\ufb01nd data structure ( Section 1.5) to \nidentify those that cause cycles, and a queue ( Section 1.3) to collect the MST edges. \nAlgorithm 4.8 is an implementation along these lines. Note that collecting the MST \nedges in a Queue means that when a client iterates through the edges it gets them in \nincreasing order of their weight. The weight() method requires iterating through the \nqueue to add the edge weights (or keeping a running total in an instance variable) and \nis left as an exercise (see Exercise 4.3.31).\nAnalyzing the running time of Kruskal\u2019s algorithm is a simple matter because we \nknow the running times of its basic operations.\nProposition N (continued).    Kruskal\u2019s algorithm uses space proportional to E and \ntime proportional to E log E (in the worst case) to compute the MST of an edge-\nweighted connected graph with E edges and V vertices.\nProof: The implementation uses the priority-queue constructor that initializes the \npriority queue with all the edges, at a cost of at most E compares (see Section 2.4). \nAfter the priority queue is built, the argument is the same as for Prim\u2019s algorithm. \nThe number of edges on the priority queue is at most E, which gives the space \nbound, and the cost per operation is at most 2 lg E compares, which gives the time \nbound. Kruskal\u2019s algorithm also performs up to E find() and V union() opera-\ntions, but that cost does not contribute to the E log E order of growth of the total \nrunning time (see Section 1.5).\nAs with Prim\u2019s algorithm the cost bound is conservative, since the algorithm terminates \nafter ", "start": 637, "end": 637}, "929": {"text": "opera-\ntions, but that cost does not contribute to the E log E order of growth of the total \nrunning time (see Section 1.5).\nAs with Prim\u2019s algorithm the cost bound is conservative, since the algorithm terminates \nafter \ufb01nding the V/H110021 MST edges. The order of growth of the actual cost is E + E0 log E, \nwhere E0 is the number of edges whose weight is less than the weight of the MST edge \nwith the highest weight. Despite this advantage, Kruskal\u2019s algorithm is generally slower \nthan Prim\u2019s algorithm because it has to do a connected() operation for each edge, in \naddition to the priority-queue operations that both algorithms do for each edge pro-\ncessed (see Exercise 4.3.39).\n6254.3 \u25a0 Minimum Spanning Trees\n  The \ufb01gure at left illustrates the algorithm\u2019s dynamic characteris-\ntics on the larger example mediumEWG.txt. The fact that the edges \nare added to the forest in order of their length is quite apparent. \nKruskal\u2019s algorithm (250 vertices)\n20%\n40%\n60%\n80%\nMST\n626 CHAPTER 4 \u25a0 Graphs\n ALGORITHM 4.8   Kruskal\u2019s MST algorithm\npublic class  KruskalMST \n{\n   private Queue<Edge> mst;\n   public KruskalMST(EdgeWeightedGraph G)\n   {\n      mst = new Queue<Edge>();\n      MinPQ<Edge> pq = new MinPQ<Edge>(G.edges());\n      UF uf = new UF(G.V());\n      while (!pq.isEmpty() && mst.size() < G.V()-1)\n      {\n         Edge e = pq.delMin();               // Get min weight edge on pq\n         int v = e.either(), w = e.other(v); //   and its vertices.\n         if (uf.connected(v, w)) continue; ", "start": 637, "end": 639}, "930": {"text": "Edge e = pq.delMin();               // Get min weight edge on pq\n         int v = e.either(), w = e.other(v); //   and its vertices.\n         if (uf.connected(v, w)) continue;   // Ignore ineligible edges.\n         uf.union(v, w);                     // Merge components.\n         mst.enqueue(e);                     // Add edge to mst.\n      }\n   }\n   public Iterable<Edge> edges()\n   {  return mst;  }\n   public double weight()           // See Exercise 4.3.31.\n}\nThis implementation of Kruskal\u2019s algorithm uses a queue to hold MST edges, a priority queue to hold \nedges not yet examined, and a union-\ufb01nd data structure for identifying ineligible edges. The MST \nedges are returned to the client in increasing order of their weights. The weight() method is left as \nan exercise.\n% java KruskalMST tinyEWG.txt \n0-7 0.16 \n2-3 0.17 \n1-7 0.19 \n0-2 0.26 \n5-7 0.28 \n4-5 0.35 \n6-2 0.40\n1.81\n6274.3 \u25a0 Minimum Spanning Trees  \n \nPerspective The MST problem is one of the most heavily studied problems that \nwe encounter in this book. Basic approaches to solving it were invented long before \nthe development of modern data structures and modern techniques for analyzing the \nperformance of algorithms, at a time when \ufb01nding the MST of a graph that contained, \nsay, thousands of edges was a daunting task. The MST algorithms that we have consid-\nered differ from these old ones essentially in their use and implementation of modern \nalgorithms and data structures for basic tasks, which (coupled with modern comput-\ning power) makes it possible for us to compute MSTs with millions or even billions ", "start": 639, "end": 640}, "931": {"text": "these old ones essentially in their use and implementation of modern \nalgorithms and data structures for basic tasks, which (coupled with modern comput-\ning power) makes it possible for us to compute MSTs with millions or even billions of \nedges.\nHistorical notes. An MST imple -\nmentation for dense graphs (see Ex-\nercise 4.3.29) was \ufb01rst presented by \nR. Prim in 1961 and, independently, \nby  E. W. Dijkstra soon thereafter. It \nis usually referred to as Prim\u2019s algo-\nrithm, although Dijkstra\u2019s presenta -\ntion was more general. But the basic \nidea was also presented by  V . Jarnik \nin 1939, so some authors refer to the \nmethod as  Jarnik\u2019s algorithm, thus \ncharacterizing Prim\u2019s (or Dijkstra\u2019s) \nrole as \ufb01nding an ef\ufb01cient imple -\nmentation of the algorithm for dense \ngraphs. As the priority-queue ADT \ncame into use in the early 1970s, its \napplication to \ufb01nding MSTs of sparse \ngraphs was straightforward; the fact that MSTs of sparse graphs could be computed in \ntime proportional to E log E became widely known without attribution to any particu-\nlar researcher. In 1984,    M. L. Fredman and  R. E. Tarjan developed the    Fibonacci heap\ndata structure, which improves the theoretical bound on the order of growth of the \nrunning time of Prim\u2019s algorithm to E + V log V.  J. Kruskal presented his algorithm \nin 1956, but, again, the relevant ADT implementations were not carefully studied for \nmany years. Other interesting historical notes are that Kruskal\u2019s paper mentioned a ver-\nsion of Prim\u2019s algorithm and that a 1926 (!) paper by  O. Boruvka mentioned both ", "start": 640, "end": 640}, "932": {"text": "carefully studied for \nmany years. Other interesting historical notes are that Kruskal\u2019s paper mentioned a ver-\nsion of Prim\u2019s algorithm and that a 1926 (!) paper by  O. Boruvka mentioned both ap-\nproaches. Boruvka\u2019s paper addressed a power-distribution application and introduced \nyet another method that is easily implemented with modern data structures (see Exer-\ncise 4.3.43 and Exercise 4.3.44). The method was rediscovered by  M. Sollin in 1961; \nalgorithm\nworst-case order of growth for \nV vertices and E edges\nspace time\nlazy Prim E E log E\neager Prim V E log V\nKruskal E E log E\nFredman-Tarjan V E + V log V\nChazelle V very, very nearly, \nbut not quite E\nimpossible ? V E ?\n P e r f o r m a n c e  c h a r a c t e r i s t i c s  o f  M S T  a l g o r i t h m s\n628 CHAPTER 4 \u25a0 Graphs\n  \nit later attracted attention as the basis for MST algorithms with ef\ufb01cient asymptotic \nperformance and as the basis for parallel MST algorithms.\nA linear-time algorithm? On the one hand, no theoretical results have been developed \nthat deny the existence of an MST algorithm that is guaranteed to run in linear time \nfor all graphs. On the other hand, the goal of developing algorithms for computing \nthe MST of sparse graphs in linear time remains elusive. Since the 1970s the appli-\ncability of the union-\ufb01nd abstraction to Kruskal\u2019s algorithm and the applicability of \nthe priority-queue abstraction to Prim\u2019s algorithm have been prime motivations for \nmany researchers to seek better implementations of those ADTs. Many researchers have \nconcentrated ", "start": 640, "end": 641}, "933": {"text": "union-\ufb01nd abstraction to Kruskal\u2019s algorithm and the applicability of \nthe priority-queue abstraction to Prim\u2019s algorithm have been prime motivations for \nmany researchers to seek better implementations of those ADTs. Many researchers have \nconcentrated on \ufb01nding ef\ufb01cient priority-queue implementations as the key to \ufb01nd -\ning ef\ufb01cient MST algorithms for sparse graphs; many other researchers have studied \nvariations of  Boruvka\u2019s algorithm as the basis for nearly linear-time MST algorithms \nfor sparse graphs. Such research still holds the potential to lead us eventually to a prac-\ntical linear-time MST algorithm and has even shown the existence of a randomized \nlinear-time algorithm. Also, researchers are getting quite close to the linear-time goal: \nB. Chazelle exhibited an algorithm in 1997 that certainly could never be distinguished \nfrom a linear-time algorithm in any conceivable practical situation (even though it is \nprovably nonlinear), but is so complicated that no one would use it in practice. While \nthe algorithms that have emerged from such research are generally quite complicated, \nsimpli\ufb01ed versions of some of them may yet be shown to be useful in practice. In the \nmeantime, we can use the basic algorithms that we have considered here to compute \nthe MST in linear time in most practical situations, perhaps paying an extra factor of \nlog V for some sparse graphs.\nIn summary, we can consider the MST problem to be \u201csolved\u2019\u2019 for practical purposes. \nFor most graphs, the cost of \ufb01nding the MST is only slightly higher than the cost of \nextracting the graph\u2019s edges. This rule holds except for huge graphs that are extremely \nsparse, but the available performance improvement over the best-known algorithms \neven in this case is a small constant factor, perhaps a factor of 10 at best. These conclu-\nsions are borne out for many graph models, ", "start": 641, "end": 641}, "934": {"text": "\nsparse, but the available performance improvement over the best-known algorithms \neven in this case is a small constant factor, perhaps a factor of 10 at best. These conclu-\nsions are borne out for many graph models, and practitioners have been using Prim\u2019s \nand Kruskal\u2019s algorithms to \ufb01nd MSTs in huge graphs for decades.\n6294.3 \u25a0 Minimum Spanning Trees\n Q&A\n \nQ. Do Prim\u2019s and Kruskal\u2019s algorithms work for directed graphs?\nA. No, not at all. That is a more dif\ufb01cult graph-processing problem known as the mini-\nmum cost arborescence problem.\n630 CHAPTER 4 \u25a0 Graphs\n EXERCISES\n4.3.1 Prove that you can rescale the weights by adding a positive constant to all of \nthem or by multiplying them all by a positive constant without affecting the MST.\n4.3.2  Draw all of the MSTs of the graph depicted at right (all edge weights are \nequal).\n4.3.3  Show that if a graph\u2019s edges all have distinct weights, the MST is unique.\n4.3.4 Consider the assertion that an edge-weighted graph has a unique MST only if its \nedge weights are distinct. Give a proof or a counterexample.\n4.3.5  Show that the greedy algorithm is valid even when edge weights are not distinct.\n4.3.6  Give the MST of the weighted graph obtained by deleting vertex 7 from \ntinyEWG.txt (see page 604).\n4.3.7 How would you \ufb01nd a maximum spanning tree of an edge-weighted graph?\n4.3.8  Prove the following, known as the cycle property : Given any cycle in an edge-\nweighted graph (all edge weights distinct), the edge of maximum weight in the cycle \ndoes not belong to the MST of the graph.\n4.3.9 ", "start": 641, "end": 643}, "935": {"text": "following, known as the cycle property : Given any cycle in an edge-\nweighted graph (all edge weights distinct), the edge of maximum weight in the cycle \ndoes not belong to the MST of the graph.\n4.3.9  Implement the constructor for EdgeWeightedGraph that reads a graph from the \ninput stream, by suitably modifying the constructor from Graph (see page 526).\n4.3.10  Develop an EdgeWeightedGraph implementation for dense graphs that uses an \nadjacency-matrix (two-dimensional array of weights) representation. Disallow parallel \nedges.\n4.3.11  Determine the amount of memory used by EdgeWeightedGraph to represent a \ngraph with V vertices and E edges, using the memory-cost model of Section 1.4.\n4.3.12 Suppose that a graph has distinct edge weights. Does its shortest edge have to \nbelong to the MST? Can its longest edge belong to the MST? Does a min-weight edge \non every cycle have to belong to the MST? Prove your answer to each question or give \na counterexample.\n4.3.13 Give a counterexample that shows why the following strategy does not neces-\nsarily \ufb01nd the MST: \u2018Start with any vertex as a single-vertex MST, then add V-1 edges \nto it, always taking next a min-weight edge incident to the vertex most recently added \n3 4 5\n6 7 8\n1 2\n6314.3 \u25a0 Minimum Spanning Trees\n  \n \nto the MST.\u2019\n4.3.14 Given an MST for an edge-weighted graph G, suppose that an edge in G that \ndoes not disconnect G is deleted. Describe how to \ufb01nd an MST of the new graph in time \nproportional to E.\n4.3.15 Given an MST for an edge-weighted graph G and a new edge e, describe how to \n\ufb01nd ", "start": 643, "end": 644}, "936": {"text": "Describe how to \ufb01nd an MST of the new graph in time \nproportional to E.\n4.3.15 Given an MST for an edge-weighted graph G and a new edge e, describe how to \n\ufb01nd an MST of the new graph in time proportional to V.\n4.3.16 Given an MST for an edge-weighted graph G and a new edge e, write a program \nthat determines the range of weights for which e is in an MST.\n4.3.17  Implement toString() for EdgeWeightedGraph.\n4.3.18 Give traces that show the process of computing the MST of the graph de\ufb01ned \nin Exercise 4.3.6 with the lazy version of Prim\u2019s algorithm, the eager version of Prim\u2019s \nalgorithm, and Kruskal\u2019s algorithm.\n4.3.19 Suppose that you use a priority-queue implementation that maintains a sorted \nlist. What would be the order of growth of the worst-case running time for Prim\u2019s algo-\nrithm and for Kruskal\u2019s algorithm for graphs with V vertices and E edges? When would \nthis method be appropriate, if ever? Defend your answer.\n4.3.20 True or false: At any point dur ing the execution of  Kruskal\u2019s algor ithm, each \nvertex is closer to some vertex in its subtree than to any vertex not in its subtree. Prove \nyour answer.\n4.3.21  Provide an implementation of edges() for PrimMST (page 622).\nSolution :\npublic Iterable<Edge> edges() \n{\n   Bag<Edge> mst = new Bag<Edge>();\n   for (int v = 1; v < edgeTo.length; v++)\n      mst.add(edgeTo[v]);\n   return mst; \n}\nEXERCISES  (continued)\n632 CHAPTER 4 \u25a0 Graphs\n CREATIVE PROBLEMS\n \n4.3.22 ", "start": 644, "end": 645}, "937": {"text": "edgeTo.length; v++)\n      mst.add(edgeTo[v]);\n   return mst; \n}\nEXERCISES  (continued)\n632 CHAPTER 4 \u25a0 Graphs\n CREATIVE PROBLEMS\n \n4.3.22    Minimum spanning forest. Develop versions of Prim\u2019s and Kruskal\u2019s algorithms \nthat compute the minimum spanning forest of an edge-weighted graph that is not nec-\nessarily connected. Use the connected-components API of Section 4.1 and \ufb01nd MSTs \nin each component.\n4.3.23    Vyssotsky\u2019s algorithm. Develop an implementation that computes the MST by \napplying the cycle property (see Exercise 4.3.8) repeatedly: Add edges one at a time to \na putative tree, deleting a maximum-weight edge on the cycle if one is formed. Note : \nThis method has received less attention than the others that we consider because of the \ncomparative dif\ufb01culty of maintaining a data structure that supports ef\ufb01cient imple -\nmentation of the \u201cdelete the maximum-weight edge on the cycle\u2019\u2019 operation.\n4.3.24    Reverse-delete algorithm. Develop an implementation that computes the MST \nas follows: Start with a graph containing all of the edges. Then repeatedly go through \nthe edges in decreasing order of weight. For each edge, check if deleting that edge will \ndisconnect the graph; if not, delete it. Prove that this algorithm computes the MST. \nWhat is the order of growth of the number of edge-weight compares performed by your \nimplementation?\n4.3.25  Worst-case generator. Develop a reasonable generator for edge-weighted graphs \nwith V vertices and E edges such that the running time of the lazy version of Prim\u2019s al-\ngorithm is nonlinear. Answer the same question for the eager version.\n4.3.26    Critical edges. An MST edge whose deletion from the graph would cause the \nMST ", "start": 645, "end": 645}, "938": {"text": "running time of the lazy version of Prim\u2019s al-\ngorithm is nonlinear. Answer the same question for the eager version.\n4.3.26    Critical edges. An MST edge whose deletion from the graph would cause the \nMST weight to increase is called a  critical edge. Show how to \ufb01nd all critical edges in a \ngraph in time proportional to E log E . Note : This question assumes that edge weights \nare not necessarily distinct (otherwise all edges in the MST are critical).\n4.3.27    Animations. Write a client program that does dynamic graphical animations \nof MST algorithms. Run your program for mediumEWG.txt to produce images like the \n\ufb01gures on page 621 and page 624.\n4.3.28    Space-ef\ufb01cient data structures. Develop an implementation of the lazy ver -\nsion of Prim\u2019s algorithm   that saves space by using lower-level data structures for \nEdgeWeightedGraph and for MinPQ instead of Bag and Edge. Estimate the amount of \nmemory saved as a function of V and E, using the memory-cost model of Section 1.4 \n(see Exercise 4.3.11).\n6334.3 \u25a0 Minimum Spanning Trees\n 4.3.29    Dense graphs. Develop an implementation of Prim\u2019s algorithm that uses an \neager approach (but not a priority queue) and computes the MST using V 2 edge-weight \ncomparisons. \n4.3.30    Euclidean weighted graphs. Modify your solution to Exercise 4.1.37 to cre-\nate an API EuclideanEdgeWeightedGraph for graphs whose vertices are points in the \nplane, so that you can work with graphical representations.\n4.3.31  MST weights.  Develop implementations of weight() for LazyPrimMST, \nPrimMST, and KruskalMST, using a lazy strategy that iterates through the MST edges ", "start": 645, "end": 646}, "939": {"text": "representations.\n4.3.31  MST weights.  Develop implementations of weight() for LazyPrimMST, \nPrimMST, and KruskalMST, using a lazy strategy that iterates through the MST edges \nwhen the client calls weight().Then develop alternate implementations that use an \neager strategy that maintains a running total as the MST is computed.\n4.3.32  Speci\ufb01ed set. Given a connected edge-weighted graph G and a speci\ufb01ed set of \nedges S (having no cycles), describe a way to \ufb01nd a minimum-weight spanning tree of \nG that contains all the edges in S.\n4.3.33   Certi\ufb01cation. Write an MST and EdgeWeightedGraph client check() that uses \nthe following    cut optimality conditions  implied by Proposition J to verify that a pro -\nposed set of edges is in fact an MST: A set of edges is an MST if it is a spanning tree and \nevery edge is a minimum-weight edge in the cut de\ufb01ned by removing that edge from \nthe tree. What is the order of growth of the running time of your method?\nCREATIVE PROBLEMS  (continued)\n634 CHAPTER 4 \u25a0 Graphs\n EXPERIMENTS\n \n4.3.34    Random sparse edge-weighted graphs. Write a random-sparse-edge-weighted-\ngraph generator based on your solution to Exercise 4.1.41. To assign edge weights, \nde\ufb01ne a random-edge-weighted digraph ADT and write two implementations: one that \ngenerates uniformly distributed weights, another that generates weights according to a \nGaussian distribution. Write client programs to generate sparse random edge-weighted \ngraphs for both weight distributions with a well-chosen set of values of V and E so that \nyou can use them to run empirical tests on graphs drawn from various distributions of \nedge weights.\n4.3.35 ", "start": 646, "end": 647}, "940": {"text": "edge-weighted \ngraphs for both weight distributions with a well-chosen set of values of V and E so that \nyou can use them to run empirical tests on graphs drawn from various distributions of \nedge weights.\n4.3.35    Random Euclidean edge-weighted graphs. Modify your solution to Exercise \n4.1.42 to assign the distance between vertices as each edge\u2019s weight.\n4.3.36    Random grid edge-weighted graphs. Modify your your solution to Exercise \n4.1.43 to assign a random weight (between 0 and 1) to each edge.\n4.3.37  Real edge-weighted graphs. Find a large weighted graph somewhere online\u2014\nperhaps a map with distances, telephone connections with costs, or an airline rate \nschedule. Write a program RandomRealEdgeWeightedGraph that builds a weighted \ngraph by choosing V vertices at random and E weighted edges at random from the \nsubgraph induced by those vertices.\nTesting all algorithms and study ing all parameters against all graph models is unrealistic. \nFor each problem listed below, write a client that addresses the problem for any given input \ngraph, then choose among the generators above to run experiments for that graph model. \nUse your judgment in selecting experiments, perhaps in response to results of previous \nexperiments. Write a narrative explaining your results and any conclusions that might be \ndrawn.\n4.3.38  Cost of laziness. Run empirical studies to compare the performance of the lazy \nversion of Prim\u2019s algorithm with the eager version, for various types of graphs.\n4.3.39    Prim versus Kruskal. Run empirical studies to compare the performance of the \nlazy and eager versions of Prim\u2019s algorithm with Kruskal\u2019s algorithm.\n4.3.40  Reduced overhead. Run empirical studies to determine the effect of using \nprimitive types instead of Edge values in EdgeWeightedGraph, as described in Exer-\ncise 4.3.28.\n6354.3 ", "start": 647, "end": 647}, "941": {"text": "Kruskal\u2019s algorithm.\n4.3.40  Reduced overhead. Run empirical studies to determine the effect of using \nprimitive types instead of Edge values in EdgeWeightedGraph, as described in Exer-\ncise 4.3.28.\n6354.3 \u25a0 Minimum Spanning Trees\n  \n \n4.3.41  Longest MST edge. Run empirical studies to analyze the length of the longest \nedge in the MST and the number of graph edges that are not longer than that one.\n4.3.42  Partitioning. Develop an implementation based on integrating Kruskal\u2019s al-\ngorithm with quicksort partitioning (instead of using a priority queue) so as to check \nMST membership of each edge as soon as all smaller edges have been checked.\n4.3.43       Boruvka\u2019s algorithm. Develop an implementation of  Boruvka\u2019s algorithm: \nBuild an MST by adding edges to a growing forest of trees, as in Kruskal\u2019s algorithm, \nbut in stages. At each stage, \ufb01nd the minimum-weight edge that connects each tree to a \ndifferent one, then add all such edges to the MST. Assume that the edge weights are all \ndifferent, to avoid cycles. Hint : Maintain in a vertex-indexed array to identify the edge \nthat connects each component to its nearest neighbor, and use the union-\ufb01nd data \nstructure. \n4.3.44    Improved Boruvka. Develop an implementation of  Boruvka\u2019s algorithm that \nuses doubly-linked circular lists to represent MST subtrees so that subtrees can be \nmerged and renamed in time proportional to E during each stage (and the union-\ufb01nd \nADT is therefore not needed).\n4.3.45  External MST. Describe how you would \ufb01nd the MST of a graph so large that \nonly V edges can \ufb01t into main memory at once.\n4.3.46 ", "start": 647, "end": 648}, "942": {"text": "not needed).\n4.3.45  External MST. Describe how you would \ufb01nd the MST of a graph so large that \nonly V edges can \ufb01t into main memory at once.\n4.3.46  Johnson\u2019s algorithm. Develop a priority-queue implementation that uses a d-\nway heap (see Exercise 2.4.41). Find the best value of d for various weighted graph \nmodels.\nEXPERIMENTS  (continued)\n636 CHAPTER 4 \u25a0 Graphs\n This page intentionally left blank \n 4.4    SHORTEST PATHS\n \nPerhaps the most intuitive  graph-processing problem is one that you encounter \nregularly, when using a map application or a navigation system to get directions from \none place to another. A graph model is immediate: vertices correspond to intersections \nand edges correspond to roads, with weights on the edges that model the cost, perhaps \ndistance or travel time. The possibility of one-way roads means that we will need to \nconsider edge-weighted digraphs. In this model, the problem is easy to formulate:\nFind the lowest-cost way to get from one vertex to another. \nBeyond direct applications of this sort, the shortest-paths model is appropriate for a \nrange of other problems, some of which do not seem to be at all related to graph pro -\ncessing. As one example, we shall consider the arbitrage problem from computational \n\ufb01nance at the end of this section.\nWe adopt a general model where \nwe work with  edge-weighted digraphs\n(combining the models of Section \n4.2 and Section 4.3 ). In Section \n4.2 we wished to know whether it is \npossible to get from one vertex to an-\nother; in this section, we take weights \ninto consideration, as we did for un-\ndirected edge-weighted graphs in \nSection 4.3 . Every directed path in \nan edge-weighted ", "start": 648, "end": 650}, "943": {"text": "from one vertex to an-\nother; in this section, we take weights \ninto consideration, as we did for un-\ndirected edge-weighted graphs in \nSection 4.3 . Every directed path in \nan edge-weighted digraph has an associated path weight, \nthe value of which is the sum of the weights of that path\u2019s \nedges. This essential measure allows us to formulate such \nproblems as \u201c\ufb01nd the lowest-weight directed path from \none vertex to another, \u2019\u2019 the topic of this section. The \ufb01g-\nure at left shows an example. \n D e f i n i t i o n .  A  shortest path from vertex s to vertex t\nin an    edge-weighted digraph is a directed path from \ns to t with the property that no other such path has a \nlower weight.\nAn edge-weighted digraph and a shortest path\n4->5  0.35 \n5->4  0.35 \n4->7  0.37 \n5->7  0.28 \n7->5  0.28 \n5->1  0.32 \n0->4  0.38\n0->2  0.26 \n7->3  0.39 \n1->3  0.29 \n2->7  0.34\n6->2  0.40 \n3->6  0.52\n6->0  0.58\n6->4  0.93 \n0->2  0.26\n2->7  0.34\n7->3  0.39\n3->6  0.52 \nedge-weighted digraph\nshortest path from 0 to 6\napplication vertex edge\nmap intersection road\nnetwork router connection\nschedule job precedence constraint\narbitrage currency exchange rate\nTypical ", "start": 650, "end": 650}, "944": {"text": "0.52 \nedge-weighted digraph\nshortest path from 0 to 6\napplication vertex edge\nmap intersection road\nnetwork router connection\nschedule job precedence constraint\narbitrage currency exchange rate\nTypical shortest-paths applications\n638\n Thus, in this section, we consider classic algorithms for the following problem:\n  S i n g l e - s o u r c e  s h o r t e s t  p a t h s .  Given an edge-weighted digraph and a source ver-\ntex s, support queries of the form Is there a directed path from  s to a given target \nvertex t? If so, \ufb01nd a shortest such path (one whose total weight is minimal).\nThe plan of the section is to cover the following list of topics:\n\u25a0 Our APIs and implementations for edge-weighted digraphs, and a single-source \nshortest-paths API\n\u25a0 The classic Dijkstra\u2019s algorithm for the problem when weights are nonnegative\n\u25a0 A faster algorithm for acyclic edge-weighted digraphs (edge-weighted DAGs) \nthat works even when edge weights can be negative\n\u25a0 \n \nThe classic Bellman-Ford algorithm for use in the general case, when cycles may \nbe present, edge weights may be negative, and we need algorithms for \ufb01nding \nnegative-weight cycles and shortest paths in edge-weighted digraphs with no \nsuch cycles \nIn the context of the algorithms, we also consider applications. \nProperties of shortest paths The basic de\ufb01nition of the shortest-paths problem  \nis succinct, but its brevity masks several points worth examining before we begin to \nformulate algorithms and data structures for solving it:\n\u25a0 Paths are directed. A shortest path must respect the direction of its edges. \n\u25a0 The weights are not necessarily distances. Geometric intuition can be helpful in \nunderstanding algorithms, so we use examples where vertices are ", "start": 650, "end": 651}, "945": {"text": "it:\n\u25a0 Paths are directed. A shortest path must respect the direction of its edges. \n\u25a0 The weights are not necessarily distances. Geometric intuition can be helpful in \nunderstanding algorithms, so we use examples where vertices are points in the \nplane and weights are Euclidean distances, such as the digraph on the facing \npage. But the weights might represent time or cost or an entirely different vari-\nable and do not need to be proportional to a distance at all. We are emphasizing \nthis point by using mixed-metaphor terminology where we refer to a shortest\npath of minimal weight or cost. \n\u25a0 Not all vertices need be reachable. If t is not reachable from s, there is no path at \nall, and therefore there is no shortest path from s to t. For simplicity, our small \nrunning example is strongly connected (every vertex is reachable from every \nother vertex).\n\u25a0 Negative weights introduce complications. For the moment, we assume that edge \nweights are positive (or zero). The surprising impact of negative weights is a \nmajor focus of the last part of this section.\n\u25a0 \n \nShortest paths are normally simple. Our algorithms ignore zero-weight edges that \nform cycles, so that the shortest paths they \ufb01nd have no cycles. \n\u25a0 Shortest paths are not necessarily unique. There may be multiple paths of the low-\n6394.4 \u25a0 Shortest Paths\n  \nest weight from one vertex to another; we are content \nto \ufb01nd any one of them.\n\u25a0 Parallel edges and  self-loops may be present. Only the \nlowest-weight among a set of parallel edges will play \na role, and no shortest path contains a self-loop (ex-\ncept possibly one of zero weight, which we ignore). \nIn the text, we implicitly assume that parallel edges \nare not present for convenience in using the notation \nv->w to refer unambiguously to the edge from v to w, \nbut ", "start": 651, "end": 652}, "946": {"text": "weight, which we ignore). \nIn the text, we implicitly assume that parallel edges \nare not present for convenience in using the notation \nv->w to refer unambiguously to the edge from v to w, \nbut our code handles them without dif\ufb01culty.\n  S h o r t e s t - p a t h s  t r e e  We focus on the single-source \nshortest-paths problem, where we are given a source ver-\ntex s. The result of the computation is a tree known as \nthe shortest-paths tree (SPT), which gives a shortest path \nfrom s to every vertex reachable from s.\nDefinition. Given an edge-weighted digraph and \na designated vertex s, a shortest-paths tree for a \nsource s is a subgraph containing s and all the ver-\ntices reachable from s that forms a directed tree \nrooted at s such that every tree path is a shortest \npath in the digraph.\nSuch a tree always exists: in general there may be two \npaths of the same length connecting s to a vertex; if that \nis the case, we can delete the \ufb01nal edge on one of them, \ncontinuing until we have only \none path connecting the source \nto each vertex (a    rooted tree). \nBy building a shortest-paths \ntree, we can provide clients \nwith the shortest path from \ns to any vertex in the graph, \nusing a    parent-link represen-\ntation, in precisely the same \nmanner as for paths in graphs \nin Section 4.1. \nShortest-paths trees\n0 6->0\n1 null\n2 6->2\n3 1->3\n4 6->4\n5 7->5\n6 3->6\n7 2->7  0 6->0\n1 5->1\n2 ", "start": 652, "end": 652}, "947": {"text": "6->2\n3 1->3\n4 6->4\n5 7->5\n6 3->6\n7 2->7  0 6->0\n1 5->1\n2 null\n3 1->3\n4 5->4\n5 7->5\n6 3->6\n7 2->7  \n0 null\n1 5->1\n2 0->2\n3 7->3\n4 0->4\n5 4->5\n6 3->6\n7 2->7  \n0 6->0\n1 5->1\n2 6->2\n3 null\n4 6->4\n5 7->5\n6 3->6\n7 2->7  0 6->0\n1 5->1\n2 6->2\n3 7->3\n4 null\n5 4->5\n6 3->6\n7 4->7  0 6->1\n1 5->1\n2 6->2\n3 1->3\n4 5->4\n5 null\n6 3->6\n7 5->7  0 6->0\n1 5->1\n2 6->2\n3 7->3\n4 6->4\n5 7->5\n6 null\n7 2->7  0 6->0\n1 5->1\n2 6->2\n3 7->3\n4 5->4\n5 7->5\n6 3->6\n7 null  \nparent-edge array\nrepresentation\nsource\nAn SPT with 250 vertices\nsource\nedges point away\nfrom the source\n640 CHAPTER 4 \u25a0 Graphs\n Edge-weighted digraph data types Our data type for ", "start": 652, "end": 653}, "948": {"text": "\nparent-edge array\nrepresentation\nsource\nAn SPT with 250 vertices\nsource\nedges point away\nfrom the source\n640 CHAPTER 4 \u25a0 Graphs\n Edge-weighted digraph data types Our data type for directed edges is sim-\npler than for undirected edges because we follow directed edges in just one direction. \nInstead of the either() and other() methods in Edge, we have from() and to()\nmethods:\npublic class  DirectedEdge \nDirectedEdge(int v, int w, double weight) \ndouble weight() weight of this edge\nint from() vertex this edge points from\nint to() vertex this edge points to\nString toString() string representation\nWeighted directed-edge API\nAs with our transition from Graph to EdgeWeightedGraph from Section\u00a04.1 to Sec-\ntion 4.3, we include an edges() method and use DirectedEdge instead of integers:\npublic class    EdgeWeightedDigraph \nEdgeWeightedDigraph(int V) empty V-vertex digraph\nEdgeWeightedDigraph(In in) construct from in \nint V() number of vertices\nint E() number of edges\nvoid addEdge(DirectedEdge e) add e to this digraph\nIterable<DirectedEdge> adj(int v) edges pointing from v\nIterable<DirectedEdge> edges() all edges in this digraph\nString toString() string representation\nEdge-weighted digraph API\n \n \nYo u  c a n  \ufb01 n d  i m p l e m e n t a t i o n s  o f  t h e s e  t wo  A P Is  o n  t h e  f o l l ow i n g  t wo  p a g e s . T h e s e  a re  \nnatural extensions of the implementations of Section 4.2 and Section 4.3. Instead of \nthe adjacency lists of integers used in Digraph, ", "start": 653, "end": 653}, "949": {"text": "wo  p a g e s . T h e s e  a re  \nnatural extensions of the implementations of Section 4.2 and Section 4.3. Instead of \nthe adjacency lists of integers used in Digraph, we have adjacency lists of DirectedEdge \nobjects in EdgeWeightedDigraph. As with the transition from Graph to Digraph from \nSection\u00a04.1 to Section 4.2, the transition from EdgeWeightedGraph in Section 4.3 to \nEdgeWeightedDigraph in this section simpli\ufb01es the code, since each edge appears only \nonce in the data structure. \n6414.4 \u25a0 Shortest Paths\n  D i r e c t e d  w e i g h t e d  e d g e  d a t a  t y p e\npublic class  DirectedEdge \n{\n   private final int v;                       // edge source\n   private final int w;                       // edge target\n   private final double weight;               // edge weight\n   public DirectedEdge(int v, int w, double weight)\n   {\n      this.v = v;\n      this.w = w;\n      this.weight = weight;\n   }\n   public double weight()\n   {  return weight;  }\n   public int from()\n   {  return v;  }\n   public int to()\n   {  return w;  }\n   public String toString()\n   {  return String.format(\"%d->%d %.2f\", v, w, weight);  }  \n}\nThis DirectedEdge implementation is simpler than the undirected weighted Edge implementation \nof Section 4.3 (see page 610) because the two vertices are distinguished. Our clients use the idiomatic \ncode int v = e.to(), w = e.from(); to access a DirectedEdge e\u2019s two vertices.\n642 CHAPTER 4 \u25a0 Graphs  E d g e - w e i g h t ", "start": 653, "end": 655}, "950": {"text": "use the idiomatic \ncode int v = e.to(), w = e.from(); to access a DirectedEdge e\u2019s two vertices.\n642 CHAPTER 4 \u25a0 Graphs  E d g e - w e i g h t e d  d i g r a p h  d a t a  t y p e\npublic class  EdgeWeightedDigraph \n{\n   private final int V;               // number of vertices\n   private int E;                     // number of edges\n   private Bag<DirectedEdge>[] adj;   // adjacency lists\n   public EdgeWeightedDigraph(int V)\n   {\n      this.V = V;\n      this.E = 0;\n      adj = (Bag<DirectedEdge>[]) new Bag[V];\n      for (int v = 0; v < V; v++) \n         adj[v] = new Bag<DirectedEdge>();\n   }\n   public EdgeWeightedDigraph(In in)\n   // See Exercise 4.4.2.\n   public int V() {  return V;  }\n   public int E() {  return E;  }\n   public void addEdge(DirectedEdge e)\n   {\n      adj[e.from()].add(e);\n      E++;\n   }\n   public Iterable<Edge> adj(int v)\n   {  return adj[v];  }\n   public Iterable<DirectedEdge> edges()\n   {\n      Bag<DirectedEdge> bag = new Bag<DirectedEdge>();\n      for (int v = 0; v < V; v++)\n         for (DirectedEdge e : adj[v])\n            bag.add(e);\n      return bag;\n   }\n}\nThis EdgeWeightedDigraph implementation is an amalgam of EdgeWeightedGraph and Digraph\nthat maintains a vertex-indexed array of bags of DirectedEdge objects. As with Digraph, every \nedge appears just once: if an edge connects v to w, it appears in v\u2019s adjacency list.  Self-loops and ", "start": 655, "end": 655}, "951": {"text": "maintains a vertex-indexed array of bags of DirectedEdge objects. As with Digraph, every \nedge appears just once: if an edge connects v to w, it appears in v\u2019s adjacency list.  Self-loops and \nparallel edges are allowed. The toString() implementation is left as Exercise 4.4.2.\n6434.4 \u25a0 Shortest Paths The \ufb01gure above shows the data structure that EdgeWeightedDigraph builds to rep-\nresent the digraph de\ufb01ned by the edges at left when they are added in the order they \nappear. As usual, we use Bag to represent    adjacency lists and depict them as linked lists, \nthe standard representation. As with the unweighted digraphs of Section\u00a04.2, only one \nrepresentation of each edge appears in the data structure.\nShortest-paths API. For shortest paths, we use the same design paradigm as for the \nDepthFirstPaths and BreadthFirstPaths APIs in Section 4.1. Our algorithms im-\nplement the following API to provide clients with shortest paths and their lengths:\npublic class    SP \nSP(EdgeWeightedDigraph G, int s) constructor\ndouble distTo(int v) distance from s\nto v, \u221e if no path\nboolean hasPathTo(int v) path from s to v?\nIterable<DirectedEdge> pathTo(int v) path from s to v,\nnull if none\n A P I  f o r  s h o r t e s t - p a t h s  i m p l e m e n t a t i o n s\nThe constructor builds the shortest-paths tree and computes shortest-paths distances; \nthe client query methods use those data structures to provide distances and iterable \npaths to the client.\nEdge-weighted digraph representation\nadj[]\n0\n1\n2\n3\n4\n5\n6\n7\n0 2 .26 ", "start": 655, "end": 656}, "952": {"text": "query methods use those data structures to provide distances and iterable \npaths to the client.\nEdge-weighted digraph representation\nadj[]\n0\n1\n2\n3\n4\n5\n6\n7\n0 2 .26 0 4 .38\nBag objects\nreference to a\nDirectedEdge\nobject\n8\n15\n4 5  0.35 \n5 4  0.35 \n4 7  0.37 \n5 7  0.28 \n7 5  0.28 \n5 1  0.32 \n0 4  0.38\n0 2  0.26 \n7 3  0.39 \n1 3  0.29 \n2 7  0.34\n6 2  0.40 \n3 6  0.52\n6 0  0.58\n6 4  0.93\n1 3 .29\n2 7 .34\n3 6 .52\n4 7 .37 4 5 .35\n5 1 .32 5 7 .28 5 4 .35\n6 4 .93 6 0 .58 6 2 .40\n7 3 .39 7 5 .28\ntinyEWD.txt\nV\nE\n644 CHAPTER 4 \u25a0 Graphs\n  \nTe s t  c l i e n t . A sample client is shown below. It takes an input stream and source vertex \nindex as command-line arguments, reads the edge-weighted digraph from the input \nstream, computes the SPT of that digraph for the source, and prints the shortest path \nfrom the source to each of the other \nvertices. We assume that all of our \nshortest-paths implementations in-\nclude this test client. Our examples \nuse the ", "start": 656, "end": 657}, "953": {"text": "digraph for the source, and prints the shortest path \nfrom the source to each of the other \nvertices. We assume that all of our \nshortest-paths implementations in-\nclude this test client. Our examples \nuse the \ufb01le tinyEWD.txt shown on \nthe facing page, which de\ufb01nes the \nedges and weights that are used in \nthe small sample digraph that we \nuse for detailed traces of shortest-\npaths algorithms. It uses the same \n\ufb01le format that we used for MST al -\ngorithms: the number of vertices V \nand the number of edges E followed \nby E lines, each with two vertex in-\ndices and a weight. Y ou can also \ufb01nd \non the booksite \ufb01les that de\ufb01ne sev-\neral larger edge-weighted digraphs, \nincluding the \ufb01le mediumEWD.txt which de\ufb01nes the 250-vertex graph drawn on page 640. \nIn the drawing of the graph, every line represents edges in both directions, so this \ufb01le \nhas twice as many lines as the corresponding \ufb01le mediumEWG.txt that we examined for \nMSTs. In the drawing of the SPT, each line represents a directed edge pointing away \nfrom the source.\npublic static void main(String[] args) \n{\n   EdgeWeightedDigraph G; \n   G = new EdgeWeightedDigraph(new In(args[0]));\n   int s = Integer.parseInt(args[1]);\n   SP sp = new SP(G, s);\n   for (int t = 0; t < G.V(); t++)\n   {\n      StdOut.print(s + \" to \" + t);\n      StdOut.printf(\" (%4.2f): \", sp.distTo(t));\n      if (sp.hasPathTo(t))\n         for (DirectedEdge e : sp.pathTo(t))\n            StdOut.print(e + \"   \");\n ", "start": 657, "end": 657}, "954": {"text": "StdOut.printf(\" (%4.2f): \", sp.distTo(t));\n      if (sp.hasPathTo(t))\n         for (DirectedEdge e : sp.pathTo(t))\n            StdOut.print(e + \"   \");\n      StdOut.println();\n   } \n}\n S h o r t e s t  p a t h s  t e s t  c l i e n t\n% java SP tinyEWD.txt 0 \n0 to 0 (0.00): \n0 to 1 (1.05): 0->4 0.38  4->5 0.35  5->1 0.32 \n0 to 2 (0.26): 0->2 0.26 \n0 to 3 (0.99): 0->2 0.26  2->7 0.34  7->3 0.39 \n0 to 4 (0.38): 0->4 0.38 \n0 to 5 (0.73): 0->4 0.38  4->5 0.35 \n0 to 6 (1.51): 0->2 0.26  2->7 0.34  7->3 0.39  3->6 0.52 \n0 to 7 (0.60): 0->2 0.26  2->7 0.34\n6454.4 \u25a0 Shortest Paths\n Data structures for shortest paths. The data structures that we need to represent \nshortest paths are straightforward:\n\u25a0 Edges on the shortest-paths tree : As for DFS, BFS, and Prim\u2019s algorithm, we use a \nparent-edge representation in the form of a vertex-indexed array edgeTo[] of \nDirectedEdge objects, where edgeTo[v] is edge that connects v to its parent in ", "start": 657, "end": 658}, "955": {"text": "BFS, and Prim\u2019s algorithm, we use a \nparent-edge representation in the form of a vertex-indexed array edgeTo[] of \nDirectedEdge objects, where edgeTo[v] is edge that connects v to its parent in \nthe tree (the last edge on a shortest path from s to v).\n\u25a0 Distance to the source : We use a vertex-indexed array distTo[] such that \ndistTo[v] is the length of the shortest known path from s to v.\nBy convention, edgeTo[s] is null and distTo[s] is 0. We also adopt the con -\nvention that distances to vertices that are not reachable from the source are all \nDouble.POSITIVE_INFINITY. As usual, we will develop data types that build these \ndata structures in the constructor and \nthen support instance methods that use \nthem to support client queries for short-\nest paths and shortest-path distances. \n  E d g e  r e l a x a t i o n .  Our shortest-paths \nimplementations are based on a sim-\nple operation known as relaxation. We \nstart knowing only the graph\u2019s edges \nand weights, with the distTo[] en -\ntry for the source initialized to 0 and all of the other distTo[] entries initialized to \nDouble.POSITIVE_INFINITY.  As an algorithm proceeds, it gathers information about \nthe shortest paths that connect the source to each vertex encountered in our edgeTo[]\nand distTo[] data structures. By updating this information when we encounter edges, \nwe can make new inferences about shortest paths. Speci\ufb01cally, we use edge relaxation, \nde\ufb01ned as follows: to relax an edge v->w means to test whether the best known way \nfrom s to w is to go from s to v, then take the edge from v to w, and, if so, update our \ndata structures to indicate that \nto be the case. The code ", "start": 658, "end": 658}, "956": {"text": "best known way \nfrom s to w is to go from s to v, then take the edge from v to w, and, if so, update our \ndata structures to indicate that \nto be the case. The code at the \nright implements this opera -\ntion. The best known distance \nto w through v is the sum of \ndistTo[v] and e.weight()\u2014\nif that value is not smaller than \ndistTo[w], we say the edge is \n     ineligible, and we ignore it; if it \nis smaller, we update the data \nShortest-paths data structures\n    edgeTo[]    distTo[]\n 0    null        0\n 1    5->1 0.32   1.05\n 2    0->2 0.26   0.26\n 3    7->3 0.37   0.97\n 4    0->4 0.38   0.38\n 5    4->5 0.35   0.73\n 6    3->6 0.52   1.49\n 7    2->7 0.34   0.60\nprivate void relax(DirectedEdge e) \n{\n   int v = e.from(), w = e.to();\n   if (distTo[w] > distTo[v] + e.weight())\n   {\n       distTo[w] = distTo[v] + e.weight();\n       edgeTo[w] = e;\n   } \n}\n E d g e  r e l a x a t i o n\n646 CHAPTER 4 \u25a0 Graphs\n  structures. The \ufb01gure at the bottom of this page illustrates the two possible outcomes of \nan edge-relaxation operation. Either the edge is ineligible (as in the example at left) and \nno changes are made, or the ", "start": 658, "end": 659}, "957": {"text": "\ufb01gure at the bottom of this page illustrates the two possible outcomes of \nan edge-relaxation operation. Either the edge is ineligible (as in the example at left) and \nno changes are made, or the edge v->w leads to a shorter path to w (as in the example \nat right) and we update edgeTo[w] and distTo[w] (which might render some other \nedges ineligible and might create some new eligible edges). The term relaxation follows \nfrom the idea of a rubber band stretched tight on a path connecting two vertices: relax-\ning an edge is akin to relaxing the tension on the rubber band along a shorter path, if \npossible. We say that an edge e can be successfully relaxed if relax() would change the \nvalues of distTo[e.to()] and edgeTo[e.to()]. \nEdge relaxation (two cases)\nv->w is ineligible v->w is eligible     \ns\nv\nwblack edges\nare in edgeTo[]\ns\nw\ns\nv\nw\nno longer in SPT\nno changes\ns\nw\nedgeTo[w]\ndistTo[v]\ndistTo[w]\nweight of v->w is 1.3\nv\nv\n3.1\n3.3\n3.1\n3.1\n7.2\n4.4\n6474.4 \u25a0 Shortest Paths\n  \n \n  V e r t e x  r e l a x a t i o n .  All of our implementations \nactually relax all the edges pointing from a given \nvertex as shown in the (overloaded) implementa -\ntion of relax() below. Note that any edge from \na vertex whose distTo[v] entry is \ufb01nite to a ver -\ntex whose distTo[] entry is in\ufb01nite is eligible and \nwill be added to edgeTo[] if relaxed. In particu-\nlar, some edge leaving the source is the \ufb01rst ", "start": 659, "end": 660}, "958": {"text": "\ufb01nite to a ver -\ntex whose distTo[] entry is in\ufb01nite is eligible and \nwill be added to edgeTo[] if relaxed. In particu-\nlar, some edge leaving the source is the \ufb01rst to be \nadded to edgeTo[]. Our algorithms choose verti -\nces judiciously, so that each vertex relaxation \ufb01nds \na shorter path than the best known so far to some \nvertex, incrementally progressing toward the goal \nof \ufb01nding shortest paths to every vertex. Vertex relaxation\ns\nv\ns\nstill ineligiblev\nnow ineligible\nbefore\nafter\nprivate void relax(EdgeWeightedDigraph G, int v) \n{\n   for (DirectedEdge e : G.adj(v))\n   {\n      int w = e.to();\n      if (distTo[w] > distTo[v] + e.weight())\n      {\n         distTo[w] = distTo[v] + e.weight();\n         edgeTo[w] = e;\n     }\n   } \n}\n V e r t e x  r e l a x a t i o n\n648 CHAPTER 4 \u25a0 Graphs\n  \nClient query methods. In a manner similar to our implementations for path\ufb01nding \nAPIs in Section 4.1 (and Exercise 4.1.13), the edgeTo[] and distTo[] data struc-\ntures directly support the pathTo(), hasPathTo(),  and distTo() client query meth-\nods, as shown below. This code is included in all of our shortest-paths implementa-\ntions. As we have noted already, distTo[v] is only meaningful when v is reachable \nfrom s and we adopt the convention that distTo() should return in\ufb01nity for vertices \nthat are not reachable from s. T o implement this convention, we initialize all distTo[]\nentries to Double.POSITIVE_INFINITY and distTo[s] to 0; ", "start": 660, "end": 661}, "959": {"text": "distTo() should return in\ufb01nity for vertices \nthat are not reachable from s. T o implement this convention, we initialize all distTo[]\nentries to Double.POSITIVE_INFINITY and distTo[s] to 0; then our shortest-paths \nimplementations will set distTo[v] to a \ufb01nite value for all vertices v that are reachable \nfrom the source. Thus, we can dispense with the marked[] array that we normally use \nto mark reachable vertices in a graph search and implement hasPathTo(v) by testing \nwhether distTo[v] equals Double.POSITIVE_INFINITY. For pathTo(), we use the \nconvention that pathTo(v) returns null if v is \nnot reachable from the source and a path with \nno edges if v is the source. For reachable vertices, \nwe travel up the tree, pushing the edges that we \n\ufb01nd on a stack, in the same manner as we did \nfor DepthFirstPaths and BreadthFirstPaths. \nThe \ufb01gure at right shows the discovery of the \npath 0->2->7->3->6 for our example. \nTrace of  pathTo() computation\n v  edgeTo[]\n 0     null\n 1    5->1\n 2    0->2\n 3    7->3\n 4    0->4\n 5    4->5\n 6    3->6\n 7    2->7\n3->6    \n7->3   3->6\n2->7   7->3 3->6\n0->2   2->7 7->3 3->6\n       0->2 2->7 7->3 3->6\ne     path\npathTo(6)\nSPT\nnull\npublic double distTo(int v) \n{   return distTo[v]; ", "start": 661, "end": 661}, "960": {"text": "0->2 2->7 7->3 3->6\ne     path\npathTo(6)\nSPT\nnull\npublic double distTo(int v) \n{   return distTo[v];   }\npublic boolean hasPathTo(int v) \n{   return distTo[v] < Double.POSITIVE_INFINITY;  }\npublic Iterable<DirectedEdge> pathTo(int v) \n{\n   if (!hasPathTo(v)) return null;\n   Stack<DirectedEdge> path = new Stack<DirectedEdge>();\n   for (DirectedEdge e = edgeTo[v]; e != null; e = edgeTo[e.from()])\n      path.push(e);\n   return path; \n}\n C l i e n t  q u e r y  m e t h o d s  f o r  s h o r t e s t  p a t h s\n6494.4 \u25a0 Shortest Paths\n Theoretical basis for shortest-paths algorithms. Edge relaxation is an easy-\nto-implement fundamental operation that provides a practical basis for our shortest-\npaths implementations. It also provides a theoretical basis for understanding the algo-\nrithms and an opportunity for us to do our algorithm correctness proofs at the outset. \nOptimality conditions. The following proposition shows an equivalence between the \nglobal condition that the distances are shortest-paths distances, and the local condition \nthat we test to relax an edge. \n \nProposition P .  (  Shortest-paths optimality conditions) Let G be an edge-weighted \ndigraph, with s a source vertex in G and distTo[] a vertex-indexed array of path \nlengths in G such that, for all v reachable from s, the value of distTo[v] is the \nlength of some path from s to v with distTo[v] equal to in\ufb01nity for all v not reach-\nable from s. These values are the lengths of shortest paths ", "start": 661, "end": 662}, "961": {"text": "of distTo[v] is the \nlength of some path from s to v with distTo[v] equal to in\ufb01nity for all v not reach-\nable from s. These values are the lengths of shortest paths if and only if they satisfy \ndistTo[w] <= distTo[v] + e.weight() for each edge e from v to w (or, in \nother words, no edge is eligible).\nProof: Suppose that distTo[w] is the length of a shortest path from s to w. If \ndistTo[w] > distTo[v] + e.weight() for some edge e from v to w, then e\nwould give a path from s to w (through v) of length less than distTo[w], a contra-\ndiction. Thus the optimality conditions are necessary.\nTo  p rove  t h a t  t h e  o p t i m a l i t y  co n d i t i o n s  a re  s u f \ufb01 c i e n t , s u p p o s e  t h a t  w is reach-\nable from s and that s = v 0->v1->v2...->vk = w is a shortest path from s to \nw, of weight OPTsw. For i from 1 to k, denote the edge from vi-1 to vi by ei. By the \noptimality conditions, we have the following sequence of inequalities:\ndistTo[w] = distTo[vk]  <= distTo[vk-1] + ek.weight()\n            distTo[vk-1] <= distTo[vk-2] + ek-1.weight()\n            ...\n            distTo[v2]  <= distTo[v1]  + e2.weight()\n            distTo[v1]  <= distTo[s]   + e1.weight()\nCollapsing these inequalities and eliminating distTo[s] ", "start": 662, "end": 662}, "962": {"text": "distTo[v2]  <= distTo[v1]  + e2.weight()\n            distTo[v1]  <= distTo[s]   + e1.weight()\nCollapsing these inequalities and eliminating distTo[s] = 0.0, we have\ndistTo[w] <= e1.weight() + ... + ek.weight() = OPTsw.\nNow, distTo[w] is the length of some path from s to w, so it cannot be smaller than \nthe length of a shortest path. Thus, we have shown that\nOPTsw <= distTo[w] <=  OPTsw\nand equality must hold.\n650 CHAPTER 4 \u25a0 Graphs\n  \n \n \n \n  C e r t i \ufb01 c a t i o n .  An important practical consequence of Proposition P is its applicabil-\nity to certi\ufb01cation. However an algorithm computes distTo[], we can check whether it \ncontains shortest-path lengths in a single pass through the edges of the graph, checking \nwhether the optimality conditions are satis\ufb01ed. Shortest-paths algorithms can be com-\nplicated, and this ability to ef\ufb01ciently test their outcome is crucial. We include a method \ncheck() in our implementations on the booksite for this purpose. This method also \nchecks that edgeTo[] speci\ufb01es paths from the source and is consistent with distTo[].\nGeneric algorithm. The optimality conditions lead immediately to a generic algo-\nrithm that encompasses all of the shortest-paths algorithms that we consider. For the \nmoment, we restrict attention to nonnegative weights.\n \n \n \n \nProposition Q.  (  Generic shortest-paths algorithm) Initialize distTo[s] to 0 and \nall other distTo[] values to in\ufb01nity, and proceed as follows: \nRelax any edge in G, continuing until no edge is eligible.\nFor all vertices w reachable from s, the ", "start": 662, "end": 663}, "963": {"text": "0 and \nall other distTo[] values to in\ufb01nity, and proceed as follows: \nRelax any edge in G, continuing until no edge is eligible.\nFor all vertices w reachable from s, the value of distTo[w] after this computation \nis the length of a shortest path from s to w (and the value of edgeTo[] is the last \nedge on that path).\nProof: Relaxing an edge v->w always sets distTo[w] to the length of some path \nfrom s (and edgeTo[w] to the last edge on that path). For any vertex w reachable \nfrom s, some edge on the shortest path to w is eligible as long as distTo[w] remains \nin\ufb01nite, so the algorithm continues until the distTo[] value of each vertex reach-\nable from s is the length of some path to that vertex. For any vertex v for which the \nshortest path is well-de\ufb01ned, throughout the algorithm distTo[v] is the length of \nsome (simple) path from s to v and is strictly monotonically decreasing. Thus, it \ncan decrease at most a \ufb01nite number of times (once for each simple path from s to \nv). When no edge is eligible, Proposition P applies.\nThe key reason for considering the optimality conditions and the generic algorithm \nis that the generic algorithm does not specify in which order the edges are to be relaxed . \nThus, all that we need to do to prove that any algorithm computes shortest paths is to \nprove that it relaxes edges until no edge is eligible.\n6514.4 \u25a0 Shortest Paths\n    D i j k s t r a \u2019 s  a l g o r i t h m  In Section 4.3, we discussed Prim\u2019s algorithm for \ufb01nding \nthe minimum spanning tree (MST) of an edge-weighted undirected graph: we build \nthe ", "start": 663, "end": 664}, "964": {"text": "g o r i t h m  In Section 4.3, we discussed Prim\u2019s algorithm for \ufb01nding \nthe minimum spanning tree (MST) of an edge-weighted undirected graph: we build \nthe MST by attaching a new edge to a single growing tree at each step. Dijkstra\u2019s algo-\nrithm is an analogous scheme to compute an SPT. We begin by initializing dist[s] to \n0 and all other distTo[] entries to positive in\ufb01nity, then we relax and add to the tree a \nnon-tree vertex with the lowest distTo[] value, continuing until all vertices are on the tree \nor no non-tree vertex has a \ufb01nite distTo[] value.\n Proposition R.  Dijkstra\u2019s algorithm solves the single-source shortest-paths prob-\nlem in edge-weighted digraphs with nonnegative weights.\nProof: If v is reachable from the source, every edge v->w is relaxed exactly once, \nwhen v is relaxed, leaving distTo[w] <= distTo[v] + e.weight() . This in -\nequality holds until the algorithm completes, since distTo[w] can only decrease \n(any relaxation can only decrease a distTo[] value) and distTo[v] never changes \n(because edge weights are nonnegative and we choose the lowest distTo[] value \nat each step, no subsequent relaxation can set any distTo[] entry to a lower value \nthan distTo[v]). Thus, after all vertices reachable from s have been added to the \ntree, the shortest-paths optimality conditions hold, and Proposition P applies.\nData structures. To  i m p l e m e n t  D i j k s t r a\u2019s  a l g o r i t h m  w e  a d d  to  o u r  distTo[] and \nedgeTo[] data structures an  index  priority queue pq to keep track of ", "start": 664, "end": 664}, "965": {"text": "a\u2019s  a l g o r i t h m  w e  a d d  to  o u r  distTo[] and \nedgeTo[] data structures an  index  priority queue pq to keep track of vertices that are \ncandidates for being the next to be relaxed. Recall that an IndexMinPQ allows us to as-\nsociate indices with keys (priorities) and to remove and return the index corresponding \nto the lowest key. For this application, we always associ-\nate a vertex v with distTo[v], and we have a direct and \nimmediate implementation of Dijkstra\u2019s algorithm as \nstated. Moreover, it is immediate by induction that the \nedgeTo[] entries corresponding to reachable vertices \nform a tree, the SPT. \nAlternative viewpoint. Another way to understand \nthe dynamics of the algorithm derives from the proof, \ndiagrammed at left: we have the invariant that distTo[]\nentries for tree vertices are shortest-paths distances and \nfor each vertex w on the priority queue, distTo[w] is \nthe weight of a shortest path from s to w that uses only \nDijkstra\u2019s shortest-paths algorithm\ns\nwv\ncrossing edge\non shortest path from s\nhaving just one crossing edge\nmust be on SPT\ntree edge\n(black) crossing edge\n(red)\n652 CHAPTER 4 \u25a0 Graphs\n  \nintermediate vertices in the tree and ends in the \ncrossing edge edgeTo[w]. The distTo[] entry for \nthe vertex with the smallest priority is a shortest-\npath weight, not smaller than the shortest-path \nweight to any vertex already relaxed, and not larger \nthan the shortest-path weight to any vertex not yet \nrelaxed. That vertex is next to be relaxed. Reachable \nvertices are relaxed in order of the weight of their \nshortest path from s. \nThe \ufb01gure at right is a trace for our ", "start": 664, "end": 665}, "966": {"text": "yet \nrelaxed. That vertex is next to be relaxed. Reachable \nvertices are relaxed in order of the weight of their \nshortest path from s. \nThe \ufb01gure at right is a trace for our small sample \ngraph tinyEWD.txt. For this example, the algo -\nrithm builds the SPT as follows:\n\u25a0 Adds 0 to the tree and its adjacent vertices 2\nand 4 to the priority queue. \n\u25a0 Removes 2 from the priority queue, adds \n0->2 to the tree, and adds 7 to the priority \nqueue.\n\u25a0 Removes 4 from the priority queue, adds \n0->4 to the tree, and adds 5 to the priority \nqueue. Edge 4->7 is ineligible.\n\u25a0 Removes 7 from the priority queue, adds \n2->7 to the tree, and adds 3 to the priority \nqueue. Edge 7->5 is ineligible. \n\u25a0 Removes 5 from the priority queue, adds \n4->5 to the tree, and adds 1 to the priority \nqueue. Edge 5->7 is ineligible.\n\u25a0 Removes 3 from the priority queue, adds \n7->3 to the tree, and adds 6 to the priority \nqueue.\n\u25a0 Removes 1 from the priority queue and adds \n5->1 to the tree. Edge 1->3  is ineligible. \n\u25a0 Removes 6 from the priority queue and adds \n3->6 to the tree. \nVer t i ce s  a re  a d d e d  to  t h e  S P T  i n  i n c re a s i n g  o rd e r  o f  \ntheir distance from the source, as indicated by the \nred arrows at the right edge of the diagram.\nTrace of Dijkstra\u2019s algorithm\n0   \n 1  5->1 0.32\n 2  0->2 ", "start": 665, "end": 665}, "967": {"text": "\ntheir distance from the source, as indicated by the \nred arrows at the right edge of the diagram.\nTrace of Dijkstra\u2019s algorithm\n0   \n 1  5->1 0.32\n 2  0->2 0.26\n 3  7->3 0.37\n 4  0->4 0.38\n 5  4->5 0.35\n6  3->6 0.52\n 7  2->7 0.34 \n0   \n 1  5->1 0.32\n 2  0->2 0.26\n 3  7->3 0.37\n 4  0->4 0.38\n 5  4->5 0.35\n6  3->6 0.52   \n 7  2->7 0.34 \n0   \n1  5->1 0.32 \n 2  0->2 0.26\n 3  7->3 0.37\n 4  0->4 0.38\n 5  4->5 0.35\n6   \n 7  2->7 0.34 \n0   \n 1   \n 2  0->2 0.26\n3  7->3 0.37\n 4  0->4 0.38\n 5  4->5 0.35\n6   \n 7  2->7 0.34 \n0   \n 1   \n 2  0->2 0.26\n3   \n 4  0->4 0.38\n5  4->5 0.35  \n 6   \n 7  2->7 0.34 \n0   \n ", "start": 665, "end": 665}, "968": {"text": "0.26\n3   \n 4  0->4 0.38\n5  4->5 0.35  \n 6   \n 7  2->7 0.34 \n0   \n 1   \n 2  0->2 0.26\n3   \n 4  0->4 0.38\n5   \n 6   \n7  2->7 0.34  \n   edgeTo[]\n 0   \n 1   \n 2  0->2 0.26\n3   \n4  0->4 0.38  \n 5   \n 6   \n 7   \n0.00\n 1.05\n 0.26\n 0.97\n 0.38\n 0.73\n1.49\n 0.60\n0.00\n 1.05\n 0.26\n 0.97\n 0.38\n 0.73\n1.49   \n 0.60 \n 0.00\n1.05 \n 0.26\n 0.97\n 0.38\n 0.73\n   \n 0.60 \n0.00\n   \n 0.26\n0.97\n 0.38\n 0.73\n   \n 0.60\n0.00\n   \n 0.26\n   \n 0.38\n0.73\n   \n 0.60 \n0.00\n   \n 0.26\n    \n 0.38\n   \n    \n0.60   \ndistTo[]\n 0.00  \n   \n 0.26\n   \n0.38\n    \n    \n   \nred: on pq\nindex\n0   \n 1  5->1 0.32\n 2  0->2 0.26\n 3  7->3 0.37\n 4  0->4 0.38\n 5  4->5 0.35\n ", "start": 665, "end": 665}, "969": {"text": "0.32\n 2  0->2 0.26\n 3  7->3 0.37\n 4  0->4 0.38\n 5  4->5 0.35\n 6  3->6 0.52\n 7  2->7 0.34 \n0.00\n 1.05\n 0.26\n 0.97\n 0.38\n 0.73\n 1.49\n 0.60\npriority\nblack: on SPT\n6534.4 \u25a0 Shortest Paths\n The implementation of Dijkstra\u2019s algorithm in DijkstraSP (Algorithm 4.9) is a ren-\ndition in code of the one-sentence description of the algorithm, enabled by adding one \nstatement to relax() to handle two cases: either the to() vertex on an edge is not yet \non the priority queue, in which case we use insert() to add it to the priority queue, \nor it is already on the priority queue and its priority lowered, in which case change()\ndoes so.\nProposition R (continued).   Dijkstra\u2019s algorithm uses extra space proportional to V\nand time proportional to E log V (in the worst case) to compute the SPT rooted at \na given source in an edge-weighted digraph with E edges and V vertices.\nProof: Same as for Prim\u2019s algorithm (see Proposition N).\n \nAs we have indicated, another way to think about  Dijkstra\u2019s algorithm is to \ncompare it to Prim\u2019s MST algorithm from Section 4.3 (see page 622). Both algorithms \nbuild a rooted tree by adding an edge to a growing tree: Prim\u2019s adds next the non-tree \nvertex that is closest to the tree; Dijkstra\u2019s adds next the non-tree vertex that is closest \nto the source. The marked[] array is not needed, ", "start": 665, "end": 666}, "970": {"text": "to a growing tree: Prim\u2019s adds next the non-tree \nvertex that is closest to the tree; Dijkstra\u2019s adds next the non-tree vertex that is closest \nto the source. The marked[] array is not needed, because the condition !marked[w]\nis equivalent to the condition that distTo[w] is in\ufb01nite. In other words, switching to \nundirected graphs and edges and omitting the references to distTo[v] in the relax()\ncode in Algorithm 4.9 gives an implementation of Algorithm 4.7, the eager version \nof Prim\u2019s algorithm (!). Also, a lazy version of Dijkstra\u2019s algorithm along the lines of \nLazyPrimMST (page 619) is not dif\ufb01cult to develop. \nVa r i a n t s . Our implementation of Dijkstra\u2019s algorithm, with suitable modi\ufb01cations, is \neffective for solving other versions of the problem, such as the following:\n     S i n g l e - s o u r c e  s h o r t e s t  p a t h s  i n  u n d i r e c t e d  g r a p h s .  Given an edge-weighted un-\ndirected graph and a source vertex s, support queries of the form Is there a path \nfrom s to a given target vertex v? If so, \ufb01nd a shortest such path (one whose total \nweight is minimal).\nThe solution to this problem is immediate if we view the undirected graph as a digraph. \nThat is, given an undirected graph, build an edge-weighted digraph with the same ver-\ntices and with two directed edges (one in each direction) corresponding to each edge \nin the graph. There is a one-to-one correspondence between paths in the digraph and \npaths in the graph, and the costs of the paths are the same\u2014the shortest-paths ", "start": 666, "end": 666}, "971": {"text": "in each direction) corresponding to each edge \nin the graph. There is a one-to-one correspondence between paths in the digraph and \npaths in the graph, and the costs of the paths are the same\u2014the shortest-paths prob-\nlems are equivalent.\n654 CHAPTER 4 \u25a0 Graphs\n ALGORITHM 4.9   Dijkstra\u2019s shortest-paths algorithm\npublic class  DijkstraSP \n{\n   private DirectedEdge[] edgeTo;\n   private double[] distTo;\n   private IndexMinPQ<Double> pq;\n   public DijkstraSP(EdgeWeightedDigraph G, int s)\n   {\n      edgeTo = new DirectedEdge[G.V()];\n      distTo = new double[G.V()];\n      pq = new IndexMinPQ<Double>(G.V());\n      for (int v = 0; v < G.V(); v++)\n         distTo[v] = Double.POSITIVE_INFINITY;\n      distTo[s] = 0.0;\n      pq.insert(s, 0.0);\n      while (!pq.isEmpty())\n         relax(G, pq.delMin())\n   }\n   private void relax(EdgeWeightedDigraph G, int v)\n   {\n      for(DirectedEdge e : G.adj(v))\n      {\n         int w = e.to();\n         if (distTo[w] > distTo[v] + e.weight())\n         {\n            distTo[w] = distTo[v] + e.weight();\n            edgeTo[w] = e;\n            if (pq.contains(w)) pq.change(w, distTo[w]);\n            else                pq.insert(w, distTo[w]);\n         }\n      } \n  }\n   public double distTo(int v)           // standard client query methods\n   public boolean hasPathTo(int v)       //   for SPT implementatations\n   public Iterable<Edge> pathTo(int v)   //   (See page 649.) \n}\nThis implementation of Dijkstra\u2019s algorithm grows the SPT by ", "start": 666, "end": 667}, "972": {"text": "hasPathTo(int v)       //   for SPT implementatations\n   public Iterable<Edge> pathTo(int v)   //   (See page 649.) \n}\nThis implementation of Dijkstra\u2019s algorithm grows the SPT by adding an edge at a time, always \nchoosing the edge from a tree vertex to a non-tree vertex whose destination w is closest to s. \n6554.4 \u25a0 Shortest Paths  \n  S o u r c e - s i n k  s h o r t e s t  p a t h s .  Given an edge-weighted digraph, a source vertex s,  \nand a target vertex t, \ufb01nd the shortest path from s to t.\nTo  s o lve  t h i s  p ro b l e m , u s e  D i j k s t r a\u2019s  a l g o r i t h m , b u t  te r m i n a te  t h e  s e a rc h  a s  s o o n  a s  t\ncomes off the priority queue.\n  A l l - p a i r s  s h o r t e s t  p a t h s .  Given an edge-weighted digraph, support queries of the \nform Given a source vertex s and a target vertex t, is there a path from s to t?  If so, \n\ufb01nd a shortest such path (one whose total weight is minimal).\nThe surprisingly compact implementation at right below solves the all-pairs shortest \npaths problem, using time and space proportional to E V log V. It builds an array of \nDijkstraSP objects, one for each vertex as the source. T o answer a client query, it uses \nthe source to access the corresponding single-source shortest-paths object and then \npasses the target as argument to the query.\nShortest paths in ", "start": 667, "end": 668}, "973": {"text": "one for each vertex as the source. T o answer a client query, it uses \nthe source to access the corresponding single-source shortest-paths object and then \npasses the target as argument to the query.\nShortest paths in  Euclidean graphs. Solve the single-source, source-sink, and \nall-pairs shortest-paths problems in graphs where vertices are points in the plane \nand edge weights are proportional to Euclidean distances between vertices.\nA simple modi\ufb01cation considerably speeds up Dijkstra\u2019s algorithm in this case (see \nExercise 4.4.27).\nThe figures on the faCING page show the emergence of the SPT as computed by Di-\njkstra\u2019s algorithm for the Euclidean graph de\ufb01ned by our test \ufb01le mediumEWD.txt (see \npage 645) for several different sources. Re-\ncall that line segments in this graph rep-\nresent directed edges in both directions. \nAgain, these \ufb01gures illustrate a fascinat -\ning dynamic process.\nNext, we consider shortest-paths algo-\nrithms for acyclic edge-weighted graphs, \nwhere we can solve the problem in linear \ntime (faster than Dijkstra\u2019s algorithm) \nand then for edge-weighted digraphs \nwith negative weights, where Dijkstra\u2019s \nalgorithm does not apply.\npublic class  DijkstraAllPairsSP \n{\n   private DijkstraSP[] all;\n   DijkstraAllPairsSP(EdgeWeightedDigraph G)\n   {\n      all = new DijkstraSP[G.V()]\n      for (int v = 0; v < G.V(); v++)\n         all[v] = new DijkstraSP(G, v);\n   }\n   Iterable<Edge> path(int s, int t)\n   {  return all[s].pathTo(t);  }\n   double dist(int s, int t)\n   {  return all[s].distTo(t);  }\n}\n A l ", "start": 668, "end": 668}, "974": {"text": "Iterable<Edge> path(int s, int t)\n   {  return all[s].pathTo(t);  }\n   double dist(int s, int t)\n   {  return all[s].distTo(t);  }\n}\n A l l - p a i r s  s h o r t e s t  p a t h s\n656 CHAPTER 4 \u25a0 Graphs\n Dijkstra\u2019s algorithm (250 vertices, various sources)\n20%\n40%\n60%\n80%\nSPT\nsource\n6574.4 \u25a0 Shortest Paths\n    A c y c l i c  e d g e - w e i g h t e d  d i g r a p h s  For many natural applications, edge-weighted \ndigraphs are known to have no directed cycles. For economy, we use the equivalent \nterm  edge-weighted DAG to refer to an acyclic edge-weighted digraph. We now consider \nan algorithm for \ufb01nding shortest paths that is simpler and faster than Dijkstra\u2019s algo-\nrithm for edge-weighted DAGs. Speci\ufb01cally, it\n\u25a0 Solves the single-source problem in linear time \n\u25a0 Handles negative edge weights\n\u25a0 \n \nSolves related problems, such as \ufb01nding longest paths.\nThese algorithms are straightforward extensions to the algorithm for topological sort \nin DAGs that we considered in Section 4.2. \nSpeci\ufb01cally, vertex relaxation, in combi -\nnation with topological sorting, immedi-\nately presents a solution to the single-source \nshortest-paths problem for edge-weighted \nDAGs. We initialize distTo[s] to 0 and all \nother distTo[] values to in\ufb01nity, then relax \nthe vertices, one by one, taking the vertices \nin topological order . An argument similar \nto (but simpler than) the argument that we \nused  for ", "start": 668, "end": 670}, "975": {"text": "distTo[] values to in\ufb01nity, then relax \nthe vertices, one by one, taking the vertices \nin topological order . An argument similar \nto (but simpler than) the argument that we \nused  for Dijkstra\u2019s algorithm on page 652 es-\ntablishes the effectiveness of this method:\n Proposition S. By relaxing vertices in  topological order, we can solve the single-\nsource shortest-paths problem for edge-weighted DAGs in time proportional to \nE + V.\nProof: Every edge v->w is relaxed exactly once, when v is relaxed, leaving \ndistTo[w] <= distTo[v] + e.weight() . This inequality holds until the algo -\nrithm completes, since distTo[v] never changes (because of the topological or -\nder, no edge pointing to v will be processed after v is relaxed) and distTo[w] can \nonly decrease (any relaxation can only decrease a distTo[] value). Thus, after all \nvertices reachable from s have been added to the tree, the shortest-paths optimal-\nity conditions hold, and  Proposition Q  applies. The time bound is immediate: \nProposition G on page 583 tells us that the topological sort takes time proportional \nto E + V, and  the second relaxation pass completes the job by relaxing each edge \nonce, again in time proportional to E + V.\nAn acyclic edge-weighted digraph with an SPT\n8\n13\n5 4  0.35 \n4 7  0.37 \n5 7  0.28 \n5 1  0.32 \n4 0  0.38\n0 2  0.26 \n3 7  0.39 \n1 3  0.29 \n7 2  0.34\n6 2  0.40 \n3 6 ", "start": 670, "end": 670}, "976": {"text": "2  0.26 \n3 7  0.39 \n1 3  0.29 \n7 2  0.34\n6 2  0.40 \n3 6  0.52\n6 0  0.58\n6 4  0.93 \ntinyEWDAG.txt\nV\nE\n658 CHAPTER 4 \u25a0 Graphs\n The \ufb01gure at right is a trace for a sample acyclic \nedge-weighted digraph tinyEWDAG.txt. For this exam-\nple, the algorithm builds the shortest-paths tree from \nvertex 5 as follows:\n\u25a0 Does a DFS to discover the topological order \n5 1 3 6 4 7 0 2.\n\u25a0 Adds to the tree 5 and all edges leaving it.\n\u25a0 Adds to the tree 1 and 1->3.\n\u25a0 Adds to the tree 3 and 3->6, but not 3->7, which \nis ineligible.\n\u25a0 Adds to the tree 6 and edges 6->2 and 6->0, but \nnot 6->4, which is ineligible.\n\u25a0 Adds to the tree 4 and 4->0, but not 4->7, which \nis ineligible. Edge 6->0 becomes ineligible.\n\u25a0 Adds to the tree 7 and 7->2. Edge 6->2 becomes \nineligible.\n\u25a0 Adds 0 to the tree, but not its incident edge 0->2, \nwhich is ineligible.\n\u25a0 \n \nAdds 2 to the tree.\nThe addition of 2 to the tree is not depicted; the last \nvertex in a topological sort has no edges leaving it.\nThe implementation, shown in Algorithm 4.10, \nis a straightforward application of code we have al-\nready considered. It assumes that Topological has \noverloaded methods for the ", "start": 670, "end": 671}, "977": {"text": "topological sort has no edges leaving it.\nThe implementation, shown in Algorithm 4.10, \nis a straightforward application of code we have al-\nready considered. It assumes that Topological has \noverloaded methods for the topological sort, using the   \nEdgeWeightedDigraph and DirectedEdge APIs of this \nsection (see Exercise 4.4.12). Note that our boolean \narray marked[] is not needed in this implementation: \nsince we are processing vertices in an acyclic digraph \nin topological order, we never re-encounter a vertex \nthat we have already relaxed. Algorithm 4.10 could \nhardly be more ef\ufb01cient: after the topological sort, the \nconstructor scans the graph, relaxing each edge exactly \nonce. It is the method of choice for \ufb01nding shortest \npaths in edge-weighted graphs that are known to be \nacyclic.\nProposition S  is signi\ufb01cant because it pro -\nvides a concrete example where the absence of cycles Trace for shortest paths in an edge-weighted DAG \ngray:\nineligible\nred: add to tree\nthick black: on tree\ntopological sort\n        5 1 3 6 4 7 0 2    edgeTo[]\n0   \n 1   5->1   \n2\n3   \n 4   5->4\n 5   \n 6   \n 7   5->7  \n0   \n 1   5->1   \n2\n 3   1->3\n 4   5->4\n 5   \n 6   \n 7   5->7  \n0   \n 1   5->1   \n2\n 3   1->3\n 4   5->4\n 5   \n 6   3->6\n 7   5->7  \n 0   6->0 ", "start": 671, "end": 671}, "978": {"text": "5->1   \n2\n 3   1->3\n 4   5->4\n 5   \n 6   3->6\n 7   5->7  \n 0   6->0 \n 1   5->1   \n 2   6->2\n 3   1->3\n 4   5->4\n 5   \n 6   3->6\n 7   5->7  \n 0   4->0 \n 1   5->1   \n 2   6->2\n 3   1->3\n 4   5->4\n 5   \n 6   3->6\n 7   5->7  \n 0   4->0 \n 1   5->1   \n 2   7->2\n 3   1->3\n 4   5->4\n 5   \n 6   3->6\n 7   5->7  \n 0   4->0 \n 1   5->1   \n 2   7->2\n 3   1->3\n 4   5->4\n 5   \n 6   3->6\n 7   5->7  \n6594.4 \u25a0 Shortest Paths\n ALGORITHM 4.10   Shortest paths in edge-weighted DAGs\npublic class  AcyclicSP \n{\n   private DirectedEdge[] edgeTo;\n   private double[] distTo;\n   public AcyclicSP(EdgeWeightedDigraph G, int s)\n   {\n      edgeTo = new DirectedEdge[G.V()];\n      distTo = new double[G.V()];\n      for (int v = 0; v < G.V(); v++)\n         distTo[v] = Double.POSITIVE_INFINITY;\n      distTo[s] = 0.0;\n ", "start": 671, "end": 672}, "979": {"text": "DirectedEdge[G.V()];\n      distTo = new double[G.V()];\n      for (int v = 0; v < G.V(); v++)\n         distTo[v] = Double.POSITIVE_INFINITY;\n      distTo[s] = 0.0;\n      Topological top = new Topological(G);\n      for (int v : top.order())\n         relax(G, v);\n   }\n   private void relax(EdgeWeightedDigraph G, int v)\n   // See page 648.\n   public double distTo(int v)          // standard client query methods\n   public boolean hasPathTo(int v)      //   for SPT implementatations\n   public Iterable<Edge> pathTo(int v)  //   (See page 649.) \n}\nThis shortest-paths algorithm for edge-weighted DAGs uses a topological sort ( Algorithm 4.5 , \nadapted to use EdgeWeightedDigraph and DirectedEdge) to enable it to relax the vertices in topo-\nlogical order, which is all that is needed to compute shortest paths.\n% java AcyclicSP tinyEWDAG.txt 5 \n5 to 0 (0.73): 5->4 0.35  4->0 0.38 \n5 to 1 (0.32): 5->1 0.32 \n5 to 2 (0.62): 5->7 0.28  7->2 0.34 \n5 to 3 (0.62): 5->1 0.32  1->3 0.29 \n5 to 4 (0.35): 5->4 0.35 \n5 to 5 (0.00): \n5 to 6 (1.13): 5->1 0.32  1->3 0.29  3->6 0.52 \n5 to 7 (0.28): ", "start": 672, "end": 672}, "980": {"text": "(0.00): \n5 to 6 (1.13): 5->1 0.32  1->3 0.29  3->6 0.52 \n5 to 7 (0.28): 5->7 0.28\n660 CHAPTER 4 \u25a0 Graphs  \n \nconsiderably simpli\ufb01es a problem. For shortest paths, the topological-sort-based meth-\nod is faster than Dijkstra\u2019s algorithm by a factor proportional to the cost of the priority-\nqueue operations in Dijkstra\u2019s algorithm. Moreover, the proof of Proposition S does \nnot depend on the edge weights being nonnegative, so we can remove that restriction \nfor edge-weighted DAGs. Next, we consider implications of this ability to allow nega-\ntive edge weights, by considering the use of the shortest-paths model to solve two other \nproblems, one of which seems at \ufb01rst blush to be quite removed from graph processing.\n  L o n g e s t  p a t h s .  Consider the problem of \ufb01nding the longest path in an edge-weighted \nDAG with edge weights that may be positive or negative. \nSingle-source longest paths in edge-weighted DAGs. Given an edge-weighted \nDAG (with negative weights allowed) and a source vertex s, support queries of the \nform: Is there a directed path from  s to a given target vertex  v? If so, \ufb01nd a longest\nsuch path (one whose total weight is maximal).\nThe algorithm just considered provides a quick solution to this problem:\nProposition T.  We can solve the  longest-paths problem in edge-weig hted DAGs in   \ntime proportional to E + V.\nProof: Given a longest-paths problem, create a copy of the given edge-weighted \nDAG that is identical to the original, except that all edge weights are negated. ", "start": 672, "end": 673}, "981": {"text": "DAGs in   \ntime proportional to E + V.\nProof: Given a longest-paths problem, create a copy of the given edge-weighted \nDAG that is identical to the original, except that all edge weights are negated. Then \nthe shortest path in this copy is the longest path in the original. T o transform the \nsolution of the shortest-paths problem to a solution of the longest-paths problem, \nnegate the weights in the solution. The running time follows immediately from \nProposition S.\n \nUsing this transformation to develop a class  AcyclicLP that \ufb01nds longest paths in \nedge-weighted DAGs is straightforward. An even simpler way to implement such a \nclass is to copy AcyclicSP, then switch the distTo[] initialization to Double.NEGA-\nTIVE_INFINITY and switch the sense of the inequality in relax(). Either way, we get \nan ef\ufb01cient solution to the longest-paths problem in edge-weighted DAGs. This result is \nto be compared with the fact that the best known algorithm for \ufb01nding longest simple \npaths in general edge-weighted digraphs (where edge weights may be negative) requires \nexponential time in the worst case (see Chapter 6)! The possibility of cycles seems to \nmake the problem exponentially more dif\ufb01cult.\n6614.4 \u25a0 Shortest Paths\n The \ufb01gure at right is a trace of the process \nof \ufb01nding longest paths in our sample edge-\nweighted DAG tinyEWDAG.txt, for comparison \nwith the shortest-paths trace for the same DAG \non page 659. For this example, the algorithm \nbuilds the longest-paths tree (LPT) from vertex \n5 as follows:\n\u25a0 Does a DFS to discover the topological \norder 5 1 3 6 4 7 0 2.\n\u25a0 Adds to the tree 5 and all edges ", "start": 673, "end": 674}, "982": {"text": "(LPT) from vertex \n5 as follows:\n\u25a0 Does a DFS to discover the topological \norder 5 1 3 6 4 7 0 2.\n\u25a0 Adds to the tree 5 and all edges leaving it.\n\u25a0 Adds to the tree 1 and 1->3.\n\u25a0 Adds to the tree 3 and edges 3->6 and \n3->7. Edge 5->7 becomes ineligible.\n\u25a0 Adds to the tree 6 and edges 6->2, 6->4, \nand 6->0.\n\u25a0 Adds to the tree 4 and edges 4->0 and \n4->7. Edges 6->0 and 3->7 become ineli-\ngible.\n\u25a0 Adds to the tree 7 and 7->2. Edge 6->2\nbecomes ineligible\n\u25a0 Adds 0 to the tree, but not 0->2, which is \nineligible.\n\u25a0 Adds 2 to the tree (not depicted).\nThe longest-paths algorithm processes the verti-\nces in the same order as the shortest-paths algo -\nrithm but produces a completely different result.\nnow ineligible\nTrace for longest paths in an acyclic network  \ntopological sort\n        5 1 3 6 4 7 0 2    edgeTo[]\n0   \n 1   5->1   \n2\n3   \n 4   5->4\n 5   \n 6   \n 7   5->7  \n0   \n 1   5->1   \n2\n 3   1->3\n 4   5->4\n 5   \n 6   \n 7   5->7  \n0   \n 1   5->1   \n2\n 3   1->3\n 4   5->4\n 5   \n 6   3->6\n 7 ", "start": 674, "end": 674}, "983": {"text": "5->7  \n0   \n 1   5->1   \n2\n 3   1->3\n 4   5->4\n 5   \n 6   3->6\n 7   3->7  \n 0   6->0 \n 1   5->1   \n 2   6->2\n 3   1->3\n 4   6->4\n 5   \n 6   3->6\n 7   3->7  \n 0   4->0 \n 1   5->1   \n 2   6->2\n 3   1->3\n 4   6->4\n 5   \n 6   3->6\n 7   4->7  \n 0   4->0 \n 1   5->1   \n 2   7->2\n 3   1->3\n 4   6->4\n 5   \n 6   3->6\n 7   4->7  \n 0   4->0 \n 1   5->1   \n 2   7->2\n 3   1->3\n 4   6->4\n 5   \n 6   3->6\n 7   4->7  \n662 CHAPTER 4 \u25a0 Graphs\n  \n  P a r a l l e l  j o b  s c h e d u l i n g .  As an example application, we revisit the class of scheduling\nproblems that we \ufb01rst considered in Section 4.2 (page 574). Speci\ufb01cally, consider the \nfollowing scheduling problem (differences from the problem on page 575 are italicized):\n P a r a l l e l  p r e c e d e n c e - c ", "start": 674, "end": 675}, "984": {"text": "Speci\ufb01cally, consider the \nfollowing scheduling problem (differences from the problem on page 575 are italicized):\n P a r a l l e l  p r e c e d e n c e - c o n s t r a i n e d  s c h e d u l i n g .  Given a set of jobs of speci\ufb01ed du-\nration to be completed, with precedence constraints that specify that certain jobs \nhave to be completed before certain other jobs are begun, how can we schedule \nthe jobs on identical processors  (as many as needed ) such that they are all com-\npleted in the minimum amount of time while still respecting the constraints?\nImplicit in the model of Section 4.2 is a single processor: we schedule the jobs in to-\npological order and the total time required is the total duration of the jobs. Now, we \nassume that we have suf\ufb01cient processors to perform as many jobs as possible, limited \nonly by precedence constraints. Again, thousands or even millions of jobs might be \ninvolved, so we require an ef\ufb01cient algorithm. Remarkably, a linear-\ntime algorithm is available\u2014an approach known as the    critical path \nmethod demonstrates that the problem is equivalent to a longest-\npaths problem in an edge-weighted DAG. This method has been used \nsuccessfully in countless industrial applications.\nWe focus on the earliest possible time that we can schedule each \njob, assuming that any available processor can handle the job for its \nduration. For example, consider the problem instance speci\ufb01ed in \nthe table at right. The solution below shows that 173.0 is the mini-\nmum possible completion time for any schedule for this problem: the \nschedule satis\ufb01es all the constraints, and no schedule can complete \nbefore time 173.0 because of the job sequence 0->9->6->8->2. ", "start": 675, "end": 675}, "985": {"text": "for any schedule for this problem: the \nschedule satis\ufb01es all the constraints, and no schedule can complete \nbefore time 173.0 because of the job sequence 0->9->6->8->2. This \nsequence is known as a  critical path for this problem. Every sequence \nof jobs, each constrained to follow the job just preceding it in the se-\nquence, represents a lower bound on the length of the schedule. If we de\ufb01ne the length \nof such a sequence to be its earliest possible completion time (total of the durations of \nits jobs), the longest sequence is known as a critical path because any delay in the start-\ning time of any job delays the best achievable completion time of the entire project.\nA job-scheduling problem\n0    41.0    1  7  9\n 1    51.0    2 \n 2    50.0    \n 3    36.0    \n 4    38.0    \n 5    45.0    \n 6    21.0    3  8\n 7    32.0    3  8\n 8    32.0    2\n 9    29.0    4  6\njob duration must complete\nbefore\nParallel job-scheduling solution\n0\n4\n3\n5\n9\n7\n6 8 2\n1\n41 700 91 123 173\n6634.4 \u25a0 Shortest Paths\n Definition.  The   critical path method for parallel scheduling is to proceed as follows: \nCreate an edge-weighted DAG with a source s, a sink t, and two vertices for each \njob (a start vertex and an end vertex). For each job, add an edge from its start vertex \nto its end vertex with weight equal to its duration. For each ", "start": 675, "end": 676}, "986": {"text": "s, a sink t, and two vertices for each \njob (a start vertex and an end vertex). For each job, add an edge from its start vertex \nto its end vertex with weight equal to its duration. For each precedence constraint \nv->w, add a zero-weight edge from the end vertex corresponding tovs to the begin-\nning vertex corresponding to w. Also add zero-weight edges from the source to each \njob\u2019s start vertex and from each job\u2019s end vertex to the sink. Now, schedule each job \nat the time given by the length of its longest path from the source. \n  \nThe \ufb01gure at the top of this page depicts this correspondence for our sample problem, \nand the \ufb01gure at the bottom of the page gives the longest-paths solution. As speci\ufb01ed, \nthe graph has three edges for each job (zero-weight edges from the source to the start \nand from the \ufb01nish to the sink, and an edge from start to \ufb01nish) and one edge for each \nprecedence constraint. The class CPM on the facing page is a straightforward implemen-\ntation of the critical path method. It transforms any instance of the job-scheduling \nproblem into an instance of the longest-paths problem in an edge-weighted DAG, uses \nAcyclicLP to solve it, then prints the job start times and schedule \ufb01nish time. \nEdge-weighted DAG representation of job scheduling\n41\n0 0\n51\n1 1\n50\n2 2\n36\n3 3\n38\n4 4\n45\n5 5\n21\n6 6\n32\n7 7 \n32\n8 8\n29 \n9 9\nprecedence constraint\n(zero weight)\njob start job finish\nduration\nzero-weight \nedge to each\njob start\nzero-weight \nedge from each\njob finishs\nt\nLongest-paths ", "start": 676, "end": 676}, "987": {"text": "8\n29 \n9 9\nprecedence constraint\n(zero weight)\njob start job finish\nduration\nzero-weight \nedge to each\njob start\nzero-weight \nedge from each\njob finishs\nt\nLongest-paths solution to job-scheduling example\n41\n0 0\n51\n1 1\n50\n2 2\n36\n3 3\n38\n4 4\n45\n5 5\n21\n6 6\n32\n7 7 \n32\n8 8\n29 \n9 9\ncritical path\nduration\ns\nt\n664 CHAPTER 4 \u25a0 Graphs\n  C r i t i c a l  p a t h  m e t h o d  f o r  p a r a l l e l  p r e c e d e n c e - c o n s t r a i n e d  j o b  s c h e d u l i n g\npublic class  CPM \n{\n   public static void main(String[] args)\n   {\n      int N = StdIn.readInt(); StdIn.readLine();\n      EdgeWeightedDigraph G;\n      G = new EdgeWeightedDigraph(2*N+2);\n      int s = 2*N, t = 2*N+1;\n      for (int i = 0; i < N; i++)\n      {\n         String[] a = StdIn.readLine().split(\"\\\\s+\");\n         double duration = Double.parseDouble(a[0]);\n         G.addEdge(new DirectedEdge(i, i+N, duration));\n         G.addEdge(new DirectedEdge(s, i, 0.0));\n         G.addEdge(new DirectedEdge(i+N, t, 0.0));\n         for (int j = 1; j < a.length; j++)\n         {\n            int successor = Integer.parseInt(a[j]);\n            G.addEdge(new DirectedEdge(i+N, successor, 0.0));\n ", "start": 676, "end": 677}, "988": {"text": "t, 0.0));\n         for (int j = 1; j < a.length; j++)\n         {\n            int successor = Integer.parseInt(a[j]);\n            G.addEdge(new DirectedEdge(i+N, successor, 0.0));\n         }\n      }\n      AcyclicLP lp = new AcyclicLP(G, s);\n      StdOut.println(\"Start times:\");\n      for (int i = 0; i < N; i++)\n         StdOut.printf(\"%4d: %5.1f\\n\", i, lp.distTo(i));\n      StdOut.printf(\"Finish time: %5.1f\\n\", lp.distTo(t));\n  }\n}\nThis implementation of the critical path method for job scheduling \nreduces the problem directly to the longest-paths problem in edge-\nweighted DAGs. It builds an edge-weighted digraph (which must be \na DAG) from the job-scheduling problem speci\ufb01cation, as prescribed \nby the critical path method, then uses AcyclicLP (see Proposition T) \nto \ufb01nd the longest-paths tree and to print the longest-paths lengths, \nwhich are precisely the start times for each job.\n% java CPM < jobsPC.txt \nStart times:\n   0:   0.0\n   1:  41.0\n   2: 123.0\n   3:  91.0\n   4:  70.0\n   5:   0.0\n   6:  70.0\n   7:  41.0\n   8:  91.0\n   9:  41.0 \nFinish time: 173.0\n% more jobsPC.txt \n10\n41.0  1 7 9\n51.0  2\n50.0\n36.0\n38.0\n45.0\n21.0 ", "start": 677, "end": 677}, "989": {"text": "\nFinish time: 173.0\n% more jobsPC.txt \n10\n41.0  1 7 9\n51.0  2\n50.0\n36.0\n38.0\n45.0\n21.0  3 8\n32.0  3 8\n32.0  2\n29.0  4 6\n6654.4 \u25a0 Shortest Paths  \n \nProposition U.  The    critical path method solves the parallel precedence-\nconstrained scheduling problem in linear time.\nProof: Why does the CPM approach work? The correctness of the algo -\nrithm rests on two facts. First, every path in the DAG is a sequence of job \nstarts and job \ufb01nishes, separated by zero-weight precedence constraints\u2014\nthe length of any path from the source s to any vertex v in the graph is a \nlower bound on the start/\ufb01nish time represented by v, because we could \nnot do better than scheduling those jobs one after another on the same ma-\nchine. In particular, the length of the longest path from s to the sink t is a \nlower bound on the \ufb01nish time of all the jobs. Second, all the start and \ufb01nish \ntimes implied by longest paths are feasible\u2014every job starts after the \ufb01nish \nof all the jobs where it appears as a successor in a precedence constraint, \nbecause the start time is the length of the longest path from the source to it. \nIn particular, the length of the longest path from s to t is an upper bound \non the \ufb01nish time of all the jobs. The linear-time performance is immediate  \nfrom Proposition T.\n \n \nParallel job scheduling with  relative deadlines. Conventional deadlines are \nrelative to the start time of the \ufb01rst job. Suppose that we allow an additional \ntype of constraint in the job-scheduling problem ", "start": 677, "end": 678}, "990": {"text": "\nfrom Proposition T.\n \n \nParallel job scheduling with  relative deadlines. Conventional deadlines are \nrelative to the start time of the \ufb01rst job. Suppose that we allow an additional \ntype of constraint in the job-scheduling problem to specify \nthat a job must begin before a speci\ufb01ed amount of time has \nelapsed, relative to the start time of another job. Such con-\nstraints are commonly needed in time-critical manufactur -\ning processes and in many other applications, but they can \nmake the job-scheduling problem considerably more dif\ufb01-\ncult to solve. For example, as shown at left, suppose that we \nneed to add a constraint to our example that job 2 must start \nno later than 12 time units after job 4 starts. This deadline is actually a constraint \non the start time of job 4: it must be no earlier than 12 time units before the start \ntime of job 2. In our example, there is room in the schedule to meet the deadline: \nwe can move the start time of job 4 to 111, 12 time units before the scheduled \nstart time of job 2. Note that, if job 4 were a long job, this change would increase \nthe \ufb01nish time of the whole schedule. Similarly, if we add to the schedule a dead-\nline that job 2 must start no later than 70 time units after job 7 starts, there is \nroom in the schedule to change the start time of job 7 to 53, without having to \nreschedule jobs 3 and 8. But if we add a deadline that job 4 must start no later \nAdded deadlines\nfor job scheduling\n2    12.0    4\n 2    70.0    7\n 4    80.0    0\njob time relative\nto\nRelative deadlines\nin job scheduling\n0 ", "start": 678, "end": 678}, "991": {"text": "scheduling\n2    12.0    4\n 2    70.0    7\n 4    80.0    0\njob time relative\nto\nRelative deadlines\nin job scheduling\n0     0.0\n 1    41.0\n 2   123.0\n 3    91.0\n 4    70.0\n 5     0.0\n 6    70.0\n 7    41.0\n 8    91.0\n 9    41.0 \njob\n0     0.0\n 1    41.0\n 2   123.0\n 3    91.0\n4   111.0\n 5     0.0\n 6    70.0\n 7    41.0\n 8    91.0\n 9    41.0 \njob start\nstart\nstart\n0     0.0\n 1    41.0\n 2   123.0\n 3    91.0\n 4   111.0\n 5     0.0\n 6    70.0\n7    53.0\n 8    91.0\n 9    41.0 \njob\ninfeasible!\noriginal\n2 by 12.0 after 4\n2 by 70.0 after 7\n4 by  80.0 after 0\n666 CHAPTER 4 \u25a0 Graphs\n than 80 time units after job 0, the schedule becomes infeasible: the constraints that 4 \nmust start no more than 80 time units after job 0 and that job 2 must start no more \nthan 12 units after job 4 imply that job 2 must ", "start": 678, "end": 679}, "992": {"text": "infeasible: the constraints that 4 \nmust start no more than 80 time units after job 0 and that job 2 must start no more \nthan 12 units after job 4 imply that job 2 must start no more than 93 time units after job \n0, but job 2 must start at least 123 time units after job 0 because of the chain 0 (41 time \nunits) precedes 9 (29 time units) precedes 6 (21 time units) precedes 8 (32 time units) \nprecedes 2. Adding more deadlines of course multiplies the possibilities and turns an \neasy problem into a dif\ufb01cult one.\nProposition V.   Parallel job scheduling with relative deadlines is a shortest-paths \nproblem in edge-weighted digraphs (with cycles and negative weights allowed).\nProof: Use the same construction as in Proposition U, adding an edge for each \ndeadline: if job v has to start within d time units of the start of job w, add an edge \nfrom v to w with negative weight d. Then convert to a shortest-paths problem by \nnegating all the weights in the digraph. The proof of correctness applies, provided \nthat the schedule is feasible. Determining whether a schedule is feasible is part of the \ncomputational burden, as you will see.\n  \nThis example illustrates that negative weights can play a critical role in practical ap-\nplication models. It says that if we can \ufb01nd an ef\ufb01cient solution to the shortest-paths \nproblem with negative weights, then we can \ufb01nd an ef\ufb01cient solution to the parallel job \nscheduling problem with relative deadlines. Neither of the algorithms we have consid-\nered can do the job: Dijkstra\u2019s algorithm requires that weights be positive (or zero), and \nAlgorithm 4.10 requires that the digraph be acyclic. Next, we consider the ", "start": 679, "end": 679}, "993": {"text": "the algorithms we have consid-\nered can do the job: Dijkstra\u2019s algorithm requires that weights be positive (or zero), and \nAlgorithm 4.10 requires that the digraph be acyclic. Next, we consider the problem of \ncoping with negative edge weights in digraphs that are not necessarily acyclic.\nzero-weight \nedge from each\njob finish\nzero-weight \nedge to each\njob start\nEdge-weighted digraph representation of parallel precedence-constrained scheduling with relative deadlines\n41\n0 0\n51\n1 1\n50\n2 2\n36\n3 3\n38\n-12\n-70\n-80\n4 4\n45\n5 5\n21\n6 6\n32\n7 7 \n32\n8 8\n29 \n9 9\ndeadline\ns\nt\n6674.4 \u25a0 Shortest Paths\n  \n \n \nShortest paths in general edge-weighted digraphs Our job-scheduling-\nwith-deadlines example just discussed demonstrates that negative weights are not \nmerely a mathematical curiosity; on the contrary, they signi\ufb01cantly extend the applica-\nbility of the shortest-paths problem as a problem-solving model. Accordingly we now \nconsider algorithms for edge-weighted digraphs that may have both cycles and negative \nweights. Before doing so, we consider some ba -\nsic properties of such digraphs to reset our in -\ntuition about shortest paths. The \ufb01gure at left is \na small example that illustrates the effects of in-\ntroducing negative weights on a digraph\u2019s short-\nest paths. Perhaps the most important effect is \nthat when negative weights are present, low-\nweight shortest paths tend to have more edges \nthan higher-weight paths. For positive weights, \nour emphasis was on looking for shortcuts; but \nwhen negative weights are present, we seek de-\ntours that use negative-weight edges. ", "start": 679, "end": 680}, "994": {"text": "shortest paths tend to have more edges \nthan higher-weight paths. For positive weights, \nour emphasis was on looking for shortcuts; but \nwhen negative weights are present, we seek de-\ntours that use negative-weight edges. This effect \nturns our intuition in seeking \u201cshort\u2019\u2019 paths into \na liability in understanding the algorithms, so \nwe need to suppress that line of intuition and \nconsider the problem on a basic abstract level.\nStrawman I. The \ufb01rst idea that suggests itself is \nto \ufb01nd the smallest (most negative) edge weight, \nthen to add the absolute value of that number \nto all the edge weights to transform the digraph \ninto one with no negative weights. This naive \napproach does not work at all, because shortest \npaths in the new digraph bear little relation to shortest paths in the old one. The more \nedges a path has, the more it is penalized by this transformation (see Exercise 4.4.14).\nStrawman II. The second idea that suggests itself is to try to adapt  Dijkstra\u2019s algorithm \nin some way. The fundamental dif\ufb01culty with this approach is that the algorithm de-\npends on examining paths in increasing order of their distance from the source. The \nproof in Proposition R that the algorithm is correct assumes that adding an edge to a \npath makes that path longer. But any edge with negative weight makes the path shorter, \nso that assumption is unfounded (see Exercise 4.4.14). \n N e g a t i v e  c y c l e s .  When we consider digraphs that could have negative edge weights, \nthe concept of a shortest path is meaningless if there is a cycle in the digraph that \nAn edge-weighted digraph with negative weights\n8\n15\n4->5  0.35 \n5->4  0.35 \n4->7 ", "start": 680, "end": 680}, "995": {"text": "path is meaningless if there is a cycle in the digraph that \nAn edge-weighted digraph with negative weights\n8\n15\n4->5  0.35 \n5->4  0.35 \n4->7  0.37 \n5->7  0.28 \n7->5  0.28 \n5->1  0.32 \n0->4  0.38\n0->2  0.26 \n7->3  0.39 \n1->3  0.29 \n2->7  0.34\n6->2 -1.20 \n3->6  0.52\n6->0 -1.40\n6->4 -1.25 \ntinyEWDn.txt\nshortest-paths tree from 0    edgeTo[] distTo[]\n0    \n 1   5->1     0.93\n 2   0->2     0.26\n 3   7->3     0.99\n 4   6->4     0.26  \n 5   4->5     0.61  \n 6   3->6     1.51 \n 7   2->7     0.60   \nnegative weights\nare dashed lines\nV\nE\n668 CHAPTER 4 \u25a0 Graphs\n has negative weight. For example, consid-\ner the digraph at left, which is identical to \nour \ufb01rst example except that edge 5->4 has \nweight -.66. Then, the weight of the cycle \n4->7->5->4 is\n       37+.28-.66 = -.01 \nWe can spin around that cycle to generate \narbitrarily short paths! Note that it is not \nnecessary for all the edges on a directed cy-\ncle to be of negative weight; ", "start": 680, "end": 681}, "996": {"text": "37+.28-.66 = -.01 \nWe can spin around that cycle to generate \narbitrarily short paths! Note that it is not \nnecessary for all the edges on a directed cy-\ncle to be of negative weight; what matters is \nthe sum of the edge weights. \nDefinition. A  negative cycle  in an edge-\nweighted digraph is a directed cycle whose \ntotal weight (sum of the weights of its \nedges) is negative.\nNow, suppose that some vertex on a path from s to a \nreachable vertex v is also on a negative cycle. In this case, \nthe existence of a shortest path from s to v would be a \ncontradiction, because we could use the cycle to construct \na path with weight lower than any given value. In other \nwords, shortest paths can be an ill-posed problem if nega-\ntive cycles are present.\nProposition W.  There exists a   shortest path from s\nto v in an edge-weighted digraph if and only if there \nexists at least one directed path from s to v and no \nvertex on any directed path from s to v is on a nega-\ntive cycle. \nProof: See discussion above and Exercise 4.4.29.\nNote that the requirement that shortest paths have no \nvertices on negative cycles implies that shortest paths are \nsimple and that we can compute a shortest-paths tree for \nsuch vertices, as we have done for positive edge weights. \nAn edge-weighted digraph with a negative cycle\n8\n15\n4 5  0.35 \n5 4 -0.66 \n4 7  0.37 \n5 7  0.28 \n7 5  0.28 \n5 1  0.32 \n0 4  0.38\n0 2  0.26 \n7 3  0.39 ", "start": 681, "end": 681}, "997": {"text": "0.28 \n7 5  0.28 \n5 1  0.32 \n0 4  0.38\n0 2  0.26 \n7 3  0.39 \n1 3  0.29 \n2 7  0.34\n6 2  0.40 \n3 6  0.52\n6 0  0.58\n6 4  0.93 \n0->4->7->5->4->7->5...->1->3->6 \nshortest path from 0 to 6\ntinyEWDnc.txt\nV\nE\nShortest-paths possibilities\nred outline: no shortest path from s exists\nblack outline: \nshortest path\n from s exists\nwhite: reachable from s\ngray: not reachable from s\ns\nnegative cycle\n6694.4 \u25a0 Shortest Paths\n  \nStrawman III. Whether or not there are negative cycles, there exists a shortest simple\npath connecting the source to each vertex reachable from the source. Why not de\ufb01ne \nshortest paths so that we seek such paths? Unfortunately, the best known algorithm for \nsolving this problem takes exponential time in the worst case (see Chapter 6). Gener-\nally, we consider such problems \u201ctoo dif\ufb01cult to solve\u201d and study simpler versions. \nThus, a well-posed and tractable version of the shortest paths problem in edge-\nweighted digraphs is to require algorithms to\n\u25a0 Assign a shortest-path weight of /H11001\u221e to vertices that are not reachable from the \nsource\n\u25a0 Assign a shortest-path weight of /H11002\u221e to vertices that are on a path from the \nsource that has a vertex that is on a negative cycle\n\u25a0 \n \n \nCompute the shortest-path weight (and tree) for all other vertices\nThroughout ", "start": 681, "end": 682}, "998": {"text": "of /H11002\u221e to vertices that are on a path from the \nsource that has a vertex that is on a negative cycle\n\u25a0 \n \n \nCompute the shortest-path weight (and tree) for all other vertices\nThroughout this section, we have been placing restrictions on the shortest-paths prob-\nlem so that we can develop algorithms to solve it. First, we disallowed negative weights, \nthen we disallowed directed cycles. We now adopt these less stringent restrictions and \nfocus on the following problems in general digraphs:\n  N e g a t i v e  c y c l e  d e t e c t i o n .  Does a given edge-weighted digraph have a negative \ncycle? If it does, \ufb01nd one such cycle.\nSingle-source shortest paths when negative cycles are not reachable. Given an \nedge-weighted digraph and a source s with no negative cycles reachable from s, \nsupport queries of the form Is there a directed path from s to a given target vertex v? \nIf so, \ufb01nd a shortest such path (one whose total weight is minimal).\nTo summarize: while shortest paths in digraphs with directed cycles is an ill-posed \nproblem and we cannot ef\ufb01ciently solve the problem of \ufb01nding simple shortest paths \nin  such digraphs, we can identify negative cycles in practical situations. For example, \nin a job-scheduling-with-deadlines problem, we might expect negative cycles to be \nrelatively rare: constraints and deadlines derive from logical real-world constraints, so \nany negative cycles are likely to stem from an error in the problem statement. Finding \nnegative cycles, correcting errors, and then \ufb01nding the schedule in a problem with no \nnegative cycles is a reasonable way to proceed. In other cases, \ufb01nding a negative cycle is \nthe goal of the computation. The following approach, developed by R. ", "start": 682, "end": 682}, "999": {"text": "\ufb01nding the schedule in a problem with no \nnegative cycles is a reasonable way to proceed. In other cases, \ufb01nding a negative cycle is \nthe goal of the computation. The following approach, developed by R. Bellman and L. \nFord in the late 1950s, provides a simple and effective basis for attacking both of these \nproblems and is also effective for digraphs with positive weights:\n670 CHAPTER 4 \u25a0 Graphs\n  \nProposition X.  (   Bellman-Ford algorithm) The following method solves the single-\nsource shortest-paths problem from a given source s for any edge-weighted di-\ngraph with V vertices and no negative cycles reachable from s: Initialize distTo[s]\nto 0 and all other distTo[] values to in\ufb01nity. Then, considering the digraph\u2019s edges \nin any order, relax all edges. Make V such passes.\nProof: For any vertex t that is reachable from s consider a speci\ufb01c shortest path \nfrom s to t: v0->v1->...->vk, where v0 is s and vk is t. Since there are no negative \ncycles, such a path exists and k can be no larger than V/H110021. We show by induction \non i that after the ith pass the algorithm computes a shortest path from s to vi. The \nbase case (i = 0) is trivial. Assuming the claim to be true for i, v0->v1->...->vi\nis a shortest path from s to vi, and distTo[vi] is its length. Now, we relax every \nvertex in the ith pass, including vi, so distTo[vi+1] is no greater than distTo[vi]\nplus the weight of vi->vi+1.  Now, after the ith pass, distTo[vi+1] must be equal \nto distTo[vi] plus the weight of ", "start": 682, "end": 683}, "1000": {"text": "distTo[vi+1] is no greater than distTo[vi]\nplus the weight of vi->vi+1.  Now, after the ith pass, distTo[vi+1] must be equal \nto distTo[vi] plus the weight of vi->vi+1. It cannot be greater because we relax \nevery vertex in the ith pass, in particular vi, and it cannot be less because that is \nthe length of v0->v1->...->vi+1, a shortest path. Thus the algorithm computes a \nshortest path from s to vi+1 after the (i+1)st pass. \nProposition W (continued). The Bellman-Ford algorithm takes time proportional \nto EV and extra space proportional to V.\nProof: Each of the V passes relaxes E edges.\n  \nThis method is very general, since it does not specify the order in which the edges are \nrelaxed. We now restrict attention to a less general method where we always relax all \nthe edges leaving any vertex (in any order). The following code exhibits the simplicity \nof the approach:\nfor (int pass = 0; pass < G.V(); pass++)\n   for (v = 0; v < G.V(); v++)\n      for (DirectedEdge e : G.adj(v))\n         relax(e);\nWe do not consider this version in detail because it always relaxes VE edges, and a \nsimple modi\ufb01cation makes the algorithm much more ef\ufb01cient for typical applications.\n6714.4 \u25a0 Shortest Paths\n Queue-based Bellman-Ford. Speci\ufb01cally, we \ncan easily determine a priori  that numerous \nedges are not going to lead to a successful \nrelaxation in any given pass: the only edges \nthat could lead to a change in distTo[] are \nthose leaving a vertex whose distTo[] value \nchanged in the previous pass. T o keep track of ", "start": 683, "end": 684}, "1001": {"text": "\nrelaxation in any given pass: the only edges \nthat could lead to a change in distTo[] are \nthose leaving a vertex whose distTo[] value \nchanged in the previous pass. T o keep track of \nsuch vertices, we use a FIFO queue. The op-\neration of the algorithm for our standard ex -\nample with positive weights is shown at right. \nShown at the left of the \ufb01gure are the queue \nentries for each pass (in red), followed by the \nqueue entries for the next pass (in black). We \nstart with the source on the queue and then \ncompute the SPT as follows:\n\u25a0 Relax 1->3 and put 3 on the queue.\n\u25a0 Relax 3->6 and put 6 on the queue.\n\u25a0 Relax 6->4, 6->0, and 6->2 and put 4, \n0, and 2 on the queue.\n\u25a0 Relax 4->7 and 4->5 and put 7 and 4 on \nthe queue. Then relax 0->4 and 0->2, \nwhich are ineligible. Then relax 2->7\n(and recolor 4->7).\n\u25a0 Relax 7->5 (and recolor 4->5) but do \nnot put 5 on the queue (it is already \nthere). Then relax 7->3, which is ineli-\ngible. Then relax 5->1,  5->4, and 5->7, \nwhich are ineligible, leaving the queue \nempty. \nImplementation. Implementing the Bell -\nman-Ford algorithm along these lines requires \nremarkably little code, as shown in Algo-\nrithm 4.11. It is based on two additional data \nstructures:\n\u25a0 A queue q of vertices to be relaxed\n\u25a0 A vertex-indexed boolean array onQ[]\nthat indicates which vertices are on the \nqueue, ", "start": 684, "end": 684}, "1002": {"text": "Algo-\nrithm 4.11. It is based on two additional data \nstructures:\n\u25a0 A queue q of vertices to be relaxed\n\u25a0 A vertex-indexed boolean array onQ[]\nthat indicates which vertices are on the \nqueue, to avoid duplicates\nTrace of the Bellman-Ford algorithm\nsource\nqueue vertices for\neach phase are in red\nrecolored edge\nred: this pass\nblack: next pass\n   edgeTo[]\n0            \n 1         \n 2   \n 3   1->3  \n4    \n 5    \n 6    \n 7    \n 1\n 3\n   edgeTo[]\n0            \n 1         \n 2   \n3   \n4    \n 5    \n6   3->6     \n 7    \n 3\n 6\n   edgeTo[]\n0   6->0   \n 1         \n2   6->2  \n3   1->3  \n 4   6->4     \n 5    \n 6   3->6     \n 7    \n 6\n 4\n 0\n 2\n   edgeTo[]\n0   6->0  \n 1        \n 2   6->2 \n 3   1->3 \n 4   6->4    \n5   4->5    \n 6   3->6 \n 7   2->7      \n 4\n0\n2\n 7\n 5     \n   edgeTo[]\n0   6->0  \n 1        \n 2   6->2  \n 3   1->3 \n 4   6->4   \n5   7->5     \n 6   3->6    \n 7   2->7     \n 7\n5     \n      \n   edgeTo[]\n0   6->0   \n 1        \n 2   6->2 \n 3   1->3\n 4   6->4 ", "start": 684, "end": 684}, "1003": {"text": "7   2->7     \n 7\n5     \n      \n   edgeTo[]\n0   6->0   \n 1        \n 2   6->2 \n 3   1->3\n 4   6->4   \n 5   7->5    \n 6   3->6   \n 7   2->7     \nq\n672 CHAPTER 4 \u25a0 Graphs\n We star t by putting the source s on the queue, then enter a loop where we take a ver -\ntex off the queue and relax it. To add vertices to the queue, we augement our relax()\nimplementation from page 646 to put the vertex pointed to by any edge that successfully\nrelaxes onto the queue, as shown in the code at right. The data structures ensure that\n\u25a0 Only one copy of each vertex \nappears on the queue\n\u25a0 Every vertex whose edgeTo[]\nand distTo[] values change in \nsome pass is processed in the \nnext pass\nTo  co m p l e te  t h e  i m p l e m e n t a t i o n , w e  \nneed to ensure that the algorithm ter-\nminates after V passes. One way to \nachieve this end is to explicitly keep \ntrack of the passes. Our implemen -\ntation BellmanFordSP (Algorithm \n4.11) uses a different approach that \nwe will consider in detail on page \n677: it checks for negative cycles in\nthe subset of digraph edges in \nedgeTo[] and terminates if it \ufb01nds \none. \n \nProposition Y .  The queue-based implementation of the  Bellman-Ford algorithm \nsolves the shortest-paths problem from a given source s (or \ufb01nds a negative cycle \nreachable from s) for any edge-weighted digraph with V vertices, in time propor-\ntional to EV and extra space proportional to V, ", "start": 684, "end": 685}, "1004": {"text": "shortest-paths problem from a given source s (or \ufb01nds a negative cycle \nreachable from s) for any edge-weighted digraph with V vertices, in time propor-\ntional to EV and extra space proportional to V, in the worst case.\nProof: If there is no negative cycle reachable from s, the algorithm terminates after \nrelaxations corresponding to the (V\u20131)st pass of the generic algorithm described in \nProposition X (since all shortest paths have fewer than V\u20131 edges). If there does \nexist a negative cycle reachable from s, the queue never empties. After relaxations \ncorresponding to the Vth pass of the generic algorithm described in Proposition \nX the edgeTo[] array has a path with a cycle (connects some vertex w to itself) and \nthat cycle must be negative, since the path from s to the second occurrence of w \nmust be shorter that the path from s to the \ufb01rst occurrence of w for w to be included \non the path the second time. In the worst case, the algorithm mimics the general \nalgorithm and relaxes all E edges in each of V passes.\nprivate void relax(EdgeWeightedDigraph G, int v) \n{\n   for (DirectedEdge e : G.adj(v)\n   {\n      int w = e.to();\n      if (distTo[w] > distTo[v] + e.weight())\n      {\n         distTo[w] = distTo[v] + e.weight();\n         edgeTo[w] = e;\n         if (!onQ[w])\n         {\n            q.enqueue(w);\n            onQ[w] = true;\n         }\n      }\n      if (cost++ % G.V() == 0)\n         findNegativeCycle();\n   } \n}\n R e l a x a t i o n  f o r  B e l l m a n - F o r d\n6734.4 \u25a0 Shortest ", "start": 685, "end": 685}, "1005": {"text": "0)\n         findNegativeCycle();\n   } \n}\n R e l a x a t i o n  f o r  B e l l m a n - F o r d\n6734.4 \u25a0 Shortest Paths\n ALGORITHM 4.11   Bellman-Ford algorithm (queue-based)\npublic class  BellmanFordSP \n{\n   private double[] distTo;               // length of path to v\n   private DirectedEdge[] edgeTo;         // last edge on path to v\n   private boolean[] onQ;                 // Is this vertex on the queue?\n   private Queue<Integer> queue;          // vertices being relaxed\n   private int cost;                      // number of calls to relax()\n   private Iterable<DirectedEdge> cycle;  // negative cycle in edgeTo[]?\n   public BellmanFordSP(EdgeWeightedDigraph G, int s)\n   { \n     distTo = new double[G.V()];\n      edgeTo = new DirectedEdge[G.V()]; \n     onQ = new boolean[G.V()];\n      queue = new Queue<Integer>();\n      for (int v = 0; v < G.V(); v++)\n         distTo[v] = Double.POSITIVE_INFINITY;\n      distTo[s] = 0.0;\n      queue.enqueue(s);\n      onQ[s] = true;\n      while (!queue.isEmpty() && !this.hasNegativeCycle())\n      {\n         int v = queue.dequeue();\n         onQ[v] = false;\n         relax(v);\n      }\n   }\n   private void relax(int v)\n   // See page 673.\n   public double distTo(int v)          // standard client query methods\n   public boolean hasPathTo(int v)      //   for SPT implementatations\n   public Iterable<Edge> pathTo(int v)  //   (See page 649.)\n   private void findNegativeCycle()\n   public boolean hasNegativeCycle()\n   public Iterable<Edge> negativeCycle()\n ", "start": 685, "end": 686}, "1006": {"text": "SPT implementatations\n   public Iterable<Edge> pathTo(int v)  //   (See page 649.)\n   private void findNegativeCycle()\n   public boolean hasNegativeCycle()\n   public Iterable<Edge> negativeCycle()\n   // See page 677. \n}\nThis implementation of the Bellman-Ford algorithm uses a version of relax() that puts vertices \npointed to by edges that successfully relax on a FIFO queue (avoiding duplicates) and periodically \nchecks for a negative cycle in edgeTo[] (see text).\n674 CHAPTER 4 \u25a0 Graphs The queue-based Bellman-Ford algorithm is an effective and \nef\ufb01cient method for solving the shortest-paths problem that is \nwidely used in practice, even for the case when edge weights \nare positive. For example, as shown in the diagram at right, our \n250-vertex example is complete in 14 passes and requires fewer \npath-length compares than Dijkstra\u2019s algorithm for the same \nproblem. \nNegative weights. The example on the next page traces the \nprogress of the Bellman-Ford algorithm in a digraph with nega-\ntive weights. We start with the source on q and then compute \nthe SPT as follows:\n\u25a0 Relax 0->2 and 0->4 and put 2 and 4 on the queue.\n\u25a0 Relax 2->7 and put 7 on the queue. Then relax 4->5 and \nput 5 on the queue. Then relax 4->7, which is ineligible.\n\u25a0 Relax 7->3 and 5->1 and put 3 and 1 on the queue. Then \nrelax 5->4 and 5->7, which are ineligible.\n\u25a0 Relax 3->6 and put 6 on the queue. Then relax 1->3, \nwhich is ineligible. \n\u25a0 \n \nRelax 6->4 and put 4 on the queue. This negative-weight ", "start": 686, "end": 687}, "1007": {"text": "ineligible.\n\u25a0 Relax 3->6 and put 6 on the queue. Then relax 1->3, \nwhich is ineligible. \n\u25a0 \n \nRelax 6->4 and put 4 on the queue. This negative-weight \nedge gives a shorter path to 4, so its edges must be relaxed \nagain (they were \ufb01rst relaxed in pass 2). The distances \nto 5 and to 1 are no longer valid but will be corrected in \nlater passes.\n\u25a0 Relax 4->5 and put 5 on the queue. Then relax 4->7, \nwhich is still ineligible.\n\u25a0 Relax 5->1 and put 1 on the queue. Then relax 5->4 and \n5->7, which are both still ineligible.\n\u25a0 Relax 1->3, which is still ineligible, leaving the queue \nempty.\nThe shortest-paths tree for this example is a single long path \nfrom 0 to 1. The edges from 4, 5, and 1 are all relaxed twice for \nthis example. Rereading the proof of Proposition X in the con-\ntext of this example is a good way to better understand it.\nBellman-Ford (250 vertices)\n4\n7\n10\n13\nSPT\nedges on queue in red\n passes\n6754.4 \u25a0 Shortest Paths\n Trace of the Bellman-Ford algorithm (negative weights)  \nsource\n   edgeTo[] distTo[]\n0 \n 1             \n 2   0->2     0.26\n 3\n 4   0->4     0.38\n 5   4->5     0.73\n 6    \n 7   2->7     0.60\n 2\n4\n 7\n 5\n   edgeTo[] distTo[]\n0 \n 1   5->1     1.05\n ", "start": 687, "end": 688}, "1008": {"text": "6    \n 7   2->7     0.60\n 2\n4\n 7\n 5\n   edgeTo[] distTo[]\n0 \n 1   5->1     1.05\n 2   0->2     0.26\n 3   7->3     0.99\n 4   0->4     0.38\n5   4->5     0.73\n 6    \n 7   2->7     0.60\n 7\n5\n 3\n 1\n   edgeTo[] distTo[]\n0  \n1   5->1     1.05\n 2   0->2     0.26\n 3   7->3     0.99\n 4   0->4     0.38\n 5   4->5     0.73\n6   3->6     1.51\n 7   2->7     0.60\n 3\n1\n 6    \n   edgeTo[] distTo[]\n0   \n 1   5->1     1.05\n 2   0->2     0.26\n 3   7->3     0.99\n 4   6->4     0.26\n5   4->5     0.73  \n 6   3->6     1.51\n 7   2->7     0.60\n 6\n 4\n      \n   edgeTo[] distTo[]\n0  \n1   5->1     1.05            \n 2   0->2     0.26\n 3   7->3     0.99\n 4   6->4     0.26\n 5 ", "start": 688, "end": 688}, "1009": {"text": "1.05            \n 2   0->2     0.26\n 3   7->3     0.99\n 4   6->4     0.26\n 5   4->5     0.61         \n 6   3->6     1.51   \n 7   2->7     0.60    \n 4\n 5\n      \n   edgeTo[] distTo[]\n0 \n 1   5->1     0.93\n 2   0->2     0.26\n 3   7->3     0.99\n 4   6->4     0.26\n 5   4->5     0.61\n 6   3->6     1.51   \n 7   2->7     0.60    \n 5\n 1\n      \n4->5  0.35 \n5->4  0.35 \n4->7  0.37 \n5->7  0.28 \n7->5  0.28 \n5->1  0.32 \n0->4  0.38\n0->2  0.26 \n7->3  0.39 \n1->3  0.29 \n2->7  0.34\n6->2 -1.20 \n3->6  0.52\n6->0 -1.40\n6->4 -1.25 \ntinyEWDn.txt\nno longer eligible!\nqueue\n676 CHAPTER 4 \u25a0 Graphs\n  N e g a t i v e   c y c l e  d e t e c t i o n .  Our implementation BellmanFordSP checks for negative \ncycles to avoid an in\ufb01nite loop. We can apply the code that ", "start": 688, "end": 689}, "1010": {"text": "v e   c y c l e  d e t e c t i o n .  Our implementation BellmanFordSP checks for negative \ncycles to avoid an in\ufb01nite loop. We can apply the code that does this check to provide \nclients with the capability to check for and extract negative cycles, as well. We do so by \nadding the following methods to the SP API on page 644:\nboolean  h a s N e g a t i v e C y c l e ( )  has a negative cycle?\nIterable<DirectedEdge> negativeCycle() a negative cycle\n(null if no negative cycles)\nShortest -paths API extensions for handling negative cycles\n Implementing these methods is not dif\ufb01cult, as shown in the code below. After running \nthe constructor in BellmanFordSP, the proof of Proposition Y tells us that the digraph \nhas a negative cycle reachable from the source if and only if the queue is nonempty after \nthe Vth pass through all the edges. Moreover, the subgraph of edges in our edgeTo[]\narray must contain a negative cycle. Accordingly, to implement negativeCycle() we \nbuild an edge-weighted digraph from the edges in edgeTo[] and look for a cycle in \nthat digraph. T o \ufb01nd the cycle, we use a version of DirectedCycle from Section 4.3, \nadapted to work for edge-weighted digraphs (see Exercise 4.4.12). We amortize the \ncost of this check by\n\u25a0 Adding an instance variable cycle and a private method findNegativeCycle()\nthat sets cycle to an iterator for \nthe edges of a negative cycle if one \nis found (and to null if none is \nfound)\n\u25a0 Calling findNegativeCycle()\nevery Vth call to relax()\nThis approach ensures that the loop in the \nconstructor terminates. Moreover, clients \ncan call hasNegativeCycle() to learn \nwhether ", "start": 689, "end": 689}, "1011": {"text": "null if none is \nfound)\n\u25a0 Calling findNegativeCycle()\nevery Vth call to relax()\nThis approach ensures that the loop in the \nconstructor terminates. Moreover, clients \ncan call hasNegativeCycle() to learn \nwhether there is a negative cycle reachable \nfrom the source (and negativeCycle()\nto get one such cycle. Adding the capa -\nbility to detect any negative cycle in the \ndigraph is also a simple extension (see \nExercise 4.4.43). \nprivate void findNegativeCycle() \n{\n   int V = edgeTo.length;\n   EdgeWeightedDigraph spt;\n   spt = new EdgeWeightedDigraph(V);\n   for (int v = 0; v < V; v++)\n      if (edgeTo[v] != null)\n         spt.addEdge(edgeTo[v]);\n   EdgeWeightedCycleFinder cf;\n   cf = new EdgeWeightedCycleFinder(spt);\n   cycle = cf.cycle(); \n}\npublic boolean hasNegativeCycle() \n{  return cycle != null;  }\npublic Iterable<Edge> negativeCycle() \n{  return cycle;  }\n N e g a t i v e  c y c l e  d e t e c t i o n  m e t h o d s  f o r  B e l l m a n - F o r d  a l g o r i t h m\n6774.4 \u25a0 Shortest Paths\n  \nThe example below traces the progress of the Bellman-Ford algorithm in a digraph \nwith a negative cycle. The \ufb01rst two passes are the same as for tinyEWDn.txt. In the \nthird pass, after relaxing 7->3 and 5->1 and putting 3 and 1 on queue, it relaxes the \nnegative-weight edge 5->4. This relaxation discovers the negative cycle  4->5->4. It puts \n5->4 ", "start": 689, "end": 690}, "1012": {"text": "5->1 and putting 3 and 1 on queue, it relaxes the \nnegative-weight edge 5->4. This relaxation discovers the negative cycle  4->5->4. It puts \n5->4 on the tree and cuts the cycle off from the source 0 in edgeTo[]. From that point \non, the algorithm spins through the cycle, lowering the distances to all the vertices \ntouched, until \ufb01nishing when the cycle is detected, with the queue not empty. The cycle \nis in the edgeTo[] array, for discovery by findNegativeCycle().\nTrace of the Bellman-Ford algorithm (negative cycle)  \nsource\n   edgeTo[] distTo[]\n0 \n 1             \n 2   0->2     0.26\n 3\n 4   0->4     0.38\n 5   4->5     0.73\n 6    \n 7   2->7     0.60\n 2\n4\n 7\n 5\n   edgeTo[] distTo[]\n0 \n 1   5->1     1.05\n 2   0->2     0.26\n 3   7->3     0.99\n4   5->4     0.07\n5   4->5     0.73\n 6    \n 7   2->7     0.60\n 7\n5\n 3\n 1\n 4\n   edgeTo[] distTo[]\n0   \n1   5->1     1.05\n 2   0->2     0.26\n 3   7->3     0.99\n 4   0->4     0.07\n 5   4->5     0.42\n6   3->6 ", "start": 690, "end": 690}, "1013": {"text": "0.26\n 3   7->3     0.99\n 4   0->4     0.07\n 5   4->5     0.42\n6   3->6     1.51\n 7   2->7     0.44\n 3\n1\n4\n 6\n 7\n 5\n   edgeTo[] distTo[]\n0 \n 1   5->1     0.74\n 2   0->2     0.26\n 3   7->3     0.83\n 4   5->4    -0.59\n 5   4->5     0.73  \n 6   3->6     1.51\n 7   2->7     0.60\n 6\n 7\n 5\n 3\n 1\n 4\n      \ntinyEWDnc.txt\nlength of\n0->4->5->4\nlength of\n0->4->5->4->5->4\n4->5  0.35 \n5->4 -0.66 \n4->7  0.37 \n5->7  0.28 \n7->5  0.28 \n5->1  0.32 \n0->4  0.38\n0->2  0.26 \n7->3  0.39 \n1->3  0.29 \n2->7  0.34\n6->2  0.40 \n3->6  0.52\n6->0  0.58\n6->4  0.93 \n. . .\nqueue\n678 CHAPTER 4 \u25a0 Graphs\n  \n \n \n \n  A r b i t r a g e .  Consider a market for ", "start": 690, "end": 691}, "1014": {"text": "0.58\n6->4  0.93 \n. . .\nqueue\n678 CHAPTER 4 \u25a0 Graphs\n  \n \n \n \n  A r b i t r a g e .  Consider a market for \ufb01nancial transactions that is based on trading com -\nmodities. Y ou can \ufb01nd a familiar example in tables that show conversion rates among \ncurrencies, such as the one in our sample \ufb01le rates.txt shown here. The \ufb01rst line \nin the \ufb01le is the number V of currencies; \nthen the \ufb01le has one line per currency, \ngiving its name followed by the conver -\nsion rates to the other currencies. For \nbrevity, this example includes just \ufb01ve \nof the hundreds of currencies that are \ntraded on modern markets: U.S. dol -\nlars ( USD), Euros ( EUR), British pounds \n(GBP), Swiss francs (CHF), and Canadian \ndollars (CAD). The tth number on line s represents a conversion rate: the number of \nunits of the currency named on row s that is needed to buy 1 unit of the currency \nnamed on row t. For example, our table says that 1,000 U.S. dollars will buy 741 euros. \nThis table is equivalent to a  complete edge-weighted digraph with a vertex correspond -\ning to each currency and an edge corresponding to each conversion rate. An edge s->t\nwith weight x corresponds to a conversion from s to t at exchange rate x. Paths in the \ndigraph specify multistep conversions. For example, com-\nbining the conversion just mentioned with an edge t->u \nwith weight y gives a path s->t->u that represents a way \nto convert 1 unit of currency s into xy units of curren-\ncy u. For example, we might buy 1,012.206 = ", "start": 691, "end": 691}, "1015": {"text": "y gives a path s->t->u that represents a way \nto convert 1 unit of currency s into xy units of curren-\ncy u. For example, we might buy 1,012.206 = 741\u00d71.366 \nCanadian dollars with our euros. Note that this gives a \nbetter rate than directly converting from U.S. dollars to \nCanadian dollars. Y ou might expect xy to be equal to the \nweight of s->u in all such cases, but such tables represent \na complex \ufb01nancial system where such consistency cannot \nbe guaranteed. Thus, \ufb01nding the path from s to u such \nthat the product of the weights is maximal is certainly of \ninterest. Even more interesting is a case where the product \nof the edge weights is smaller than the weight of the edge \nfrom the last vertex back to the \ufb01rst. In our example, sup-\npose that the weight of u->s is z and xyz > 1. Then cycle s->t->u->s gives a way to \nconvert 1 unit of currency s into more than 1 unit (xyz) of currency s. In other words, \nwe can make a 100(xyz - 1) percent pro\ufb01t by converting from s to t to u back to s. \nFor example, if we convert our 1,012.206 Canadian dollars back to US dollars, we get \n1,012.206*.995 = 1,007.14497 dollars, a 7.14497-dollar pro\ufb01t. That might not seem like \n% more rates.txt \n5 \nUSD  1      0.741  0.657  1.061  1.005 \nEUR  1.349  1      0.888  1.433  1.366 \nGBP  1.521  1.126 ", "start": 691, "end": 691}, "1016": {"text": "1.061  1.005 \nEUR  1.349  1      0.888  1.433  1.366 \nGBP  1.521  1.126  1      1.614  1.538 \nCHF  0.942  0.698  0.619  1      0.953 \nCAD  0.995  0.732  0.650  1.049  1    \nAn arbitrage opportunity\nUSD\n0.741\n1.349\n0.888\n1.126\n0.619\n1.614\n1.049\n0.953\n1.005\n0.995\n0.650\n1.538\n0.732\n1.366\n0.657\n1.5211.061\n0.942\n1.433\n0.698\nEUR\nGBP\nCHFCAD\n0.741 * 1.366 * .995 = 1.00714497\n6794.4 \u25a0 Shortest Paths\n  A r b i t r a g e  i n  c u r r e n c y  e x c h a n g e\npublic class  Arbitrage \n{\n   public static void main(String[] args)\n   {\n      int V = StdIn.readInt();\n      String[] name = new String[V];\n      EdgeWeightedDigraph G = new EdgeWeightedDigraph(V);\n      for (int v = 0; v < V; v++) \n      {\n         name[v] = StdIn.readString();\n         for (int w = 0; w < V; w++)\n         {\n            double rate = StdIn.readDouble();\n            DirectedEdge e = new DirectedEdge(v, w, -Math.log(rate));\n            G.addEdge(e);\n         }\n      }\n      BellmanFordSP ", "start": 691, "end": 692}, "1017": {"text": "V; w++)\n         {\n            double rate = StdIn.readDouble();\n            DirectedEdge e = new DirectedEdge(v, w, -Math.log(rate));\n            G.addEdge(e);\n         }\n      }\n      BellmanFordSP spt = new BellmanFordSP(G, 0);\n      if (spt.hasNegativeCycle())\n      {\n         double stake = 1000.0;\n         for (DirectedEdge e : spt.negativeCycle())\n         {\n            StdOut.printf(\"%10.5f %s \", stake, name[e.from()]);\n            stake *= Math.exp(-e.weight());\n            StdOut.printf(\"= %10.5f %s\\n\", stake, name[e.to()]);\n         }\n      }\n      else StdOut.println(\"No arbitrage opportunity\");\n   } \n}\n \n \nThis BellmanFordSP client \ufb01nds an arbitrage opportunity in a currency exchange table by construct-\ning a complete-graph representation of the exchange table and then using the Bellman-Ford algo-\nrithm to \ufb01nd a negative cycle in the graph.\n% java Arbitrage < rates.txt \n1000.00000 USD =  741.00000 EUR\n 741.00000 EUR = 1012.20600 CAD \n1012.20600 CAD = 1007.14497 USD\n680 CHAPTER 4 \u25a0 Graphs  \nmuch, but a currency trader might have 1 million dollars and be able to execute these \ntransactions every minute, which would lead to pro\ufb01ts of over $7,000 per minute, or \n$420,000 per hour! This situation is an example of an arbitrage opportunity that would \nallow traders to make unlimited pro\ufb01ts were it not for forces outside the model, such \nas transaction fees or limitations on the size of transactions. Even with these forces, \narbitrage is plenty pro\ufb01table in the real world. What does this problem have to do with ", "start": 692, "end": 693}, "1018": {"text": "forces outside the model, such \nas transaction fees or limitations on the size of transactions. Even with these forces, \narbitrage is plenty pro\ufb01table in the real world. What does this problem have to do with \nshortest paths? The answer to this question is remarkably simple:\nProposition Z.  The  arbitrage problem is a negative-cycle-detection problem in \nedge-weighted digraphs.\nProof: Replace each weight by its logarithm, negated. With this change, comput -\ning path weights by multiplying edge weights in the original problem corresponds \nto adding them in the transformed problem. Speci\ufb01cally, any product w1w2 . . . wk \ncorresponds to a sum /H11002ln(w1) /H11002 ln(w2) /H11002 . . . /H11002 ln(wk). The transformed edge \nweights might be negative or positive, a path from v to w gives a way of converting \nfrom currency v to currency w, and any negative cycle is an arbitrage opportunity.\nIn our example, where all transactions are possible, the digraph is a  complete graph, so \nany negative cycle is reachable from any vertex. In general commodity exchanges, some \nedges may be absent, so the one-argument constructor described in Exercise 4.4.43 is \nneeded. No ef\ufb01cient algorithm for \ufb01nding the best arbi-\ntrage opportunity (the most negative cycle in a digraph) \nis known (and the graph does not have to be very big \nfor this computational burden to be overwhelming), but \nthe fastest algorithm to \ufb01nd any arbitrage opportunity \nis crucial\u2014a trader with that algorithm is likely to sys-\ntematically wipe out numerous opportunities before the \nsecond-fastest algorithm \ufb01nds any.\nThe transformation in the proof of Proposition Z\nis useful even in the absence of arbitrage, because it ", "start": 693, "end": 693}, "1019": {"text": "that algorithm is likely to sys-\ntematically wipe out numerous opportunities before the \nsecond-fastest algorithm \ufb01nds any.\nThe transformation in the proof of Proposition Z\nis useful even in the absence of arbitrage, because it re-\nduces currency conversion to a shortest-paths problem. \nSince the logarithm function is monotonic (and we ne -\ngated the logarithms), the product is maximized precisely \nwhen the sum is minimized. The edge weights might be \nnegative or positive, and a shortest path from v to w gives \na best way of converting from currency v to currency w. \nA negative cycle that represents\nan arbitrage opportunity\nUSD\n.2998\n-.2999\n.1188\n-.1187\n.4797\n-.4787\n-.0478\n.0481\n-.0080\n.0050\n.4308\n-.4305\n.3120\n-.3119.4201\n-.4914-.0592\n.0598\n-.3598\n.3595\nEUR\nGBP\nCHFCAD\nreplace each\nweight w\nwith /H11002ln(w)\n.2998 - .3119 + .0050 = -.0071\n-ln(.741) -ln(1.366) -ln(.995)\n6814.4 \u25a0 Shortest Paths\n Perspective The table below summarizes the important characteristics of the \nshortest-paths algorithms that we have considered in this section. The \ufb01rst reason to \nchoose among the algorithms has to do with basic properties of the digraph at hand. \nDoes it have negative weights? Does it have cycles? Does it have negative cycles? Be-\nyond these basic characteristics, the characteristics of edge-weighted digraphs can vary \nwidely, so choosing among the algorithms requires some experimentation when more \nthan one can apply.\nalgorithm restriction\npath length compares\n(order of growth) extra \nspace sweet ", "start": 693, "end": 694}, "1020": {"text": "the characteristics of edge-weighted digraphs can vary \nwidely, so choosing among the algorithms requires some experimentation when more \nthan one can apply.\nalgorithm restriction\npath length compares\n(order of growth) extra \nspace sweet spot\ntypical worst case\nDijkstra (eager) positive edge \nweights E log V E log V V worst-case \nguarantee\ntopological sort edge-weighted \nDAGs E + V E + V V optimal for acyclic\nBellman-Ford \n(queue-based)\nno negative \ncycles E + V VE V widely applicable\nPerformance characteristics of shortest-paths algorithms\nHistorical notes. Shortest-paths problems have been intensively studied and widely \nused since the 1950s. The history of Dijkstra\u2019s algorithm for computing shortest paths \nis similar (and related) to the history of Prim\u2019s algorithm for computing the MST. The \nname Dijkstra\u2019s algorithm is commonly used to refer both to the abstract method of \nbuilding an SPT by adding vertices in order of their distance from the source and to \nits implementation as the optimal algorithm for the adjacency-matrix representation, \nbecause  E. W. Dijkstra presented both in his 1959 paper (and also showed that the same \napproach could compute the MST). Performance improvements for sparse graphs are \ndependent on later improvements in priority-queue implementations that are not spe-\nci\ufb01c to the shortest-paths problem. Improved performance of Dijkstra\u2019s algorithm is \none of the most important applications of that technology (for example, with a data \nstructure known as a  Fibonacci heap, the worst-case bound can be reduced to E + V log \nV). The Bellman-Ford algorithm has proven to be useful in practice and has found wide \napplication, particularly for general edge-weighted digraphs. While the running time of \nthe Bellman-Ford algorithm is likely to be linear for typical applications, its worst-case ", "start": 694, "end": 694}, "1021": {"text": "to be useful in practice and has found wide \napplication, particularly for general edge-weighted digraphs. While the running time of \nthe Bellman-Ford algorithm is likely to be linear for typical applications, its worst-case \nrunning time is VE. The development of a worst-case linear-time shortest-paths algo -\nrithm for sparse graphs remains an open problem. The basic Bellman-Ford algorithm \n682 CHAPTER 4 \u25a0 Graphs\n was developed in the 1950s by  L. Ford and  R. Bellman; despite the dramatic strides in \nperformance that we have seen for many other graph problems, we have not yet seen \nalgorithms with better worst-case performance for digraphs with negative edge weights \n(but no negative cycles).\n6834.4 \u25a0 Shortest Paths\n Q&A\n \n \nQ. Why de\ufb01ne separate data types for undirected graphs, directed graphs, edge-weight-\ned undirected graphs, and edge-weighted digraphs? \nA. We do so both for clar it y in client code and for simpler and more ef\ufb01cient imple -\nmentation code in unweighted graphs. In applications or systems where all types of \ngraphs are to be processed, it is a textbook exercise in software engineering to de\ufb01ne an \nADT from which ADTs can be derived for Graph, the unweighted undirected graphs of \nSection 4.1; Digraph, the unweighted digraphs of Section 4.2; EdgeWeightedGraph, \nthe edge-weighted undirected graphs of Section 4.3; or EdgeWeightedDigraph, the \nedge-weighted directed graphs of this section.\nQ. How can we \ufb01nd shortest paths in undirected (edge-weighted) graphs?\n A .  For positive edge weights, Dijkstra\u2019s algorithm does the job. We just build an \nEdgeWeightedDigraph corresponding to the given EdgeWeightedGraph ", "start": 694, "end": 696}, "1022": {"text": "\ufb01nd shortest paths in undirected (edge-weighted) graphs?\n A .  For positive edge weights, Dijkstra\u2019s algorithm does the job. We just build an \nEdgeWeightedDigraph corresponding to the given EdgeWeightedGraph (by adding \ntwo directed edges corresponding to each undirected edge, one in each direction) and \nthen run Dijkstra\u2019s algorithm. If edge weights can be negative, ef\ufb01cient algorithms are \navailable, but they are more complicated than the Bellman-Ford algorithm.\n684 CHAPTER 4 \u25a0 Graphs\n EXERCISES\n \n4.4.1  True or false: Adding a constant to ever y edge weig ht does not change the solu-\ntion to the single-source shortest-paths problem.\n4.4.2  Provide an implementation of toString() for EdgeWeightedDigraph.\n4.4.3  Develop an implementation of EdgeWeightedDigraph for dense graphs that \nuses an adjacency-matrix (two-dimensional array of weights) representation (see Ex-\nercise 4.3.9). Ignore parallel edges.\n4.4.4  Draw the SPT for source 0 of the edge-weighted digraph obtained by deleting \nvertex 7 from tinyEWD.txt (see page 644), and give the parent-link representation of the \nSPT. Answer the question for the same graph with all edge reversed.\n4.4.5  Change the direction of edge 0->2 in tinyEWD.txt (see page 644). Draw two differ-\nent SPTs that are rooted at 2 for this modi\ufb01ed edge-weighted digraph.\n4.4.6  Give a trace that shows the process of computing the SPT of the digraph de\ufb01ned \nin Exercise 4.4.5 with the eager version of Dijkstra\u2019s algorithm.\n4.4.7 Develop a version of DijkstraSP that supports ", "start": 696, "end": 697}, "1023": {"text": "of computing the SPT of the digraph de\ufb01ned \nin Exercise 4.4.5 with the eager version of Dijkstra\u2019s algorithm.\n4.4.7 Develop a version of DijkstraSP that supports a client method that returns a \nsecond shortest path from s to t in an edge-weighted digraph (and returns null if there \nis only one shortest path from s to t).\n4.4.8 The    diameter of a digraph is the length of the maximum-length shortest path \nconnecting two vertices. Write a DijkstraSP client that \ufb01nds the diameter of a given \nEdgeWeightedDigraph that has nonnegative weights.\n4.4.9 The table below, from an old published road map, purports to give the length of \nthe shortest routes connecting the cities. It contains an error. Correct the table. Also, add \na table that shows how to achieve the shortest routes.\nProvidence Westerly New London Norwich\nProvidence - 53 54 48\nWesterly 53 - 18 101\nNew London 54 18 - 12\nNorwich 48 101 12 -\n6854.4 \u25a0 Shortest Paths\n 4.4.10 Consider the edges in the digraph de\ufb01ned in Exercise 4.4.4 to be undirected \nedges such that each edge corresponds to equal-weight edges in both directions in the \nedge-weighted digraph. Answer Exercise 4.4.6 for this corresponding edge-weighted \ndigraph.\n4.4.11 Use the memory-cost model of Section 1.4 to determine the amount of mem-\nory used by EdgeWeightedDigraph to represent a graph with V vertices and E edges,.\n4.4.12  Adapt the DirectedCycle and Topological classes from Section 4.2 to use \nthe EdgeweightedDigraph and DirectedEdge APIs of this ", "start": 697, "end": 698}, "1024": {"text": "a graph with V vertices and E edges,.\n4.4.12  Adapt the DirectedCycle and Topological classes from Section 4.2 to use \nthe EdgeweightedDigraph and DirectedEdge APIs of this section, thus implementing \nEdgeWeightedCycleFinder and EdgeWeightedTopological classes.\n4.4.13 Show, in the style of the trace in the text, the process of computing the SPT \nwith Dijkstra\u2019s algorithm for the digraph obtained by removing the edge 5->7 from \ntinyEWD.txt (see page 644). \n4.4.14  Show the paths that would be discovered by the two strawman approaches de-\nscribed on page 668 for the example tinyEWDn.txt shown on that page.\n4.4.15 What happens to Bellman-Ford if there is a negative cycle on the path from s to \nv and then you call pathTo(v)?\n4.4.16 Suppose that we convert an EdgeWeightedGraph into an EdgeWeightedDigraph\nby creating two DirectedEdge objects in the EdgeWeightedDigraph (one in each di-\nrection) for each Edge in the EdgeWeightedGraph (as described for Dijkstra\u2019s algorithm \nin the Q&A on page 684) and then use the Bellman-Ford algorithm. Explain why this ap-\nproach fails spectacularly.\n4.4.17 What happens if you allow a vertex to be enqueued more than once in the same \npass in the Bellman-Ford algorithm? \nAnswer : The running time of the algorithm can go exponential. For example, describe \nwhat happens for the complete edge-weighted digraph whose edge weights are all -1.\n4.4.18 Write a CPM client that prints all critical paths.\n4.4.19 Find the lowest-weight cycle (best arbitrage opportunity) in the example shown \nin the text.\nEXERCISES  (continued)\n686 ", "start": 698, "end": 698}, "1025": {"text": "-1.\n4.4.18 Write a CPM client that prints all critical paths.\n4.4.19 Find the lowest-weight cycle (best arbitrage opportunity) in the example shown \nin the text.\nEXERCISES  (continued)\n686 CHAPTER 4 \u25a0 Graphs\n  \n4.4.20  Find a currency-conversion table online or in a newspaper. Use it to build an \narbitrage table. Note : Avoid tables that are derived (calculated) from a few values and \nthat therefore do not give suf\ufb01ciently accurate conversion information to be interesting. \nExtra credit : Make a killing in the money-exchange market!\n4.4.21 Show, in the style of the trace in the text, the process of computing the SPT with \nthe Bellman-Ford algorithm for the edge-weighted digraph of Exercise 4.4.5.\n6874.4 \u25a0 Shortest Paths\n CREATIVE PROBLEMS\n \n4.4.22  Ver tex we ig hts. Show that shortest-paths computations in edge-weighted di-\ngraphs with nonnegative weights on vertices (where the weight of a path is de\ufb01ned to \nbe the sum of the weights of the vertices) can be handled by building an edge-weighted \ndigraph that has weights on only the edges.\n4.4.23  Source-sink shortest paths. Develop an API and implementation that use a ver-\nsion of Dijkstra\u2019s algorithm to solve the source-sink shortest path problem on edge-\nweighted digraphs.\n4.4.24  Multisource shortest paths. Develop an API and implementation that uses Di-\njkstra\u2019s algorithm to solve the multisource shortest-paths problem on edge-weighted \ndigraphs with positive edge weights: given a set of sources, \ufb01nd a shortest-paths forest \nthat enables implementation of a method that returns to clients the shortest path from \nany ", "start": 698, "end": 700}, "1026": {"text": "shortest-paths problem on edge-weighted \ndigraphs with positive edge weights: given a set of sources, \ufb01nd a shortest-paths forest \nthat enables implementation of a method that returns to clients the shortest path from \nany source to each vertex. Hint : Add a dummy vertex with a zero-weight edge to each \nsource, or initialize the priority queue with all sources, with their distTo[] entries set \nto 0.\n4.4.25  Shortest path between two subsets.  Given a digraph with positive edge weights, \nand two distinguished subsets of vertices S and T, \ufb01nd a shortest path from any vertex \nin S to any vertex in T. Your algorithm should run in time proportional to E log V, in \nthe worst case.\n4.4.26  Single-source shortest paths in dense graphs. Develop a version of Dijkstra\u2019s al-\ngorithm that can \ufb01nd the SPT from a given vertex in a dense edge-weighted digraph in \ntime proportional to V 2. Use an adjacency-matrix representation (see Exercise 4.4.3 \nand Exercise 4.3.29).\n4.4.27    Shortest paths in Euclidean graphs. Adapt our APIs to speed up Dijkstra\u2019s algo-\nrithm in the case where it is known that vertices are points in the plane.\n4.4.28    Longest paths in DAGs. Develop an implementation AcyclicLP that can solve \nthe longest-paths problem in edge-weighted DAGs, as described in Proposition T.\n4.4.29    General optimality. Complete the proof of Proposition W by showing that if \nthere exists a directed path from s to v and no vertex on any path from s to v is on a \nnegative cycle, then there exists a shortest path from s to v (Hint : See Proposition P.)\n4.4.30  All-pairs shortest path in ", "start": 700, "end": 700}, "1027": {"text": "and no vertex on any path from s to v is on a \nnegative cycle, then there exists a shortest path from s to v (Hint : See Proposition P.)\n4.4.30  All-pairs shortest path in graphs with negative cycles. Articulate an API like the \none implemented on page 656 for the all-pairs shortest-paths problem in graphs with no \n688 CHAPTER 4 \u25a0 Graphs\n  \nnegative cycles. Develop an implementation that runs a version of Bellman-Ford to \nidentify weights pi[v] such that for any edge v->w, the edge weight plus the differ -\nence between pi[v] and pi[w] is nonnegative. Then use these weights to reweight \nthe graph, so that Dijkstra\u2019s algorithm is effective for \ufb01nding all shortest paths in the \nreweighted graph.\n4.4.31  All-pairs shortest path on a line. Given a weighted line graph (undirected con-\nnected graph, all vertices of degree 2, except two endpoints which have degree 1), devise \nan algorithm that preprocesses the graph in linear time and can return the distance of \nthe shortest path between any two vertices in constant time.\n4.4.32  Parent-checking heuristic. Modify Bellman-Ford to visit a vertex v only if its \nSPT parent edgeTo[v] is not currently on the queue. This heuristic has been reported \nby Cherkassky, Goldberg, and Radzik to be useful in practice. Prove that it correctly \ncomputes shortest paths and that the worst-case running time is proportional to EV.\n4.4.33  Shortest path in a grid. Given an N-by-N matrix of positive integers, \ufb01nd the \nshortest path from the (0, 0) entry to the ( N/H110021, N/H110021) entry, where the length of the \npath is the sum of the integers ", "start": 700, "end": 701}, "1028": {"text": "integers, \ufb01nd the \nshortest path from the (0, 0) entry to the ( N/H110021, N/H110021) entry, where the length of the \npath is the sum of the integers in the path. Repeat the problem but assume you can only \nmove right and down.\n4.4.34   Monotonic shortest path. Given a weighted digraph, \ufb01nd a monotonic shortest \npath from s to every other vertex. A path is monotonic if the weight of every edge on \nthe path is either strictly increasing or strictly decreasing. The path should be simple \n(no repeated vertices). Hint : Relax edges in ascending order and \ufb01nd a best path; then \nrelax edges in descending order and \ufb01nd a best path.\n4.4.35    Bitonic shortest path. Given a digraph, \ufb01nd a bitonic shortest path from s to ev-\nery other vertex (if one exists). A path is bitonic if there is an intermediate vertex v such \nthat the edges on the path from s to v are strictly increasing and the edges on the path \nfrom v to t are strictly decreasing. The path should be simple (no repeated vertices).\n4.4.36  Neighbors. Develop an SP client that \ufb01nds all vertices within a given distance \nd of a given vertex in a given edge-weighted digraph.The running time of your method \nshould be proportional to the size of the subgraph induced by those vertices and the \nvertices incident on them, or V (to initialize data structures), whichever is larger.\n6894.4 \u25a0 Shortest Paths\n  4.4.37    Critical edges. Develop an algorithm for \ufb01nding an edge whose removal causes \nmaximal increase in the shortest-paths length from one given vertex to another given \nvertex in a given edge-weighted digraph.\n4.4.38  Sensitivity. ", "start": 701, "end": 702}, "1029": {"text": "algorithm for \ufb01nding an edge whose removal causes \nmaximal increase in the shortest-paths length from one given vertex to another given \nvertex in a given edge-weighted digraph.\n4.4.38  Sensitivity. Develop an SP client that performs a sensitivity analysis on the \nedge-weighted digraph\u2019s edges with respect to a given pair of vertices s and t: Compute \na V-by-V boolean matrix such that, for every v and w, the entry in row v and column w\nis true if v->w is an edge in the edge-weighted digraphs whose weight can be increased \nwithout the shortest-path length from v to w being increased and is false otherwise.\n4.4.39    Lazy implementation of Dijkstra\u2019s algorithm. Develop an implementation of the \nlazy version of Dijkstra\u2019s algorithm that is described in the text.\n4.4.40     Bottleneck SPT. Show that an MST of an undirected graph is equivalent to a \nbottleneck SPT of the graph: For every pair of vertices v and w, it gives the path connect-\ning them whose longest edge is as short as possible.\n4.4.41   Bidirectional search. Develop a class for the source-sink shortest-paths prob-\nlem that is based on code like Algorithm 4.9 but that initializes the priority queue with \nboth the source and the sink. Doing so leads to the growth of an SPT from each vertex; \nyour main task is to decide precisely what to do when the two SPTs collide.\n4.4.42  Worst case (Dijkstra). Describe a family of graphs with V vertices and E edges \nfor which the worst-case running time of Dijkstra\u2019s algorithm is achieved.\n4.4.43    Negative cycle detection. Suppose that we add a constructor to Algorithm 4.11 \nthat differs from the constructor given only in that it omits the second argument and ", "start": 702, "end": 702}, "1030": {"text": "Dijkstra\u2019s algorithm is achieved.\n4.4.43    Negative cycle detection. Suppose that we add a constructor to Algorithm 4.11 \nthat differs from the constructor given only in that it omits the second argument and \nthat it initializes all distTo[] entries to 0. Show that, if a client uses that constructor, a \nclient call to hasNegativeCycle() returns true if and only if the graph has a negative \ncycle (and negativeCycle() returns that cycle).\nAnswer : Consider a digraph formed from the original by adding a new source with an \nedge of weight 0 to all the other vertices. After one pass, all distTo[] entries are 0, and \n\ufb01nding a negative cycle reachable from that source is the same as \ufb01nding a negative \ncycle anywhere in the original graph.\n4.4.44  Worst case (Bellman-Ford). Describe a family of graphs for which Algorithm \n4.11 takes time proportional to VE.\nCREATIVE PROBLEMS  (continued)\n690 CHAPTER 4 \u25a0 Graphs\n 4.4.45  Fast Bellman-Ford. Develop an algorithm that breaks the linearithmic running  \ntime barrier for the single-source shortest-paths problem in general edge-weighted di-\ngraphs for the special case where the weights are integers known to be bounded in \nabsolute value by a constant.\n4.4.46  Animate. Write a client program that does dynamic graphical animations of \nDijkstra\u2019s algorithm.\n6914.4 \u25a0 Shortest Paths\n EXPERIMENTS\n \n4.4.47  Random sparse edge-weighted digraphs. Modify your solution to Exercise       \n4.3.34 to assign a random direction to each edge.\n4.4.48  Random Euclidean edge-weighted digraphs. Modify your solution to Exercise \n4.3.35 to assign a random direction to each edge.\n4.4.49 ", "start": 702, "end": 704}, "1031": {"text": "direction to each edge.\n4.4.48  Random Euclidean edge-weighted digraphs. Modify your solution to Exercise \n4.3.35 to assign a random direction to each edge.\n4.4.49  Random grid edge-weighted digraphs. Modify your solution to Exercise 4.3.36 \nto assign a random direction to each edge.\n4.4.50    Negative weights I. Modify your random edge-weighted digraph generators \nto generate weights between x and y (where x and y are both between /H110021 and 1) by \nrescaling.\n4.4.51  Negative weights II. Modify your random edge-weighted digraph generators to \ngenerate negative weights by negating a \ufb01xed percentage (whose value is supplied by the \nclient) of the edge weights.\n4.4.52  Negative weights III. Develop client programs that use your edge-weighted \ndigraph to produce edge-weighted digraphs that have a large percentage of negative \nweights but have at most a few negative cycles, for as large a range of values of V and E\nas possible.\n692 CHAPTER 4 \u25a0 Graphs\n Testing all algorithms and study ing all parameters against all edge-weighted digraph mod-\nels is unrealistic. For each problem listed below, write a client that addresses the problem \nfor any given input digraph, then choose among the generators above to run experiments \nfor that graph model. Use your judgment in selecting experiments, perhaps in response to \nresults of previous experiments. Write a narrative explaining your results and any conclu-\nsions that might be drawn.\n4.4.53  Prediction. Estimate, to within a factor of 10, the largest graph with E = 10V\nthat your computer and programming system could handle if you were to use Dijkstra\u2019s \nalgorithm to compute all its shortest paths in 10 seconds.\n4.4.54  Cost ", "start": 704, "end": 705}, "1032": {"text": "the largest graph with E = 10V\nthat your computer and programming system could handle if you were to use Dijkstra\u2019s \nalgorithm to compute all its shortest paths in 10 seconds.\n4.4.54  Cost of laziness. Run empirical studies to compare the performance of the lazy \nversion of Dijkstra\u2019s algorithm with the eager version, for various edge-weighted di -\ngraph models.\n4.4.55  Johnson\u2019s algorithm. Develop a priority-queue implementation that uses a d-\nway heap. Find the best value of d for various edge-weighted digraph models.\n4.4.56  Arbitrage model. Develop a model for generating random arbitrage problems.\nYo u r  g o a l  i s  t o  g e n e r a t e  t a b l e s  t h a t  a re  a s  s i m i l a r  a s  p o s s i b l e  t o  t h e  t a b l e s  t h a t  yo u  u s e d  \nin Exercise 4.4.20.\n4.4.57  Parallel job-scheduling-with-deadlines model. Develop a model for generating \nrandom instances of the parallel job-scheduling-with-deadlines problem. Y our goal is \nto generate nontrivial problems that are likely to be feasible.\n6934.4 \u25a0 Shortest Paths\n 5.1 String Sorts   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  702\n5.2 Tries .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  730\n5.3 Substring ", "start": 705, "end": 706}, "1033": {"text": ".  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  730\n5.3 Substring Search .  .  .  .  .  .  .  .  .  .  .  .  .  758\n5.4 Regular Expressions .  .  .  .  .  .  .  .  .  .  .  788\n5.5 Data Compression .  .  .  .  .  .  .  .  .  .  .  .  810\nFIVE\n S t r i n g s W\n \ne communicate by exchanging strings of characters. Accordingly, numerous \nimportant and familiar applications are based on processing strings. In this \nchapter, we consider classic algorithms for addressing the underlying com-\nputational challenges surrounding applications such as the following:\nInformation processing. When you search for web pages containing a given keyword, \nyou are using a string-processing application. In the modern world, virtually all in-\nformation is encoded as a sequence of strings, and the applications that process it are \nstring-processing applications of crucial importance.\nGenomics. Computational biologists work with a genetic code  that reduces DNA to \n(very long) strings formed from four characters ( A, C, T, and G). Vast databases giving \ncodes describing all manner of living organisms have been developed in recent years, \nso that string processing is a cornerstone of modern research in computational biology.\nCommunications systems. When you send a text message or an email or download \nan ebook, you are transmitting a string from one place to another. Applications that \nprocess strings for this purpose were an original motivation for the development of \nstring-processing algorithms.\nProgramming systems. Programs are strings. Compilers, interpreters, and other appli-\ncations that convert programs into ", "start": 706, "end": 707}, "1034": {"text": "another. Applications that \nprocess strings for this purpose were an original motivation for the development of \nstring-processing algorithms.\nProgramming systems. Programs are strings. Compilers, interpreters, and other appli-\ncations that convert programs into machine instructions are critical applications that \nuse sophisticated string-processing techniques. Indeed, all written languages are ex-\npressed as strings, and another motivation for the development of string-processing \nalgorithms was the theory of formal languages, the study of describing sets of strings. \nThis list of a few signi\ufb01cant examples illustrates the diversity and importance of string-\nprocessing algorithms.\n695\n  \n \nThe plan of this chapter is as follows: After addressing basic properties of strings, we \nrevisit in Sections 5.1 and 5.2 the sorting and search ing APIs from Chapters 2 and\u00a03. \nAlgorithms that exploit special properties of string keys are faster and more \ufb02exible \nthan the algorithms that we considered earlier. In Section 5.3 we consider algorithms \nfor substring search , including a famous algorithm due to Knuth, Morris, and Pratt. \nIn Section 5.4  we introduce regular expressions , the basis of the  pattern-matching\nproblem, a generalization of substring search, and a quintes sential search tool known \nas grep. These classic algorithms are based on the related con ceptual devices known as \nformal languages and \ufb01nite automata. Section 5.5 is devoted to a central application: \ndata compression, where we try to reduce the size of a string as much as possible.\nRules of the game For clarity and ef\ufb01ciency, our implementations are expressed \nin terms of the Java String class, but we intentionally use as few operations as possible \nfrom that class to make it easier to adapt our algorithms for use on other string-like \ntypes of data and to other programming languages. W e introduced strings in detail in \nSection ", "start": 707, "end": 708}, "1035": {"text": "but we intentionally use as few operations as possible \nfrom that class to make it easier to adapt our algorithms for use on other string-like \ntypes of data and to other programming languages. W e introduced strings in detail in \nSection 1.2 but brie\ufb02y review here their most important characteristics.\n  C h a r a c t e r s .  A String is a sequence of characters. Characters are of type char and can \nhave one of 216 possible values. For many decades, programmers restricted attention to  \ncharacters encoded in 7-bit ASCII (see page 815 for a conversion table) or 8-bit extended \n A S C I I ,  b u t  m a n y  m o d e r n  a p p l i c a t i o n s  c a l l  f o r  1 6 - b i t   U n i c o d e .\n  I m m u t a b i l i t y .  String objects are immutable, so that we can use them in assignment \nstatements and as arguments and return values from methods without having to worry \nabout their values changing. \n  I n d e x i n g .  The operation that we perform most often is extract a speci\ufb01ed character \nfrom a string that the charAt() method in Java\u2019s String class provides. We expect \ncharAt() to complete its work in constant time, as if the string were stored in a char[]\narray. As discussed in Chapter 1, this expectation is quite reasonable.\n  L e n g t h .  In Java, the \ufb01nd the length of a string operation is implemented in the length()\nmethod in String. Again, we expect length() to complete its work in constant time, \nand again, this expectation is reasonable, although some care is needed in some pro-\ngramming environments.\n   S u b s ", "start": 708, "end": 708}, "1036": {"text": "length()\nmethod in String. Again, we expect length() to complete its work in constant time, \nand again, this expectation is reasonable, although some care is needed in some pro-\ngramming environments.\n   S u b s t r i n g .  Java\u2019s substring() method implements the extract a speci\ufb01ed substring op-\neration. Again, we expect a constant-time implementation of this method, as in Java\u2019s \nstandard implementation. If you are not familiar with substring() and the reason that \nit is constant-time, be sure to reread our discussion of Java\u2019s standard string implementa-\ntion in Section 1.2 (see page 80 and page 204). \n696 CHAPTER 5 \u25a0 Strings\n   C o n c a t e n a t i o n .  In Java, the create a new \nstring formed by appending one string to \nanother operation is a built-in operation \n(using the + operator) that takes time pro-\nportional to the length of the result. For \nexample, we avoid forming a string by ap -\npending one character at a time because \nthat is a quadratic process in Java. (Java has \na  StringBuilder class for that use.)\nCharacter arrays. The Java String is decidedly not a primitive type. The standard \nimplementation provides the operations just described to facilitate client program -\nming. By contrast, many of the algorithms that we consider can work with a low-level \nrepresentation such as an array of char values, and many clients might prefer such a \nrepresentation, because it consumes less space and takes less time. For several of the \nalgorithms that we consider, the cost of converting from one representation to the other \nwould be higher than the cost of running the algorithm. As indicated in the table below, \nthe differences in code that processes the two representations are minor (substring()\nis more complicated and is omitted), so use of one representation or the other is no \nbarrier ", "start": 708, "end": 709}, "1037": {"text": "cost of running the algorithm. As indicated in the table below, \nthe differences in code that processes the two representations are minor (substring()\nis more complicated and is omitted), so use of one representation or the other is no \nbarrier to understanding the algorithm. \nUnderstanding the efficiency of these operations is a key ingredient in under-\nstanding the ef\ufb01ciency of several string-processing algorithms. Not all programming \nlanguages provide String implementations with these performance characteristics. \nFor example, the substring operation and determining the length of a string take time \nproportional to the number of characters in the string in the widely used C program-\nming language. Adapting the algorithms that we describe to such languages is always \npossible (implement an ADT like Java\u2019s String), but also might present different chal-\nlenges and opportunities. \noperation array of characters Java string\ndeclare char[] a String s\nindexed character access a[i] s.charAt(i)\nlength a.length s.length()\nconvert a = s.toCharArray(); s = new String(a);\nTwo ways to represent strings in Java\nFundamental constant-time  String  operations\n0  1  2  3  4  5  6  7  8  9 10 11 12\nA  T  T  A  C  K  A  T  D  A  W  N  s\ns.charAt(3)\ns.length()\ns.substring(7, 11)\n697CHAPTER 5 \u25a0 Strings\n We pr imar ily use the String data type in the text, with liberal use of indexing and \nlength and occasional use of substring extraction and concatenation. When appropri-\nate, we also provide on the booksite the corresponding code for char arrays. In perfor-\nmance-critical applications, the primary consideration in choosing between the two for \nclients is often the cost of accessing a character ( a[i] is ", "start": 709, "end": 710}, "1038": {"text": "also provide on the booksite the corresponding code for char arrays. In perfor-\nmance-critical applications, the primary consideration in choosing between the two for \nclients is often the cost of accessing a character ( a[i] is likely to be much faster than \ns.charAt(i) in typical Java implementations).\n A l p h a b e t s  Some applications involve strings taken from a restricted alphabet. In \nsuch applications, it often makes sense to use an Alphabet class with the following API:\npublic class  Alphabet\nAlphabet(String s) create a new alphabet from chars in s \nchar toChar(int index) convert index to corresponding alphabet char\nint toIndex(char c) convert c to an index between 0 and R-1\nboolean contains(char c) is c in the alphabet?\nint R() radix (number of characters in alphabet)\nint lgR() number of bits to represent an index\nint[] toIndices(String s) convert s to base-R integer\nString toChars(int[] indices) convert base-R integer to string over this alphabet\n A l p h a b e t  A P I\nThis API is based on a constructor that takes as argument an R-character string that \nspeci\ufb01es the alphabet and the toChar() and toIndex() methods for converting (in \nconstant time) between string characters and int values between 0 and R-1. It also \nincludes a contains() method for checking whether a given character is in the alpha-\nbet, the methods R() and lgR() for \ufb01nding the number of characters in the alphabet \nand the number of bits needed to represent them, and the methods toIndices() and \ntoChars() for converting between strings of characters in the alphabet and int arrays. \nFor convenience, we also include the built-in alphabets in the table at the top of the \nnext page, which you can access with code such as Alphabet.UNICODE. ", "start": 710, "end": 710}, "1039": {"text": "of characters in the alphabet and int arrays. \nFor convenience, we also include the built-in alphabets in the table at the top of the \nnext page, which you can access with code such as Alphabet.UNICODE. Implementing \nAlphabet is a straightforward exercise (see Exercise 5.1.12). We will examine a sample \nclient on page 699. \nCharacter-indexed arrays. One of the most important reasons to use Alphabet is that \nmany algorithms gain ef\ufb01ciency through the use of character-indexed arrays, where \nwe associate information with each character that we can retrieve with a single array \n698 CHAPTER 5 \u25a0 Strings\n name R() lgR() characters\nBINARY 2 1 01\nDNA 4 2 ACTG\nOCTAL 8 3 01234567\nDECIMAL 10 4 0123456789\nHEXADECIMAL 16 4 0123456789ABCDEF\nPROTEIN 20 5 ACDEFGHIKLMNPQRSTVWY\nLOWERCASE 26 5 abcdefghijklmnopqrstuvwxyz\nUPPERCASE 26 5 ABCDEFGHIJKLMNOPQRSTUVWXYZ\nBASE64 64 6 ABCDEFGHIJKLMNOPQRSTUVWXYZabcdef \nghijklmnopqrstuvwxyz0123456789+/\nASCII 128 7 ASCII characters\nEXTENDED_ASCII 256 8 extended ASCII characters\nUNICODE16 65536 16 Unicode characters\nStandard alphabets\npublic class  Count \n{\n   public static void main(String[] args)\n   {\n      Alphabet alpha = new Alphabet(args[0]);\n      int R = alpha.R();\n      int[] count = new int[R];\n      String s = StdIn.readAll();\n      int N = s.length();\n      for (int i = 0; i < N;  i++)\n         if (alpha.contains(s.charAt(i)))\n           count[alpha.toIndex(s.charAt(i))]++;\n ", "start": 710, "end": 711}, "1040": {"text": "= StdIn.readAll();\n      int N = s.length();\n      for (int i = 0; i < N;  i++)\n         if (alpha.contains(s.charAt(i)))\n           count[alpha.toIndex(s.charAt(i))]++;\n      for (int c = 0; c < R; c++)\n         StdOut.println(alpha.toChar(c)\n                             + \" \" + count[c]);\n   } \n}\n T y p i c a l  Alphabet client\n% more abra.txt \nABRACADABRA!\n% java Count ABCDR < abra.txt \nA 5 \nB 2 \nC 1 \nD 1 \nR 2\n699CHAPTER 5 \u25a0 Strings\n access. With a Java String, we have to use an array of size 65,536; with Alphabet, we \njust need an array with one entry for each alphabet character. Some of the algorithms \nthat we consider can produce huge numbers of such arrays, and in such cases, the space \nfor arrays of size 65,536 can be prohibitive. As an example, consider the class Count at \nthe bottom of the previous page, which takes a string of characters from the command \nline and prints a table of the frequency of occurrence of those characters that appear on \nstandard input. The count[] array that holds the frequencies in Count is an example of \na character-indexed array. This calculation may seem to you to be a bit frivolous; actu-\nally, it is the basis for a family of fast sorting methods that we will consider in Section \n5.1.\nNumbers. As you can see from several of the standard Alphabet examples, we often \nrepresent numbers as strings. The method toIndices() converts any String over a \ngiven Alphabet into a base-R number represented as an int[] array with all values \nbetween 0 and R/H110021. In some situations, doing this conversion at the start leads to com-\npact ", "start": 711, "end": 712}, "1041": {"text": "toIndices() converts any String over a \ngiven Alphabet into a base-R number represented as an int[] array with all values \nbetween 0 and R/H110021. In some situations, doing this conversion at the start leads to com-\npact code, because any digit can be used as an index in a character-indexed array. For \nexample, if we know that the input consists only of characters from the alphabet, we \ncould replace the inner loop in Count with the more compact code\n   int[] a = alpha.toIndices(s);\n   for (int i = 0; i < N; i++)\n      count[a[i]]++;\nIn this context, we refer to R as the  radix, the base of the number system. Several of the \nalgorithms that we consider are often referred to as \u201cradix\u201d methods because they work \nwith one digit at a time. \n% more pi.txt \n3141592653 \n5897932384 \n6264338327 \n9502884197 \n... [100,000 digits of pi]\n% java Count 0123456789 < pi.txt \n0 9999 \n1 10137 \n2 9908 \n3 10026 \n4 9971 \n5 10026 \n6 10028 \n7 10025 \n8 9978 \n9 9902\n700 CHAPTER 5 \u25a0 Strings\n Despite the advantages of using a data type such as Alphabet in string-processing \nalgorithms (particularly for small alphabets), we do not develop our implementations \nin the book for strings taken from a general Alphabet because\n\u25a0 The preponderance of clients just use String\n\u25a0 Conversion to and from indices tends to fall in the inner loop and slow down \nimplementations considerably\n\u25a0  The code is more complicated, and therefore more dif\ufb01cult to understand\nAccordingly we use String, use the constant R = 256 in the code and R ", "start": 712, "end": 713}, "1042": {"text": "inner loop and slow down \nimplementations considerably\n\u25a0  The code is more complicated, and therefore more dif\ufb01cult to understand\nAccordingly we use String, use the constant R = 256 in the code and R as a parameter \nin the analysis, and discuss performance for general alphabets when appropriate. Y ou \ncan \ufb01nd full Alphabet-based implementations on the booksite.\n701CHAPTER 5 \u25a0 Strings\n 5.1    STRING SORTS\n \n  \nFor many sorting applications , the keys that de\ufb01ne the order are strings. In this \nsection, we look at methods that take advantage of special properties of strings to de-\nvelop sorts for string keys that are more ef\ufb01cient than the general-purpose sorts that we \nconsidered in Chapter 2. \nWe consider two fundamentally different approaches to str ing sor ting . Both of  them \nare venerable methods that have served programmers well for many decades.\nThe \ufb01rst approach examines the characters in the keys in a right-to-left order. Such \nmethods are generally referred to as least-signi\ufb01cant-digit (LSD) string sorts.  Use of \nthe term digit instead of character traces back to the application of the same basic meth-\nod to numbers of various types. Thinking of a string as a base-256 number, considering \ncharacters from right to left amounts to considering \ufb01rst the least signi\ufb01cant digits. \nThis approach is the method of choice for string-sorting applications where all the keys \nare the same length.\nThe second approach examines the characters in the keys in a left-to-right order, \nworking with the most signi\ufb01cant character \ufb01rst.  These methods are generally referred \nto as most-signi\ufb01cant-digit (MSD) string sorts\u2014we will consider two such methods \nin this section. MSD string sorts are attractive because they can get a sorting job done \nwithout necessarily examining all ", "start": 713, "end": 714}, "1043": {"text": "referred \nto as most-signi\ufb01cant-digit (MSD) string sorts\u2014we will consider two such methods \nin this section. MSD string sorts are attractive because they can get a sorting job done \nwithout necessarily examining all of the input characters.  MSD string sorts are similar \nto quicksort, because they partition the array to be sorted into independent pieces such \nthat the sort is completed by recursively applying the same method to the subarrays. \nThe difference is that MSD string sorts use just the \ufb01rst character of the sort key to \ndo the partitioning, while quicksort uses comparisons that could involve examining \nthe whole key. The \ufb01rst method that we consider creates a partition for each character \nvalue; the second always creates three partitions, for sort keys whose \ufb01rst character is \nless than, equal to, or greater than the partitioning key\u2019s \ufb01rst character.\nThe number of characters in the alphabet is an important parameter when analyz-\ning string sorts. Though we focus on extended ASCII strings ( R = 256), we will also \nconsider strings taken from much smaller alphabets (such as genomic sequences) and \nfrom much larger alphabets (such as the 65,536-character Unicode alphabet that is an \ninternational standard for encoding natural languages).\n702\n   K e y - i n d e x e d  c o u n t i n g  As a warmup, we consider a sim-\nple method for sorting that is effective whenever the keys are \nsmall integers. This method, known as key-indexed counting, is \nuseful in its own right and is also the basis for  two of the three \nstring sorts that we consider in this section.\nConsider the following data-processing problem, which \nmight be faced by a teacher maintaining grades for a class with \nstudents assigned to sections, which are numbered 1, 2, 3, and \nso forth. On ", "start": 714, "end": 715}, "1044": {"text": "section.\nConsider the following data-processing problem, which \nmight be faced by a teacher maintaining grades for a class with \nstudents assigned to sections, which are numbered 1, 2, 3, and \nso forth. On some occasions, it is necessary to have the class \nlisted by section. Since the section numbers are small integers, \nsorting by key-indexed counting is appropriate. T o describe \nthe method, we assume that the information is kept in an array \na[] of items that each contain a name and a section number, \nthat section numbers are integers between 0 and R-1, and that \nthe code a[i].key() returns \nthe section number for the in-\ndicated student. The method \nbreaks down into four steps, \nwhich we describe in turn.\nCompute frequency counts. The \ufb01rst step is to count \nthe frequency of occurrence of each key value, using \nan int array count[]. For each item, we use the key to \naccess an entry in count[] and increment that entry. If \nthe key value is r, we increment count[r+1]. (Why +1? \nThe reason for that will become clear in the next step.) \nIn the example at left, we \ufb01rst increment count[3]\nbecause Anderson is in section 2, then we increment \ncount[4] twice because Brown and Davis are in sec-\ntion 3, and so forth. Note that count[0] is always 0, \nand that count[1] is 0 in this example (no students are \nin section 0).\nComputing frequency counts\n                 count[]\n               0 1 2 3 4 5\n               0 0 0 0 0 0\nAnderson  2    0 0 0 1 0 0\nBrown     3    0 0 0 1 ", "start": 715, "end": 715}, "1045": {"text": "5\n               0 0 0 0 0 0\nAnderson  2    0 0 0 1 0 0\nBrown     3    0 0 0 1 1 0\nDavis     3    0 0 0 1 2 0\nGarcia    4    0 0 0 1 2 1\nHarris    1    0 0 1 1 2 1\nJackson   3    0 0 1 1 3 1\nJohnson   4    0 0 1 1 3 2\nJones     3    0 0 1 1 4 2\nMartin    1    0 0 2 1 4 2\nMartinez  2    0 0 2 2 4 2\nMiller    2    0 0 2 3 4 2\nMoore     1    0 0 3 3 4 2\nRobinson  2    0 0 3 4 4 2\nSmith     4    0 0 3 4 4 3\nTaylor    3    0 0 3 4 5 3\nThomas    4    0 0 3 4 5 4\nThompson  4    0 0 3 4 5 5\nWhite     2    0 0 3 5 5 5\nWilliams  3    0 0 3 5 6 5\nWilson    4    0 0 3 5 6 6\nfor (i = 0; i < N; i++)\n  count[a[i].key() ", "start": 715, "end": 715}, "1046": {"text": "0 0 3 5 6 5\nWilson    4    0 0 3 5 6 6\nfor (i = 0; i < N; i++)\n  count[a[i].key() + 1]++;\nnumber of 3s\nalways 0\nAnderson  2       Harris    1\nBrown     3       Martin    1\nDavis     3       Moore     1\nGarcia    4       Anderson  2\nHarris    1       Martinez  2\nJackson   3       Miller    2\nJohnson   4       Robinson  2\nJones     3       White     2\nMartin    1       Brown     3\nMartinez  2       Davis     3\nMiller    2       Jackson   3\nMoore     1       Jones     3\nRobinson  2       Taylor    3\nSmith     4       Williams  3\nTaylor    3       Garcia    4\nThomas    4       Johnson   4\nThompson  4       Smith     4\nWhite     2       Thomas    4\nWilliams  3       Thompson  4\nWilson    4       Wilson    4\nTypical candidate for key-indexed counting\ninput sorted result\nkeys are\nsmall integers \nsection (by section) name\n7035.1 \u25a0 String Sorts\n Transfor m counts to indices. Next, we use count[]\nto compute, for each key value, the starting index \npositions in the sorted order of items with that key. \nIn our example, since there are three items with key \n1 and \ufb01ve items with key 2, then the items with key \n3 start at position 8 in the sorted array. In general, \nto get the starting index for items with any given key \nvalue ", "start": 715, "end": 716}, "1047": {"text": "key \n1 and \ufb01ve items with key 2, then the items with key \n3 start at position 8 in the sorted array. In general, \nto get the starting index for items with any given key \nvalue we sum the frequency counts of smaller val-\nues. For each key value r, the sum of the counts for \nkey values less than r+1 is equal to the sum of the \ncounts for key values less than r plus count[r], so \nit is easy to proceed from left to right to transform \ncount[] into an index table that we can use to sort \nthe data.   \nDistribute the data. With the \ncount[] array transformed into \nan index table, we accomplish the \nactual sort by moving the items to \nan auxiliary array aux[]. We move \neach item to the position in aux[]\nindicated by the count[] entry cor-\nresponding to its key, and then in -\ncrement that entry to maintain the \nfollowing invariant for count[]: \nfor each key value r, count[r] is \nthe index of the position in aux[]\nwhere the next item with key value \nr (if any) should be placed. This \nprocess produces a sorted result \nwith one pass through the data, as \nillustrated at left. Note : In one of \nour applications, the fact that this \nimplementation is stable is critical: \nitems with equal keys are brought \ntogether but kept in the same rela -\ntive order.\n   \n               \n               \nAnderson  2       Harris    1\nBrown     3       Martin    1\nDavis     3       Moore     1\nGarcia    4       Anderson  2\nHarris    1       Martinez  2\nJackson   3       Miller    2\nJohnson   4       Robinson  2\nJones     3       White     2\nMartin    1       Brown ", "start": 716, "end": 716}, "1048": {"text": "2\nHarris    1       Martinez  2\nJackson   3       Miller    2\nJohnson   4       Robinson  2\nJones     3       White     2\nMartin    1       Brown     3\nMartinez  2       Davis     3\nMiller    2       Jackson   3\nMoore     1       Jones     3\nRobinson  2       Taylor    3\nSmith     4       Williams  3\nTaylor    3       Garcia    4\nThomas    4       Johnson   4\nThompson  4       Smith     4\nWhite     2       Thomas    4\nWilliams  3       Thompson  4\nWilson    4       Wilson    4\nDistributing the data (records with key 3 highlighted)\n  count[]\n1  2  3  4\n0  3  8 14\n0  4  8 14\n0  4  9 14\n0  4 10 14\n0  4 10 15\n1  4 10 15\n1  4 11 15\n1  4 11 16\n1  4 12 16\n2  4 12 16\n2  5 12 16\n2  6 12 16\n3  6 12 16\n3  7 12 16\n3  7 12 17\n3  7 13 17\n3  7 13 18\n3  7 13 19\n3  8 13 19\n3  8 14 19\n3  8 14 20\n3  8 14 20\n i\n 0\n ", "start": 716, "end": 716}, "1049": {"text": "7 13 19\n3  8 13 19\n3  8 14 19\n3  8 14 20\n3  8 14 20\n i\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n a[0]\n a[1]\n a[2]\n a[3]\n a[4]\n a[5]\n a[6]\n a[7]\n a[8]\n a[9]\na[10]\na[11]\na[12]\na[13]\na[14]\na[15]\na[16]\na[17]\na[18]\na[19]\naux[0]\naux[1]\naux[2]\naux[3]\naux[4]\naux[5]\naux[6]\naux[7]\naux[8]\naux[9]\naux[10]\naux[11]\naux[12]\naux[13]\naux[14]\naux[15]\naux[16]\naux[17]\naux[18]\naux[19]\nfor (int i = 0; i < N; i++)\n   aux[count[a[i].key()]++] = a[i];\nTransforming counts to start indices\nnumber of keys less than 3\n(start index of 3s in output)\nfor (int r = 0; r < R; r++)\n   count[r+1] += count[r];\n     count[]\n0  1  2  3  4  5\n0  0  3  5  6  6\n0  0  3  5  6  6\n0  0  3  5  6  6\n0 ", "start": 716, "end": 716}, "1050": {"text": "3  5  6  6\n0  0  3  5  6  6\n0  0  3  5  6  6\n0  0  3  8  6  6\n0  0  3  8 14  6\n0  0  3  8 14 20\n0  0  3  8 14 20\nalways 0\n r\n 0\n 1\n 2\n 3\n 4\n 5\n704 CHAPTER 5 \u25a0 Strings\n Copy back. Since we accomplished the sort by moving the items to an auxiliary array, \nthe last step is to copy the sorted result back to the original array. \nProposition A.   Key-indexed counting uses 8N /H11001 3 R /H11001 1  array accesses to  stably \nsort N items whose keys are integers between 0 and R /H11002 1.\nProof: Immediate from the code. Initializing the arrays uses N /H11001 R /H11001 1 array ac-\ncesses. The \ufb01rst loop increments a counter for each of the N items (2 N array ac-\ncesses); the second loop does R additions (2R array accesses); the third loop does \nN  counter increments and N data moves (3N array accesses); and the fourth loop \ndoes N  data moves (2N array accesses). Both moves preserve the relative order of \nequal keys.\n \nKey-indexed counting is an extremely effective \nand often overlooked sorting method for applica-\ntions where keys are small integers. Understand-\ning how it works is a \ufb01rst step toward understand-\ning string sorting. Proposition A  implies that \nkey-indexed counting breaks through the N log \nN ", "start": 716, "end": 717}, "1051": {"text": "applica-\ntions where keys are small integers. Understand-\ning how it works is a \ufb01rst step toward understand-\ning string sorting. Proposition A  implies that \nkey-indexed counting breaks through the N log \nN lower bound that we proved for sorting. How \ndoes it manage to do so? Proposition I in SEc-\ntion 2.2 is a lower bound on the number of com-\npares needed (when data is accessed only through \ncompareTo())\u2014key-indexed counting does no\ncompares (it accesses data only through key()). \nWhen R is within a constant factor of N, we have \na linear-time sort.\nint N = a.length;\nString[] aux = new String[N]; \nint[] count = new int[R+1];\n// Compute frequency counts. \nfor (int i = 0; i < N; i++)\n   count[a[i].key() + 1]++; \n// Transform counts to indices. \nfor (int r = 0; r < R; r++)\n   count[r+1] += count[r]; \n// Distribute the records. \nfor (int i = 0; i < N; i++)\n   aux[count[a[i].key()]++] = a[i]; \n// Copy back. \nfor (int i = 0; i < N; i++)\n   a[i] = aux[i];\nKey-indexed counting (a[].key is an int in [0, R).\n. . .... ...\ncount[R-1]\nR-1 R-1 R-1\ncount[2]count[1]count[0]\naux[] R-12 21 ... 21 1 100\n. . .\ncount[R-1]\nR-1 R-1 R-1\ncount[2]count[1]count[0]\naux[] 2 21 1 10\n. . .\ncount[R-1]count[2]count[1]count[0]\naux[]\nKey-indexed ", "start": 717, "end": 717}, "1052": {"text": ".\ncount[R-1]\nR-1 R-1 R-1\ncount[2]count[1]count[0]\naux[] 2 21 1 10\n. . .\ncount[R-1]count[2]count[1]count[0]\naux[]\nKey-indexed counting (distribution phase)\nbefore\nduring\nafter\n7055.1 \u25a0 String Sorts\n    L S D  s t r i n g  s o r t  The \ufb01rst string-sorting method that we consider is known as least-\nsigni\ufb01cant-digit \ufb01rst (LSD) string sort. Consider the following motivating application: \nSuppose that a highway engineer sets up a device that records \nthe license plate numbers of all vehicles using a busy highway \nfor a given period of time and wants to know the number of \ndifferent vehicles that used the highway. As you know from \nSection 2.1, one easy way to solve this problem is to sort the \nnumbers, then make a pass through to count the  different val-\nues, as in Dedup (page 490). License plates are a mixture of num-\nbers and letters, so it is natural to represent them as strings. In \nthe simplest situation (such as the California license plate ex-\namples at right) the strings all have the same number of char-\nacters. This situation is often found in sort applications\u2014for \nexample, telephone numbers, bank account numbers, and IP \naddresses are typically \ufb01xed-length strings.\nSorting such strings can be done with key-indexed count-\ning, as shown in Algorithm 5.1 ( LSD) and the example be-\nlow it on the facing page. If the strings are each of length W, \nwe sort the strings W times with key-indexed counting, using \neach of the positions as the key, proceeding from right to left. It is not easy, at \ufb01rst, ", "start": 717, "end": 718}, "1053": {"text": "page. If the strings are each of length W, \nwe sort the strings W times with key-indexed counting, using \neach of the positions as the key, proceeding from right to left. It is not easy, at \ufb01rst, to \nbe convinced that the method produces a sorted array\u2014in fact, it does not work at all \nunless the key-indexed count implementation is stable.  Keep this fact in mind and refer \nto the example when studying this proof of correctness :\nProposition B.   LSD string sort stably sorts \ufb01xed-length strings.\nProof: This fact depends crucially on the key-indexed counting implementation \nbeing  stable, as indicated in Proposition A. After sorting keys on their i trailing \ncharacters (in a stable manner), we know that any two keys appear in proper order \nin the array (considering just those characters) either because the \ufb01rst of their i\ntrailing characters is different, in which case the sort on that character puts them in \norder, or because the \ufb01rst of their ith trailing characters is the same, in which case \nthey are in order because of stability (and by induction, for i-1).  \nAnother way to state the proof is to think about the future: if the characters that have \nnot been examined for a pair of keys are identical, any difference between the keys is re-\nstricted to the characters already examined, so the keys have been properly ordered and \nwill remain so because of stability.  If, on the other hand, the characters that have not \nTypical  candidate for\nLSD string sort\nkeys are all\nthe same length\n4PGC938\n2IYE230\n3CIO720\n1ICK750\n1OHV845\n4JZY524\n1ICK750\n3CIO720\n1OHV845\n1OHV845\n2RLA629\n2RLA629\n3ATW723\ninput ", "start": 718, "end": 718}, "1054": {"text": "length\n4PGC938\n2IYE230\n3CIO720\n1ICK750\n1OHV845\n4JZY524\n1ICK750\n3CIO720\n1OHV845\n1OHV845\n2RLA629\n2RLA629\n3ATW723\ninput sorted result\n1ICK750\n1ICK750\n1OHV845\n1OHV845\n1OHV845\n2IYE230\n2RLA629\n2RLA629\n3ATW723\n3CIO720\n3CIO720\n4JZY524\n4PGC938\n706 CHAPTER 5 \u25a0 Strings\n 4PGC938\n2IYE230\n3CIO720\n1ICK750\n1OHV845\n4JZY524\n1ICK750\n3CIO720\n1OHV845\n1OHV845\n2RLA629\n2RLA629\n3ATW723\n2IYE230\n3CIO720\n1ICK750\n1ICK750\n3CIO720\n3ATW723\n4JZY524\n1OHV845\n1OHV845\n1OHV845\n4PGC938\n2RLA629\n2RLA629\n3CIO720\n3CIO720\n3ATW723\n4JZY524\n2RLA629\n2RLA629\n2IYE230\n4PGC938\n1OHV845\n1OHV845\n1OHV845\n1ICK750\n1ICK750\n2IYE230\n4JZY524\n2RLA629\n2RLA629\n3CIO720\n3CIO720\n3ATW723\n1ICK750\n1ICK750\n1OHV845\n1OHV845\n1OHV845\n4PGC938\n2RLA629\n2RLA629\n4PGC938\n2IYE230\n1ICK750\n1ICK750\n3CIO720\n3CIO720\n1OHV845\n1OHV845\n1OHV845\n3ATW723\n4JZY524\n1ICK750\n1ICK750\n4PGC938\n1OHV845\n1OHV845\n1OHV845\n3CIO720\n3CIO720\n2RLA629\n2RLA629\n3ATW723\n2IYE230\n4JZY524\n3ATW723\n3CIO720\n3CIO720\n1ICK750\n1ICK750\n2IYE230\n4JZY524\n1OHV845\n1OHV845\n1OHV845\n4PGC938\n2RLA629\n2RLA629\n1ICK750\n1ICK750\n1OHV845\n1OHV845\n1OHV845\n2IYE230\n2RLA629\n2RLA629\n3ATW723\n3CIO720\n3CIO720\n4JZY524\n4PGC938\n1ICK750\n1ICK750\n1OHV845\n1OHV845\n1OHV845\n2IYE230\n2RLA629\n2RLA629\n3ATW723\n3CIO720\n3CIO720\n4JZY524\n4PGC938\ninput ", "start": 718, "end": 719}, "1055": {"text": "4PGC938\n2IYE230\n3CIO720\n1ICK750\n1OHV845\n4JZY524\n1ICK750\n3CIO720\n1OHV845\n1OHV845\n2RLA629\n2RLA629\n3ATW723\n2IYE230\n3CIO720\n1ICK750\n1ICK750\n3CIO720\n3ATW723\n4JZY524\n1OHV845\n1OHV845\n1OHV845\n4PGC938\n2RLA629\n2RLA629\n3CIO720\n3CIO720\n3ATW723\n4JZY524\n2RLA629\n2RLA629\n2IYE230\n4PGC938\n1OHV845\n1OHV845\n1OHV845\n1ICK750\n1ICK750\n2IYE230\n4JZY524\n2RLA629\n2RLA629\n3CIO720\n3CIO720\n3ATW723\n1ICK750\n1ICK750\n1OHV845\n1OHV845\n1OHV845\n4PGC938\n2RLA629\n2RLA629\n4PGC938\n2IYE230\n1ICK750\n1ICK750\n3CIO720\n3CIO720\n1OHV845\n1OHV845\n1OHV845\n3ATW723\n4JZY524\n1ICK750\n1ICK750\n4PGC938\n1OHV845\n1OHV845\n1OHV845\n3CIO720\n3CIO720\n2RLA629\n2RLA629\n3ATW723\n2IYE230\n4JZY524\n3ATW723\n3CIO720\n3CIO720\n1ICK750\n1ICK750\n2IYE230\n4JZY524\n1OHV845\n1OHV845\n1OHV845\n4PGC938\n2RLA629\n2RLA629\n1ICK750\n1ICK750\n1OHV845\n1OHV845\n1OHV845\n2IYE230\n2RLA629\n2RLA629\n3ATW723\n3CIO720\n3CIO720\n4JZY524\n4PGC938\n1ICK750\n1ICK750\n1OHV845\n1OHV845\n1OHV845\n2IYE230\n2RLA629\n2RLA629\n3ATW723\n3CIO720\n3CIO720\n4JZY524\n4PGC938\ninput (W ", "start": 719, "end": 719}, "1056": {"text": "(W = 7) d = 6 d = 5 d = 4 d = 3 d = 2 d = 1 d = 0 output\nALGORITHM 5.1   LSD string sort\npublic class  LSD \n{\n   public static void sort(String[] a, int W)\n   {  // Sort a[] on leading W characters.\n      int N = a.length;\n      int R = 256;\n      String[] aux = new String[N];\n      for (int d = W-1; d >= 0; d--)\n      { // Sort by key-indexed counting on dth char.\n         int[] count = new int[R+1];     // Compute frequency counts.\n         for (int i = 0; i < N; i++)\n             count[a[i].charAt(d) + 1]++;\n         for (int r = 0; r < R; r++)     // Transform counts to indices.\n            count[r+1] += count[r];\n         for (int i = 0; i < N; i++)     // Distribute.\n            aux[count[a[i].charAt(d)]++] = a[i];\n         for (int i = 0; i < N; i++)     // Copy back.\n            a[i] = aux[i];\n        }\n    } \n}\nTo  s o r t  a n  a r r ay  a[] of strings that each have exactly W characters, we do W key-indexed counting \nsorts: one for each character position, proceeding from right to left.\n7075.1 \u25a0 String Sorts  \n \n \nbeen examined are different, the characters already examined do not matter, \nand a later pass will correctly order the pair based on the more signi\ufb01cant \ndifferences.\nLSD radix sorting is the method used by the old punched-card-sorting \nmachines that were developed at the beginning of the 20th century ", "start": 719, "end": 720}, "1057": {"text": "order the pair based on the more signi\ufb01cant \ndifferences.\nLSD radix sorting is the method used by the old punched-card-sorting \nmachines that were developed at the beginning of the 20th century and thus \npredated the use of computers in commercial data processing by several de-\ncades.  Such machines had the capability of distributing a deck of punched \ncards among 10 bins, according to the pattern of holes punched in the select-\ned columns.  If a deck of cards had numbers punched in a particular set of \ncolumns, an operator could sort the cards by running them through the ma-\nchine on the rightmost digit, then picking up and stacking the output decks \nin order, then running them through the machine on the next-to-rightmost \ndigit, and so forth, until getting to the \ufb01rst digit.  The physical stacking of the \ncards is a stable process, which is mimicked by key-indexed counting sort. \nNot only was this version of LSD radix sorting important in commercial \napplications up through the 1970s, but it was also used by many cautious \nprogrammers (and students!), who would have to keep their programs on \npunched cards (one line per card) and would punch sequence numbers in \nthe \ufb01nal few columns of a program deck so as to be able to put the deck back \nin order mechanically if it were accidentally dropped. This method is also a \nneat way to sort a deck of playing cards: deal them into thirteen piles (one for \neach value), pick up the piles in order, then deal into four piles (one for each \nsuit). The (stable) dealing process keeps the cards in order within each suit, \nso picking up the piles in suit order yields a sorted deck.\nIn many string-sorting applications (even license plates, for some states), \nthe keys are not all be the same length. It is possible ", "start": 720, "end": 720}, "1058": {"text": "each suit, \nso picking up the piles in suit order yields a sorted deck.\nIn many string-sorting applications (even license plates, for some states), \nthe keys are not all be the same length. It is possible to adapt LSD string sort \nto work for such applications, but we leave this task for exercises because \nwe will next consider two other methods that are speci\ufb01cally designed for \nvariable-length keys.\nFrom a theoretical standpoint, LSD string sort is signi\ufb01cant because it is a \nlinear-time sort for typical applications.  No matter how large the value of N, \nit makes W passes through the data. Speci\ufb01cally:\nSorting a card deck with\nLSD string sort\n{A\n{2\n{3\n{4\n{5\n{6\n{7\n{8\n{9\n{10\n{J\n{Q\n{K\nxA\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\nxJ\nxQ\nxK\nzA\nz2\nz3\nz4\nz5\nz6\nz7\nz8\nz9\nz10\nzJ\nzQ\nzK\nyA\ny2\ny3\ny4\ny5\ny6\ny7\ny8\ny9\ny10\nyJ\nyQ\nyK\nyJ\nx6\nzA\nxA\n{K\nxJ\nzQ\ny6\n{J\nyA\nz9\nx9\nz8\n{9\nyK\nz4\n{5\nyQ\nx3\n{2\ny10\ny9\nx7\ny4\nx4\nz10\n{A\nz5\n{3\nx8\ny2\nzK\n{4\ny7\nxQ\nzJ\n{6\ny3\n{7\n{8\n{10\nz3\nx10\nz7\n{Q\nx2\nz2\ny5\nxK\nx5\nz6\ny8\nzA\nxA\nyA\n{A\n{2\ny2\nx2\nz2\nx3\n{3\ny3\nz3\nz4\ny4\nx4\n{4\n{5\nz5\ny5\nx5\nx6\ny6\n{6\nz6\nx7\ny7\n{7\nz7\nz8\nx8\n{8\ny8\nz9\nx9\n{9\ny9\ny10\nz10\n{10\nx10\nyJ\nxJ\n{J\nzJ\nzQ\nyQ\nxQ\n{Q\n{K\nyK\nzK\nxK\n708 ", "start": 720, "end": 720}, "1059": {"text": "sort\n{A\n{2\n{3\n{4\n{5\n{6\n{7\n{8\n{9\n{10\n{J\n{Q\n{K\nxA\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\nxJ\nxQ\nxK\nzA\nz2\nz3\nz4\nz5\nz6\nz7\nz8\nz9\nz10\nzJ\nzQ\nzK\nyA\ny2\ny3\ny4\ny5\ny6\ny7\ny8\ny9\ny10\nyJ\nyQ\nyK\nyJ\nx6\nzA\nxA\n{K\nxJ\nzQ\ny6\n{J\nyA\nz9\nx9\nz8\n{9\nyK\nz4\n{5\nyQ\nx3\n{2\ny10\ny9\nx7\ny4\nx4\nz10\n{A\nz5\n{3\nx8\ny2\nzK\n{4\ny7\nxQ\nzJ\n{6\ny3\n{7\n{8\n{10\nz3\nx10\nz7\n{Q\nx2\nz2\ny5\nxK\nx5\nz6\ny8\nzA\nxA\nyA\n{A\n{2\ny2\nx2\nz2\nx3\n{3\ny3\nz3\nz4\ny4\nx4\n{4\n{5\nz5\ny5\nx5\nx6\ny6\n{6\nz6\nx7\ny7\n{7\nz7\nz8\nx8\n{8\ny8\nz9\nx9\n{9\ny9\ny10\nz10\n{10\nx10\nyJ\nxJ\n{J\nzJ\nzQ\nyQ\nxQ\n{Q\n{K\nyK\nzK\nxK\n708 CHAPTER ", "start": 720, "end": 720}, "1060": {"text": "CHAPTER 5 \u25a0 Strings\n Proposition B (continued).  LSD string sort uses ~7WN /H11001 3WR array accesses and \nextra space proportional to N /H11001 R to sort N items whose keys are W-character \nstrings taken from an R-character alphabet.\nProof: The method is W passes of key-indexed counting, except that the aux[] ar-\nray is initialized just once. The total is immediate from the code and Proposition\u00a0A.\nFor typical applications, R is far smaller than N, so Proposition B implies that the to-\ntal running time is proportional to WN. An input array of N strings that each have W \ncharacters has a total of WN characters, so the running time of LSD string sort is linear\nin the size of the input. \n7095.1 \u25a0 String Sorts\n  \n \n \n  M S D  s t r i n g  s o r t  To  i m p l e m e n t  a  g e n e r a l - p u r p o s e  s t r i n g  s o r t , w h e re  \nstrings are not necessarily all the same length, we consider the characters \nin left-to-right order. We know that strings that start with a should appear \nbefore strings that start with b, and so forth. The natural way to implement \nthis idea is a recursive method known as most-signi\ufb01cant-digit-\ufb01rst (MSD) \nstring sort. We use key-indexed counting to sort the strings according to \ntheir \ufb01rst character, then (recursively) sort the subarrays corresponding to \neach character (excluding the \ufb01rst character, which we know to be the same \nfor each string in each subarray). Like quicksort, MSD string sort parti-\ntions the array into subarrays that can be sorted independently to complete \nthe job, but it partitions the array ", "start": 720, "end": 722}, "1061": {"text": "know to be the same \nfor each string in each subarray). Like quicksort, MSD string sort parti-\ntions the array into subarrays that can be sorted independently to complete \nthe job, but it partitions the array into one subarray for each possible value \nof the \ufb01rst character, instead of the two or three partitions in quicksort. \nEnd-of-string convention. We need to pay par ticular attention to reaching \nthe ends of strings in MSD string sort. For a proper sort, we need the sub-\narray for strings whose charac-\nters have all been examined to \nappear as the \ufb01rst subarray, and \nwe do not want to recursively \nsort this subarray. T o facilitate \nthese two parts of the compu-\ntation we use a private two-\nargument toChar() method to \nconvert from an indexed string \ncharacter to an array index \nthat returns -1 if the speci\ufb01ed \ncharacter position is past the \nend of the string. Then, we just \nadd 1 to each returned value, to \nget a nonnegative int that we \ncan use to index count[]. This \nconvention means that we have \nR+1 different possible charac-\nter values at each string posi -\ntion: 0 to signify end of string , \n1 for the \ufb01rst alphabet charac-\nter, 2 for the second alphabet \ncharacter, and so forth. Since \nOverview of MSD string sort\n0\n0\n0\n.\n.\n.\n0\n0\n1\n1\n1\n.\n.\n.\n1\n1\n2\n2\n2\n.\n.\n.\n2\n2\nr\nr\nr\n.\n.\n.\nr\nr\nsort on first character value\nto partition into subarrays\nrecursively sort subarrays\n(excluding first character)\n.\n.\n.\n.\n.\n.\n0\n0\n0\n.\n.\n.\n0\n0\n1\n1\n1\n.\n.\n.\n1\n1\n2\n2\n2\n.\n.\n.\n2\n2\nr\nr\nr\n.\n.\n.\nr\nr\n.\n.\n.\n.\n.\n.\nSorting ", "start": 722, "end": 722}, "1062": {"text": "sort\n0\n0\n0\n.\n.\n.\n0\n0\n1\n1\n1\n.\n.\n.\n1\n1\n2\n2\n2\n.\n.\n.\n2\n2\nr\nr\nr\n.\n.\n.\nr\nr\nsort on first character value\nto partition into subarrays\nrecursively sort subarrays\n(excluding first character)\n.\n.\n.\n.\n.\n.\n0\n0\n0\n.\n.\n.\n0\n0\n1\n1\n1\n.\n.\n.\n1\n1\n2\n2\n2\n.\n.\n.\n2\n2\nr\nr\nr\n.\n.\n.\nr\nr\n.\n.\n.\n.\n.\n.\nSorting a card deck with\nMSD string sort\n{A\n{2\n{3\n{4\n{5\n{6\n{7\n{8\n{9\n{10\n{J\n{Q\n{K\nxA\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\nxJ\nxQ\nxK\nzA\nz2\nz3\nz4\nz5\nz6\nz7\nz8\nz9\nz10\nzJ\nzQ\nzK\nyA\ny2\ny3\ny4\ny5\ny6\ny7\ny8\ny9\ny10\nyJ\nyQ\nyK\n{K\n{J\n{9\n{5\n{2\n{A\n{3\n{4\n{6\n{7\n{8\n{10\n{Q\nx6\nxA\nxJ\nx9\nx3\nx7\nx4\nx8\nxQ\nx10\nx2\nxK\nx5\nzA\nzQ\nz9\nz8\nz4\nz10\nz5\nzK\nzJ\nz3\nz7\nz2\nz6\nyJ\ny6\nyA\nyK\nyQ\ny10\ny9\ny4\ny2\ny7\ny3\ny5\ny8\nyJ\nx6\nzA\nxA\n{K\nxJ\nzQ\ny6\n{J\nyA\nz9\nx9\nz8\n{9\nyK\nz4\n{5\nyQ\nx3\n{2\ny10\ny9\nx7\ny4\nx4\nz10\n{A\nz5\n{3\nx8\ny2\nzK\n{4\ny7\nxQ\nzJ\n{6\ny3\n{7\n{8\n{10\nz3\nx10\nz7\n{Q\nx2\nz2\ny5\nxK\nx5\nz6\ny8\n710 ", "start": 722, "end": 722}, "1063": {"text": "sort\n{A\n{2\n{3\n{4\n{5\n{6\n{7\n{8\n{9\n{10\n{J\n{Q\n{K\nxA\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\nxJ\nxQ\nxK\nzA\nz2\nz3\nz4\nz5\nz6\nz7\nz8\nz9\nz10\nzJ\nzQ\nzK\nyA\ny2\ny3\ny4\ny5\ny6\ny7\ny8\ny9\ny10\nyJ\nyQ\nyK\n{K\n{J\n{9\n{5\n{2\n{A\n{3\n{4\n{6\n{7\n{8\n{10\n{Q\nx6\nxA\nxJ\nx9\nx3\nx7\nx4\nx8\nxQ\nx10\nx2\nxK\nx5\nzA\nzQ\nz9\nz8\nz4\nz10\nz5\nzK\nzJ\nz3\nz7\nz2\nz6\nyJ\ny6\nyA\nyK\nyQ\ny10\ny9\ny4\ny2\ny7\ny3\ny5\ny8\nyJ\nx6\nzA\nxA\n{K\nxJ\nzQ\ny6\n{J\nyA\nz9\nx9\nz8\n{9\nyK\nz4\n{5\nyQ\nx3\n{2\ny10\ny9\nx7\ny4\nx4\nz10\n{A\nz5\n{3\nx8\ny2\nzK\n{4\ny7\nxQ\nzJ\n{6\ny3\n{7\n{8\n{10\nz3\nx10\nz7\n{Q\nx2\nz2\ny5\nxK\nx5\nz6\ny8\n710 CHAPTER ", "start": 722, "end": 722}, "1064": {"text": "CHAPTER 5 \u25a0 Strings\n key-indexed counting already needs one extra position, we use the \ncode int count[] = new int[R+2]; to create the array of fre -\nquency counts (and set all of its values to 0). Note : Some languages, \nnotably C and C++, have a built-in end-of-string convention, so \nour code needs to be adjusted accordingly for such languages.\nWith these preparations , the implementation of MSD string \nsort, in Algorithm 5.2, requires very little new code. We add a test \nto cutoff to insertion sort for small subarrays (using a specialized \ninsertion sort that we will consider later), and we add a loop to \nkey-indexed counting to do the recursive calls. As summarized in \nthe table at the bottom of this page, the values in the count[] array \n(after serving to count the frequencies, transform counts to indices, \nand distribute the data) give us precisely the information that we \nneed to (recursively) sort the subarrays corresponding to each character value. \nSpeci\ufb01ed alphabet. The cost of MSD string sort depends strongly on the number of \npossible characters in the alphabet. It is easy to modify our sort method to take an \nAlphabet as argument, to allow for improved ef\ufb01ciency in clients involving strings \ntaken from relatively small alphabets. The following changes will do the job:\n\u25a0 Save the alphabet in an instance variable alpha in the constructor.\n\u25a0 Set R to alpha.R() in the constructor.\n\u25a0 Replace s.charAt(d) with alpha.toIndex(s.charAt(d)) in charAt().\nTypical candidate for MSD string sort\nvarious\nkey \nlengths\nshe\nsells\nseashells\nby\nthe\nseashore\nthe\nshells\nshe\nsells\nare\nsurely\nseashells\nare\nby\nseashells\nseashells\nseashore\nsells\nsells\nshe\nshe\nshells\nsurely\nthe\nthe\ninput ", "start": 722, "end": 723}, "1065": {"text": "\nlengths\nshe\nsells\nseashells\nby\nthe\nseashore\nthe\nshells\nshe\nsells\nare\nsurely\nseashells\nare\nby\nseashells\nseashells\nseashore\nsells\nsells\nshe\nshe\nshells\nsurely\nthe\nthe\ninput sorted result\nat completion \nof phase for \ndth character\nvalue of count[r] is\nr = 0 r = 1 r between 2 and R-1 r  = R r  = R+1\ncount \nfrequencies 0 (not used)\nnumber of \nstrings of \nlength d\nnumber of strings whose\ndth character value is r-2\ntransform\ncounts to\nindices\nstart index of subarray for \nstrings of length d\nstart index of subarray for strings whose\ndth character value is r-1\nnot\nused\ndistribute\nstart index of subarray for strings\nwhose dth character value is r\nnot\nused\n1 + end index of subarray \nfor strings of length d\n1 + end index of subarray for strings \nwhose dth character value is r-1\nnot\nused\nInterpretation of count[] values during MSD string sort\n7115.1 \u25a0 String Sorts\n ALGORITHM 5.2   MSD string sort\npublic class  MSD \n{\n   private static int R = 256;        // radix\n   private static final int M = 15;   // cutoff for small subarrays\n   private static String[] aux;       // auxiliary array for distribution\n   private static int charAt(String s, int d)\n   {  if (d < s.length()) return s.charAt(d); else return -1;  }\n   public static void sort(String[] a)\n   {\n      int N = a.length;\n      aux = new ", "start": 723, "end": 724}, "1066": {"text": "d)\n   {  if (d < s.length()) return s.charAt(d); else return -1;  }\n   public static void sort(String[] a)\n   {\n      int N = a.length;\n      aux = new String[N];\n      sort(a, 0, N-1, 0);\n   }\n   private static void sort(String[] a, int lo, int hi, int d)\n   {  // Sort from a[lo] to a[hi], starting at the dth character. \n      if (hi <= lo + M)\n      {  Insertion.sort(a, lo, hi, d); return;  }\n      int[] count = new int[R+2];        // Compute frequency counts.\n      for (int i = lo; i <= hi; i++)\n         count[charAt(a[i], d) + 2]++;\n      for (int r = 0; r < R+1; r++)      // Transform counts to indices.\n         count[r+1] += count[r];\n      for (int i = lo; i <= hi; i++)     // Distribute. \n         aux[count[charAt(a[i], d) + 1]++] = a[i];\n      for (int i = lo; i <= hi; i++)     // Copy back.\n         a[i] = aux[i - lo];\n      // Recursively sort for each character value.\n      for (int r = 0; r < R; r++)\n         sort(a, lo + count[r], lo + count[r+1] - 1, d+1);\n    }\n}\n To  s o r t  a n  a r r ay  a[] of strings, we sort them on their \ufb01rst character using key-indexed counting, then \n(recursively) sort the subarrays corresponding to each \ufb01rst-character value.\n712 CHAPTER 5 \u25a0 Strings In our running examples, we use strings made ", "start": 724, "end": 725}, "1067": {"text": "them on their \ufb01rst character using key-indexed counting, then \n(recursively) sort the subarrays corresponding to each \ufb01rst-character value.\n712 CHAPTER 5 \u25a0 Strings In our running examples, we use strings made up of lowercase letters. It is also easy to \nextend LSD string sort to provide this feature, but typically with much less impact on \nperformance than for MSD string sort.\nThe code in Algorithm 5.2  is deceptively simple, masking a rather sophisticated \ncomputation. It is de\ufb01nitely worth your while to study the trace of the top level at \nthe bottom of this page and the trace of recursive calls on the next page, to be sure \nthat you understand the intricacies of the algorithm. This trace uses a cutoff-for-small-\nsubarrays threshold value ( M) of 0, so that you can see the sort to completion for this \nsmall example. The strings in this example are taken from Alphabet.LOWERCASE, with \nR = 26; bear in mind that typical applications might use Alphabet.EXTENDED.ASCII, \nwith R = 256 , or Alphabet.UNICODE, with R = 65536 . For large alphabets, MSD \nstring sort is so simple as to be dangerous\u2014improperly used, it can consume outra-\ngeous amounts of time and space. Before considering performance characteristics in \ndetail, we shall discuss three important issues (all of which we have considered before, \nin Chapter 2) that must be addressed in any application.\nSmall subarrays. The basic idea behind MSD string sort is quite effective: in typi-\ncal applications, the strings will be in order after examining only a few characters in \nthe key. Put another way, the method quickly divides the array to be sorted into small \nTrace of MSD string sort: top level of sort(a, 0, 14, 0)\n0     0\n1  a  0\n2 ", "start": 725, "end": 725}, "1068": {"text": "Put another way, the method quickly divides the array to be sorted into small \nTrace of MSD string sort: top level of sort(a, 0, 14, 0)\n0     0\n1  a  0\n2  b  1\n3  c  2\n4  d  2\n5  e  2\n6  f  2\n7  g  2\n8  h  2\n9  i  2\n10 j  2\n11 k  2\n12 l  2\n13 m  2 \n14 n  2\n15 o  2\n16 p  2\n17 q  2\n18 r  2\n19 s  2\n20 t 12\n21 u 14\n22 v 14 \n23 w 14\n24 x 14\n25 y 14\n26 z 14\n27   14\n0     0\n1  a  0\n2  b  1\n3  c  1\n4  d  0\n5  e  0\n6  f  0\n7  g  0\n8  h  0\n9  i  0\n10 j  0\n11 k  0\n12 l  0\n13 m  0 \n14 n  0\n15 o  0\n16 p  0\n17 q  0\n18 r  0\n19 s  0\n20 t 10\n21 u  2\n22 v  0 \n23 w  0\n24 x  0\n25 y  0\n26 z  0\n27    0\n0  0  0\n1  a  1\n2  b  2\n3 ", "start": 725, "end": 725}, "1069": {"text": "0\n24 x  0\n25 y  0\n26 z  0\n27    0\n0  0  0\n1  a  1\n2  b  2\n3  c  2\n4  d  2\n5  e  2\n6  f  2\n7  g  2\n8  h  2\n9  i  2\n10 j  2\n11 k  2\n12 l  2\n13 m  2 \n14 n  2\n15 o  2\n16 p  2\n17 q  2\n18 r  2\n19 s 12\n20 t 14\n21 u 14\n22 v 14 \n23 w 14\n24 x 14\n25 y 14\n26 z 14\n27   14\nsort(a, 0, 0, 1);\nsort(a, 1, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, ", "start": 725, "end": 725}, "1070": {"text": "1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 1, 1);\nsort(a, 2, 11, 1);\nsort(a, 12, 13, 1);\nsort(a, 14, 13, 1);\nsort(a, 14, 13, 1);\nsort(a, 14, 13, 1);\nsort(a, 14, 13, 1);\nsort(a, 14, 13, 1);\nsort(a, 14, 13, 1);\nsort(a, 14, 13, 1);\nsort(a, 14, 13, 1);\nshe\nsells\nseashells\nby\nthe\nsea\nshore\nthe\nshells\nshe\nsells\nare\nsurely\nseashells\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\nare\nby\nshe\nsells\nseashells\nsea\nshore\nshells\nshe\nsells\nsurely\nseashells\nthe\nthe\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshe\nshells\nshore\nsurely\nthe\nthe\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\ncount\nfrequencies\ntransform ", "start": 725, "end": 725}, "1071": {"text": "9\n10\n11\n12\n13\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshe\nshells\nshore\nsurely\nthe\nthe\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\ncount\nfrequencies\ntransform counts\nto indices\ndistribute \nand copy back\nindices at completion\nof distribute phase\nrecursively sort subarraysuse key-indexed counting on first character\nstart of s subarray\n1 + end of s subarray\n7135.1 \u25a0 String Sorts\n  \nsubarrays. But this is a double-edged sword: we are certain to have to handle huge num-\nbers of tiny subarrays, so we had better be sure that we handle them ef\ufb01ciently. Small \nsubarrays are of critical importance in the performance of MSD string sort. We have seen \nthis situation for other recursive sorts (quicksort and mergesort), but it is much more \ndramatic for MSD string sort. For example, suppose that you are sorting millions of \nASCII strings (R = 256) that are all different, with no cutoff for small subarrays. Each \nstring eventually \ufb01nds its way to its own subarray, so you will sort millions of subarrays \nof size 1. But each such sort involves initializing the 258 entries of the count[] array \nto 0 and transforming them all to indices. This cost is likely to dominate the rest of the \nsort. With Unicode (R = 65536) the sort might be thousands of times slower. Indeed, \nmany unsuspecting sort clients have seen their running times explode from minutes to \nshe\nsells\nseashells\nby\nthe\nsea\nshore\nthe\nshells\nshe\nsells\nare\nsurely\nseashells\nare\nby\nshe\nsells\nseashells\nsea\nshore\nshells\nshe\nsells\nsurely\nseashells\nthe\nthe\nare\nby\nsells\nseashells\nsea\nsells\nseashells\nshe\nshore\nshells\nshe\nsurely\nthe\nthe\ninput\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshe\nshells\nshore\nsurely\nthe\nthe\noutput\nare\nby\nseashells\nsea\nseashells\nsells\nsells\nshe\nshore\nshells\nshe\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshore\nshells\nshe\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshore\nshells\nshe\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshore\nshells\nshe\nsurely\nthe\nthe\nare\nby\nseas\nseashells\nseashells\nsells\nsells\nshe\nshells\nshore\nshe\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshells\nshore\nshe\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshells\nshe\nshore\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshells\nshe\nshore\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshells\nshe\nshore\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshells\nshe\nshore\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshe\nshells\nshore\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshe\nshells\nshore\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshe\nshells\nshore\nsurely\nthe\nthe\nTrace ", "start": 725, "end": 726}, "1072": {"text": "of times slower. Indeed, \nmany unsuspecting sort clients have seen their running times explode from minutes to \nshe\nsells\nseashells\nby\nthe\nsea\nshore\nthe\nshells\nshe\nsells\nare\nsurely\nseashells\nare\nby\nshe\nsells\nseashells\nsea\nshore\nshells\nshe\nsells\nsurely\nseashells\nthe\nthe\nare\nby\nsells\nseashells\nsea\nsells\nseashells\nshe\nshore\nshells\nshe\nsurely\nthe\nthe\ninput\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshe\nshells\nshore\nsurely\nthe\nthe\noutput\nare\nby\nseashells\nsea\nseashells\nsells\nsells\nshe\nshore\nshells\nshe\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshore\nshells\nshe\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshore\nshells\nshe\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshore\nshells\nshe\nsurely\nthe\nthe\nare\nby\nseas\nseashells\nseashells\nsells\nsells\nshe\nshells\nshore\nshe\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshells\nshore\nshe\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshells\nshe\nshore\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshells\nshe\nshore\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshells\nshe\nshore\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshells\nshe\nshore\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshe\nshells\nshore\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshe\nshells\nshore\nsurely\nthe\nthe\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshe\nshells\nshore\nsurely\nthe\nthe\nTrace of ", "start": 726, "end": 726}, "1073": {"text": "of recursive calls for MSD string sort (no cutoff for small subarrays, subarrays of size 0 and 1 omitted)\nend of string\ngoes before any\nchar value\nneed to examine\nevery character\nin equal keys\nd\nlo\nhi\n714 CHAPTER 5 \u25a0 Strings\n  \nhours on switching from ASCII to Unicode, for precisely this reason. Accordingly, the \nswitch to insertion sort for small subarrays is a must for MSD string sort. T o avoid the \ncost of reexamining characters that we know to be equal, we use the version of insertion \nsort given at the top of the page, which takes an extra argument d and assumes that the \n\ufb01rst d characters of all the strings to be sorted are known to be equal.  The ef\ufb01ciency of \nthis code depends on substring() being a constant-time \noperation. As with quicksort and mergesort, most of the \nbene\ufb01t of this improvement is achieved with a small value \nof the cutoff, but the savings here are much more dramat-\nic. The diagram at right shows the results of experiments \nwhere using a cutoff to insertion sort for subarrays of size \n10 or less decreases the running time by a factor of 10 for \na typical application.\n E q u a l  k e y s .  A second pitfall for MSD string sort is that \nit can be relatively slow for subarrays containing large \nnumbers of equal keys. If a substring occurs suf\ufb01ciently \noften that the cutoff for small subarrays does not ap-\nply, then a recursive call is needed for every character \nin all of the equal keys. Moreover, key-indexed count-\ning is an inef\ufb01cient way to determine that the charac-\nters are all equal: not only does each character need to \nbe examined and each string moved, but all the counts \nhave to be initialized, converted ", "start": 726, "end": 727}, "1074": {"text": "is an inef\ufb01cient way to determine that the charac-\nters are all equal: not only does each character need to \nbe examined and each string moved, but all the counts \nhave to be initialized, converted to indices, and so forth. \nThus, the worst case for MSD string sorting is when all \nkeys are equal. The same problem arises when large num-\nbers of keys have long common pre\ufb01xes, a situation often \nfound in applications.\npublic static void sort(String[] a, int lo, int hi, int d) \n{  // Sort from a[lo] to a[hi], starting at the dth character.\n   for (int i = lo; i <= hi; i++)\n      for (int j = i; j > lo && less(a[j], a[j-1], d); j--)\n         exch(a, j, j-1); \n}\nprivate static boolean less(String v, String w, int d) \n{  return v.substring(d).compareTo(w.substring(d)) < 0;  }\n I n s e r t i o n  s o r t  f o r  s t r i n g s  w h o s e  f i r s t  d characters are equal\nEffect of cutoff  for small subarrays\nin MSD string sort\n50%\n100%\n25%\n0\n10%\n10 50\nN = 100,000\nN random CA plates\n100 trials per point\nrunning time as a percentage of time with no cutoff\ncutoff value\n7155.1 \u25a0 String Sorts\n Extra space. To  d o  t h e  p a r t i t i o n i n g , M S D  u s e s  t wo  a u x i l i a r y  a r r ay s : t h e  te m p o r a r y ", "start": 727, "end": 728}, "1075": {"text": "o n i n g , M S D  u s e s  t wo  a u x i l i a r y  a r r ay s : t h e  te m p o r a r y  \narray for distributing keys ( aux[]) and the array that holds the counts that are trans-\nformed into partition indices (count[]).  The aux[] array is of size N and can be cre-\nated outside the recursive sort() method. This extra space can be eliminated by sac-\nri\ufb01cing stability (see Exercise 5.1.17), but it is often not a major concern in practical \napplications of MSD string sort. Space for the count[] array, on the other hand, can be \nan important issue (because it cannot be created outside the recursive sort() method) \nas addressed in Proposition D below.\n  R a n d o m  s t r i n g  m o d e l .  To  s t u d y  t h e  p e r f o r m a n ce  o f  M S D  s t r i n g  s o r t , w e  u s e  a  random \nstring model , where each string consists of (independently) random characters, with \nno bound on their length. Long equal keys are \nessentially ignored, because they are extremely \nunlikely. The behavior of MSD string sort in \nthis model is similar to its behavior in a model \nwhere we consider random \ufb01xed-length keys \nand also to its performance for typical real \ndata; in all three, MSD string sort tends to ex-\namine just a few characters at the beginning of \neach key, as we will see.\nPerformance. The running time of MSD \nstring sort depends on the data. For compare-\nbased methods, we were primarily concerned \nwith the order of the keys; for MSD string sort, \nthe ", "start": 728, "end": 728}, "1076": {"text": "key, as we will see.\nPerformance. The running time of MSD \nstring sort depends on the data. For compare-\nbased methods, we were primarily concerned \nwith the order of the keys; for MSD string sort, \nthe order of the keys is immaterial, but we are \nconcerned with the values of the keys.\n\u25a0 For random inputs, MSD string sort \nexamines just enough characters to distinguish among the keys, and the running \ntime is  sublinear in the number of characters in the data (it examines a small \nfraction of the input characters). \n\u25a0 For nonrandom inputs, MSD string sort still could be sublinear but might need \nto examine more characters than in the random case, depending on the data. In \nparticular, it has to examine all the characters in equla keys, so the running time \nis nearly linear in the number of characters in the data when signi\ufb01cant num-\nbers of equal keys are present.\n\u25a0 In the worst case, MSD string sort examines all the characters in the keys, so the \nrunning time is linear in the number of characters in the data (like LSD string \nsort). A worst-case input is one with all strings equal.\n1EIO402\n1HYL490\n1ROZ572\n2HXE734\n2IYE230\n2XOR846\n3CDB573\n3CVP720\n3IGJ319\n3KNA382\n3TAV879\n4CQP781\n4QGI284\n4YHV229\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\nnonrandom\nwith duplicates\n(nearly linear)\nrandom\n(sublinear)\nworst ", "start": 728, "end": 728}, "1077": {"text": "equal.\n1EIO402\n1HYL490\n1ROZ572\n2HXE734\n2IYE230\n2XOR846\n3CDB573\n3CVP720\n3IGJ319\n3KNA382\n3TAV879\n4CQP781\n4QGI284\n4YHV229\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\n1DNB377\nnonrandom\nwith duplicates\n(nearly linear)\nrandom\n(sublinear)\nworst case\n(linear)\nCharacters examined by MSD string sort\nare\nby\nsea\nseashells\nseashells\nsells\nsells\nshe\nshe\nshells\nshore\nsurely\nthe\nthe\n716 CHAPTER 5 \u25a0 Strings\n Some applications involve distinct keys that are well-modeled by the random string \nmodel; others have signi\ufb01cant numbers of equal keys or long common pre\ufb01xes, so the \nsort time is closer to the worst case. Our license-plate-processing application, for ex-\nample, can fall anywhere between these extremes: if our engineer takes an hour of data \nfrom a busy interstate, there will not be many duplicates and the random model will \napply; for a week\u2019s worth of data on a local road, there will be numerous duplicates and \nperformance will be closer to the worst case. \nProposition C. To  s o r t  N random strings from an R-character alphabet,  MSD \nstring sort examines about N log R N characters, on average.\nProof sketch: We expect the subar rays to be all about the same size, so the recur -\nrence CN = RCN/R ", "start": 728, "end": 729}, "1078": {"text": "an R-character alphabet,  MSD \nstring sort examines about N log R N characters, on average.\nProof sketch: We expect the subar rays to be all about the same size, so the recur -\nrence CN = RCN/R + N approximately describes the performance, which leads to the \nstated result, generalizing our argument for quicksort in Chapter 2. Again, this \ndescription of the situation is not entirely accurate, because N/R  is not necessarily \nan integer, and the subarrays are the same size only on the average (and because \nthe number of characters in real keys is \ufb01nite).  These effects turn out to be less \nsigni\ufb01cant for MSD string sort than for standard quicksort, so the leading term of \nthe running time is the solution to this recurrence. The detailed analysis that proves \nthis fact is a classical example in the analysis of algorithms, \ufb01rst done by Knuth in \nthe early 1970s. \n \n \nAs food for thought and to indicate why the proof is beyond the scope of this book, \nnote that key length does not play a role. Indeed, the random-string model allows key \nlength to approach in\ufb01nity. There is a nonzero probability that two keys will match for \nany speci\ufb01ed number of characters, but this probability is so small as to not play a role \nin our performance estimates. \nAs we have discussed, the number of characters examined is not the full story for \nMSD string sort. We also have to take into account the time and space required to count \nfrequencies and turn the counts into indices. \nProposition D.  MSD string sort uses between 8N /H11001 3R and ~7wN /H11001 3WR array ac-\ncesses to sort N strings taken from an R-character alphabet, where w is the average \nstring length.\nProof: Immediate ", "start": 729, "end": 729}, "1079": {"text": "/H11001 3R and ~7wN /H11001 3WR array ac-\ncesses to sort N strings taken from an R-character alphabet, where w is the average \nstring length.\nProof: Immediate from the code, Proposition A, and Proposition B. In the best \ncase MSD sort uses just one pass; in the worst case, it performs like LSD string sort. \n7175.1 \u25a0 String Sorts\n When N is small, the factor of R dominates. Though precise analysis of the total cost \nbecomes dif\ufb01cult and complicated, you can estimate the effect of this cost just by con -\nsidering small subarrays when keys are distinct. With no cutoff for small subarrays, \neach key appears in its own subarray, so NR array accesses are needed for just these \nsubarrays. If we cut off to small subarrays of size M, we have about N/M subarrays of \nsize M, so we are trading off NR/M array accesses with NM/4 compares, which tells us \nthat we should choose M to be proportional to the square root of R.\n \nProposition D (continued). To  s o r t  N strings taken from an R-character alphabet, \nthe amount of space needed by  MSD string sort is proportional to R times the \nlength of the longest string (plus N ), in the worst case.\nProof: The count[] array must be created within sort(), so the total amount of \nspace needed is proportional to R times the depth of recursion (plus N for the aux-\niliary array). Precisely, the depth of the recursion is the length of the longest string \nthat is a pre\ufb01x of two or more of the strings to be sorted. \nAs just discussed, equal keys cause the depth of the recursion to be proportional to the \nlength of the keys. The immediate practical lesson to be drawn from Proposition D \nis that it is ", "start": 729, "end": 730}, "1080": {"text": "of the strings to be sorted. \nAs just discussed, equal keys cause the depth of the recursion to be proportional to the \nlength of the keys. The immediate practical lesson to be drawn from Proposition D \nis that it is quite possible for MSD string sort to run out of time or space when sorting \nlong strings taken from large alphabets, particularly if long equal keys are to be ex-\npected. For example, with Alphabet.UNICODE and more than M equal 1,000-character \nstrings, MSD.sort() would require space for over 65 million counters!\nThe main challenge in getting maximum ef\ufb01ciency from MSD string sort on keys \nthat are long strings is to deal with lack of randomness in the data. Typically, keys may \nhave long stretches of equal data, or parts of them might fall in only a narrow range.   \nFor example, an information-processing application for student data might have keys \nthat include graduation year (4 bytes, but one of four different values), state names \n(perhaps 10 bytes, but one of 50 different values), and gender (1 byte with one of two \ngiven values), as well as a person\u2019s name (more similar to random strings, but probably \nnot short, with nonuniform letter distributions, and with trailing blanks in a \ufb01xed-\nlength \ufb01eld). Restrictions like these lead to large numbers of empty subarrays during \nthe MSD string sort. Next, we consider a graceful way to adapt to such situations.\n718 CHAPTER 5 \u25a0 Strings\n  \n   T h r e e - w a y  s t r i n g  q u i c k s o r t  We can also \nadapt quicksort to MSD string sorting by using \n3-way partitioning on the leading character of the \nkeys, moving to the next character on only the mid-\ndle subarray (keys with leading character equal to \nthe partitioning character). This method is not dif\ufb01-\ncult ", "start": 730, "end": 731}, "1081": {"text": "\n3-way partitioning on the leading character of the \nkeys, moving to the next character on only the mid-\ndle subarray (keys with leading character equal to \nthe partitioning character). This method is not dif\ufb01-\ncult to implement, as you can see in Algorithm 5.3: \nwe just add an argument to the recursive method \nin Algorithm 2.5 that keeps track of the current \ncharacter, adapt the 3-way partitioning code to use \nthat character, and appropriately modify the recur -\nsive calls.\nAlthough it does the computation in a different \norder, 3-way string quicksort amounts to sorting the \narray on the leading characters of the keys (using \nquicksort), then applying the method recursively on \nthe remainder of the keys. For sorting strings, the \nmethod compares favorably with normal quicksort \nand with MSD string sort.  Indeed, it is a hybrid of \nthese two algorithms.\nThree-way string quicksort divides the array into \nonly three parts, so it involves more data movement \nthan MSD string sort when the number of nonempty partitions is large because it has to \ndo a series of 3-way partitions \nto get the effect of the multiway \npartition.  On the other hand, \nMSD string sort can create \nlarge numbers of (empty) sub-\narrays, whereas 3-way string \nquicksort always has just three. \nThus, 3-way string quicksort \nadapts well to handling equal \nkeys, keys with long common \npre\ufb01xes,  keys that fall into a \nsmall range, and small arrays\u2014\nall situations where MSD \nstring sort runs slowly.  Of par-\nticular importance is that the \nOverview of 3-way string quicksort\n<v\n<v\n<v\n.\n.\n.\n<v\n<v\n<v\n<v\n<v\n.\n.\n.\n<v\n<v\nv\nv\nv\n.\n.\n.\nv\nv\nuse ", "start": 731, "end": 731}, "1082": {"text": "par-\nticular importance is that the \nOverview of 3-way string quicksort\n<v\n<v\n<v\n.\n.\n.\n<v\n<v\n<v\n<v\n<v\n.\n.\n.\n<v\n<v\nv\nv\nv\n.\n.\n.\nv\nv\nuse first character value\nto partition into \u201cless, \u201d \u201cequal, \u201d\nand \u201cgreater\u201d subarrays\nrecursively sort subarrays\n(excluding first character\nfor \u201cequal\u2019 subarray)\nv\nv\nv\n.\n.\n.\nv\nv\n>v\n>v\n>v\n.\n.\n.\n>v\n>v\n>v\n>v\n>v\n.\n.\n.\n>v\n>v\nTypical 3-way string quicksort candidate\nduplicate\nkeys\nedu.princeton.cs\ncom.apple\nedu.princeton.cs\ncom.cnn\ncom.google\nedu.uva.cs\nedu.princeton.cs\nedu.princeton.cs.www\nedu.uva.cs\nedu.uva.cs\nedu.uva.cs\ncom.adobe\nedu.princeton.ee\ncom.adobe\ncom.apple\ncom.cnn\ncom.google\nedu.princeton.cs\nedu.princeton.cs\nedu.princeton.cs\nedu.princeton.cs.www\nedu.princeton.ee\nedu.uva.cs\nedu.uva.cs\nedu.uva.cs\nedu.uva.cs \nlong\nprefix\nmatch\ninput sorted result\n7195.1 \u25a0 String Sorts\n ALGORITHM 5.3   Three-way string quicksort\npublic class  Quick3string \n{\n   private static int charAt(String s, int d)\n   {  if (d < s.length()) return s.charAt(d); else return -1;  }\n   public static void sort(String[] a)\n   {  sort(a, 0, a.length - 1, 0);  }\n   private static void sort(String[] a, int lo, int hi, int d)\n   { \n      if ", "start": 731, "end": 732}, "1083": {"text": "sort(String[] a)\n   {  sort(a, 0, a.length - 1, 0);  }\n   private static void sort(String[] a, int lo, int hi, int d)\n   { \n      if (hi <= lo) return;\n      int lt = lo, gt = hi;\n      int v = charAt(a[lo], d);\n      int i = lo + 1;\n      while (i <= gt)\n      {\n         int t = charAt(a[i], d);\n         if      (t < v) exch(a, lt++, i++);\n         else if (t > v) exch(a, i, gt--);\n         else            i++;\n      }\n      // a[lo..lt-1] < v = a[lt..gt] < a[gt+1..hi]\n      sort(a, lo, lt-1, d);\n      if (v >= 0) sort(a, lt, gt, d+1);\n      sort(a, gt+1, hi, d);\n   }\n}\n \n \nTo  s o r t  a n  a r r ay  a[] of strings, we 3-way partition them on their \ufb01rst character, then (recursively) sort \nthe three resulting subarrays: the strings whose \ufb01rst character is less than the partitioning character, \nthe strings whose \ufb01rst character is equal to the partitioning character (excluding their \ufb01rst character \nin the sort), and the strings whose \ufb01rst character is greater than the partitioning character.\n720 CHAPTER 5 \u25a0 Strings  \n \npartitioning adapts to different kinds of structure in different parts of the key. Also, like \nquicksort, 3-way string quicksort does not use extra space (other than the implicit stack \nto support recursion), which is an important advantage over MSD string sort, which \nrequires space for both frequency counts and an ", "start": 732, "end": 733}, "1084": {"text": "\nquicksort, 3-way string quicksort does not use extra space (other than the implicit stack \nto support recursion), which is an important advantage over MSD string sort, which \nrequires space for both frequency counts and an auxiliary array.  \nThe \ufb01gure at the bottom of this page shows all of the recursive calls that  Quick3string\nmakes for our example. Each subarray is sorted using precisely three recursive calls, ex-\ncept when we skip the recursive call on reaching the ends of the (equal) string(s) in the \nmiddle subarray. \nAs usual, in practice, it is worthwhile to consider various standard improvements to \nthe implementation in Algorithm 5.3:\nSmall subarrays. In any recursive algorithm, we can gain ef\ufb01ciency by treating small \nsubarrays differently. In this case, we use the insertion sort from page 715, which skips the \ncharacters that are known to be equal. The improvement due to this change is likely to \nbe signi\ufb01cant, though not nearly as important as for MSD string sort.\nRestricted alphabet. To  h a n d l e  s p e c i a l i z e d  a l p h a b e t s , w e  co u l d  a d d  a n  Alphabet\nargument alpha to each of the methods and replace s.charAt(d) with \nalpha.toIndex(s.charAt(d)) in charAt(). In this case, there is no bene\ufb01t to doing \nso, and adding this code is likely to substantially slow the algorithm down because this \ncode is in the inner loop.\nTrace of recursive calls for 3-way string quicksort (no cutoff for small subarrays)\nshe\nsells\nseashells\nby\nthe\nsea\nshore\nthe\nshells\nshe\nsells\nare\nsurely\nseashells\nby\nare\nseashells\nshe\nseashells\nsea\nshore\nsurely\nshells\nshe\nsells\nsells\nthe\nthe\nare\nby\nseashells\nsells\nseashells\nsea\nsells\nshells\nshe\nsurely\nshore\nshe\nthe\nthe\nare\nby\nseashells\nsea\nseashells\nsells\nsells\nshe\nshells\nshe\nshore\nsurely\nthe\nthe\nshe\nshe\nshells\nshore\n ", "start": 733, "end": 733}, "1085": {"text": "small subarrays)\nshe\nsells\nseashells\nby\nthe\nsea\nshore\nthe\nshells\nshe\nsells\nare\nsurely\nseashells\nby\nare\nseashells\nshe\nseashells\nsea\nshore\nsurely\nshells\nshe\nsells\nsells\nthe\nthe\nare\nby\nseashells\nsells\nseashells\nsea\nsells\nshells\nshe\nsurely\nshore\nshe\nthe\nthe\nare\nby\nseashells\nsea\nseashells\nsells\nsells\nshe\nshells\nshe\nshore\nsurely\nthe\nthe\nshe\nshe\nshells\nshore\n  \nthe\nthe\nsea\nseashells\nseashells\nsells\nsells\nsea\nseashells\nseashells\nsells\nsells\nseashells\nseashells \nsells\nsells\nseashells\nseashells\nsells\nsells\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\ntwo more passes\nto reach end\ngray bars represent\nempty subarrays\nno recursive calls\n(end of string)\nshells \nthe\nthe\n7215.1 \u25a0 String Sorts\n  \n R a n d o m i z a t i o n .  As with any quicksort, it is generally worthwhile to shuf\ufb02e the array \nbeforehand or to use a random paritioning item by swapping the \ufb01rst item with a ran-\ndom one. The primary reason to do so is to protect against worst-case performance in \nthe case that the array is already sorted or nearly sorted.\nFor string keys, standard quicksort and all the other sorts in ", "start": 733, "end": 734}, "1086": {"text": "a ran-\ndom one. The primary reason to do so is to protect against worst-case performance in \nthe case that the array is already sorted or nearly sorted.\nFor string keys, standard quicksort and all the other sorts in Chapter 2 are actually \nMSD string sorts, because the compareTo() method in String accesses the charac-\nters in left-to-right order. That is, compareTo() accesses only the leading characters \nif they are different, the leading two characters if the \ufb01rst characters are the same and \nthe second different, and so forth. For example, if the \ufb01rst characters of the strings are \nall different, the standard sorts will examine just those characters, thus automatically \nrealizing some of the same performance gain that we seek in MSD string sorting. The \nessential idea behind 3-way quicksort is to take special action when the leading char -\nacters are equal.  Indeed, one way to think of Algorithm 5.3 is as a way for standard \nquicksort to keep track of  leading characters that are known to be equal. In the small \nsubarrays, where most of the compares in the sort are done, the strings are likely to have \nnumerous equal leading characters. The standard algorithm has to scan over all those \ncharacters for each compare; the 3-way algorithm avoids doing so.\nPerformance. Consider a case where the string keys are long (and are all the same \nlength, for simplicity), but most of the leading characters are equal. In such a situa-\ntion, the running time of standard quicksort is proportional to the string length times\n2N ln N, whereas the running time of 3-way string quicksort is proportional to N times \nthe string length (to discover all the leading equal characters) plus 2N ln N character \ncomparisons (to do the sort on the remaining short keys). That is, 3-way string quick-\nsort requires up ", "start": 734, "end": 734}, "1087": {"text": "\nthe string length (to discover all the leading equal characters) plus 2N ln N character \ncomparisons (to do the sort on the remaining short keys). That is, 3-way string quick-\nsort requires up to a factor of 2 ln N fewer character compares than normal quicksort. \nIt is not unusual for keys in practical sorting applications to have characteristics similar \nto this arti\ufb01cial example.\n722 CHAPTER 5 \u25a0 Strings\n Proposition E.  To  s o r t  a n  a r r ay  o f  N random strings,  3-way string quicksort uses \n~ 2N ln N character compares, on the average.\nProof: There are two instructive ways to understand this result. First, consider -\ning the method to be equivalent to quicksort partitioning on the leading char -\nacter, then (recursively) using the same method on the subarrays, we should not \nbe surprised that the total number of operations is about the same as for normal \nquicksort\u2014but they are single-character compares, not full-key compares.  Second, \nconsidering the method as replacing key-indexed counting by quicksort, we expect \nthat the N log R N running time from Proposition D should be multiplied by a fac-\ntor of 2 ln R because it takes quicksort 2R ln R steps to sort R characters, as opposed \nto R steps for the same characters in the MSD string sort. We omit the full proof.\n \n \nAs emphasized on page 716, considering random strings is instructive, but more detailed \nanalysis is needed to predict performance for practical situations. Researchers have \nstudied this algorithm in depth and have proved that no algorithm can beat 3-way \nstring quicksort (measured by number of character compares) by more than a constant \nfactor, under very general assumptions. T o appreciate its versatility, note that 3-way \nstring quicksort ", "start": 734, "end": 735}, "1088": {"text": "algorithm can beat 3-way \nstring quicksort (measured by number of character compares) by more than a constant \nfactor, under very general assumptions. T o appreciate its versatility, note that 3-way \nstring quicksort has no direct dependencies on the size of the alphabet. \nExample: web logs. As an example where 3-way string quicksort shines, we can con-\nsider a typical modern data-processing task. Suppose that you have built a website and \nwant to analyze the traf\ufb01c that it generates. Y ou can have your system administrator \nsupply you with a web log of all transactions on your site. Among the information asso-\nciated with a transaction is the domain name of the originating machine. For example, \nthe \ufb01le week.log.txt on the booksite is a log of one week\u2019s transactions on our book-\nsite. Why does 3-way string quicksort do well on such a \ufb01le? Because the sorted result is \nreplete with long common pre\ufb01xes that this method does not have to reexamine.\n7235.1 \u25a0 String Sorts\n Which string-sorting algorithm should I use? Naturally, we are interested \nin how the string-sorting methods that we have considered compare to the general-\npurpose methods that we considered in Chapter 2. The following table   summarizes \nthe important characteristics of the string-sort algorithms that we have discussed in \nthis section (the rows for quicksort, mergesort, and 3-way quicksort are included from \nChapter 2, for comparison).\nalgorithm stable? inplace?\norder of growth of\ntypical number calls to charAt() \n to sort N strings\nfrom an R-character alphabet\n(average length w, max length W)\nsweet spot\nrunning time extra space\ninsertion sort\nfor strings yes yes between\nN and N 2 1 small arrays,\narrays in order\nquicksort no yes N ", "start": 735, "end": 736}, "1089": {"text": "alphabet\n(average length w, max length W)\nsweet spot\nrunning time extra space\ninsertion sort\nfor strings yes yes between\nN and N 2 1 small arrays,\narrays in order\nquicksort no yes N log 2 N log N general-purpose\nwhen space is tight\nmergesort yes no N log 2 N N general-purpose\nstable sort\n3-way quicksort no yes between\nN and N log N log N large numbers of\nequal keys\nLSD string sort yes no NW N short fixed-length \nstrings\nMSD string sort yes no between\nN and Nw N + WR random strings\n3-way string \nquicksort no yes between\nN and Nw  W + log N\ngeneral-purpose,\nstrings with long \nprefix matches\nPerformance characteristics of string-sorting algorithms\nAs in Chapter 2, multiplying these growth rates by appropriate algorithm- and data-\ndependent constants gives an effective way to predict running time.\nAs explored in the examples that we have already considered and in many other \nexamples in the exercises, different speci\ufb01c situations call for different methods, with \nappropriate parameter settings. In the hands of an expert (maybe that\u2019s you, by now), \ndramatic savings can be realized for certain situations.\n724 CHAPTER 5 \u25a0 Strings\n Q&A\nQ. Does the Java system sort use one of these methods for String sorts?\nA. No, but the standard implementation includes a fast string compare that makes \nstandard sorts competitive with the methods considered here. \nQ. So, I should just use the system sort for String keys?\nA. Probably yes in Java, though if you have huge numbers of strings or need an excep-\ntionally fast sort, you may wish to switch to char arrays instead of String values and \nuse a radix sort.\nQ. What is explanation of the log2 N factors on the table in the previous page?\nA. ", "start": 736, "end": 737}, "1090": {"text": "excep-\ntionally fast sort, you may wish to switch to char arrays instead of String values and \nuse a radix sort.\nQ. What is explanation of the log2 N factors on the table in the previous page?\nA. They re\ufb02ect the idea that most of the comparisons for these algorithms wind up \nbeing between keys with a common pre\ufb01x of length log  N. Recent research has estab-\nlished this fact for random strings with careful mathematical analysis (see booksite for \nreference).\n7255.1 \u25a0 String Sorts\n EXERCISES\n \n5.1.1 Develop a sort implementation that counts the number of different key values, \nthen uses a symbol table to apply key-indexed counting to sort the array. (This method \nis not for use when the number of different key values is large.)\n5.1.2 Give a trace for LSD string sort for the keys\n no is th ti fo al go pe to co to th ai of th pa\n5.1.3 Give a trace for MSD string sort for the keys\n no is th ti fo al go pe to co to th ai of th pa\n5.1.4 Give a trace for 3-way string quicksort for the keys\n no is th ti fo al go pe to co to th ai of th pa\n5.1.5 Give a trace for MSD string sort for the keys\n now is the time for all good people to come to the aid of\n5.1.6 Give a trace for 3-way string quicksort for the keys\n now is the time for all good people to come to the aid of\n5.1.7 Develop an implementation of key-indexed counting that makes use of an array \nof Queue objects.\n5.1.8 Give the number of characters examined by MSD string sort and 3-way string \nquicksort for a \ufb01le of N keys a, ", "start": 737, "end": 738}, "1091": {"text": "key-indexed counting that makes use of an array \nof Queue objects.\n5.1.8 Give the number of characters examined by MSD string sort and 3-way string \nquicksort for a \ufb01le of N keys a, aa, aaa, aaaa, aaaaa, . . . \n5.1.9 Develop an implementation of LSD string sort that works for variable-length \nstrings.\n5.1.10 What is the total number of characters examined by 3-way string quicksort \nwhen sorting N \ufb01xed-length strings (all of length W), in the worst case?\n726 CHAPTER 5 \u25a0 Strings\n CREATIVE PROBLEMS\n \n5.1.11  Queue sort. Implement MSD string sorting using queues, as follows: Keep one \nqueue for each bin. On a \ufb01rst pass through the items to be sorted, insert each item into \nthe appropriate queue, according to its leading character value. Then, sort the sublists \nand stitch together all the queues to make a sorted whole. Note that this method does \nnot involve keeping the count[] arrays within the recursive method.\n5.1.12    Alphabet. Develop an implementation of the Alphabet API that is given on \npage 698 and use it to develop LSD and MSD sorts for general alphabets.\n5.1.13  Hybrid sort. Investigate the idea of using standard MSD string sort for large ar-\nrays, in order to get the advantage of multiway partitioning, and 3-way string quicksort \nfor smaller arrays, in order to avoid the negative effects of large numbers of empty bins.\n5.1.14  Array sort. Develop a method that uses 3-way string quicksort for keys that are \narrays of int values.\n5.1.15  Sublinear sort. Develop a sort implementation for int values that makes two \npasses through the array to do an LSD sort on the leading 16 bits of the keys, ", "start": 738, "end": 739}, "1092": {"text": "are \narrays of int values.\n5.1.15  Sublinear sort. Develop a sort implementation for int values that makes two \npasses through the array to do an LSD sort on the leading 16 bits of the keys, then does \nan insertion sort.\n5.1.16  Linked-list sort. Develop a sort implementation that takes a linked list of nodes \nwith String key values as argument and rearranges the nodes so that they appear in \nsorted order (returning a link to the node with the smallest key). Use 3-way string \nquicksort.\n5.1.17    In-place key-indexed counting. Develop a version of key-indexed counting that \nuses only a constant amount of extra space. Prove that your version is stable or provide \na counterexample.\n7275.1 \u25a0 String Sorts\n 728 CHAPTER 5 \u25a0 Strings\nEXPERIMENTS\n  \n \n \n5.1.18  Random decimal keys. Write a static method randomDecimalKeys that takes \nint values N and W as arguments and returns an array of N string values that are each \nW-digit decimal numbers.\n5.1.19  Random CA license plates. Write a static method randomPlatesCA that takes \nan int value N as argument and returns an array of N String values that represent CA \nlicense plates as in the examples in this section.\n5.1.20  Random \ufb01xed-length words. Write a static method randomFixedLengthWords\nthat takes int values N and W as arguments and returns an array of N string values that \nare each strings of W characters from the alphabet.\n5.1.21  Random items. Write a static method randomItems that takes an int value N as \nargument and returns an array of N string values that are each strings of length between \n15 and 30 made up of three \ufb01elds: a 4-character \ufb01eld with one of a set of 10 ", "start": 739, "end": 740}, "1093": {"text": "as \nargument and returns an array of N string values that are each strings of length between \n15 and 30 made up of three \ufb01elds: a 4-character \ufb01eld with one of a set of 10 \ufb01xed strings; \na 10-char \ufb01eld with one of a set of 50 \ufb01xed strings; a 1-character \ufb01eld with one of two \ngiven values; and a 15-byte \ufb01eld with random left-justi\ufb01ed strings of letters equally \nlikely to be 4 through 15 characters long. \n5.1.22  Timings. Compare the running times of MSD string sort and 3-way string \nquicksort, using various key generators. For \ufb01xed-length keys, include LSD string sort.\n5.1.23  Array accesses. Compare the number of array accesses used by MSD string sort \nand 3-way string sort, using various key generators. For \ufb01xed-length keys, include LSD \nstring sort.\n5.1.24  Rightmost character accessed. Compare the position of the rightmost character \naccessed for MSD string sort and 3-way string quicksort, using various key generators.\n This page intentionally left blank \n 5.2       TRIES\nAs with sorting, we can take advantage of properties of strings to develop search meth-\nods (symbol-table implementations) that can be more ef\ufb01cient than the general-pur -\npose methods of Chapter 3 for typical applications where search keys are strings.\nSpeci\ufb01cally, the methods that we consider in this section achieve the following per -\nformance characteristics in typical applications, even for huge tables:\n\u25a0 Search hits take time proportional to the length of the search key.\n\u25a0 Search misses involve examining only a few characters.\nOn re\ufb02ection, these performance characteristics are quite remarkable, one of the \ncrowning achievements of algorithmic technology and a ", "start": 740, "end": 742}, "1094": {"text": "proportional to the length of the search key.\n\u25a0 Search misses involve examining only a few characters.\nOn re\ufb02ection, these performance characteristics are quite remarkable, one of the \ncrowning achievements of algorithmic technology and a primary factor in enabling \nthe development of the computational infrastructure we now enjoy that has made so \nmuch information instantly accessible. Moreover, we can extend the symbol-table API \nto include character-basedoperations de\ufb01ned for string keys (but not necessarily for \nall Comparable types of keys) that are powerful and quite useful in practice, as in the \nfollowing API: \n    public class  StringST< Value>\nStringST() create a symbol table\nvoid put(String key, Value val) put key-value pair into the table\n(remove key if value is null)\nValue get(String key) value paired with key\n(null if key is absent)\nvoid delete(String key) remove key (and its value)\nboolean contains(String key) is there a value paired with key?\nboolean isEmpty() is the table empty?\nString longestPrefixOf(String s) the longest key that is a prefix of s\n Iterable<String> keysWithPrefix(String s) all the keys having s as a prefix\nIterable<String> keysThatMatch(String s) all the keys that match s\n(where . matches any character)\nint size() number of key-value pairs\nIterable<String> keys() all the keys in the table\n A P I  f o r  a  s y m b o l  t a b l e  w i t h  s t r i n g  k e y s\n730\n This API differs from the symbol-table API introduced in Chapter\u00a03 in the follow-\ning aspects:\n\u25a0 We replace the gener ic t y pe Key with the concrete type String.\n\u25a0 \n \n \nWe add three new methods,     longestPrefixOf(), keysWithPrefix() and \nkeysThatMatch().\nWe retain the basic conventions of  our ", "start": 742, "end": 743}, "1095": {"text": "We replace the gener ic t y pe Key with the concrete type String.\n\u25a0 \n \n \nWe add three new methods,     longestPrefixOf(), keysWithPrefix() and \nkeysThatMatch().\nWe retain the basic conventions of  our sy mbol-table implementations in Chapter 3 \n(no duplicate or null keys and no null values). \nAs we saw for sorting with string keys, it is often quite important to be able to work \nwith strings from a speci\ufb01ed alphabet. Simple and ef\ufb01cient implementations that are \nthe method of choice for small alphabets turn out to be useless for large alphabets be-\ncause they consume too much space. In such cases, it is certainly worthwhile to add a \nconstructor that allows clients to specify the alphabet. We will consider the implemen-\ntation of such a constructor later in this section but omit it from the API for now, in \norder to concentrate on string keys.\nThe following descriptions of the three new methods use the keys she sells sea \nshells by the sea shore to give examples:\n\u25a0 longestPrefixOf() takes a string as argument and returns the longest \nkey in the symbol table that is a pre\ufb01x of that string. For the keys above, \nlongestPrefixOf(\"shell\") is she and longestPrefixOf(\"shellsort\") is \nshells. \n\u25a0 keysWithPrefix() takes a string as argument and returns all the keys \nin the symbol table having that string as pre\ufb01x. For the keys above, \nkeysWithPrefix(\"she\") is she and shells, and keysWithPrefix(\"se\") is \nsells and sea. \n\u25a0 keysThatMatch() takes a string as argument and returns all the keys in the \nsymbol table that match that string, in the sense that a period (.) in the argu-\nment string matches any character. For the keys above, keysThatMatch(\".he\")\nreturns she and ", "start": 743, "end": 743}, "1096": {"text": "all the keys in the \nsymbol table that match that string, in the sense that a period (.) in the argu-\nment string matches any character. For the keys above, keysThatMatch(\".he\")\nreturns she and the, and keysThatMatch(\"s..\") returns she and sea. \nWe w ill consider in detail implementations and applications of  these operations after \nwe have seen the basic symbol-table methods. These particular operations are repre -\nsentative of what is possible with string keys; we discuss several other possibilities in \nthe exercises.\nTo  f o c u s  o n  t h e  m a i n  i d e a s , w e  co n ce n t r a te  o n  put(), get(), and the new methods; \nwe assume (as in Chapter 3) default implementations of contains() and isEmpty(); \nand we leave implementations of size() and delete() for exercises. Since strings are \nComparable, extending the API to also include the ordered operations de\ufb01ned in the \nordered symbol-table API in Chapter 3 is possible (and worthwhile); we leave those \nimplementations (which are generally straightforward) to exercises and booksite code. \n7315.2 \u25a0 Tries\n  \n  T r i e s  In this section, we consider a search tree known as a trie, a data structure built \nfrom the characters of the string keys that allows us to use the characters of the search \nkey to guide the search. The name \u201ctrie\u201d is a bit of wordplay introduced by E. Fredkin in \n1960 because the data structure is used for retrieval, but we pronounce it \u201ctry\u201d to avoid \nconfusion with \u201ctree.\u201d We begin with a high-level description of the basic properties \nof tries, including search and insert algorithms, and then proceed to the details of the \nrepresentation and Java implementation.\nBasic properties. As with search trees, tries ", "start": 743, "end": 744}, "1097": {"text": "\u201ctree.\u201d We begin with a high-level description of the basic properties \nof tries, including search and insert algorithms, and then proceed to the details of the \nrepresentation and Java implementation.\nBasic properties. As with search trees, tries are data structures composed of nodes that \ncontain links that are either null or references to other nodes. Each node is pointed to \nby just one other node, which is called its parent (except for one node, the root, which \nhas no nodes pointing to it), and each node has R links, where R is the alphabet size. \nOften, tries have a substantial number of null links, so when we draw a trie, we typi-\ncally omit null links. Although links point to nodes, we can view each link as point-\ning to a trie, the trie whose root is the referenced node. Each link corresponds to a \ncharacter value\u2014since each link points \nto exactly one node, we label each node \nwith the character value corresponding \nto the link that points to it (except for \nthe root, which has no link pointing to \nit). Each node also has a corresponding \nvalue, which may be null or the value as-\nsociated with one of the string keys in \nthe symbol table. Speci\ufb01cally, we store \nthe value associated with each key in the \nnode corresponding to its last character. \nIt is very important to bear in mind the \nfollowing fact: nodes with null values ex-\nist to facilitate search in the trie and do \nnot correspond to keys.  An example of a \ntrie is shown at right.\nSearch in a trie. Finding the value as-\nsociated with a given string key in a trie is a simple process, guided by the characters \nin the search key. Each node in the trie has a link corresponding to each possible string \ncharacter. We start at the root, then follow the link associated with the \ufb01rst ", "start": 744, "end": 744}, "1098": {"text": "is a simple process, guided by the characters \nin the search key. Each node in the trie has a link corresponding to each possible string \ncharacter. We start at the root, then follow the link associated with the \ufb01rst character in \nthe key; from that node we follow the link associated with the second character in the \nkey; from that node we follow the link associated with the third character in the key and \nAnatomy of a trie\nt\nh\ne 5\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\nb\ny 4\na 2\nvalue for she in node\ncorresponding to\nlast key character\nlabel each node with\ncharacter associated\nwith incoming link\nlink to trie for all keys\nthat start with s\nlink to trie for all keys\nthat start with she\nroot\nkey value\n4by\n2sea\nsells 1\n0she\nshells 3\n5the\n732 CHAPTER 5 \u25a0 Strings\n  \nso forth, until reaching the last character of the key or a null link. At this point, one of \nthe following three conditions holds (refer to the \ufb01gure above for examples): \n\u25a0 The value at the node corresponding to the last character in the key is not null\n(as in the searches for shells and she depicted at left above). This result is \na  search hit\u2014the value associated with the key is the value in the node corre-\nsponding to its last character.\n\u25a0 The value in the node corresponding to the last character in the key is null (as \nin the search for shell depicted at top right above). This result is a  search miss:\nthe key is not in the table.\n\u25a0 The search terminated with a null link (as in the search for shore depicted at \nbottom right above). This result is also a search miss. ", "start": 744, "end": 745}, "1099": {"text": "above). This result is a  search miss:\nthe key is not in the table.\n\u25a0 The search terminated with a null link (as in the search for shore depicted at \nbottom right above). This result is also a search miss. \nIn all cases, the search is accomplished just by examining nodes along a path from the \nroot to another node in the trie.\nTrie search examples\nt\nh\ne 5\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\nb\ny 4\na 2\nhits misses\nget(\"shells\")\nt\nh\ne 5\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\nb\ny 4\na 2\nget(\"she\")\nt\nh\ne 5\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\nb\ny 4\na 2\nget(\"shell\")\nt\nh\ne 5\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\nb\ny 4\na 2\nno link for the o,\nso return null\nget(\"shore\")\nreturn the value in the\nnode corresponding to\nthe last key character\nsearch may terminate\nat an internal node\nthe value in the node \ncorresponding to the last key\ncharacter is null, so return null\n7335.2 \u25a0 Tries\n  I n s e r t i o n  i n t o  a  t r i e .  As with binary search trees, we insert by \ufb01rst doing a search: in \na trie that means using the characters of the key to guide us down the trie until ", "start": 745, "end": 746}, "1100": {"text": "t o  a  t r i e .  As with binary search trees, we insert by \ufb01rst doing a search: in \na trie that means using the characters of the key to guide us down the trie until reach-\ning the last character of the key or a null link. At this point, one of the following two \nconditions holds: \n\u25a0 We encountered a null link before reaching the last character of  the key. In this \ncase, there is no trie node corresponding to the last character in the key, so we \nneed to create nodes for each of the characters in the key not yet encountered \nand set the value in the last one to the value to be associated with the key.\n\u25a0 We encountered the last character of  the key before reaching a null link. In this \ncase, we set that node\u2019s value to the value to be associated with the key (whether \nor not that value is null), as usual with our associative array convention.\nIn all cases, we examine or create a node in the trie for each key character. The construc-\ntion of the trie for our standard indexing client from Chapter 3 with the input\nshe sells sea shells by the sea shore\nis shown on the facing page. \n  N o d e  r e p r e s e n t a t i o n .  As mentioned at the outset, our trie diagrams do not quite cor-\nrespond to the data structures our programs will build, because we do not draw null \nlinks. Taking null links into account emphasizes the following important characteristics \nof tries:\n\u25a0 Every node has R links, one for each possible character.\n\u25a0 Characters and keys are implicitly stored in the data structure.\nFor example, the \ufb01gure below depicts a trie for keys made up of lowercase letters, with \neach node having a value and 26 links. The \ufb01rst link points to a subtrie for keys begin-\nning with a, ", "start": 746, "end": 746}, "1101": {"text": "the \ufb01gure below depicts a trie for keys made up of lowercase letters, with \neach node having a value and 26 links. The \ufb01rst link points to a subtrie for keys begin-\nning with a, the second points to a subtrie for substrings beginning with b, and so forth. \nTrie representation (R = 26)\neach node has\nan array of links\nand a value\ncharacters are implicitly\ndefined by link indexs\nh\ne 0\ne\nl\nl\ns 1\na\ns\nh\ne\ne\nl\nl\ns\na\n2 0\n2\n1\n734 CHAPTER 5 \u25a0 Strings\n s\nh\ne\nTrie construction trace for standard indexing client\nkey value key value\nroot\none node\nfor each\nkey character\nvalue is in node\ncorresponding to\nlast character\nkey is sequence\nof characters from\nroot to value\n0\nt\nh\ne 5\ns\nh\ne 0\ne\nl\nl\ns 1\ns\nh\ne 0\ne\nl\nl\ns 1\na 2\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3 \no\nr\ne 7\na 2\n0she\n1sells\n2sea\n3shells\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\nb\ny 4\na 2\n4by\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\nb\ny 4\na 2\nt\nh\ne 5\ns\nh\ne 0\ne\nl\nl\ns ", "start": 746, "end": 747}, "1102": {"text": "0\ne\nl\nl\ns 1\nl\nl\ns 3\nb\ny 4\na 2\nt\nh\ne 5\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\nb\ny 4\na 6\n5the\n6sea\nt\nh\ne 5\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\nb\ny 4\na 6\n7shore\nnode corresponding to\nthe last key character\nexists, so reset its value\nnodes corresponding to\ncharacters at the end of the\nkey do not exist, so create them\nand set the value of the last one\n7355.2 \u25a0 Tries\n Keys in the trie are implicitly represented by paths from the root that end at nodes with \nnon-null values. For example, the string sea is associated with the value 2 in the trie \nbecause the 19th link in the root (which points to the trie for all keys that start with s) \nis not null and the 5th link in the node that link refers to (which points to the trie for \nall keys that start with se) is not null, and the \ufb01rst link in the node that link refers to \n(which points to the trie for all keys that starts with sea) has the value 2. Neither the \nstring sea nor the characters s, e, and a are stored in the data structure. Indeed, the data \nstructure contains no characters or strings, just links and values. Since the parameter R\nplays such a critical role, we refer to a trie for an R-character alphabet as an R-way trie.\nWith these preparations, the symbol-table implementation TrieST on the facing \npage is straightforward. It uses recursive methods like ", "start": 747, "end": 748}, "1103": {"text": "R\nplays such a critical role, we refer to a trie for an R-character alphabet as an R-way trie.\nWith these preparations, the symbol-table implementation TrieST on the facing \npage is straightforward. It uses recursive methods like those that we used for search \ntrees in Chapter 3, based on a private Node class with instance variable val for client \nvalues and an array next[] of Node references. The methods are compact recursive \nimplementations that are worthy of careful study. Next, we discuss implementations of \nthe constructor that takes an Alphabet as argument and the methods size(), keys(), \nlongestPrefixOf(), keysWithPrefix(), keysThatMatch(), and delete(). These \nare also easily understood recursive methods, each slightly more complicated than the \nlast.\nSize. As for the binary search trees of Chapter 3, three straightforward options are \navailable for implementing size():\n\u25a0 An eager implementation where we maintain the number of keys in an instance \nvariable N.\n\u25a0 A very eager implementation where we maintain the number of keys in a subtrie \nas a node instance variable that we update after the recursive calls in put() and \ndelete(). \n\u25a0 A lazy recursive implementation like \nthe one at right. It traverses all of the \nnodes in the trie, counting the number \nhaving a non-null value.\nAs with binary search trees, the lazy im -\nplementation is instructive but should be \navoided because it can lead to performance \nproblems for clients. The eager implementa-\ntions are explored in the exercises.\npublic int size() \n{  return size(root);  }\nprivate int size(Node x) \n{  \n   if (x == null) return 0;\n   int cnt = 0;\n   if (x.val != null) cnt++;\n   for (char c = 0; c < R; c++)\n      cnt += size(next[c]);\n   return cnt; \n}\nLazy recursive size() for tries\n736 CHAPTER 5 ", "start": 748, "end": 748}, "1104": {"text": "(x.val != null) cnt++;\n   for (char c = 0; c < R; c++)\n      cnt += size(next[c]);\n   return cnt; \n}\nLazy recursive size() for tries\n736 CHAPTER 5 \u25a0 Strings\n ALGORITHM 5.4   Trie symbol table\npublic class  TrieST<Value> \n{\n   private static int R = 256; // radix\n   private Node root;          // root of trie\n   private static class Node\n   {\n      private Object val;\n      private Node[] next = new Node[R];\n   }\n   public Value get(String key)\n   {\n      Node x = get(root, key, 0);\n      if (x == null) return null;\n      return (Value) x.val;\n   }\n   private Node get(Node x, String key, int d)\n   {  // Return value associated with key in the subtrie rooted at x.\n      if (x == null) return null;\n      if (d == key.length()) return x;\n      char c = key.charAt(d); // Use dth key char to identify subtrie.\n      return get(x.next[c], key, d+1);\n   }\n   public void put(String key, Value val)\n   {  root = put(root, key, val, 0);  }\n   private Node put(Node x, String key, Value val, int d)\n   {  // Change value associated with key if in subtrie rooted at x.\n      if (x == null) x = new Node();\n      if (d == key.length()) {  x.val = val; return x; }\n      char c = key.charAt(d); // Use dth key char to identify subtrie.\n      x.next[c] = put(x.next[c], key, val, d+1);\n      return x;\n   } \n}\nThis code uses an R-way trie to implement a symbol table. Additional methods in the string symbol-\ntable ", "start": 748, "end": 749}, "1105": {"text": "x.next[c] = put(x.next[c], key, val, d+1);\n      return x;\n   } \n}\nThis code uses an R-way trie to implement a symbol table. Additional methods in the string symbol-\ntable API of page 730 are presented in the next several pages. Modifying this code to handle keys from \nspecialized alphabets is straighforward (see page 740). The value in Node has to be an Object because \nJava does not support arrays of generics; we cast values back to Value in get().\n7375.2 \u25a0 Tries  \n \n C o l l e c t i n g  k e y s .  Because characters and keys are represented implicitly in tries, provid-\ning clients with the ability to iterate through the keys presents a challenge. As with bina-\nry search trees, we accumulate the string keys in a Queue, but for tries we need to create \nexplicit representations of all of the string keys, not just \ufb01nd them in the data structure. \nWe do so w ith a recursive pr ivate method collect() that is similar to size() but also \nmaintains a string with the sequence of characters on the path from the root. Each time \nthat we visit a node via a call to collect() with that node as \ufb01rst argument, the second \nargument is the string associated \nwith that node (the sequence of \ncharacters on the path from the \nroot to the node). To visit a node, \nwe add its associated string to the \nqueue if its value is not null, then \nvisit (recursively) all the nodes \nin its array of links, one for each \npossible character. T o create the \nkey for each call, we append the \ncharacter corresponding to the \nlink to the current key. We use \nthis collect() method to col -\nlect keys for both the keys() and \nthe keysWithPrefix() methods \nin ", "start": 749, "end": 750}, "1106": {"text": "\nkey for each call, we append the \ncharacter corresponding to the \nlink to the current key. We use \nthis collect() method to col -\nlect keys for both the keys() and \nthe keysWithPrefix() methods \nin the API. T o implement keys()\nwe call keysWithPrefix() with \nthe empty string as argument; to implement \nkeysWithPrefix(), we call get() to \ufb01nd the \ntrie node corresponding to the given pre\ufb01x \n(null if there is no such node), then use the \ncollect() method to complete the job. The \ndiagram at left shows a trace of collect()\n(or keysWithPrefix(\"\")) for an example \ntrie, giving the value of the second argument \nkey and the contents of the queue for each \ncall to collect(). The diagram at the top \nof the facing page illustrates the process for \nkeysWithPrefix(\"sh\"). \npublic Iterable<String> keys() \n{  return keysWithPrefix(\"\");  }\npublic Iterable<String> keysWithPrefix(String pre) \n{\n   Queue<String> q = new Queue<String>();\n   collect(get(root, pre, 0), pre, q);\n   return q; \n}\nprivate void collect(Node x, String pre,\n                             Queue<String> q) \n{\n   if (x == null) return;\n   if (x.val != null) q.enqueue(pre);\n   for (char c = 0; c < R; c++)\n      collect(x.next[c], pre + c, q); \n}\nCollecting the keys in a trie\no\nr\ne 7\nt\nh\ne 5\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\nb\ny 4\na 6\nb\nby\ns\nse\nsea\nsel\nsell\nsells\nsh\nshe\nshell\nshells\nsho\nshor\nshore\nt\nth\nthe\nby\nby ", "start": 750, "end": 750}, "1107": {"text": "1\nl\nl\ns 3\nb\ny 4\na 6\nb\nby\ns\nse\nsea\nsel\nsell\nsells\nsh\nshe\nshell\nshells\nsho\nshor\nshore\nt\nth\nthe\nby\nby sea\nby sea sells\nby sea sells she\nby sea sells she shells\nby sea sells she shells shore\nby sea sells she shells shore the\nCollecting the keys in a trie (trace)\nkey q\nkeysWithPrefix(\"\");\n738 CHAPTER 5 \u25a0 Strings\n   \n W i l d c a r d  m a t c h .  To  i m p l e m e n t  keysThatMatch(), we use a similar process, but add \nan argument specifying the pattern to collect() and add a test to make a recursive call \nfor all links when the pattern character is a wildcard or only for the link corresponding \nto the pattern character otherwise, as in the code below. Note also that we do not need \nto consider keys longer than the pattern.\n L o n g e s t  p r e \ufb01 x .  To  \ufb01 n d  t h e  l o n g e s t  ke y  t h a t  i s  a  p re \ufb01 x  o f  a  g ive n  s t r i n g , w e  u s e  a  re c u r-\nsive method like get() that keeps track of the length of the longest key found on the \nsearch path (by passing it as a parameter to the recursive method, updating the value \no\nr\ne 7\nt\nh\ne 5\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\nb\ny 4\na 6\nfind subtrie ", "start": 750, "end": 751}, "1108": {"text": "7\nt\nh\ne 5\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\nb\ny 4\na 6\nfind subtrie for all\nkeys beginning with \"sh\"\no\nr\ne 7\nt\nh\ne 5\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\nb\ny 4\na 6\ncollect keys\nin that subtrie \nkeysWithPrefix(\"sh\");\nPrefix match in a trie\nsh\nshe\nshel\nshell\nshells\nsho\nshor\nshore\nshe\nshe shells\nshe shells shore\nkey q\npublic Iterable<String> keysThatMatch(String pat) \n{\n   Queue<String> q = new Queue<String>();\n   collect(root, \"\", pat, q);\n   return q; \n}\npublic void collect(Node x, String pre, String pat, Queue<String> q) \n{\n   int d = pre.length();\n   if (x == null) return;\n   if (d == pat.length() && x.val != null) q.enqueue(pre);\n   if (d == pat.length()) return;\n   char next = pat.charAt(d);\n   for (char c = 0; c < R; c++)\n      if (next == '.' || next == c)\n         collect(x.next[c], pre + c, pat, q);\n }\nWildcard match in a trie\n7395.2 \u25a0 Tries\n whenever a node with a non-null value is encountered). The \nsearch ends when the end of the string or a null link is en-\ncountered, whichever comes \ufb01rst.\n D e l e t i o n .  The \ufb01rst step needed to delete a key-value pair \nfrom a trie is to use a normal search to \ufb01nd the ", "start": 751, "end": 752}, "1109": {"text": "en-\ncountered, whichever comes \ufb01rst.\n D e l e t i o n .  The \ufb01rst step needed to delete a key-value pair \nfrom a trie is to use a normal search to \ufb01nd the node cor -\nresponding to the key and set the corresponding value to \nnull. If that node has a non-null link to a child, then no \nmore work is required; if all the links are null, we need to \nremove the node from the data structure. If doing so leaves \nall the links null in its parent, we need to remove that node, \nand so forth. The implementation on the facing page dem-\nonstrates that this action can be accomplished with remark-\nably little code, using our standard recursive setup: after the \nrecursive calls for a node x, we return null if the client value \nand all of the links in a node are null; otherwise we return \nx.\npublic String longestPrefixOf(String s) \n{\n   int length = search(root, s, 0, 0);\n   return s.substring(0, length); \n}\nprivate int search(Node x, String s, int d, int length) \n{\n   if (x == null) return length;\n   if (x.val != null) length = d;\n   if (d == s.length()) return length;\n   char c = s.charAt(d);\n   return search(x.next[c], s, d+1, length); \n}\nMatching the longest prefix of a given string\nPossibilities for longestPrefixOf()\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\na 2\n\"she\"\n\"shell\"\nsearch ends at\nend of string\nvalue is not null\n return she\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\na ", "start": 752, "end": 752}, "1110": {"text": "2\n\"she\"\n\"shell\"\nsearch ends at\nend of string\nvalue is not null\n return she\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\na 2 search ends at \nend of string\nvalue is null\nreturn she\n(last key on path)\n\"shellsort\"\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\na 2\nsearch ends at\n null link\nreturn shells\n(last key on path)\nsearch ends at\n null link\nreturn she\n(last key on path)\n\"shelters\"\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\na 2\n740 CHAPTER 5 \u25a0 Strings\n  A l p h a b e t .  As usual, Algorithm 5.4 \nis coded for Java String keys, but it is \na simple matter to modify the imple-\nmentation to handle keys taken from \nany alphabet, as follows:\n\u25a0 Implement a constructor that \ntakes an Alphabet as argument, \nwhich sets an Alphabet in-\nstance variable to that argument \nvalue and the instance variable \nR to the number of characters \nin the alphabet.\n\u25a0 Use the toIndex() method \nfrom Alphabet in get() and \nput() to convert string char-\nacters to indices between 0 and \nR/H110021. \n\u25a0 Use the toChar() method from \nAlphabet to convert indices between 0 and R/H110021 to char values. This operation \nis not needed in get() and put() but is important in the implementations of \nkeys(), keysWithPrefix(), and keysThatMatch().\nWith these changes, you can save a considerable amount of space (use only R links per \nnode) when you know that your keys are taken from a small ", "start": 752, "end": 753}, "1111": {"text": "implementations of \nkeys(), keysWithPrefix(), and keysThatMatch().\nWith these changes, you can save a considerable amount of space (use only R links per \nnode) when you know that your keys are taken from a small alphabet, at the cost of the \ntime required to do the conversions between characters and indices. \nDeleting a key (and its associated value) from a trie\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\ns 3\na 2\ndelete(\"shells\");\ns\nh\ne 0\ne\nl\nl\ns 1\nl\nl\na 2\nnull value and links,\nso remove node\n(return null link)\ns\nh\ne 0\ne\nl\nl\ns 1\nl\na 2\ns\nh\ne 0\ne\nl\nl\ns 1\na 2\nnon-null value\nso do not remove node\n(return link to node)\nnon-null link\nso do not remove node\n(return link to node)\ns\nh\ne 0\ne\nl\nl\ns 1\na 2\ns\nh\ne 0\ne\nl\nl\ns 1\na 2\ns\nh\ne 0\ne\nl\nl\ns 1\na 2\nset value\nto null\npublic void delete(String key) \n{  root = delete(root, key, 0);  }\nprivate Node delete(Node x, String key, int d) \n{\n   if (x == null) return null;\n   if (d == key.length())\n      x.val = null;\n   else\n   { \n      char c = key.charAt(d);\n      x.next[c] = delete(x.next[c], key, d+1);\n   }\n   if (x.val ", "start": 753, "end": 753}, "1112": {"text": "key.length())\n      x.val = null;\n   else\n   { \n      char c = key.charAt(d);\n      x.next[c] = delete(x.next[c], key, d+1);\n   }\n   if (x.val != null) return x;\n   for (char c = 0; c < R; c++)\n      if (x.next[c] != null) return x;\n   return null; \n}\nDeleting a key (and its associated value) from a trie\n7415.2 \u25a0 Tries\n The code that we have considered is a compact and complete implementation \nof the string symbol-table API that has broadly useful practical applications. Several \nvariations and extensions are discussed in the exercises. Next, we consider basic proper-\nties of tries, and some limitations on their utility.\n P r o p e r t i e s  o f  t r i e s  As usual, we are interested in knowing the amount of time and \nspace required to use tries in typical applications. Tries have been extensively studied \nand analyzed, and their basic properties are relatively easy to understand and to apply. \nProposition F. The linked structure (shape) of a  trie is independent of the key in-\nsertion/deletion order: there is a unique trie for any given set of keys.\nProof: Immediate, by induction on the subtries.\n \nThis fundamental fact is a distinctive feature of tries: for all of the other search tree \nstructures that we have considered so far, the tree that we construct depends both on \nthe set of keys and on the order in which we insert those keys.\nWorst-case t ime bound for search and inser t. How long does it take to \ufb01nd the value \nassociated with a key? For BSTs, hashing, and other methods in Chapter 4, we needed \nmathematical analysis to study this question, but for tries it is very easy to answer:\nProposition ", "start": 753, "end": 754}, "1113": {"text": "the value \nassociated with a key? For BSTs, hashing, and other methods in Chapter 4, we needed \nmathematical analysis to study this question, but for tries it is very easy to answer:\nProposition G.  The number of array accesses when searching in a trie or inserting \na key into a trie is at most 1 plus the length of the key. \nProof: Immediate from the code. The recursive get() and put() implementations \ncarry an argument d that starts at 0, increments for each call, and is used to stop the \nrecursion when it reaches the key length.\nFrom a theoretical standpoint, the implication of Proposition G is that tries are opti-\nmal for search hit\u2014we could not expect to do better than search time proportional to \nthe length of the search key. Whatever algorithm or data structure we are using, we can-\nnot know that we have found a key that we seek without examining all of its characters. \nFrom a practical standpoint this guarantee is important because it does not depend on \nthe number of keys : when we are working with 7-character keys like license plate num-\nbers, we know that we need to examine at most 8 nodes to search or insert; when we are \nworking with 20-digit account numbers, we only need to examine at most 21 nodes to \nsearch or insert.\n742 CHAPTER 5 \u25a0 Strings\n   \nExpected time bound for search miss. Suppose that we are searching for a key in a \ntrie and \ufb01nd that the link in the root node that corresponds to its \ufb01rst character is null. \nIn this case, we know that the key is not in the table on the basis of examining just one\nnode. This case is typical: one of the most important properties of tries is that search \nmisses typically require examining just a few nodes. If we assume that the keys are \ndrawn from the random string model (each character is ", "start": 754, "end": 755}, "1114": {"text": "This case is typical: one of the most important properties of tries is that search \nmisses typically require examining just a few nodes. If we assume that the keys are \ndrawn from the random string model (each character is equally likely to have any one \nof the R different character values) we can prove this fact:\nProposition H.  The  average number of nodes examined for search miss in a trie \nbuilt from N random keys over an alphabet of size R is ~log R N . \nProof sketch (for readers who are familiar with probabilistic analysis): The prob-\nability that each of the N keys in a random trie differs from a random search key in \nat least one of the leading t characters is (1 /H11002 R/H11002t )N. Subtracting this quantity from \n1 gives the probability that one of the keys in the trie matches the search key in all \nof the leading t characters. In other words, 1 /H11002 (1 /H11002 R/H11002t )N is the probability that \nthe search requires more than t character compares. From probabilistic analysis, \nthe sum for t = 0, 1, 2, . . . of the probabilities that an integer random variable is >t\nis the average value of that random variable, so the average search cost is \n1 /H11002 (1 /H11002 R/H110021)N /H11001 1 /H11002 (1 /H11002 R/H110022)N /H11001. . ./H11001 1 /H11002 (1 /H11002 R/H11002t )N /H11001. . .   \nUsing the elementary approximation (1/H110021/x)x ~ e/H110021, we \ufb01nd the search cost to be \napproximately\n(1 /H11002 e /H11002N/R1) ", "start": 755, "end": 755}, "1115": {"text": ". .   \nUsing the elementary approximation (1/H110021/x)x ~ e/H110021, we \ufb01nd the search cost to be \napproximately\n(1 /H11002 e /H11002N/R1) /H11001 (1 /H11002 e /H11002N/R2) /H11001. . ./H11001  (1 /H11002 e /H11002N/Rt) /H11001. . .  \nThe summand is extremely close to 1 for approximately ln R N terms with  Rt sub-\nstantially smaller than N; it is extremely close to 0 for all the terms with Rt substan-\ntially greater than N; and it is somewhere between 0 and 1 for the few terms with   \nRt /H33360 N. So the grand total is about log R N.  \nFrom a practical standpoint, the most important implication of this proposition is that \nsearch miss does not depend on the key length.  For example, it says that unsuccessful \nsearch in a trie built with 1 million random keys will require examining only three or \nfour nodes, whether the keys are 7-digit license plates or 20-digit account numbers. \nWhile it is unreasonable to expect truly random keys in practical applications, it is rea-\nsonable to hypothesize that the behavior of trie algorithms for keys in typical applica-\n7435.2 \u25a0 Tries\n tions is described by this model. Indeed, this sort of behavior is widely seen in practice \nand is an important reason for the widespread use of tries. \n  S p a c e .  How much space is needed for a trie? Addressing this question (and under -\nstanding how much space is available) is critical to using tries effectively.\n \nProposition I.  The number of links in a  trie is between RN and RNw, where w is \nthe average key ", "start": 755, "end": 756}, "1116": {"text": "(and under -\nstanding how much space is available) is critical to using tries effectively.\n \nProposition I.  The number of links in a  trie is between RN and RNw, where w is \nthe average key length.\nProof: Every key in the trie has a node containing its associated value that also has \nR links, so the number of links is at least RN. If the \ufb01rst characters of all the keys are \ndifferent, then there is a node with R links for every key character, so the number of \nlinks is R times the total number of key characters, or RNw. \nThe table on the facing page shows the costs for some typical applications that we have \nconsidered. It illustrates the following rules of thumb for tries:\n\u25a0 When keys are short, the number of links is close to RN.\n\u25a0 When keys are long, the number of links \nis close to RNw.\n\u25a0 Therefore, decreasing R can save a huge \namount of space. \nA more subtle message of this table is that it is \nimportant to understand the properties of the \nkeys to be inserted before deploying tries in an \napplication. \n  O n e - w a y  b r a n c h i n g .  The primary reason \nthat trie space is excessive for long keys is that \nlong keys tend to have long tails in the trie, \nwith each node having a single link to the next \nnode (and, therefore, R/H110021 null links). This \nsituation is not dif\ufb01cult to correct (see Exer-\ncise 5.2.11). A trie might also have internal \none-way branching. For example, two long \nkeys may be equal except for their last charac-\nter. This situation is a bit more dif\ufb01cult to ad -\ndress (see Exercise 5.2.12). These changes can \nmake trie space usage a less important factor ", "start": 756, "end": 756}, "1117": {"text": "equal except for their last charac-\nter. This situation is a bit more dif\ufb01cult to ad -\ndress (see Exercise 5.2.12). These changes can \nmake trie space usage a less important factor \n1\n1 2\n2\nput(\"shells\", 1);\nput(\"shellfish\", 2);\nRemoving one-way branching in a trie\nh\ne\nl\nf\ni\ns\nh\nl\ns\ns s\nshell\nfish\ninternal\none-way\nbranching\nexternal\none-way\nbranching\nstandard\ntrie\nno one-way\nbranching\n744 CHAPTER 5 \u25a0 Strings\n  \nthan for the straightforward implementation that we have considered, but they are not \nnecessarily effective in practical applications. Next, we consider an alternative approach \nto reducing space usage for tries.\nThe bottom line is this: do not try to use Algorithm 5.4 for large numbers of long keys \ntaken from large alphabets, because it will require space proportional to R times the total \nnumber of key characters. Otherwise, if you can afford the space, trie performance is \ndif\ufb01cult to beat.\napplication typical key\naverage \nlength \nw\nalphabet \nsize \nR\nlinks in trie\nbuilt from 1 million keys\nCA license plates 4PGC938 7 256 256 million\naccount numbers 02400019992993299111 20\n256 4 billion\n10 256 million\nURLs www.cs.princeton.edu 28 256 4 billion\ntext processing seashells 11 256 256 million\nproteins in\ngenomic data ACTGACTG 8\n256 256 million\n4 4 million\nSpace requirements for typical tries\n7455.2 \u25a0 Tries\n  \n   T e r n a r y  s e a r c h  t r i e s  ( T ", "start": 756, "end": 758}, "1118": {"text": "256 million\n4 4 million\nSpace requirements for typical tries\n7455.2 \u25a0 Tries\n  \n   T e r n a r y  s e a r c h  t r i e s  ( T S T s )  To  h e l p  u s  \navoid the excessive space cost associated with \nR-way tries, we now consider an alternative \nrepresentation: the  ternary search trie  (TST). \nIn a TST, each node has a character, three links, \nand a value. The three links correspond to keys \nwhose current characters are less than, equal \nto, or greater than the node\u2019s character. In the \nR-way tries of Algorithm 5.4, trie nodes are \nrepresented by R links, with the character cor-\nresponding to each non-null link implictly \nrepresented by its index. In the corresponding \nTST, characters appear explicitly in nodes\u2014we \n\ufb01nd characters corresponding to keys only \nwhen we are traversing the middle links.\n  S e a r c h  a n d  i n s e r t .  The search and insert code \nfor implementing our symbol-table API with \nTSTs writes itself. T o search, we compare the \n\ufb01rst character in the key with the character at \nthe root. If it is less, we take the left link; if it is \ngreater, we take the right link; and if it is equal, \nwe take the middle link and move to the next \nsearch key character.  In each case, we apply \nthe algorithm \nrecursively.  We \nterminate with \na search miss if we encounter a null link or if the node \nwhere the search ends has a null value, and we terminate \nwith a search hit if the node where the search ends has a \nnon-null value.  T o insert a new key, we search, then add ", "start": 758, "end": 758}, "1119": {"text": "node \nwhere the search ends has a null value, and we terminate \nwith a search hit if the node where the search ends has a \nnon-null value.  T o insert a new key, we search, then add \nnew nodes for the characters in the tail of the key, just as \nwe did for tries. Algorithm 5.5 gives the details of the \nimplementation of these methods.\nUsing this arrangement is equivalent to implement -\ning each R-way trie node as a binary search tree that uses \nas keys the characters corresponding to non-null links. \nBy contrast, Algorithm 5.4 uses a key-indexed array. A \nTST representation of a trie\neach node has\nthree links\nlink to TST for all keys\nthat start with s\nlink to TST for all keys\nthat start with\na letter before s\nt\nh\ne 8\na\nr\ne 12\ns\nh u\ne 10\ne\nl\nl\ns 11\nl\nl\ns 15\nr 0\ne\nl\ny 13\no\n7\nr\ne\nb\ny 4\na 14\nt\nh\ne 8\na\nr\ne 12\ns\nh u\ne 10\ne\nl\nl\ns 11\nl\nl\ns 15\nr 0\ne\nl\ny 13\no\n7\nr\ne\nb\ny 4\na\n14\nTST search example\nreturn value\nassociated with\nlast key character\nmatch: take middle link,\nmove to next char\nmismatch: take left or right link,\n do not move to next char\nt\nh\ne 8\na\nr\ne 12\ns\nh u\ne 10\ne\nl\nl\ns 11\nl\nl\ns ", "start": 758, "end": 758}, "1120": {"text": "left or right link,\n do not move to next char\nt\nh\ne 8\na\nr\ne 12\ns\nh u\ne 10\ne\nl\nl\ns 11\nl\nl\ns 15\nr\ne\nl\ny 13\no\n7\nr\ne\nb\ny 4\na14\nget(\"sea\")\n746 CHAPTER 5 \u25a0 Strings\n ALGORITHM 5.5   TST symbol table\npublic class  TST<Value> \n{\n   private Node root;            // root of trie\n   private  class Node\n   {\n      char c;                    // character\n      Node left, mid, right;     // left, middle, and right subtries\n      Value val;                 // value associated with string\n   }\n   public Value get(String key)  // same as for tries (See page 737).\n   private Node get(Node x, String key, int d)\n   {\n      if (x == null) return null;\n      char c = key.charAt(d);\n      if      (c < x.c) return get(x.left,  key, d);\n      else if (c > x.c) return get(x.right, key, d);\n      else if (d < key.length() - 1)\n                        return get(x.mid,   key, d+1);\n      else return x;\n   }\n   public void put(String key, Value val)\n   {  root = put(root, key, val, 0);  }\n   private Node put(Node x, String key, Value val, int d)\n   {\n      char c = key.charAt(d);\n      if (x == null) { x = new Node(); x.c = c; }\n      if      (c < x.c) x.left  = put(x.left,  key, val, d);\n      else if (c > x.c) ", "start": 758, "end": 759}, "1121": {"text": "null) { x = new Node(); x.c = c; }\n      if      (c < x.c) x.left  = put(x.left,  key, val, d);\n      else if (c > x.c) x.right = put(x.right, key, val, d);\n      else if (d < key.length() - 1)\n                        x.mid   = put(x.mid,   key, val, d+1);\n      else x.val = val;\n      return x;\n   }\n}\nThis implementation uses a char value c and three links per node to build string search tries where \nsubtries have keys whose \ufb01rst character is less than c (left), equal to c (middle), and greater than c\n(right).\n7475.2 \u25a0 Tries TST and its corresponding trie are illustrated above. Continuing the correspondence \ndescribed in Chapter 3 between binary search trees and sorting algorithms, we see \nthat TSTs correspond to 3-way string quicksort in the same way that BSTs correspond \nto quicksort and tries correspond to MSD sorting.  The \ufb01gures on page 714 and 721, \nwhich show the recursive call structure for MSD and 3-way three-way string quicksort \n(respectively), correspond precisely to the trie and TST drawn on page 746 for that set \nof keys. Space for links in tries corresponds to the space for counters in string sorting; \n3-way branching provides an effective solution to both problems.\nTrie node representations\ns\ne h u\nlink for keys\nthat start with s\nlink for keys\nthat start with su\nh ue\nstandard array of links (R = 26) ternary search tree (TST)\ns\n748 CHAPTER 5 \u25a0 Strings\n Properties of TSTs A TST is a compact representation of an R-way trie, but the \ntwo data structures have remarkably different properties. Perhaps the most important \ndifference ", "start": 759, "end": 761}, "1122": {"text": "(TST)\ns\n748 CHAPTER 5 \u25a0 Strings\n Properties of TSTs A TST is a compact representation of an R-way trie, but the \ntwo data structures have remarkably different properties. Perhaps the most important \ndifference is that Property A does not hold for TSTs: the BST representations of each \ntrie node depend on the order of key insertion, as with any other BST.\nSpace. The most important property of TSTs is that they have just three links in each \nnode, so a TST requires far less space than the corresponding trie.\nProposition J. The number of links in a  TST built from N string keys of average \nlength w is between 3N and 3Nw.\nProof. Immediate, by the same argument as for Proposition I. \n \nActual space usage is generally less than the upper bound of three links per character, \nbecause keys with common pre\ufb01xes share nodes at high levels in the tree. \nSearch cost. To  d e te r m i n e  t h e  co s t  o f  s e a rc h  ( a n d  i n s e r t )  i n  a  T S T, w e  m u l t i p l y  t h e  co s t  \nfor the corresponding trie by the cost of traversing the BST representation of each trie \nnode.. \nProposition K.  A search miss in a TST built from N random string keys requires \n~ln N character compares, on the average. A search hit or an insertion in a TST uses \na character compare for each character in the search key. \nProof: The search hit/insertion cost is immediate from the code. The search miss \ncost is a consequence of the same arguments discussed in the proof sketch of Prop-\nosition H. We assume that all but a constant number of the nodes on the search \npath ", "start": 761, "end": 761}, "1123": {"text": "hit/insertion cost is immediate from the code. The search miss \ncost is a consequence of the same arguments discussed in the proof sketch of Prop-\nosition H. We assume that all but a constant number of the nodes on the search \npath (a few at the top) act as random BSTs on R character values with average path \nlength ln R, so we multiply the time cost logR N = ln N / ln R by ln R. \n \n \nIn the worst case, a node might be a full R-way node that is unbalanced, stretched out \nlike a singly linked list, so we would need to multiply by a factor of R. More typically, \nwe might expect to do ln R or fewer character compares at the \ufb01rst level (since the root \nnode behaves like a random BST on the R different character values) and perhaps at a \nfew other levels (if there are keys with a common pre\ufb01x and up to R different values on \nthe character following the pre\ufb01x), and to do only a few compares for most characters \n(since most trie nodes are sparsely populated with non-null links).  Search misses are \nlikely to involve only a few character compares, ending at a null link high in the trie, \n7495.2 \u25a0 Tries\n  \nand search hits involve only about one compare per search key character, since most of \nthem are in nodes with one-way branching at the bottom of the trie.\n A l p h a b e t .  The prime virtue of using TSTs is that they adapt gracefully to irregularities \nin search keys that are likely to appear in practical applications. In particular, note that \nthere is no reason to allow for strings to be built from a client-supplied alphabet, as was \ncrucial for tries. There are two main effects. First, keys in practical applications come \nfrom large alphabets, and usage of particular characters ", "start": 761, "end": 762}, "1124": {"text": "for strings to be built from a client-supplied alphabet, as was \ncrucial for tries. There are two main effects. First, keys in practical applications come \nfrom large alphabets, and usage of particular characters in the character sets is far from \nuniform. With TSTs, we can use a 256-character ASCII encoding or a 65,536-character \nUnicode encoding without having to worry about the excessive costs of nodes with 256- \nor 65,536-way branching, and without having to determine which sets of characters are \nrelevant.  Unicode strings in non-Roman alphabets can have thousands of characters\u2014\nTSTs are especially appropriate for standard Java String keys that consist of such char-\nacters. Second, keys in practical applications often have a structured format, differing \nfrom application to application, perhaps using only letters in one part of the key, only \ndigits in another part of the key. In our CA license plate example, the second, third, and \nfourth characters are uppercase letter ( R = 26) and the other characters are decimal \ndigits (R = 10). In a TST for such keys, some of the trie nodes will be represented as \n10-node BSTs (for places where all keys have digits) and others will be represented as \n26-node BSTs (for places where all keys have letters). This structure develops automati-\ncally, without any need for special analysis of the keys.\n   P r e \ufb01 x  m a t c h ,  c o l l e c t i n g  k e y s ,  a n d   w i l d c a r d  m a t c h .  Since a TST represents a \ntrie, implementations of longestPrefixOf(), keys(), keysWithPrefix(), and \nkeysThatMatch() are easily adapted from the corresponding code for tries in the pre-\nvious section, and a ", "start": 762, "end": 762}, "1125": {"text": "Since a TST represents a \ntrie, implementations of longestPrefixOf(), keys(), keysWithPrefix(), and \nkeysThatMatch() are easily adapted from the corresponding code for tries in the pre-\nvious section, and a worthwhile exercise for you to cement your understanding of both \ntries and TSTs (see Exercise 5.2.9). The same tradeoff as for search (linear memory \nusage but an extra ln R multiplicative factor per character compare) holds. \n D e l e t i o n .  The delete() method for TSTs requires more work. Essentially, each char-\nacter in the key to be deleted belongs to a BST. In a trie, we could remove the link cor-\nresponding to a character by setting the corresponding entry in the array of links to \nnull; in a TST, we have to use BST node deletion to remove the node corresponding to \nthe character.\nHybrid TSTs. An easy improvement to TST-based search is to use a large explicit mul-\ntiway node at the root. The simplest way to proceed is to keep a table of R TSTs: one \nfor each possible value of the \ufb01rst character in the keys. If R is not large, we might use \nthe \ufb01rst two letters of the keys (and a table of size R 2).  For this method to be effec-\ntive, the leading digits of the keys must be well-distributed. The resulting hybrid search \n750 CHAPTER 5 \u25a0 Strings\n  \nalgorithm corresponds to the way that a human might search for names in a telephone \nbook. The \ufb01rst step is a multiway decision (\u201cLet\u2019s see, it starts with \u2018A \u2019 , \u2019\u2019), followed per-\nhaps by some two-way decisions (\u201cIt\u2019s before \u2018Andrews, \u2019 but after \u2018Aitken, \u2019\u2019\u2019), followed \nby sequential character matching  (\u201c\u2018Algonquin,\u2019 ", "start": 762, "end": 763}, "1126": {"text": "\u2019 , \u2019\u2019), followed per-\nhaps by some two-way decisions (\u201cIt\u2019s before \u2018Andrews, \u2019 but after \u2018Aitken, \u2019\u2019\u2019), followed \nby sequential character matching  (\u201c\u2018Algonquin,\u2019 ... No, \u2018Algorithms\u2019 isn\u2019t listed,  because \nnothing starts with \u2018Algor\u2019!\u2019\u2019). These programs are likely to be among the fastest avail-\nable for searching with string keys.\n  O n e - w a y  b r a n c h i n g .  Just as with tries, we can make TSTs more ef\ufb01cient in their use of \nspace by putting keys in leaves at the point where they are distinguished and by elimi-\nnating one-way branching between internal nodes. \nProposition L. A search or an insertion in a  TST built from N random string keys \nwith no external one-way branching and R t-way branching at the root requires \nroughly ln N /H11002 t ln R character compares, on the average. \nProof: These rough estimates follow from the same argument we used to prove   \nProposition K. We assume that all but a constant number of the nodes on the \nsearch path (a few at the top) act as random BSTs on R character values, so we \nmultiply the time cost by ln R. \nDespite the temptation to tune the algorithm to peak performance, we should not \nlose sight of the fact that one of the most attractive features of TSTs is that they free us \nfrom having to worry about application-speci\ufb01c dependencies, often providing good \nperformance without any tuning. \n7515.2 \u25a0 Tries\n  \nWhich string symbol-table implementation should I use? As with string \nsorting, we are naturally interested in how the string-searching methods that we have \nconsidered compare to the general-purpose methods that we considered in Chapter \n3. The following table summarizes the important characteristics ", "start": 763, "end": 764}, "1127": {"text": "use? As with string \nsorting, we are naturally interested in how the string-searching methods that we have \nconsidered compare to the general-purpose methods that we considered in Chapter \n3. The following table summarizes the important characteristics of the algorithms that \nwe have discussed in this section (the rows for BSTs, red-black BSTs and hashing are \nincluded from Chapter 3 , for comparison). For a particular application, these en -\ntries must be taken as indicative, not de\ufb01nitive, since so many factors (such as char -\nacteristics of keys and mix of operations) come into play when studying symbol-table \nimplementations.\nalgorithm\n(data structure)\ntypical growth rate for N strings\nfrom an R-character alphabet\n(average length w)\nsweet spot\ncharacters \nexamined for\nsearch miss\nmemory usage\nbinary tree search\n(BST) c1 (lg N )2 64N randomly ordered\nkeys\n2-3 tree search\n(red-black BST) c2 (lg N )2 64N guaranteed \nperformance\nlinear probing\u2020\n(parallel arrays) w 32N to 128N built-in types\ncached hash values\ntrie search\n(R-way trie) log R N (8R/H1100156)N to (8R/H1100156)Nw short keys\nsmall alphabets\ntrie search\n(TST) 1.39 lg N 64 N to 64Nw nonrandom keys\nPerformance characteristics of string-searching algorithms\n \nIf space is available, R-way tries provide the fastest search, essentially completing the \njob with a constant number of character compares. For large alphabets, where space \nmay not be available for R-way tries, TSTs are preferable, since they use a logarithmic \nnumber of character compares, while BSTs use a logarithmic number of key compares. \nHashing can be competitive, but, as usual, cannot support ", "start": 764, "end": 764}, "1128": {"text": "tries, TSTs are preferable, since they use a logarithmic \nnumber of character compares, while BSTs use a logarithmic number of key compares. \nHashing can be competitive, but, as usual, cannot support ordered symbol-table opera-\ntions or extended character-based API operations such as pre\ufb01x or wildcard match.\n752 CHAPTER 5 \u25a0 Strings\n Q&A\nQ. Does the Java system sort use one of these methods for searching with String keys?\nA. No. \n7535.2 \u25a0 Tries\n EXERCISES\n5.2.1 Draw the R-way trie that results when the keys\n no is th ti fo al go pe to co to th ai of th pa\nare inserted in that order into an initially empty trie (do not draw null links).\n5.2.2 Draw the TST that results when the keys\n no is th ti fo al go pe to co to th ai of th pa\nare inserted in that order into an initially empty TST.\n5.2.3 Draw the R-way trie that results when the keys\n now is the time for all good people to come to the aid of\nare inserted in that order into an initially empty trie (do not draw null links).\n5.2.4 Draw the TST that results when the keys\n now is the time for all good people to come to the aid of\nare inserted in that order into an initially empty TST.\n5.2.5 Develop nonrecursive versions of TrieST and TST.\n5.2.6  Implement the following API, for a StringSET data type:\npublic class  StringSET \nStringSET() create a string set\nvoid add(String key) put key into the set\nvoid delete(String key) remove key from the set\nboolean contains(String key) is key in the set?\nboolean isEmpty() is the set empty?\nint size() number of keys in the set\nint ", "start": 764, "end": 766}, "1129": {"text": "key into the set\nvoid delete(String key) remove key from the set\nboolean contains(String key) is key in the set?\nboolean isEmpty() is the set empty?\nint size() number of keys in the set\nint toString() string representation of the set\nAPI for a string set data type\n754 CHAPTER 5 \u25a0 Strings\n CREATIVE PROBLEMS\n \n \n \n \n5.2.7  Empty string in TSTs. The code in TST does not handle the empty string prop-\nerly. Explain the problem and suggest a correction.\n5.2.8  Ordered operations for tries. Implement the floor(), ceil(), rank(), and \nselect() (from our standard ordered ST API from Chapter 3) for TrieST.\n5.2.9    Extended operations for TSTs. Implement keys() and the extended opera-\ntions introduced in this section\u2014 longestPrefixOf(), keysWithPrefix(), and \nkeysThatMatch()\u2014for TST.\n5.2.10  Size. Implement very eager size() (that keeps in each node the number of \nkeys in its subtree) for TrieST and TST.\n5.2.11    External    one-way branching. Add code to TrieST and TST to eliminate external \none-way branching.\n5.2.12    Internal one-way branching. Add code to TrieST and TST to eliminate internal \none-way branching.\n5.2.13  Hybrid TST with R2-way branching at the root. Add code to TST to do multiway \nbranching at the \ufb01rst two levels, as described in the text.\n5.2.14    Unique substrings of length L. Write a TST client that reads in text from stan-\ndard input and calculates the number of unique substrings of length L that it contains. \nFor example, if the input is cgcgggcgcg, then there are \ufb01ve unique ", "start": 766, "end": 767}, "1130": {"text": "reads in text from stan-\ndard input and calculates the number of unique substrings of length L that it contains. \nFor example, if the input is cgcgggcgcg, then there are \ufb01ve unique substrings of length \n3: cgc, cgg, gcg, ggc, and ggg. Hint : Use the string method substring(i, i + L) to \nextract the ith substring, then insert it into a symbol table.\n5.2.15  Unique substrings. Write a TST client that reads in text from standard input \nand calculates the number of distinct substrings of any length. This can be done very \nef\ufb01ciently with a suf\ufb01x tree\u2014see Chapter 6 \n5.2.16  Document similarity. Write a TST client with a static method that takes an int\nvalue L and two \ufb01le names as command-line arguments and computes the L-similarity \nof the two documents: the Euclidean distance between the frequency vectors de\ufb01ned by \nthe number of occurrences of each trigram divided by the number of trigrams. Include \na static method main() that takes an int value L as command-line argument and a list \nof \ufb01le names from standard input and prints a matrix showing the L-similarity of all \npairs of documents.\n7555.2 \u25a0 Tries\n 5.2.17  Spell checking. Write a TST client SpellChecker that takes as command-line \nargument the name of a \ufb01le containing a dictionary of words in the English language, \nand then reads a string from standard input and prints out any word that is not in the \ndictionary. Use a string set.\n5.2.18  Whitelist. Write a TST client that solves the whitelisting problem presented in \nSection 1.1 and revisited in Section 3.5 (see page 491).\n5.2.19 ", "start": 767, "end": 768}, "1131": {"text": "string set.\n5.2.18  Whitelist. Write a TST client that solves the whitelisting problem presented in \nSection 1.1 and revisited in Section 3.5 (see page 491).\n5.2.19  Random phone numbers. Write a TrieST client (with R = 10 ) that takes as \ncommand line argument an int value N and prints N random phone numbers of the \nform (xxx) xxx-xxxx. Use a symbol table to avoid choosing the same number more \nthan once. Use the \ufb01le AreaCodes.txt from the booksite to avoid printing out bogus \narea codes. \n5.2.20  Contains pre\ufb01x. Add a method containsPrefix() to StringSET (see Exer-\ncise 5.2.6) that takes a string s as input and returns true if there is a string in the set \nthat contains s as a pre\ufb01x.\n5.2.21  Substring matches. Given a list of (short) strings, your goal is to support que-\nries where the user looks up a string s and your job is to report back all strings in the \nlist that contain s. Design an API for this task and develop a TST client that implements  \nyour API. Hint : Insert the suf\ufb01xes of each word (e.g., string, tring, ring, ing, ng, g) \ninto the TST.\n5.2.22  Typing monke ys. Suppose that a typing monkey creates random words by ap -\npending each of 26 possible letter with probability p to the current word and \ufb01nishes the \nword with probability 1 /H11002 26p. Write a program to estimate the frequency distribution \nof the lengths of words produced. If \"abc\" is produced more than once, count it only \nonce.\nCREATIVE PROBLEMS  (continued)\n756 ", "start": 768, "end": 768}, "1132": {"text": "/H11002 26p. Write a program to estimate the frequency distribution \nof the lengths of words produced. If \"abc\" is produced more than once, count it only \nonce.\nCREATIVE PROBLEMS  (continued)\n756 CHAPTER 5 \u25a0 Strings\n EXPERIMENTS\n \n \n5.2.23  Duplicates (revisited again). Redo Exercise 3.5.30 using StringSET (see Ex-\nercise 5.2.6) instead of HashSET. Compare the running times of the two approaches. \nThen use Dedup to run the experiments for N = 10 7, 10 8, and10 9, repeat the experi -\nments for random long values and discuss the results.\n5.2.24    Spell checker. Redo Exercise 3.5.31, which uses the \ufb01le dictionary.txt from \nthe booksite and the BlackFilter client on page 491 to print all misspelled words in a text \n\ufb01le. Compare the performance of TrieST and TST for the \ufb01le war.txt with this client \nand discuss the results. \n5.2.25  Dictionary. Redo Exercise 3.5.32: Study the performance of a client like \nLookupCSV (using TrieST and TST) in a scenario where performance matters. Speci\ufb01-\ncally, design a query-generation scenario instead of taking commands from standard \ninput, and run performance tests for large inputs and large numbers of queries. \n5.2.26  Indexing. Redo Exercise 3.5.33: Study a client like LookupIndex (using \nTrieST and TST) in a scenario where performance matters. Speci\ufb01cally, design a query-\ngeneration scenario instead of taking commands from standard input, and run perfor-\nmance tests for large inputs and large numbers of queries.\n7575.2 \u25a0 Tries\n ", "start": 768, "end": 769}, "1133": {"text": "matters. Speci\ufb01cally, design a query-\ngeneration scenario instead of taking commands from standard input, and run perfor-\nmance tests for large inputs and large numbers of queries.\n7575.2 \u25a0 Tries\n 5.3    SUBSTRING SEARCH\nA fundamental operation on strings is substring search: given a text string of length \nN and a pattern string of length M, \ufb01nd an occurrence of the pattern within the text. \nMost algorithms for this problem can easily be extended to \ufb01nd all occurrences of the \npattern in the text, to count the number of occurrences of the pattern in the text, or to \nprovide context (substrings of the text surrounding each occurrence of the pattern). \nWhen you search for a word while using a text editor or a web browser, you are doing \nsubstring search. Indeed, the original motivation for this problem was to support such \nsearches. Another classic application is searching for some important pattern in an in-\ntercepted communication. A military leader might be interested in \ufb01nding the pattern \nATTACK AT DAWN  s o m e w h e r e  i n  a n  i n t e r c e p t e d  t e x t  m e s s a g e ;  a  h a c k e r  m i g h t  b e  \ninterested in \ufb01nding the pattern Password:  s o m e w h e r e  i n  y o u r  c o m p u t e r \u2019 s  m e m-\nory. In today\u2019s world, we are often searching through the vast amount of information \navailable on the web. \nTo  b e s t  a p p re c i a te  t h e  a l g o r i t h m s , t h i n k  o f  t h e  p ", "start": 769, "end": 770}, "1134": {"text": "web. \nTo  b e s t  a p p re c i a te  t h e  a l g o r i t h m s , t h i n k  o f  t h e  p a t te r n  a s  b e i n g  re l a t ive l y  s h o r t  ( w i t h  \nM equal to, say, 100 or 1,000) and the text as being relatively long (with N equal to, say, \n1 million or 1 billion). In substring search, we typically preprocess the pattern in order \nto be able to support fast searches for that pattern in the text.\nSubstring search is an interesting and classic problem: several very different (and \nsurprising) algorithms have been discovered that not only provide a spectrum of use-\nful practical methods but also illustrate a spectrum of fundamental algorithm design \ntechniques.\nSubstring search \nN  E  E  D  L  E\nI  N  A  H  A  Y  S  T  A  C  K  N  E  E  D  L  E  I  N  A\nmatch\npattern\ntext\n758\n  \n \nA short history The algorithms that we examine have an interesting history; we \nsummarize it here to help place the various methods io perspective.\nThere is a simple brute-force algorithm for substring search that is in widespread \nuse. While it has a worst-case running time proportional to MN, the strings that arise \nin many applications lead to a running time that is (except in pathological cases) pro-\nportional to M /H11001 N. Furthermore, it is well-suited to standard architectural features on \nmost computer systems, so an optimized version provides a standard benchmark that \nis dif\ufb01cult to beat, even with a clever algorithm.\nIn 1970,  S. Cook proved ", "start": 770, "end": 771}, "1135": {"text": "well-suited to standard architectural features on \nmost computer systems, so an optimized version provides a standard benchmark that \nis dif\ufb01cult to beat, even with a clever algorithm.\nIn 1970,  S. Cook proved a theoretical result about a particular type of abstract ma -\nchine that implied the existence of an algorithm that solves the substring search prob -\nlem in time proportional to M /H11001 N in the worst case.  D. E. Knuth and  V . R. Pratt labori-\nously followed through the construction Cook used to prove his theorem (which was \nnot intended to be practical) and re\ufb01ned it into a relatively simple and practical algo-\nrithm. This seemed a rare and satisfying example of a theoretical result with immediate \n(and unexpected) practical applicability. But it turned out that  J. H. Morris had discov-\nered virtually the same algorithm as a solution to an annoying problem confronting \nhim when implementing a text editor (he wanted to avoid having to \u201cback up\u2019\u2019 in the \ntext string). The fact that the same algorithm arose from two such different approaches \nlends it credibility as a fundamental solution to the problem.\nKnuth, Morris, and Pratt didn\u2019t get around to publishing their algorithm until 1976, \nand in the meantime  R. S. Boyer and  J. S. Moore (and, independently,  R. W. Gosper) \ndiscovered an algorithm that is much faster in many applications, since it often exam-\nines only a fraction of the characters in the text string. Many text editors use this algo-\nrithm to achieve a noticeable decrease in response time for substring search.\nBoth the Knuth-Morris-Pratt (KMP) and the Boyer-Moore algorithms require some \ncomplicated preprocessing on the pattern that is dif\ufb01cult to understand and has lim -\nited the extent to which they are used. ", "start": 771, "end": 771}, "1136": {"text": "Knuth-Morris-Pratt (KMP) and the Boyer-Moore algorithms require some \ncomplicated preprocessing on the pattern that is dif\ufb01cult to understand and has lim -\nited the extent to which they are used. (In fact, the story goes that an unknown systems \nprogrammer found Morris\u2019s algorithm too dif\ufb01cult to understand and replaced it with \na brute-force implementation.)\nIn 1980,  M. O. Rabin and  R. M. Karp used hashing to develop an algorithm almost as \nsimple as the brute-force algorithm that runs in time proportional to M /H11001 N  with very \nhigh probability. Furthermore, their algorithm extends to two-dimensional patterns \nand text, which makes it more useful than the others for image processing.\nThis story illustrates that the search for a better algorithm is still very often justi\ufb01ed; \nindeed, one suspects that there are still more developments on the horizon even for this \nclassic problem.\n7595.3 \u25a0 Substring Search\n  \nBrute-force substring search An obvious method for substring search is to \ncheck, for each possible position in the text at which the pattern could match, whether \nit does in fact match.  The search() method below operates in this way to \ufb01nd the \ufb01rst \noccurrence of a pattern string pat in a text string txt. The program keeps one pointer \n(i) into the text and another point-\ner (j) into the pattern. For each i, it \nresets j to 0 and increments it until \n\ufb01nding a mismatch or the end of \nthe pattern ( j == M ). If we reach \nthe end of the text ( i == N-M+1 ) \nbefore the end of the pattern, then \nthere is no match: the pattern does \nnot occur in the text. Our conven-\ntion is to return the value N to indi-\ncate ", "start": 771, "end": 772}, "1137": {"text": "== N-M+1 ) \nbefore the end of the pattern, then \nthere is no match: the pattern does \nnot occur in the text. Our conven-\ntion is to return the value N to indi-\ncate a mismatch.\nIn a typical text-processing ap -\nplication, the j index rarely incre-\nments so the running time is pro-\nportional to N. Nearly all of the \n  \n \ncompares \ufb01nd a mismatch with the \ufb01rst character of the pattern. For example, suppose \nthat you search for the pattern pattern in the text of this paragraph. There are 191 \ncharacters up to the end of the \ufb01rst occurrence of the pattern, only 7 of which are the \ncharacter p (and there are no occurrences of pa), so the total number of character com-\npares is 191+7, for an average of 1.036 compares per character in the text. On the other \nhand, there is no guarantee that the algorithm will always be so ef\ufb01cient. For example, a \npattern might begin with a long string of As. If it does, and the text also has long strings \nof As, then the substring search \nwill be slow.\npublic static int search(String pat, String txt) \n{\n   int M = pat.length();\n   int N = txt.length();\n   for (int i = 0; i <= N - M; i++)\n   {   \n      int j;\n      for (j = 0; j < M; j++)\n         if (txt.charAt(i+j) != pat.charAt(j))\n            break;\n      if (j == M) return i;  // found\n   }\n   return N;                 // not found \n}\nBrute-force substring search\nBrute-force substring search\ni   j  i+j  0  1  2  3  4  5  6  7 ", "start": 772, "end": 772}, "1138": {"text": "not found \n}\nBrute-force substring search\nBrute-force substring search\ni   j  i+j  0  1  2  3  4  5  6  7  8  9 10\n             A  B  A  C  A  D  A  B  R  A  C \n 0   2   2   A  B  R  A \n 1   0   1      A  B  R  A \n 2   1   3         A  B  R  A \n 3   0   3            A  B  R  A \n 4   1   5               A  B  R  A \n 5   0   5                  A  B  R  A \n6   4  10                     A  B  R  A \n   \nentries in gray are\nfor reference only\nentries in black\nmatch the text\nreturn i when j is M\nentries in red are\nmismatches\ntxt\npat\nmatch\n760 CHAPTER 5 \u25a0 Strings\n Proposition M. Brute-force  substring search requires ~NM character compares to \nsearch for a pattern of length M in a text of length N, in the worst case.\nProof: A worst-case input is when both pattern and text are all As followed by a B. \nThen for each of the  N /H11002 M /H11001 1 possible match positions, all the characters in the \npattern are checked against the text, for a total cost of M(N /H11002 M /H11001 1). Normally M\nis very small compared to N, so the total is ~NM. \nSuch degenerate strings are not likely to appear in English text, but they may well occur \nin other applications (for example, in binary ", "start": 772, "end": 773}, "1139": {"text": "Normally M\nis very small compared to N, so the total is ~NM. \nSuch degenerate strings are not likely to appear in English text, but they may well occur \nin other applications (for example, in binary texts), so we seek better algorithms.\nThe alternate implementation at \nthe bottom of this page is instructive. \nAs before, the program keeps one \npointer (i) into the text and another \npointer (j) into the pattern. As long \nas they point to matching characters, \nboth pointers are incremented. This \ncode performs precisely the same \ncharacter compares as the previous \nimplementation. T o understand it, \nnote that i in this code maintains the value of i+j in the previous code: it points to the \nend of the sequence of already-matched characters in the text (where i pointed to the \nbeginning of the sequence before). If i and j point to mismatching characters, then we \nback up both pointers: j to point to the beginning of the pattern and i to correspond to \nmoving the pattern to the right one position for matching against the text. \npublic static int search(String pat, String txt) \n{\n   int j, M = pat.length();\n   int i, N = txt.length();\n   for (i = 0, j = 0; i < N && j < M; i++)\n   {\n      if (txt.charAt(i) == pat.charAt(j)) j++;\n      else { i -= j; j = 0;  }\n   }\n   if (j == M) return i - M;  // found  \n   else            return N;  // not found \n}\nAlternate implementation of brute-force substring search (explicit backup)\nBrute-force substring search (worst case)\ni   j  i+j   0  1  2  3  4  5  6  7  8  9\n ", "start": 773, "end": 773}, "1140": {"text": "backup)\nBrute-force substring search (worst case)\ni   j  i+j   0  1  2  3  4  5  6  7  8  9\n              A  A  A  A  A  A  A  A  A  B \n 0   4   4    A  A  A  A  B \n 1   4   5       A  A  A  A  B \n 2   4   6          A  A  A  A  B \n 3   4   7             A  A  A  A  B \n 4   4   8                A  A  A  A  B \n5   5  10                   A  A  A  A  B\n   \ntxt\npat\n7615.3 \u25a0 Substring Search\n  \n \n  K n u t h - M o r r i s - P r a t t  s u b s t r i n g  s e a r c h  The basic idea behind the algorithm \ndiscovered by Knuth, Morris, and Pratt is this: whenever we detect a mismatch, we \nalready know some of the characters in the text (since they matched the pattern charac-\nters prior to the mismatch). We can take advantage of this information to avoid backing \nup the text pointer over all those known characters. \nAs a speci\ufb01c example, suppose that we have a two-character alphabet and are search-\ning for the pattern B A A A A A A A A A . Now, suppose that we match \ufb01ve char-\nacters in the pattern, with a mismatch on the sixth. When the mismatch is detected, \nwe know that the six previous \ncharacters in the text must \nbe B A A A A B (the \ufb01rst ", "start": 773, "end": 774}, "1141": {"text": "char-\nacters in the pattern, with a mismatch on the sixth. When the mismatch is detected, \nwe know that the six previous \ncharacters in the text must \nbe B A A A A B (the \ufb01rst \n\ufb01ve match and the sixth does \nnot), with the text pointer \nnow pointing at the B at the \nend. The key observation is \nthat we need not back up the \ntext pointer i, since the previ-\nous four characters in the text \nare all As and do not match \nthe \ufb01rst character in the pat-\ntern. Furthermore, the char -\nacter currently pointed to by \ni is a B and does match the \ufb01rst character in the pattern, so we can increment i and \ncompare the next character in the text with the second character in the pattern. This \nargument leads to the observation that, for this pattern, we can change the else clause \nin the alternate brute-force implementation to just set j = 1 (and not decrement i). \nSince the value of i does not change within the loop, this method does at most N char-\nacter compares. The practical effect of this particular change is limited to this particular \npattern, but the idea is worth thinking about\u2014the Knuth-Morris-Pratt algorithm is a \ngeneralization of it. Surprisingly, it is always possible to \ufb01nd a value to set the j pointer \nto on a mismatch, so that the i pointer is never decremented.\nFully skipping past all the matched characters when detecting a mismatch will not \nwork when the pattern could match itself at any position overlapping the point of the \nmismatch. For example, when searching for the pattern A A B A A A  in the text \nA A B A A B A A A A, we \ufb01rst detect the mismatch at position 5, but we had better \nrestart at position 3 to continue the search, since otherwise we ", "start": 774, "end": 774}, "1142": {"text": "in the text \nA A B A A B A A A A, we \ufb01rst detect the mismatch at position 5, but we had better \nrestart at position 3 to continue the search, since otherwise we would miss the match. \nThe insight of the KMP algorithm is that we can decide ahead of time exactly how to \nrestart the search, because that decision depends only on the pattern.\nText pointer backup in substring searching\nA  B  A  A  A  A  B  A  A  A  A  A  A  A  A  A \n   B  A  A  A  A  A  A  A  A  A\n      B  A  A  A  A  A  A  A  A  A \n         B  A  A  A  A  A  A  A  A  A \n            B  A  A  A  A  A  A  A  A  A \n               B  A  A  A  A  A  A  A  A  A \n                  B  A  A  A  A  A  A  A  A  A\n                  B  A  A  A  A  A  A  A  A  A\n   \ni\nafter mismatch\non sixth char\nbut no backup\nis needed\nbrute-force backs\nup to try this\nand this\nand this\nand this\nand this\npattern\ntext\n762 CHAPTER 5 \u25a0 Strings\n Backing up the pattern pointer. In KMP sub-\nstring search, we never back up the text pointer \ni, and we use an array dfa[][] to record how \nfar to back up the pattern pointer j when a \nmismatch is detected. For every character c, \ndfa[c][j] is the pattern position to compare \nagainst the next text position after compar -\ning c ", "start": 774, "end": 775}, "1143": {"text": "how \nfar to back up the pattern pointer j when a \nmismatch is detected. For every character c, \ndfa[c][j] is the pattern position to compare \nagainst the next text position after compar -\ning c with pat.charAt(j). During the search, \ndfa[txt.charAt(i)][j] is the pattern position \nto compare with txt.charAt(i+1) after com-\nparing txt.charAt(i) with pat.charAt(j). \nFor a match, we want to just move on to the \nnext character, so dfa[pat.charAt(j)][j] is \nalways j+1. For a mismatch, we know not just \ntxt.charAt(i), but also the j-1 previous char-\nacters in the text: they are the \ufb01rst j-1 characters \nin the pattern. For each character c, imagine that \nwe slide a copy of the pattern over these j char-\nacters (the \ufb01rst j-1 characters in the pattern fol-\nlowed by c\u2014we are deciding what to do when \nthese characters are txt.charAt(i-j+1..i)), \nfrom left to right, stopping when all overlap-\nping characters match (or there are none). \nThis gives the next possible place the pattern \ncould match. The index of the pattern char -\nacter to compare with txt.charAt(i+1)\n(dfa[txt.charAt(i)][j]) is precisely the \nnumber of overlapping characters.\nKMP search method. Once we have computed \nthe dfa[][] array, we have the substring search \nmethod at the top of the next page: when i and \nj point to mismatching characters (testing for a \npattern match beginning at position i-j+1 in the text string), then the next possible \nposition for a pattern match is beginning at position  i-dfa[txt.charAt(i)][j]. But \nby construction, the \ufb01rst dfa[txt.charAt(i)][j] characters at ", "start": 775, "end": 775}, "1144": {"text": "string), then the next possible \nposition for a pattern match is beginning at position  i-dfa[txt.charAt(i)][j]. But \nby construction, the \ufb01rst dfa[txt.charAt(i)][j] characters at that position match \nthe \ufb01rst dfa[txt.charAt(i)][j] characters of the pattern, so there is no need to back \nup the i pointer: we can simply set j to dfa[txt.charAt(i)][j] and increment i, \nwhich is precisely what we do when i and j point to matching characters. \nABABAC\nA\nB\nABABAC\nC\nABABAC\nAB\nA A\nA BABAC\nAC\n ABABAC\nABA\nAB B\n   ABABAC\nAB C\n   ABABAC\nABAB\nABA A\n   A BABAC\nABA C\n    ABABAC\nABABA\nABAB B\n    ABABAC\nABAB C\n    ABABAC\nABABAC\nABABA A\n     A BABAC\nABABA B\n  ABAB AC\nj  pat.charAt(j)  dfa[][j]\n               A  B  C\n0      A       1  \n               \n                  0\n                     0 \n     \n1      B          2 \n           \n               1\n                     0\n2      A       3\n              \n                  0\n                     0\n3      B          4\n           \n               1\n                     0\n     \n4      A       5\n              \n                  0 \n                     0\n     \n5      C             6    \n           \n               1\n                  4\n     \nPattern backup for ABABAC  in KMP substring search\nbackup is length of max overlap\nof beginning of pattern\nwith known text chars\nmatch (move to next char)\nset dfa[pat.charAt(j)][j]\nto j+1 \nknown ", "start": 775, "end": 775}, "1145": {"text": "in KMP substring search\nbackup is length of max overlap\nof beginning of pattern\nwith known text chars\nmatch (move to next char)\nset dfa[pat.charAt(j)][j]\nto j+1 \nknown text char\non mismatch\ntext (pattern itself)\nmismatch\n(back up in pattern)\n7635.3 \u25a0 Substring Search\n  \n D F A  s i m u l a t i o n .  A useful way to describe this process is in terms of a deterministic \n\ufb01nite-state automaton (DFA). Indeed, as indicated by its name, our dfa[][] array pre-\ncisely de\ufb01nes a DFA. The graphical DFA represention shown at the bottom of this page \nconsists of states (indicated by circled \nnumbers) and transitions (indicated by \nlabeled lines). There is one state for each \ncharacter in the pattern, each such state \nhaving one transition leaving it for each \ncharacter in the alphabet. For the sub-\nstring-matching DFAs that we are con-\nsidering, one of the transitions is a match\ntransition (going from j to j+1 and labeled \nwith pat.charAt(j)) and all the others \nare mismatch transition (going left). The states correspond to character compares, one \nfor each value of the pattern index. The transitions correspond to changing the value \nof the pattern index. When examining the text character i when in the state labeled j, \nthe machine does the following: \u201cTake the transition to dfa[txt.charAt(i)][j] and \nmove to the next character (by incrementing i). \u2019\u2019 For a match transition, we move to \nthe right one position because dfa[pat.charAt(j)][j] is always j+1; for a mismatch \ntransition we move to the left. The automaton reads the text characters one at a time, \nfrom left to right, moving to a ", "start": 775, "end": 776}, "1146": {"text": "dfa[pat.charAt(j)][j] is always j+1; for a mismatch \ntransition we move to the left. The automaton reads the text characters one at a time, \nfrom left to right, moving to a new state each time it reads a character. We also include a \nhalt state M that has no transitions. We start the machine at state 0: if the machine reach-\nes state M, then a substring of the text \nmatching the pattern has been found \n(and we say that the DFA recognizes the \npattern); if the machine reaches the \nend of the text before reaching state \nM, then we know the pattern does not \nappear as a substring of the text. Each \npattern corresponds to an automaton \n(which is represented by the dfa[][]\narray that gives the transitions). The \nKMP substring search() method is \na Java program that simulates the op-\neration of such an automaton. \nTo  g e t  a  f e e l i n g  f o r  t h e  o p e r a t i o n  o f  \na substring search DFA, consider two \nof the simplest things that it does. At \npublic int search(String txt) \n{  // Simulate operation of DFA on txt.\n   int i, j, N = txt.length();\n   for (i = 0, j = 0; i < N && j < M; i++)\n      j = dfa[txt.charAt(i)][j];\n   if (j == M) return i - M;  // found\n   else        return N;      // not found \n}\nKMP substring search (DFA simulation)\nDFA corresponding to the string A  B  A  B  A  C \nmatch\ntransition\n(increment)\nmismatch\ntransition\n(back up)\nhalt state\n0 1 2 3 4 5 6A ", "start": 776, "end": 776}, "1147": {"text": "to the string A  B  A  B  A  C \nmatch\ntransition\n(increment)\nmismatch\ntransition\n(back up)\nhalt state\n0 1 2 3 4 5 6A B A A\nB,C\nA\nCB,CC\nB\nAB,C A\nC\n     0   1   2   3   4   5\n     A   B   A   B   A   C\n     1   1   3   1   5   1\n     0   2   0   4   0   4\n     0   0   0   0   0   6\ndfa[][j]\npat.charAt(j)\nj\nA\nB\nC\ngraphical representation\ninternal representation\nB\n764 CHAPTER 5 \u25a0 Strings\n  \nthe beginning of the process, when started in state 0 at the beginning of the text, it stays \nin state 0, scanning text characters, until it \ufb01nds a text character that is equal to the \ufb01rst \npattern character, when it moves to the next state and is off and running. At the end of \nthe process, when it \ufb01nds a match, it matches pattern characters with the right end of \nthe text, incrementing the state until reaching state M. The trace at the top of this page \ngives a typical example of the operation of our example DFA. Each match moves the \nDFA to the next state (which is equivalent to incrementing the pattern index j); each \nmismatch moves the DFA to an earlier state (which is equivalent to setting the pattern \nindex j to a smaller value). The text index i marches from left to right, one position at \na time, while the pattern index j bounces around in the pattern as directed by the DFA.\nConstructing ", "start": 776, "end": 777}, "1148": {"text": "the pattern \nindex j to a smaller value). The text index i marches from left to right, one position at \na time, while the pattern index j bounces around in the pattern as directed by the DFA.\nConstructing the DFA. Now that you understand the mechanism, we are ready to ad-\ndress the key question for the KMP algorithm: How do we compute the dfa[][] array \ncorresponding to a given pattern? Remarkably, the answer to this question lies in the \nDFA itself   (!) using the ingenious (and rather tricky) construction that was developed \nby Knuth, Morris, and Pratt. When we have a mismatch at pat.charAt(j), our interest \nis in knowing in what state the DFA would be if we were to back up the text index and \nrescan the text characters that we just saw after shifting to the right one position. We do \nnot want to actually do the backup, just restart the DFA as if we had done the backup. \nA  B  A  B  A  C\n   A  B  A  B  A  C\n      A  B  A  B  A  C\n         A  B  A  B  A  C\n         A  B  A  B  A  C\n            A  B  A  B  A  C\n            A  B  A  B  A  C\n            A  B  A  B  A  C\n                        A  B  A  B  A  C\n                        A  B  A  B  A  C\n                           A  B  A  B  A  C\n                           A  B  A  B  A  C\n                           A  B  A  B  A  C\n                           A  B  A  B  A  C\n                           A  B  A  B  A  C\n ", "start": 777, "end": 777}, "1149": {"text": "B  A  C\n                           A  B  A  B  A  C\n                           A  B  A  B  A  C\n                           A  B  A  B  A  C\n                           A  B  A  B  A  C\nTrace of KMP substring search (DFA simulation) for A  B  A  B  A  C\n         0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16\n         B  C  B  A  A  B  A  C  A  A  B  A  B  A  C  A  A \n         0  0  0  0  1  1  2  3  0  1  1  2  3  4  5  6\n                                        \nfound\nreturn i - M = 9\nmismatch:\n    set j to dfa[txt.charAt(i)][j]\n         implies pattern shift to align\n    pat.charAt(j) with\n    txt.charAt(i+1)\nmatch:\n  set j to  dfa[txt.charAt(i)][j] \n     = dfa[pat.charAt(j)][j]\n     = j+1\nread this char\nin this state\ngo to this state\ni\ntxt.charAt(i)\nj\n7655.3 \u25a0 Substring Search\n  \n \nThe key observation is that the characters in the text that would need to be rescanned \nare precisely pat.charAt(1) through pat.charAt(j-1): we drop the \ufb01rst character \nto shift right one position and the last character because of the mismatch. These are \npattern characters that we know, so we can \ufb01gure out ahead of \ntime, ", "start": 777, "end": 778}, "1150": {"text": "pat.charAt(j-1): we drop the \ufb01rst character \nto shift right one position and the last character because of the mismatch. These are \npattern characters that we know, so we can \ufb01gure out ahead of \ntime, for each possible mismatch position, the state where we \nneed to restart the DFA. The \ufb01gure at left shows the possibilities \nfor our example. Be sure that you understand this concept. \nWhat should the DFA do with the next character? Exactly \nwhat it would have done if we had backed up, except if it \ufb01nds \na match with pat.charAt(j), when it should go to state j+1. \nFor example, to decide what the DFA should do when we have \na mismatch at j = 5 for A B A B A C, we use the DFA to learn \nthat a full backup would leave us in state 3 for B A B A, so we \ncan copy dfa[][3] to dfa[][5], then set the entry for C to 6\nbecause pat.charAt(5) is C (a match). Since we only need to \nknow how the DFA runs for j-1 characters when we are build-\ning the jth state, we can always get the information that we need \nfrom the partially built DFA.\nThe \ufb01nal crucial detail to the computation is to observe that maintaining the restart \nposition X when working on column j of dfa[][] is easy because X < j so that we can use \nthe partially built DFA to do the job\u2014the next value of X is dfa[pat.charAt(j)][X]). \nContinuing our example from the previous paragraph, we would update the value of \nX to dfa['C'][3] = 0 (but we do not use that value because the DFA construction is \ncomplete).\nThe discussion above leads to the remarkably compact code below for constructing \nthe DFA corresponding to a ", "start": 778, "end": 778}, "1151": {"text": "\nX to dfa['C'][3] = 0 (but we do not use that value because the DFA construction is \ncomplete).\nThe discussion above leads to the remarkably compact code below for constructing \nthe DFA corresponding to a given pattern. For each j, it\n\u25a0 Copies dfa[][X] to dfa[][j] (for mismatch cases)\n\u25a0 Sets dfa[pat.charAt(j)][j] to j+1 (for the match case)\n\u25a0 Updates X\nThe diagram on the facing page traces \nthis code for our example.  T o make sure \nthat you understand it, work Exercise \n5.3.2 and Exercise 5.3.3.\ndfa[pat.charAt(0)][0] = 1; \nfor (int X = 0, j = 1; j < M; j++) \n{  // Compute dfa[][j].\n   for (int c = 0; c < R; c++)\n      dfa[c][j] = dfa[c][X];\n   dfa[pat.charAt(j)][j] = j+1;\n   X = dfa[pat.charAt(j)][X]; \n}\nConstructing the DFA for KMP substring search\nDFA simulations to compute\nrestart states for A  B  A  B  A  C\n  A   B   A   B   A\n      0   0   1   2   3   \n  A   B   A   B\n      0   0   1   2 \n  A   B   A \n      0   0   1  \n  A   B   \n      0   0  \n  A   \n      0  \nrestart\nstates\nX\n1\n2\n3\n4\n5\n766 CHAPTER 5 \u25a0 Strings\n Constructing the DFA for KMP substring search for A  B  A  B  A ", "start": 778, "end": 779}, "1152": {"text": "\n      0  \nrestart\nstates\nX\n1\n2\n3\n4\n5\n766 CHAPTER 5 \u25a0 Strings\n Constructing the DFA for KMP substring search for A  B  A  B  A  C\n0 1 2 3 4 5 6A B A A\nB,C\nA\nCB,CC\nB\nAB,C A\nB C\n     0   1   2   3   4   5\n     A   B   A   B   A   C\n     1   1   3   1   5   1\n     0   2   0   4   0   4\n     0   0   0   0   0   6\ndfa[][j]\nA\nB\nC\nX\nX j\nX\nX\nX\nX\nj\nj\nj\nj\nj\n0 1 2 3 4 5A B A A\nB,C\nA\nCB,CC\nB,C A\nB\n     0   1   2   3   4\n     A   B   A   B   A\n     1   1   3   1   5\n     0   2   0   4   0\n     0   0   0   0   0\ndfa[][j]\nA\nB\nC\nX\n0 1 2 3 4A B A\nA\nCB,CC\nB,C A\nB\n     0   1   2   3 \n     A   B   A   B\n     1   1   3   1 \n     0   2   0   4\n ", "start": 779, "end": 779}, "1153": {"text": "0   1   2   3 \n     A   B   A   B\n     1   1   3   1 \n     0   2   0   4\n     0   0   0   0\ndfa[][j]\nA\nB\nC\nX\n0 1 2 3A B A\nB,CC\nB,C A\n     0   1   2   \n     A   B   A \n     1   1   3 \n     0   2   0\n     0   0   0\ndfa[][j]\nA\nB\nC\nX\n0 1 2A B\nC\nB,C A     0   1\n     A   B\n     1   1\n     0   2\n     0   0\ndfa[][j]\nA\nB\nC\nX\n0 1A\nB,C     0 \n     A \n     1 \n     0 \n     0 \ndfa[][j]\npat.charAt(j)\nj\npat.charAt(j)\nj\npat.charAt(j)\nj\npat.charAt(j)\nj\npat.charAt(j)\nj\npat.charAt(j)\nj\nA\nB\nC\ncopy dfa[][X] to dfa[][j]\ndfa[pat.charAt(j)][j] = j+1;\nX = dfa[pat.charAt(j)][X];\n7675.3 \u25a0 Substring Search\n ALGORITHM 5.6   Knuth-Morris-Pratt substring search\npublic class  KMP \n{\n   private String pat;\n   private int[][] dfa;\n   public KMP(String pat)\n   {  // Build DFA from pattern.\n      this.pat = pat;\n      int M = pat.length();\n      int R = 256;\n      dfa ", "start": 779, "end": 780}, "1154": {"text": "private int[][] dfa;\n   public KMP(String pat)\n   {  // Build DFA from pattern.\n      this.pat = pat;\n      int M = pat.length();\n      int R = 256;\n      dfa = new int[R][M];\n      dfa[pat.charAt(0)][0] = 1;\n      for (int X = 0, j = 1; j < M; j++)\n      {  // Compute dfa[][j].\n         for (int c = 0; c < R; c++)\n            dfa[c][j] = dfa[c][X];            // Copy mismatch cases.\n         dfa[pat.charAt(j)][j] = j+1;         // Set match case.\n         X = dfa[pat.charAt(j)][X];           // Update restart state.\n      }\n   }\n   public int search(String txt)\n   {  // Simulate operation of DFA on txt.\n      int i, j, N = txt.length(), M = pat.length();\n      for (i = 0, j = 0; i < N && j < M; i++)\n         j = dfa[txt.charAt(i)][j];\n      if (j == M) return i - M;  // found (hit end of pattern)\n      else        return N;      // not found (hit end of text)\n   }\n   public static void main(String[] args)\n   // See page 769. \n}\n \nThe constructor in this implementation of the Knuth-Morris-Pratt algorithm for substring search \nbuilds a DFA from a pattern string, to support a search() method that can \ufb01nd the pattern in a given \ntext string. This program does the same job as the brute-force method, but it runs faster for patterns \nthat are self-repetitive.\n% java KMP AACAA AABRAACADABRAACAADABRA \ntext:    AABRAACADABRAACAADABRA ", "start": 780, "end": 780}, "1155": {"text": "does the same job as the brute-force method, but it runs faster for patterns \nthat are self-repetitive.\n% java KMP AACAA AABRAACADABRAACAADABRA \ntext:    AABRAACADABRAACAADABRA \npattern:             AACAA \n768 CHAPTER 5 \u25a0 Strings Algorithm 5.6 on the facing page implements the following API:\npublic class  KMP\nKMP(String pat) create a DFA that can search for pat \nint search(String txt) find index of pat in txt \nSubstring search API\nYo u  c a n  s e e  a  t y p i c a l  t e s t  c l i e n t  a t  t h e  b o t t o m  o f  t h i s  p a g e . T h e  co n s t r u c t o r  b u i l d s  a  D FA  \nfrom a pattern that the search() method uses to search for the pattern in a given text.\nProposition N.  Knuth-Morris-Pratt substring search accesses no more than M /H11001 N\ncharacters to search for a pattern of length M in a text of length N.\nProof. Immediate from the code: we access each pattern character once when com-\nputing dfa[][] and each text character once (in the worst case) in search(). \nAnother parameter comes into play: for an R-character alphabet, the total running time \n(and space) required to build the DFA is proportional to MR. It is possible to remove \nthe factor of R by building a DFA where each state has a match transition and a mis-\nmatch transition (not transitions for each possible character), though the construction \nis somewhat more intricate.\nThe linear-time worst-case guarantee provided by the KMP algorithm is a signi\ufb01cant \ntheoretical result. ", "start": 780, "end": 781}, "1156": {"text": "and a mis-\nmatch transition (not transitions for each possible character), though the construction \nis somewhat more intricate.\nThe linear-time worst-case guarantee provided by the KMP algorithm is a signi\ufb01cant \ntheoretical result. In practice, the speedup over the brute-force method is not often \nimportant because few applications involve searching for highly self-repetitive patterns \nin highly self-repetitive text. Still, the method \nhas the practical advantage that it never backs \nup in the input. This property makes KMP \nsubstring search more convenient for use \non an input stream of undetermined length \n(such as standard input) than algorithms re-\nquiring backup, which need some complicat-\ned buffering in this situation. Ironically, when \nbackup is easy, we can do signi\ufb01cantly better \nthan KMP . Next, we consider a method that \ngenerally leads to substantial performance \ngains precisely because it can back up in the \ntext.\npublic static void main(String[] args) \n{\n   String pat = args[0];\n   String txt = args[1];\n   KMP kmp = new KMP(pat);\n   StdOut.println(\"text:    \" + txt);\n   int offset = kmp.search(txt);\n   StdOut.print(\"pattern: \");\n   for (int i = 0; i < offset; i++)\n      StdOut.print(\" \");\n   StdOut.println(pat);\n}\n K M P  s u b s t r i n g  s e a r c h  t e s t  c l i e n t\n7695.3 \u25a0 Substring Search\n  \n \n \n  B o y e r - M o o r e  s u b s t r i n g  s e a r c h  When backup in the text string is not a prob -\nlem, we can develop a signi\ufb01cantly faster substring-searching method by scanning the ", "start": 781, "end": 782}, "1157": {"text": "b s t r i n g  s e a r c h  When backup in the text string is not a prob -\nlem, we can develop a signi\ufb01cantly faster substring-searching method by scanning the   \npattern from right to left  when trying to match it against the text. For example, when \nsearching for the substring BAABBAA , if we \ufb01nd matches on the seventh and sixth \ncharacters but not on the \ufb01fth, then we can immediately slide the pattern seven posi -\ntions to the right, and check the 14th character in the text next, because our partial \nmatch found XAA  w h e r e  X i s  n o t  B , which does not appear elsewhere in the pattern. In \ngeneral, the pattern at the end might appear elsewhere, so we need an array of restart \npositions as for Knuth-Morris-Pratt. We will not explore this approach in further detail \nbecause it is quite similar to our implementation of the Knuth-Morris-Pratt method. \nInstead, we will consider another suggestion by Boyer and Moore that is typically even \nmore effective in right-to-left pattern scanning.\nAs with our implementation of KMP substring search, we decide what to do next on \nthe basis of the character that caused the mismatch in the text as well as the pattern. The \npreprocessing step is to decide, for each possible character that could occur in the text, \nwhat we would do if that character were to cause the mismatch. The simplest realiza-\ntion of this idea leads immediately to an ef\ufb01cient and useful substring search method. \n M i s m a t c h e d  c h a r a c t e r  h e u r i s t i c .  Consider the \ufb01gure at the bottom of this page, which \nshows a search for the pattern NEEDLE  i n ", "start": 782, "end": 782}, "1158": {"text": "h a r a c t e r  h e u r i s t i c .  Consider the \ufb01gure at the bottom of this page, which \nshows a search for the pattern NEEDLE  i n  t h e  t e x t  FINDINAHAYSTACKNEEDLE . \nProceeding from right to left to match the pattern, we \ufb01rst compare the rightmost E in \nthe pattern with the N (the character at position 5) in the text. Since N appears in the \npattern, we slide the pattern \ufb01ve positions to the right to line up the N in the text with \nthe (rightmost) N in the pattern. Then we compare the rightmost E in the pattern with \nthe S (the character at position 10) in the text. This is also a mismatch, but S does not\nappear in the pattern, so we can slide the pattern six positions to the right.We match \nthe rightmost E in the pattern against the E at position 16 in the text, then \ufb01nd a mis-\nmatch and discover the N at position 15 and slide to the right \ufb01ve positions, as at the \nbeginning. Finally, we verify, moving from right to left starting at position 20, that the \npattern is in the text. This method brings us to the match position at a cost of only four \ncharacter compares (and six more to verify the match)!\nMismatched character heuristic for right-to-left (Boyer-Moore) substring search \n i   j   0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n         F  I  N  D  I  N  A  H  A ", "start": 782, "end": 782}, "1159": {"text": "10 11 12 13 14 15 16 17 18 19 20 21 22 23\n         F  I  N  D  I  N  A  H  A  Y  S  T  A  C  K  N  E  E  D  L  E  I  N  A\n 0   5   N  E  E  D  L  E\n 5   5                  N  E  E  D  L  E\n11   4                                    N  E  E  D  L  E\n15   0                                                N  E  E  D  L  E \n   return i = 15\n pattern\n text\n770 CHAPTER 5 \u25a0 Strings\n Starting point. To  i m p l e m e n t  t h e  m i s m a tc h e d  \ncharacter heuristic, we use an array right[] that \ngives, for each character in the alphabet, the index \nof its rightmost occurrence  in the pattern (or -1\nif the character is not in the pattern). This value \ntells us precisely how far to skip if that character \nappears in the text and causes a mismatch during \nthe string search. T o initialize the right[] array, \nwe set all entries to -1 and then, for j from 0 to \nM-1, set right[pat.charAt(j)] to j, as shown \nin the example at right for our example pattern \nNEEDLE . \nSubstring search. With the right[] array pre -\ncomputed, the implementation in Algorithm 5.7 is straightforward. We have an index \ni moving from left to right through the text and an index j moving from right to left \nthrough the pattern. The inner loop tests whether the pattern aligns with the text at \nposition i. If txt.charAt(i+j) ", "start": 782, "end": 783}, "1160": {"text": "have an index \ni moving from left to right through the text and an index j moving from right to left \nthrough the pattern. The inner loop tests whether the pattern aligns with the text at \nposition i. If txt.charAt(i+j) is equal to pat.charAt(j) for all j from M-1 down to \n0, then there is a match. Otherwise, there is a character mismatch, and we have one of \nthe following three cases:\n\u25a0 If the character causing the mismatch is \nnot found in the pattern, we can slide the \npattern j+1 positions to the right (incre-\nment i by j+1). Anything less would align \nthat character with some pattern character.\nActually, this move aligns some known \ncharacters at the beginning of the pattern \nwith known characters at the end of the \npattern so that we could further increase \ni by precomputing a KMP-like table (see \nexample at right).\n\u25a0 \n \nIf the character c causing the mismatch is \nfound in the pattern, we use the right[] array to line up the pattern with the \ntext so that character will match its rightmost occurrence in the pattern. To do \nso, we increment i by j minus right[c]. Again, anything less would align that \ntext character with a pattern character it could not match (one to the right of its \nrightmost occurrence). Again, there is a possibility that we could do better with a \nKMP-like table, as indicated in the top example in the \ufb01gure on page 773.\nBoyer-Moore skip table computation\nc right[c]\n          N   E   E   D   L   E\n          0   1   2   3   4   5\nA    -1  -1  -1  -1  -1  -1  -1     -1\nB    -1  -1  -1 ", "start": 783, "end": 783}, "1161": {"text": "2   3   4   5\nA    -1  -1  -1  -1  -1  -1  -1     -1\nB    -1  -1  -1  -1  -1  -1  -1     -1\nC    -1  -1  -1  -1  -1  -1  -1     -1\nD    -1  -1  -1  -1   3   3   3      3\nE    -1  -1   1   2   2   2   5      5\n...                                 -1\nL    -1  -1  -1  -1  -1   4   4      4\nM    -1  -1  -1  -1  -1  -1  -1     -1\nN    -1   0   0   0   0   0   0      0\n...                                 -1\nMismatched character heuristic (mismatch not in pattern)\n increment i by j+1 \n reset j to M-1 \n.  .  .  .  .  .  T  L  E  .  .  .  .  . \n         N  E  E  D  L  E\ni\nj\nj\ni+j\n.  .  .  .  .  .  T  L  E  .  .  .  .  .\n                     N  E  E  D  L  E\ni  could do better with\nKMP-like table\n7715.3 \u25a0 Substring Search\n ALGORITHM 5.7   Boyer-Moore substring search (mismatched character heuristic)\npublic class  BoyerMoore \n{\n   private int[] ", "start": 783, "end": 784}, "1162": {"text": "with\nKMP-like table\n7715.3 \u25a0 Substring Search\n ALGORITHM 5.7   Boyer-Moore substring search (mismatched character heuristic)\npublic class  BoyerMoore \n{\n   private int[] right;\n   private String pat;\n   BoyerMoore(String pat)\n   {  // Compute skip table.\n      this.pat = pat;\n      int M = pat.length();\n      int R = 256;\n      right = new int[R];\n      for (int c = 0; c < R; c++)\n         right[c] = -1;                // -1 for chars not in pattern\n      for (int j = 0; j < M; j++)      // rightmost position for\n         right[pat.charAt(j)] = j;     //   chars in pattern\n   }\n   public int search(String txt)\n   {  // Search for pattern in txt.\n      int N = txt.length();\n      int M = pat.length();\n      int skip;\n      for (int i = 0; i <= N-M; i += skip)\n      {  // Does the pattern match the text at position i ?\n         skip = 0;\n         for (int j = M-1; j >= 0; j--)\n            if (pat.charAt(j) != txt.charAt(i+j))\n            {\n               skip = j - right[txt.charAt(i+j)];\n               if (skip < 1) skip = 1;\n               break;\n            }\n         if (skip == 0) return i;          // found.   \n      }\n      return N;                            // not found.\n    }\n   public static void main(String[] args)  // See page 769. \n}\nThe constructor in this substring search algorithm builds a table giving the rightmost occurrence in \nthe pattern of each possible character. The search method scans from right to left in the pattern, skip-\nping to align any character ", "start": 784, "end": 784}, "1163": {"text": "769. \n}\nThe constructor in this substring search algorithm builds a table giving the rightmost occurrence in \nthe pattern of each possible character. The search method scans from right to left in the pattern, skip-\nping to align any character causing a mismatch with its rightmost occurrence in the pattern.\n772 CHAPTER 5 \u25a0 Strings \u25a0 If this computation would not in-\ncrease i, we just increment i instead, \nto make sure that the pattern always \nslides at least one position to the \nright. The bottom example in the \ufb01g-\nure at right illustrates this situation.\nAlgorithm 5.7 is a straightforward imple-\nmentation of this process. Note that the \nconvention of using -1 in the right[]\narray entries corresponding to characters \nthat do not appear in the pattern uni-\n\ufb01es the \ufb01rst two cases (increment i by \nj - right[txt.charAt(i+j)]).\nThe full Boyer-Moore algorithm takes \ninto account precomputed mismatches of \nthe pattern with itself (in a manner simi-\nlar to the KMP algorithm) and provides a \nlinear-time worst-case guarantee (whereas \nAlgorithm 5.7 can take time proportional \nto NM in the worst case\u2014see Exercise \n5.3.19). We omit this computation because \nthe mismatched character heuristic con-\ntrols the performance in typical practical \napplications. \nProperty O. On typical inputs, substring search with the  Boyer-Moore mismatched \ncharacter heuristic uses ~N/H11408M character compares to search for a pattern of length \nM in a text of length N.\nDiscussion: This result can be proved for various random string models, but such \nmodels tend to be unrealistic, so we shall skip the details. In many practical situa-\ntions it is true that all but a few of the alphabet characters appear nowhere in the \npattern, so nearly all compares lead to M characters being skipped, ", "start": 784, "end": 785}, "1164": {"text": "unrealistic, so we shall skip the details. In many practical situa-\ntions it is true that all but a few of the alphabet characters appear nowhere in the \npattern, so nearly all compares lead to M characters being skipped, which gives the \nstated result.\nMismatched character heuristic (mismatch in pattern) \n increment i by j - right['N']\n to line up text with N in pattern\n reset j to M-1 \n.  .  .  .  .  .  N  L  E  .  .  .  .  .  .\n         N  E  E  D  L  E\ni\nj\nj\n reset j to M-1 \nj\ni+j\n.  .  .  .  .  .  N  L  E  .  .  .  .  .  .\n                  N  E  E  D  L  E\ni\nbasic idea\n lining up text with rightmost E\nwould shift pattern left\n could do better with\nKMP-like table\n could do better with\nKMP-like table\n.  .  .  .  .  .  E  L  E  .  .  .  .  .  .\n         N  E  E  D  L  E\ni\nj\ni+j\n.  .  .  .  .  .  E  L  E  .  .  .  .  .  .\n   N  E  E  D  L  E\n so increment i by 1\n.  .  .  .  .  .  E  L  E  .  .  .  .  .  .\n            N  E  E  D  L  E\ni\nheuristic is no help\n7735.3 \u25a0 Substring Search\n    R a b i n - K a ", "start": 785, "end": 786}, "1165": {"text": ".  .  .  .\n            N  E  E  D  L  E\ni\nheuristic is no help\n7735.3 \u25a0 Substring Search\n    R a b i n - K a r p  \ufb01 n g e r p r i n t  s e a r c h  The method developed by M. O. Rabin and \nR. A. Karp is a completely different approach to substring search that is based on hash-\ning.  We compute a hash function for the pattern and then look for a match by using \nthe same hash function for each possible M-character substring of the text. If we \ufb01nd \na text substring with the same hash value as the pattern, we can check for a match. \nThis process is equivalent to storing the pattern in a hash table, then doing a search \nfor each substring of the text, but we do not need to reserve the memory for the hash \ntable because it would have just one entry.  A straightforward implementation based \non this description would be much slower than a brute-force search (since comput-\ning a hash function that involves every character is likely to be much more expensive \nthan just comparing characters), but Rabin and Karp showed that it is easy to compute \nhash functions for M-character substrings in constant time (after some preprocessing), \nwhich leads to a linear-time substring search in practical situations.\n B a s i c  p l a n .  A string of length M corresponds to an M-digit base-R number. T o use a hash \ntable of size Q for keys of this type, we need a hash function to convert an M-digit base-R\nnumber to an int value between 0 and Q-1.  Modular hashing (see Section 3.4) pro-\nvides an answer: take the remainder when dividing the number by Q. In practice, we use \na random  prime Q, ", "start": 786, "end": 786}, "1166": {"text": "0 and Q-1.  Modular hashing (see Section 3.4) pro-\nvides an answer: take the remainder when dividing the number by Q. In practice, we use \na random  prime Q, taking as large a value as possible while avoiding over\ufb02ow (because \nwe do not actually need to store a hash table). The method is simplest to understand for \nsmall Q and R = 10, shown in the example below.  To \ufb01nd the pattern 26535  i n  t h e  \ntext 3141592653589793 , we choose a table size Q (997 in the example), compute \nthe hash value 26535 % 997 = 613, and then look for a match by computing hash val-\nues for each \ufb01ve-digit substring \nin the text. In the example, we \nget the hash values 508, 201, 715, \n971, 442, and 929 before \ufb01nding \nthe match 613. \nComputing the  hash func -\ntion. With \ufb01ve-digit values, we \ncould just do all the necessary \ncalculations with int values, but \nwhat do we do when M is 100 or \n1,000? A simple application of \nHorner\u2019s method, precisely like \nthe method that we examined in \nSection 3.4 for strings and other \ntypes of keys with multiple values, Basis for Rabin-Karp substring search \n                      txt.charAt(i)\ni    0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 ", "start": 786, "end": 786}, "1167": {"text": "15\n     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9  3\n0    3  1  4  1  5  % 997 = 508\n1       1  4  1  5  9  % 997 = 201\n2          4  1  5  9  2  % 997 = 715\n3             1  5  9  2  6  % 997 = 971\n4                5  9  2  6  5  % 997 = 442\n5                   9  2  6  5  3  % 997 = 929 \n6                      2  6  5  3  5  % 997 = 613\n     pat.charAt(j)\nj    0  1  2  3  4\n     2  6  5  3  5  % 997 = 613\n                                              \n return i = 6\n match\n774 CHAPTER 5 \u25a0 Strings\n leads to the code shown at right, which computes the hash function for an M-digit base-\nR number represented as a char array in time proportional to M. (We pass M as an argu-\nment so that we can use the method for both the pattern and the text, as you will see.) \nFor each digit in the number, we multiply by R, add the digit, and take the remainder \nwhen divided by Q. For example, computing the \nhash function for our pattern using this process \nis shown at the bottom of the page. The same \nmethod can ", "start": 786, "end": 787}, "1168": {"text": "by R, add the digit, and take the remainder \nwhen divided by Q. For example, computing the \nhash function for our pattern using this process \nis shown at the bottom of the page. The same \nmethod can work for computing the hash func-\ntions in the text, but the cost for the substring \nsearch would be a multiplication, addition, and \nremainder calculation for each text character, \nfor a total of NM operations in the worst case, \nno improvement over the brute-force method.\nKey idea. The Rabin-Karp method is based on ef\ufb01ciently computing the hash func-\ntion for position i+1 in the text, given its value for position i. It follows directly from a \nsimple mathematical formulation. Using the notation ti for txt.charAt(i), the num-\nber corresponding to the M-character substring of txt that starts at position i is\nxi = ti R M/H110021 /H11001 ti+1 R M/H110022 /H11001 .  .  . /H11001 ti+M/H110021R 0\nand we can assume that we know the value of h(xi) = xi mod Q . Shifting one position \nright in the text corresponds to replacing xi by\nxi+1 =  (xi /H11002 ti R M/H110021) R /H11001 ti+M  .\nWe subtract off  the leading dig it, multiply by R, then add the trailing digit. Now, the \ncrucial point is that we do not have to maintain the values of the numbers, just the \nvalues of their remainders when divided by Q. A fundamental property of the modu -\nlus operation is that if we take the remainder when divided by Q after each arithmetic \noperation, then we get the same answer as if we were to perform all of the arithmetic \noperations, then take the remainder \nwhen divided by Q. We took advan-\ntage ", "start": 787, "end": 787}, "1169": {"text": "take the remainder when divided by Q after each arithmetic \noperation, then we get the same answer as if we were to perform all of the arithmetic \noperations, then take the remainder \nwhen divided by Q. We took advan-\ntage of this property once before, \nwhen implementing modular hash-\ning with Horner\u2019s method (see page \n460). The result is that we can effec-\ntively move right one position in the \ntext in constant time, whether M is 5 \nor 100 or 1,000.\nprivate long hash(String key, int M) \n{  // Compute hash for key[0..M-1].\n   long h = 0;\n   for (int j = 0; j < M; j++)\n      h = (R * h + key.charAt(j)) % Q;\n   return h; \n}\n H o r n e r \u2019 s  m e t h o d ,  a p p l i e d  t o  m o d u l a r  h a s h i n g\nComputing the hash value for the pattern with Horner\u2019s method\n         pat.charAt(j)\n i   0  1  2  3  4\n     2  6  5  3  5\n 0   2  % 997 = 2\n 1   2  6  % 997 = (2*10 + 6) % 997 = 26\n 2   2  6  5  % 997 = (26*10 + 5) % 997 = 265\n 3   2  6  5  3  % 997 = (265*10 + 3) % 997 = 659\n 4   2  6  5  3  5  % ", "start": 787, "end": 787}, "1170": {"text": "6  5  3  % 997 = (265*10 + 3) % 997 = 659\n 4   2  6  5  3  5  % 997 = (659*10 + 5) % 997 = 613\nQR\n7755.3 \u25a0 Substring Search\n Implementation. This discussion leads directly \nto the substring search implementation in Al-\ngorithm 5.8. The constructor computes a hash \nvalue patHash for the pattern; it also computes \nthe value of RM/H110021mod Q in the variable RM. The \nhashSearch() method begins by computing the \nhash function for the \ufb01rst M characters of the text \nand comparing that value against the hash value \nfor the pattern. If that is not a match, it proceeds \nthrough the text string, using the technique above \nto maintain the hash function for the M charac-\nters starting at position i for each i in a variable \ntxtHash and  comparing each new hash value to \npatHash. (An extra Q is added during the txtHash calculation to make sure that every-\nthing stays positive so that the remainder operation works as it should.)  \nA trick:    Monte Carlo correctness. After \ufb01nding a hash value for an M-character sub-\nstring of txt that matches the pattern hash value, you might expect to see code to com-\npare those characters with the pattern to ensure that we have a true match, not just a \nhash collision. We do not do that test because using it requires backup in the text string. \nInstead, we make the hash table \u201csize\u201d Q as large as we wish, since we are not actually \nbuilding a hash table, just testing for a collision with one key, our pattern. We will use \na long value greater than 1020, making the probability that a random key hashes to the \nRabin-Karp ", "start": 787, "end": 788}, "1171": {"text": "actually \nbuilding a hash table, just testing for a collision with one key, our pattern. We will use \na long value greater than 1020, making the probability that a random key hashes to the \nRabin-Karp substring search example \n i   0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9  3\n 0   3  % 997 = 3\n 1   3  1  % 997 = (3*10 + 1) % 997 = 31\n 2   3  1  4  % 997 = (31*10 + 4) % 997 = 314\n 3   3  1  4  1  % 997 = (314*10 + 1) % 997 = 150\n 4   3  1  4  1  5  % 997 = (150*10 + 5) % 997 = 508\n 5      1  4  1  5  9  % 997 = ((508 + 3*(997 - 30))*10 + 9) % 997 = 201\n 6         4  1  5  9  2  % 997 = ((201 + 1*(997 - 30))*10 + 2) % 997 = 715\n 7            1  5  9  2  6  % 997 = ", "start": 788, "end": 788}, "1172": {"text": "= ((201 + 1*(997 - 30))*10 + 2) % 997 = 715\n 7            1  5  9  2  6  % 997 = ((715 + 4*(997 - 30))*10 + 6) % 997 = 971\n 8               5  9  2  6  5  % 997 = ((971 + 1*(997 - 30))*10 + 5) % 997 = 442\n 9                  9  2  6  5  3  % 997 = ((442 + 5*(997 - 30))*10 + 3) % 997 = 929\n10                     2  6  5  3  5  % 997 = ((929 + 9*(997 - 30))*10 + 5) % 997 = 613\nQ\nRM R\n return i-M+1 = 6\n match\nKey computation in Rabin-Karp substring search\n(move right one position in the text)\n i   ...  2  3  4  5  6  7  ...\n       1  4  1  5  9  2  6  5\n          4  1  5  9  2  6  5\n          \n          4  1  5  9  2\n       -  4  0  0  0  0\n             1  5  9  2\n                *  1  0\n          1  5  9  2  0\n                   +  6\n          1  5  9  2  6\ncurrent ", "start": 788, "end": 788}, "1173": {"text": "2\n                *  1  0\n          1  5  9  2  0\n                   +  6\n          1  5  9  2  6\ncurrent value\nsubtract leading digit\nmultiply by radix\nadd new trailing digit\nnew value\ncurrent value\nnew value  text\n776 CHAPTER 5 \u25a0 Strings\n ALGORITHM 5.8   Rabin-Karp fingerprint substring search\npublic class  RabinKarp \n{\n   private String pat;       // pattern (only needed for Las Vegas)\n   private long patHash;     // pattern hash value \n   private int M;            // pattern length\n   private long Q;           // a large prime\n   private int R = 256;      // alphabet size\n   private long RM;          // R^(M-1) % Q                                              \n   public RabinKarp(String pat)\n   {\n      this.pat = pat;        // save pattern (only needed for Las Vegas)\n      this.M = pat.length();\n      Q = longRandomPrime();           // See Exercise 5.3.33.\n      RM = 1;\n      for (int i = 1; i <= M-1; i++)   // Compute R^(M-1) % Q for use\n         RM = (R * RM) % Q;            //   in removing leading digit.\n      patHash = hash(pat, M);\n   }\n   public boolean check(int i)  // Monte Carlo (See text.)\n   {  return true;  }  //   For Las Vegas, check pat vs txt(i..i-M+1).\n   private long hash(String key, int M)\n   // See text (page 775).\n   private int search(String txt)\n   {  // Search for hash match in text.\n      int N = txt.length();\n      long txtHash = hash(txt, M);\n ", "start": 788, "end": 789}, "1174": {"text": "int M)\n   // See text (page 775).\n   private int search(String txt)\n   {  // Search for hash match in text.\n      int N = txt.length();\n      long txtHash = hash(txt, M);\n      if (patHash == txtHash) return 0;            // Match at beginning.\n      for (int i = M; i < N; i++)\n      {  // Remove leading digit, add trailing digit, check for match.\n         txtHash = (txtHash + Q - RM*txt.charAt(i-M) % Q) % Q;\n         txtHash = (txtHash*R + txt.charAt(i)) % Q;\n         if (patHash == txtHash)                    \n           if (check(i - M + 1)) return i - M + 1; // match\n      }\n      return N;                                    // no match found\n    } \n}\nThis substring search algorithm is based on hashing.  It computes a hash value for the pattern in the \nconstructor, then searches through the text looking for a hash match.\n7775.3 \u25a0 Substring Search same value as our pattern less than 10\u201320, an exceedingly small value. If that value is not \nsmall enough for you, you could run the algorithms again to get a probability of fail-\nure of less than 10\u201340. This algorithm is an early and famous example of a Monte Carlo\nalgorithm that has a guaranteed completion time but fails to output a correct answer \nwith a small probability. The alternative method of checking for a match could be slow \n(it might amount to the brute-force algorithm, with a very small probability) but is \nguaranteed correct. Such an algorithm is known as a   Las Vegas algorithm.\n \nProperty P . The Monte Carlo version of  Rabin-Karp substring search is linear-time \nand extremely likely to be correct, and the Las V egas version of Rabin-Karp sub-\nstring search is ", "start": 789, "end": 790}, "1175": {"text": "algorithm.\n \nProperty P . The Monte Carlo version of  Rabin-Karp substring search is linear-time \nand extremely likely to be correct, and the Las V egas version of Rabin-Karp sub-\nstring search is correct and extremely likely to be linear-time.\nDiscussion: The use of the very large value of Q, made possible by the fact that we \nneed not maintain an actual hash table, makes it extremely unlikely that a collision \nwill occur. Rabin and Karp showed that when Q is properly chosen, we get a hash \ncollision for random strings with probability 1/ Q, which implies that, for practi -\ncal values of the variables, there are no hash matches when there are no substring \nmatches and only one hash match if there is a substring match. Theoretically, a text \nposition could lead to a hash collision and not a substring match, but in practice it \ncan be relied upon to \ufb01nd a match.\n  \nIf your belief in probability theory (or in the random string model and the code we \nuse to generate random numbers) is more half-hearted than resolute, you can add to \ncheck() the code to check that the text matches the pattern, which turns Algorithm \n5.8 into the Las V egas version of the algorithm (see Exercise 5.3.12). If you also add a \ncheck to see whether that code is ever executed, you might develop more faith in prob-\nability theory as time wears on.\nRabin-Karp substring search is known as a \ufb01ngerprint search because it uses a small \namount of information to represent a (potentially very large) pattern. Then it looks \nfor this \ufb01ngerprint (the hash value) in the text. The algorithm is ef\ufb01cient because the \n\ufb01ngerprints can be ef\ufb01ciently computed and compared. \n778 CHAPTER 5 \u25a0 Strings\n  \nSummary The table at the bottom ", "start": 790, "end": 791}, "1176": {"text": "hash value) in the text. The algorithm is ef\ufb01cient because the \n\ufb01ngerprints can be ef\ufb01ciently computed and compared. \n778 CHAPTER 5 \u25a0 Strings\n  \nSummary The table at the bottom of the page summarizes the algorithms that we \nhave discussed for substring search. As is often the case when we have several algo-\nrithms for the same task, each of them has attractive features. Brute-force search is easy \nto implement and works well in typical cases (Java\u2019s  indexOf() method in String uses \nbrute-force search); Knuth-Morris-Pratt is guaranteed linear-time with no backup in \nthe input; Boyer-Moore is  sublinear (by a factor of M) in typical situations; and Rabin-\nKarp is linear. Each also has drawbacks: brute-force might require time proportional \nto MN; Knuth-Morris-Pratt and Boyer-Moore use extra space; and Rabin-Karp has a \nrelatively long inner loop (several arithmetic operations, as opposed to character com-\npares in the other methods). These characteristics are summarized in the table below.\nalgorithm version operation count backup\nin input? correct? extra\nspaceguarantee typical\nbrute force \u2014 M N 1.1 N yes yes 1\nKnuth-Morris-Pratt\nfull DFA\n(Algorithm 5.6 ) 2 N 1.1 N no yes MR\nmismatch \ntransitions only 3 N 1.1 N no yes M\nBoyer-Moore\nfull algorithm 3 N N / M yes yes R\nmismatched char \nheuristic only\n(Algorithm 5.7 )\nM N N / M yes yes R\nRabin-Karp\u2020\nMonte Carlo\n(Algorithm 5.8 ) 7 N 7 N no yes \u2020 1\nLas Vegas 7 ", "start": 791, "end": 791}, "1177": {"text": "5.7 )\nM N N / M yes yes R\nRabin-Karp\u2020\nMonte Carlo\n(Algorithm 5.8 ) 7 N 7 N no yes \u2020 1\nLas Vegas 7 N  \u2020 7 N yes yes 1\n\u2020 probabilisitic guarantee, with uniform and independent hash function\nCost summary for substring search implementations\n7795.3 \u25a0 Substring Search\n Q&A\nQ. This substring search problem seems like a bit of a toy problem.  Do I really need to \nunderstand these complicated algorithms?\nA. Well, the factor of  M speedup available with Boyer-Moore can be quite impressive in \npractice. Also, the ability to stream input (no backup) leads to many practical applica-\ntions for KMP and Rabin-Karp. Beyond these direct practical applications, this topic \nprovides an interesting introduction to the use of abstract machines and randomiza-\ntion in algorithm design.  \nQ. Why not simplify things by converting each character to binary, treating all text as \nbinary text?\nA. That idea is not quite effective because of false matches across character boundaries. \n780 CHAPTER 5 \u25a0 Strings\n EXERCISES\n \n \n \n5.3.1  Develop a brute-force substring search implementation Brute, using the same \nAPI as Algorithm 5.6.\n5.3.2  Give the dfa[][] array for the Knuth-Morris-Pratt algorithm for the pattern \nA A A A A A A A A, and draw the DFA, in the style of the \ufb01gures in the text.\n5.3.3  Give the dfa[][] array for the Knuth-Morris-Pratt algorithm for the pattern \nA B R A C A D A B R A, and draw the DFA, in the style of the \ufb01gures in the text.\n5.3.4 Write ", "start": 791, "end": 793}, "1178": {"text": "Knuth-Morris-Pratt algorithm for the pattern \nA B R A C A D A B R A, and draw the DFA, in the style of the \ufb01gures in the text.\n5.3.4 Write an ef\ufb01cient method that takes a string txt and an integer M as arguments \nand returns the position of the \ufb01rst occurrence of M consecutive blanks in the string, \ntxt.length if there is no such occurrence. Estimate the number of character compares \nused by your method, on typical text and in the worst case.\n5.3.5 Develop a brute-force substring search implementation BruteForceRL that pro-\ncesses the pattern from right to left (a simpli\ufb01ed version of Algorithm 5.7).\n5.3.6 Give the right[] array computed by the constructor in Algorithm 5.7 for the \npattern A B R A C A D A B R A.\n5.3.7 Add to our brute-force implementation of substring search a count() method to \ncount occurrences and a searchAll() method to print all occurrences.\n5.3.8 Add to KMP a count() method to count occurrences and a searchAll() method \nto print all occurrences.\n5.3.9 Add to BoyerMoore a count() method to count occurrences and a searchAll()\nmethod to print all occurrences.\n5.3.10 Add to RabinKarp a count() method to count occurrences and a searchAll()\nmethod to print all occurrences.\n5.3.11 Construct a worst-case example for the Boyer-Moore implementation in Algo-\nrithm 5.7 (which demonstrates that it is not linear-time).\n5.3.12  Add the code to check() in RabinKarp (Algorithm 5.8) that turns it into a \nLas V egas algorithm (check that the pattern matches the text at the position given as ", "start": 793, "end": 793}, "1179": {"text": "linear-time).\n5.3.12  Add the code to check() in RabinKarp (Algorithm 5.8) that turns it into a \nLas V egas algorithm (check that the pattern matches the text at the position given as \nargument).\n5.3.13 In the Boyer-Moore implementation in Algorithm 5.7, show that you can set \n7815.3 \u25a0 Substring Search\n  \nright[c] to the penultimate occurrence of c when c is the last character in the pattern.\n5.3.14 Develop versions of the substring search implementations in this section that \nuse char[] instead of String to represent the pattern and the text.\n5.3.15 Design a brute-force substring search algorithm that scans the pattern from \nright to left.\n5.3.16 Show the trace of the brute-force algorithm in the style of the \ufb01gures in the text  \nfor the following pattern and text strings\na. pattern: AAAAAAAB     text: AAAAAAAAAAAAAAAAAAAAAAAAB\nb. pattern: ABABABAB     text: ABABABABAABABABABAAAAAAAA\n5.3.17 Draw the KMP DFA for the following pattern strings.\na. AAAAAAB\nb. AACAAAB\nc. ABABABAB\nd. ABAABAAABAAAB\ne. ABAABCABAABCB\n5.3.18 Suppose that the pattern and text are random strings over an alphabet of size \nR (which is at least 2). Show that the expected number of character compares for the \nbrute-force method is (N /H11002 M + 1) (1 /H11002 R/H11002M) / (1 /H11002 R/H110021) /H11349 2(N /H11002 M + 1).\n5.3.19  Construct an example where the Boyer-Moore ", "start": 793, "end": 794}, "1180": {"text": "R/H11002M) / (1 /H11002 R/H110021) /H11349 2(N /H11002 M + 1).\n5.3.19  Construct an example where the Boyer-Moore algorithm (with only the mis -\nmatched character heuristic) performs poorly.\n5.3.20 How would you modify the Rabin-Karp algorithm to determine whether any of \na subset of k patterns (say, all of the same length) is in the text? \nSolution : Compute the hashes of the k patterns and store the hashes in a StringSET\n(see Exercise 5.2.6). \n5.3.21 How would you modify the Rabin-Karp algorithm to search for a given pattern \nwith the additional proviso that the middle character is a \u201cwildcard\u201d (any text character \nEXERCISES  (continued)\n782 CHAPTER 5 \u25a0 Strings\n at all can match it).\n5.3.22 How would you modify the Rabin-Karp algorithm to search for an H-by-V pat-\ntern in an N-by-N text? \n5.3.23 Write a program that reads characters one at a time and reports at each instant \nif the current string is a  palindrome. Hint : Use the Rabin-Karp hashing idea.\n7835.3 \u25a0 Substring Search\n CREATIVE PROBLEMS\n \n5.3.24  Find all occurrences. Add a method findAll() to each of the four substring \nsearch algorithms given in the text that returns an Iterable<Integer> that allows \nclients to iterate through all offsets of the pattern in the text.\n5.3.25  Streaming. Add a search() method to KMP that takes variable of type In as \nargument, and searches for the pattern in the speci\ufb01ed input stream without using any \nextra instance variables. Then do the same for RabinKarp. \n5.3.26 ", "start": 794, "end": 796}, "1181": {"text": "takes variable of type In as \nargument, and searches for the pattern in the speci\ufb01ed input stream without using any \nextra instance variables. Then do the same for RabinKarp. \n5.3.26   Cyclic rotation check. Write a program that, given two strings, determines \nwhether one is a cyclic rotation of the other, such as example and ampleex.\n5.3.27   Ta n d e m  re p e a t  s e a rc h . A tandem repeat of a base string b in a string s is a \nsubstring of s having at least two consecutive copies b (nonoverlapping). Develop and \nimplement a linear-time algorithm that, given two strings b and s, returns the index of \nthe beginning of the longest tandem repeat of b in s. For example, your program should \nreturn 3 when b is abcab and s is abcabcababcababcababcab. \n5.3.28  Buffering in brute-force search. Add a search() method to your solution to \nExercise 5.3.1 that takes an input stream (of type In) as argument and searches for the \npattern in the given input stream. Note : Y ou need to maintain a buffer that can keep at \nleast the previous M characters in the input stream. Y our challenge is to write ef\ufb01cient \ncode to initialize, update, and clear the buffer for any input stream.\n5.3.29  Buffering in Boyer-Moore. Add a search() method to Algorithm 5.7 that \ntakes an input stream (of type In) as argument and searches for the pattern in the given \ninput stream.\n5.3.30  Two-dime nsional s earch. Implement a version of the Rabin-Karp algorithm to \nsearch for patterns in two-dimensional text. Assume both pattern and text are rect-\nangles of characters.\n5.3.31 ", "start": 796, "end": 796}, "1182": {"text": "stream.\n5.3.30  Two-dime nsional s earch. Implement a version of the Rabin-Karp algorithm to \nsearch for patterns in two-dimensional text. Assume both pattern and text are rect-\nangles of characters.\n5.3.31  Random patterns. How many character compares are needed to do a substring \nsearch for a random pattern of length 100 in a given text?\nAnswer: None. The method\npublic boolean search(char[] txt) \n{  return false; }\n784 CHAPTER 5 \u25a0 Strings\n     int i = -1; \nsm: i++; \ns0: if (txt[i]) != 'A' goto sm; \ns1: if (txt[i]) != 'A' goto s0; \ns2: if (txt[i]) != 'B' goto s0; \ns3: if (txt[i]) != 'A' goto s2; \ns4: if (txt[i]) != 'A' goto s0; \ns5: if (txt[i]) != 'A' goto s3;\n    return i-8;\nStraight-line substring search for A A B A A A\n \nis quite effective for this problem, since the chances of a random pattern of length 100 \nappearing in any text are so low that you may consider it to be 0.\n5.3.32  Unique substrings. Solve Exercise 5.2.14 using the idea behind the Rabin-\nKarp method.\n5.3.33    Random  primes. Implement longRandomPrime() for RabinKarp (Algorithm \n5.8). Hint : A random n-digit number is prime with probability proportional to 1/n.\n5.3.34  Straight-line code. The Java Virtual Machine (and your computer\u2019s assembly \nlanguage) support a goto instruction so that the search can be \u201cwired in\u2019\u2019 to machine \ncode, like the ", "start": 796, "end": 797}, "1183": {"text": "1/n.\n5.3.34  Straight-line code. The Java Virtual Machine (and your computer\u2019s assembly \nlanguage) support a goto instruction so that the search can be \u201cwired in\u2019\u2019 to machine \ncode, like the program at right (which is exactly equiva-\nlent to simulating the DFA for the pattern as in KMPdfa, \nbut likely to be much more ef\ufb01cient). T o avoid check -\ning whether the end of the text has been reached each \ntime i is incremented, we assume that the pattern it-\nself is stored at the end of the text as a sentinel, as the \nlast M characters of the text. The goto labels in this code \ncorrespond precisely to the dfa[] array. Write a static \nmethod that takes a pattern as input and produces as \noutput a straight-line program like this that searches for \nthe pattern.\n5.3.35  Boyer-Moore in binary strings. The mismatched character heuristic does not \nhelp much for binary strings, because there are only two possibilities for characters that \ncause the mismatch (and these are both likely to be in the pattern). Develop a substring \nsearch class for binary strings that groups bits together to make \u201ccharacters\u2019\u2019 that can \nbe used exactly as in Algorithm 5.7. Note : If you take b bits at a time, then you need \na right[] array with 2b entries. The value of b should be chosen small enough so that \nthis table is not too large, but large enough that most b-bit sections of the text are not \nlikely to be in the pattern\u2014there are M/H11002b/H110011 different b-bit sections in the pattern (one \nstarting at each bit position from 1 through M/H11002b/H110011), so we want M/H11002b/H110011 to be sig-\nni\ufb01cantly less than 2b. ", "start": 797, "end": 797}, "1184": {"text": "pattern (one \nstarting at each bit position from 1 through M/H11002b/H110011), so we want M/H11002b/H110011 to be sig-\nni\ufb01cantly less than 2b. For example, if you take 2b to be about lg (4M), then the right[]\narray will be more than three-quarters \ufb01lled with -1 entries, but do not let b become less \nthan M/2, since otherwise you could miss the pattern entirely, if it were split between   \ntwo b-bit text sections.\n7855.3 \u25a0 Substring Search\n EXPERIMENTS\n5.3.36  Random text. Write a program that takes integers M and N as arguments, gener-\nates a random binary text string of length N, then counts the number of other occur-\nrences of the last M bits elsewhere in the string. Note : Different methods may be appro-\npriate for different values of M.\n5.3.37  KMP for random text. Write a client that takes integers M, N, and T as input and \nruns the following experiment T times: Generate a random pattern of length M and a \nrandom text of length N, counting the number of character compares used by KMP to \nsearch for the pattern in the text. Instrument KMP to provide the number of compares, \nand print the average count for the T trials.\n5.3.38  Boyer-Moore for random text. Answer the previous exercise for BoyerMoore.\n5.3.39  Timings. Write a program that times the four methods for the task of searchng \nfor the substring\nit is a far far better thing that i do than i have ever done\nin the text of Ta l e  of  Tw o  Ci t i e s (tale.txt). Discuss the extent to which your results \nvalidate the hypthotheses ", "start": 797, "end": 798}, "1185": {"text": "that i do than i have ever done\nin the text of Ta l e  of  Tw o  Ci t i e s (tale.txt). Discuss the extent to which your results \nvalidate the hypthotheses about performance that are stated in the text.\n786 CHAPTER 5 \u25a0 Strings\n This page intentionally left blank \n 5.4   REGULAR EXPRESSIONS\n \nIN many applications, we need to do substring searching with somewhat less than \ncomplete information about the pattern to be found. A user of a text editor may wish \nto specify only part of a pattern, or to specify a pattern that could match a few different \nwords, or to specify that any one of a number of patterns would do. A biologist might \nsearch for a genomic sequence satisfying certain conditions. In this section we will con-\nsider how pattern matching of this type can be done ef\ufb01ciently.\nThe algorithms in the previous section fundamentally depend on complete speci\ufb01-\ncation of the pattern, so we have to consider different methods. The basic mechanisms \nwe will consider make possible a very powerful string-searching facility that can match \ncomplicated M-character patterns in N-character text strings in time proportional to \nMN in the worst case, and much faster for typical applications.\nFirst, we need a way to describe the patterns: a rigorous way to specify the kinds of \npartial-substring-searching problems suggested above. This speci\ufb01cation needs to in-\nvolve more powerful primitive operations than the \u201ccheck if the i th character of the text \nstring matches the j th character of the pattern\u2019\u2019 operation used in the previous section. \nFor this purpose, we use regular expressions, which describe patterns in combinations \nof three natural, basic, and powerful operations.\nProgrammers have used regular expressions for decades. With the explosive growth \nof search opportunities on the web, their use is becoming even more widespread. We \nwill discuss a number of ", "start": 798, "end": 800}, "1186": {"text": "\nof three natural, basic, and powerful operations.\nProgrammers have used regular expressions for decades. With the explosive growth \nof search opportunities on the web, their use is becoming even more widespread. We \nwill discuss a number of speci\ufb01c applications at the beginning of the section, not only \nto give you a feeling for their utility and power, but also to enable you to become more \nfamiliar with their basic properties.\nAs with the KMP algorithm in the previous section, we consider the three basic op -\nerations in terms of an abstract machine that can search for patterns in a text string. \nThen, as before, our pattern-matching algorithm will construct such a machine and \nthen simulate its operation. Naturally, pattern-matching machines are typically more \ncomplicated than KMP DFAs, but not as complicated as you might expect.\nAs you will see, the solution we develop to the pattern-matching problem is inti -\nmately related to fundamental processes in computer science. For example, the method \nwe will use in our program to perform the string-searching task implied by a given \npattern description is akin to the method used by the Java system to transform a given \nJava program into a machine-language program for your computer. We also encounter \nthe concept of nondeterminism, which plays a critical role in the search for ef\ufb01cient \nalgorithms (see Chapter 6).\n788\n  \n \n \n \nDescribing patterns with regular expressions We focus on pattern descr ip-\ntions made up of characters that serve as operands for three fundamental operations. \nIn this context, we use the word language speci\ufb01cally to refer to a set of strings (pos-\nsibly in\ufb01nite) and the word pattern to refer to a language speci\ufb01cation. The rules that \nwe consider are quite analogous to familiar rules for specifying arithmetic expressions.\n C o n c a t e n a t i o ", "start": 800, "end": 801}, "1187": {"text": "in\ufb01nite) and the word pattern to refer to a language speci\ufb01cation. The rules that \nwe consider are quite analogous to familiar rules for specifying arithmetic expressions.\n C o n c a t e n a t i o n .  The \ufb01rst fundamental operation is the one used in the last section. \nWhen we write AB , we are specifying the language { AB } that has one two-character \nstring, formed by concatenating A and B.\n O r .  The second fundamental operation allows us to specify alternatives in the pattern. \nIf we have an or between two alternatives, then both are in the language. We will use the \nvertical bar symbol | to denote this operation. For example, A|B  s p e c i \ufb01 e s  t h e  l a n g u a g e  \n{ A , B }  and A|E|I|O|U speci\ufb01es the language  { A , E , I , O , U }. Concatenation has \nhigher  precedence than or, so AB|BCD speci\ufb01es the language  { AB , BCD } .\n C l o s u r e .  The third fundamental operation allows parts of the pattern to be repeated \narbitrarily. The closure of a pattern is the language of strings formed by concatenating \nthe pattern with itself any number of times (including zero). We denote closure by \nplacing a * after the pattern to be repeated. Closure has higher precedence than con-\ncatenation, so AB*  s p e c i \ufb01 e s  t h e  l a n g u a g e  c o n s i s t i n g  o f  s t r i n g s  w i t h  a n  A followed by 0\nor more Bs, while A*B  s p e c i \ufb01 ", "start": 801, "end": 801}, "1188": {"text": "s t i n g  o f  s t r i n g s  w i t h  a n  A followed by 0\nor more Bs, while A*B  s p e c i \ufb01 e s  t h e  l a n g u a g e  c o n s i s t i n g  o f  s t r i n g s  w i t h  0 or more As \nfollowed by a B. The   empty string, which we denote by /H9280, is found in every text string \n(and in A*).\n P a r e n t h e s e s .  We use parentheses to over r ide the default precedence rules. For exam-\nple, C(AC|B)D  s p e c i \ufb01 e s  t h e  l a n g u a g e  {  CACD , CBD }; (A|C)((B|C)D)  s p e c i-\n\ufb01es the language { ABD ,  CBD ,  ACD ,  CCD }; and (AB)*  s p e c i \ufb01 e s  t h e  l a n g u a g e  o f  \nstrings formed by concatenating \nany number  of occurrences of \nAB , including no occurrences: \n{ /H9280,  AB ,  ABAB ,  . . .}.\nThese simple rules allow us \nto write down REs that, while \ncomplicated, clearly and com -\npletely describe languages (see \nthe table at right for a few ex -\namples). Often, a language can be simply described in some other way, but discovering \nsuch a description can be a challenge. For example, the RE in the bottom row of the \ntable speci\ufb01es the subset of (A|B)*  with an even number of Bs.\nRE ", "start": 801, "end": 801}, "1189": {"text": "way, but discovering \nsuch a description can be a challenge. For example, the RE in the bottom row of the \ntable speci\ufb01es the subset of (A|B)*  with an even number of Bs.\nRE matches does not match\n(A|B)(C|D) AC AD BC BD every other string\nA(B|C)*D AD ABD ACD ABCCBD BCD ADD ABCBC \nA* | (A*BA*BA*)* AAA BBAABB BABAAA ABA BBB BABBAAA\nExamples of regular expressions\n7895.4 \u25a0 Regular Expressions\n Regular expressions are extremely simple formal objects, even simpler than the \narithmetic expressions that you learned in grade school. Indeed, we will take advantage \nof their simplicity to develop compact and ef\ufb01cient algorithms for processing them. \nOur starting point will be the following formal de\ufb01nition:\nDefinition. A  regular expression (RE) is either\n\u25a0 Empty\n\u25a0 A single character\n\u25a0 A regular expression enclosed in parentheses\n\u25a0 Two or more concatenated regular expressions\n\u25a0 Two or more regular expressions separ ated by the or operator (|)\n\u25a0 A regular expression followed by the closure operator (*)\nThis de\ufb01nition describes the syntax of regular expressions, telling us what constitutes \na legal regular expression. The semantics that tells us the meaning of a given regular \nexpression is the point of the informal descriptions that we have given in this section. \nFor review, we summarize these by continuing the formal de\ufb01nition:\n Definition (continued). Each RE represents a set of strings, de\ufb01ned as follows:\n\u25a0 The empty RE represents the empty set of strings, with 0 elements.\n\u25a0 A character represents the set of strings with one element, itself.\n\u25a0 An RE enclosed in parentheses represents the same set of strings as the RE \nwithout the parentheses.\n\u25a0 The RE consisting of two concatenated REs represents the cross ", "start": 801, "end": 802}, "1190": {"text": "elements.\n\u25a0 A character represents the set of strings with one element, itself.\n\u25a0 An RE enclosed in parentheses represents the same set of strings as the RE \nwithout the parentheses.\n\u25a0 The RE consisting of two concatenated REs represents the cross product of \nthe sets of strings represented by the individual components (all possible \nstrings that can be formed by taking one string from each and concatenating \nthem, in the same order as the REs).\n\u25a0 The RE consisting of the or of two REs represents the union of the sets rep-\nresented by the individual components.\n\u25a0 The RE consisting of the closure of an RE represents /H9280 (the empty string) \nor the union of the sets represented by the concatenation of any number of \ncopies of the RE.\nIn general, the language described by a given RE might be very large, possibly in\ufb01nite. \nThere are many different ways to describe each language: we must try to specify succinct \npatterns just as we try to write compact programs and implement ef\ufb01cient algorithms. \n790 CHAPTER 5 \u25a0 Strings\n  \n \n S h o r t c u t s  Typical applications adopt various additions to these basic rules to en -\nable us to develop succinct descriptions of languages of practical interest. From a theo-\nretical standpoint, these are each simply a shortcut for a sequence of operations involv-\ning many operands; from a practical standpoint, they are a quite useful extention to the \nbasic operations that enable us to develop compact patterns.\nSet-of-characters descriptors. It is often convenient \nto be able to use a single character or a short sequence \nto directly specify sets of characters. The dot character \n(.) is a  wildcard that represents any single character. A \nsequence of characters within square brackets repre-\nsents any one of those characters. The sequence may \nalso be speci\ufb01ed as a range of characters. If preceded by \na ^, a ", "start": 802, "end": 803}, "1191": {"text": "character. A \nsequence of characters within square brackets repre-\nsents any one of those characters. The sequence may \nalso be speci\ufb01ed as a range of characters. If preceded by \na ^, a sequence within square brackets represents any \ncharacter but one of those characters. These notations \nare simply shortcuts for a sequence of or operations.\nClosure shortcuts. The closure operator speci\ufb01es any \nnumber of copies of its operand. In practice, we want the \ufb02exibility to specify the num-\nber of copies, or a range on the number. In particular, we use the plus sign (+) to specify \nat least one copy, the question mark ( ?) to specify zero or one copy, and a count or a \nrange within braces ({}) to specify a given number of copies. Again, these notations are \nshortcuts for a sequence of the basic concatenation, or, and closure operations.\nEscape sequences. Some characters, such as  \\,   .,  |,  *,  (,  and ),  are metacha-\nracters  that we use to form regular expressions. We use escape sequences  that begin \nwith a backslash character \\ separating metacharacters from characters in the alphabet. \nAn escape sequence may be a \\ followed by a single metacharacter (which represents \nthat character). For example, \\\\ represents \\. Other escape sequences represent special \ncharacters and whitespace. For example, \\t represents a tab character, \\n represents a \nnewline, and \\s represents any whitespace character.\nname notation example\nwildcard . A.B\nspecified set enclosed in [] [AEIOU]*\nrange enclosed in [] \nseparated by -\n[A-Z] \n[0-9]\ncomplement enclosed in [] \npreceded by ^\n[^AEIOU]*\nSet-of-characters descriptors\noption notation example shortcut for in language not in language\nat ", "start": 803, "end": 803}, "1192": {"text": "\nseparated by -\n[A-Z] \n[0-9]\ncomplement enclosed in [] \npreceded by ^\n[^AEIOU]*\nSet-of-characters descriptors\noption notation example shortcut for in language not in language\nat least 1 + (AB)+ (AB)(AB)* AB ABABAB  /H9280 BBBAAA\n0 or 1 ? (AB)? /H9280  | AB /H9280  AB any other string\n specific count in {} (AB){3} (AB)(AB)(AB) ABABAB any other string\nrange range in {} (AB){1-2} (AB)|(AB)(AB) AB ABAB any other string\nClosure shortcuts (for specifying the number of copies of the operand)\n7915.4 \u25a0 Regular Expressions\n REs in applications REs have proven to be remarkably versatile in describing lan-\nguages that are relevant in practical applications. Accordingly, REs are heavily used and \nhave been heavily studied. T o familiarize you with regular expressions while at the same \ntime giving you some appreciation for their utility, we consider a number of practical \napplications before addressing the RE pattern-matching algorithm. REs also play an \nimportant role in theoretical computer science. Discussing this role to the extent it \ndeserves is beyond the scope of this book, but we sometimes allude to relevant funda-\nmental theoretical results. \nSubstring search. Our general goal is to develop an algorithm that determines wheth-\ner a given text string is in the set of strings described by a given regular expression. If a \ntext is in the language described by a pattern, we say that the text matches the pattern. \nPattern matching with REs vastly generalizes the substring search problem of Section \n5.3. Precisely, to search for a substring pat in a text string txt is to check whether txt\nis in the language described by the pattern . * ", "start": 803, "end": 804}, "1193": {"text": "REs vastly generalizes the substring search problem of Section \n5.3. Precisely, to search for a substring pat in a text string txt is to check whether txt\nis in the language described by the pattern . * pat. * or not.   \nVa l i d i t y  c h e c k i n g . Yo u  f re q u e n t l y  e n co u n t e r  R E  m a t c h i n g  w h e n  yo u  u s e  t h e  w e b. \nWhen you type in a date or an account number on a commercial website, the input-\nprocessing program has to check that your response is in the right format. One ap-\nproach to performing such a check is to write code that checks all the cases: if you were \nto type in a dollar amount, the code might check that the \ufb01rst symbol is a $, that the \n$ is followed by a set of digits, and so forth. A better approach is to de\ufb01ne an RE that \ndescribes the set of all legal inputs. Then, checking whether your input is legal is pre-\ncisely the pattern-matching problem: is your input in the language described by the RE? \nLibraries of REs for common checks have sprung up on the web as this type of checking \nhas come into widespread use. Typically, an RE is a much more precise and concise ex-\npression of the set of all valid strings than would be a program that checks all the cases.\ncontext regular expression matches\nsubstring search .*NEEDLE.* A HAYSTACK NEEDLE IN\nphone number \\([0-9]{3}\\)\\ [0-9]{3}-[0-9]{4} (800) 867-5309\nJava identifier [$_A-Za-z][$_A-Za-z0-9]* ", "start": 804, "end": 804}, "1194": {"text": "\\([0-9]{3}\\)\\ [0-9]{3}-[0-9]{4} (800) 867-5309\nJava identifier [$_A-Za-z][$_A-Za-z0-9]* Pattern_Matcher\ngenome marker gcg(cgg|agg)*ctg gcgaggaggcggcggctg\nemail address [a-z]+@([a-z]+\\.)+(edu|com) rs@cs.princeton.edu\nTypical regular expressions in applications (simplified versions)\n792 CHAPTER 5 \u25a0 Strings\n  \n \n \nProgrammer\u2019s toolbox. The origin of regular expression pattern matching is the Unix \ncommand grep, which prints all lines matching a given RE. This capability has proven \ninvaluable for generations of programmers, and REs are built into many modern pro-\ngramming systems, from awk and emacs to Perl, Python, and JavaScript. For example, \nsuppose that you have a directory with dozens of .java \ufb01les, and you want to know \nwhich of them has code that uses StdIn. The command\n% grep StdIn *.java\nwill immediately give the answer. It prints all lines that match .*StdIn.* for each \ufb01le.  \nGenomics. Biologists use REs to help address important scienti\ufb01c problems. For \nexample, the human gene sequence has a region that can be described with the RE \ngcg(cgg)*ctg, where the number of repeats of the cgg pattern is highly variable \namong individuals, and a certain genetic disease that can cause mental retardation and \nother symptoms is known to be associated with a high number of repeats.\nSearch. Web search eng ines suppor t REs, thoug h not always in their full g lor y. Ty pi-\ncally, if you want to specify alternatives with | or repetition with *, you can do so.\nPossibilities. A \ufb01rst introduction ", "start": 804, "end": 805}, "1195": {"text": "REs, thoug h not always in their full g lor y. Ty pi-\ncally, if you want to specify alternatives with | or repetition with *, you can do so.\nPossibilities. A \ufb01rst introduction to theoretical computer science is to think about the \nset of languages that can be speci\ufb01ed with an RE. For example, you might be surprised \nto know that you can implement the modulus operation with an RE: for example, \n(0 | 1(01*0)*1)*  d e s c r i b e s  a l l  s t r i n g s  o f  0s and 1s that are the binary repre-\nsentatons of numbers that are multiples of three (!): 11 , 110 , 1001 , and 1100  a r e  \nin the language, but 10 , 1011 , and 10000  are not. \nLimitations. Not all languages can be speci\ufb01ed with REs. A thought-provoking ex -\nample is that no RE can describe the set of all strings that specify legal REs. Simpler \nversions of this example are that we cannot use REs to check whether parentheses are \nbalanced or to check whether a string has an equal number of As and Bs. \nThese examples just scratch the surface. Suf\ufb01ce it to say that REs are a useful part \nof our computational infrastructure and have played an important role in our under -\nstanding of the nature of computation. As with KMP , the algorithm that we describe \nnext is a byproduct of the search for that understanding.\n7935.4 \u25a0 Regular Expressions\n  \n  N o n d e t e r m i n i s t i c  \ufb01 n i t e - s t a t e  a u t o m a t a  Recall that we can ", "start": 805, "end": 806}, "1196": {"text": "Expressions\n  \n  N o n d e t e r m i n i s t i c  \ufb01 n i t e - s t a t e  a u t o m a t a  Recall that we can view the Knuth-\nMorris-Pratt algorithm as a \ufb01nite-state machine constructed from the search pattern \nthat scans the text. For regular expression pattern matching, we will generalize this idea.\nThe \ufb01nite-state automaton for KMP changes from state to state by looking at a char-\nacter from the text string and then changing to another state, depending on the char -\nacter. The automaton reports a match if and only if it reaches the accept state. The al-\ngorithm itself is a simulation of the automaton. The characteristic of the machine that \nmakes it easy to simulate is that it is deterministic: each state transition is completely \ndetermined by the next character in the text.\nTo  h a n d l e  re g u l a r  e x p re s s i o n s , w e  co n s i d e r  a  m o re  p ow e r f u l  a b s t r a c t  m a c h i n e . B e-\ncause of the or operation, the automaton cannot determine whether or not the pattern \ncould occur at a given point by examining just one character; indeed, because of clo -\nsure, it cannot even determine how many characters might need to be examined before \na mismatch is discovered. T o overcome these problems, we will endow the automaton \nwith the power of  nondeterminism: when faced with more than one way to try to match \nthe pattern, the machine can \u201cguess\u2019\u2019 the right one! This power might seem to you to \nbe impossible to realize, but we will see that it is easy to write a program to build a \nnondeterministic ", "start": 806, "end": 806}, "1197": {"text": "\nthe pattern, the machine can \u201cguess\u2019\u2019 the right one! This power might seem to you to \nbe impossible to realize, but we will see that it is easy to write a program to build a \nnondeterministic \ufb01nite-state automaton (NFA) and to ef\ufb01ciently simulate its operation. \nThe overview of our RE pattern matching algorithm is the nearly the same as for KMP:\n\u25a0 Build the NFA corresponding to the given RE.\n\u25a0 \n \nSimulate the operation of that NFA on the given text.\nKleene\u2019s Theorem, a fundamental result of theoretical computer science, asserts that \nthere is an NFA corresponding to any given RE (and vice versa). We will consider a \nconstructive proof of this fact that will demonstrate how to transform any RE into an \nNFA; then we simulate the operation of the NFA to complete the job.\nBefore we consider how to build pattern-matching NFAs, we will consider an exam-\nple that illustrates their properties and the basic rules for operating them. Consider the \n\ufb01gure below, which shows an NFA that determines whether a text string is in the lan-\nguage described by the RE ((A*B|AC)D) . As illustrated in this example, the NFAs \nthat we de\ufb01ne have the following characteristics:\n\u25a0 The NFA corresponding to an RE of length M has exactly one state per pattern \ncharacter, starts at state 0, and has a (virtual) accept state M.\nNFA corresponding to the pattern   ( ( A * B | A C ) D )\n( ( A * B | A C ) D )\n0       1       2       3       4       5       6       7       8       9       10      11\naccept statestart state\n794 CHAPTER 5 \u25a0 Strings\n \u25a0 States corresponding to a character from the alphabet ", "start": 806, "end": 807}, "1198": {"text": "4       5       6       7       8       9       10      11\naccept statestart state\n794 CHAPTER 5 \u25a0 Strings\n \u25a0 States corresponding to a character from the alphabet have an outgoing edge \nthat goes to the state corresponding to the next character in the pattern (black \nedges in the diagram).\n\u25a0 States corresponding to the metacharacters (, ), |, and * have at least one outgo-\ning edge (red edges in the diagram), which may go to any other state.\n\u25a0 \n \n \nSome states have multiple outgoing edges, but no state has more than one out-\ngoing black edge.\nBy convention, we enclose all patterns in parentheses, so the \ufb01rst state corresponds to a \nleft parenthesis and the \ufb01nal state corresponds to a right parenthesis (and has a transi-\ntion to the accept state).\nAs with the DFAs of the previous section, we start the NFA at state 0, reading the \n\ufb01rst character of a text. The NFA moves from state to state, sometimes reading text \ncharacters, one at a time, from left to right. However, there are some basic differences \nfrom DFAs:\n\u25a0 Characters appear in the nodes, not the edges, in the diagrams.\n\u25a0 Our NFA recognizes a text string only after explicitly reading all its characters, \nwhereas our DFA recognizes a pattern in a text without necessarily reading all \nthe text characters.\nThese differences are not critical\u2014we have picked the version of each machine that is \nbest suited to the algorithms that we are studying.\nOur focus now is on checking whether the text matches the pattern\u2014for that, we \nneed the machine to reach its accept state and consume all the text. The rules for mov-\ning from one state to another are also different than for DFAs\u2014an NFA can do so in \none of two ways: \n\u25a0 If the current state corresponds to a character in the ", "start": 807, "end": 807}, "1199": {"text": "the text. The rules for mov-\ning from one state to another are also different than for DFAs\u2014an NFA can do so in \none of two ways: \n\u25a0 If the current state corresponds to a character in the alphabet and the current \ncharacter in the text string matches the character, the automaton can scan past \nthe character in the text string and take the (black) transition to the next state. \nWe refer to such a transition as a  match transition.\n\u25a0 The automaton can follow any red edge to another state without scanning any \ntext character. We refer to such a transition as an   /H9280-transition, referring to the \nidea that it corresponds to \u201cmatching\u201d the empty string /H9280.\nFinding a pattern with    ( ( A * B | A C ) D ) NFA\n      A     A     A     A     B        D\n0  1  2  3  2  3  2  3  2  3  4  5  8  9  10  11\naccept state reached\nand all text characters scanned:\nNFA recognizes text\nmatch transition:\nscan to next input character\nand change state\n-transition:\nchange state\nwith no match\n7955.4 \u25a0 Regular Expressions\n  \n \n \nFor example, suppose that our NFA for \n( ( A * B | A C ) D )  is started (at state 0) \nwith the text A A A A B D as input. The \ufb01gure \nat the bottom of the previous page shows a se-\nquence of state transitions ending in the accept \nstate. This sequence demonstrates that the text \nis in the set of strings described by the RE\u2014the \ntext matches the pattern. With respect to the \nNFA, we say that the NFA recognizes that text.\nThe examples shown at left illustrate that it ", "start": 807, "end": 808}, "1200": {"text": "the text \nis in the set of strings described by the RE\u2014the \ntext matches the pattern. With respect to the \nNFA, we say that the NFA recognizes that text.\nThe examples shown at left illustrate that it \nis also possible to \ufb01nd transition sequences that \ncause the NFA to stall, even for input text such \nas A A A A B D  that it should recognize. For \nexample, if the NFA takes the transition to state \n4 before scanning all the As, it is left with nowhere to go, since the only way out of state \n4 is to match a B. These two examples demonstrate the nondeterministic nature of the \nautomaton. After scanning an A and \ufb01nding itself in state 3, the NFA has two choices: \nit could go on to state 4 or it could go back to state 2. The choices make the difference \nbetween getting to the accept state (as in the \ufb01rst example just discussed) or stalling (as \nin the second example just discussed). This NFA also has a choice to make at state 1 \n(whether to take an /H9280-transition to state 2 or to state 6).\nThese examples illustrate the key difference between NFAs and DFAs: since an NFA \nmay have multiple edges leaving a given state, the transition from such a state is not \ndeterministic\u2014it might take one transition at one point in time and a different transi-\ntion at a different point in time, without scanning past any text character. T o make \nsome sense of the operation of such an automaton, imagine that an NFA has the power \nto guess which transition (if any) will lead to the accept state for the given text string. \nIn other words, we say that an NFA recognizes a text string if and only if there is some   \nsequence of transitions that scans all the text ", "start": 808, "end": 808}, "1201": {"text": "(if any) will lead to the accept state for the given text string. \nIn other words, we say that an NFA recognizes a text string if and only if there is some   \nsequence of transitions that scans all the text characters and ends in the accept state when \nstarted at the beginning of the text in state 0 . Conversely, an NFA does not recognize a \ntext string if and only if there is no sequence of match transitions and /H9280-transitions that \ncan scan all the text characters and lead to the accept state for that string.\nAs with DFAs, we have been tracing the operation of the NFA on a text string simply \nby listing the sequence of state changes, ending in the \ufb01nal state. Any such sequence is a \nproof that the machine recognizes the text string (there may be other proofs). But how \ndo we \ufb01nd such a sequence for a given text string? And how do we prove that there is no \nsuch sequence for another given text string?  The answers to these questions are easier \nthan you might think: we systematically try all possibilities.\nStalling sequences for ( ( A * B | A C ) D ) NFA\nno way out\nof state 4\nno way out\nof state 4\n     A     A     A\n0  1  2  3  2  3  4\nno way out\nof state 7\nwrong guess if input is\nA  A  A  A  B  D\n     A\n0  1  6  7\n     A     A     A     A     C\n0  1  2  3  2  3  2  3  2  3  4\n796 CHAPTER 5 \u25a0 Strings\n  \n S i m u l a t i n g  a n  N F A ", "start": 808, "end": 809}, "1202": {"text": "2  3  2  3  2  3  4\n796 CHAPTER 5 \u25a0 Strings\n  \n S i m u l a t i n g  a n  N F A  The idea of an automaton that can guess the state transitions \nit needs to get to the accept state is like writing a program that can guess the right an-\nswer to a problem: it seems ridiculous. On re\ufb02ection, you will see that the task is con-\nceptually not at all dif\ufb01cult: we make sure that we check all possible sequences of state \ntransitions, so if there is one that gets to the accept state, we will \ufb01nd it.\nRepresentation. To  b e g i n , w e  n e e d  a n  N FA  re p re s e n t a t i o n . T h e  c h o i ce  i s  c l e a r : t h e  \nRE itself gives the state names (the integers between 0 and M, where M is the number of \ncharacters in the RE). We keep the RE itself in an array re[] of char values that de-\n\ufb01nes the match transitions (if re[i] is in the alphabet, then there is a match transition \nfrom i to i+1). The natural representation for the  /H9280-transitions is a digraph\u2014they are \ndirected edges (red edges in our diagrams) connecting vertices between 0 and M (one \nfor each state). Accordingly, we represent all the /H9280-transitions as a digraph G. We will \nconsider the task of building the digraph associated with a given RE after we consider \nthe simulation process. For our example, the digraph consists of the nine edges\n0 \u2192 1  1 \u2192 2  1 \u2192 6  2 \u2192 3 ", "start": 809, "end": 809}, "1203": {"text": "with a given RE after we consider \nthe simulation process. For our example, the digraph consists of the nine edges\n0 \u2192 1  1 \u2192 2  1 \u2192 6  2 \u2192 3  3 \u2192 2  3 \u2192 4  5 \u2192 8  8 \u2192 9  10 \u2192 11\nNFA simulation and reachability. To  s i m u l a t e  a n  N FA , w e  ke e p  t r a c k  o f  t h e  set of \nstates that could possibly be encountered while the automaton is examining the cur -\nrent input character. The key computation is the familiar  multiple-source reachability\ncomputation that we addressed in Algorithm 4.4 (page 571). T o initialize this set, we \ufb01nd \nthe set of states reachable via /H9280-transitions from state 0. For each such state, we check \nwhether a match transition for the \ufb01rst input character is possible. This check gives \nus the set of possible states for the NFA just after matching the \ufb01rst input character. \nTo  t h i s  s e t , w e  a d d  a l l  s t a te s  t h a t  co u l d  b e  re a c h e d  v i a  /H9280-transitions from one of the \nstates in the set. Given the set of possible states for the NFA just after matching the \ufb01rst \ncharacter in the input, the solution to the multiple-source reachability problem in the \n-transition digraph gives the set of states that could lead to match transitions for the \nsecond character in the input. For example, the initial set of states for our example NFA \nis 0 1 2 3 4 6; ", "start": 809, "end": 809}, "1204": {"text": "gives the set of states that could lead to match transitions for the \nsecond character in the input. For example, the initial set of states for our example NFA \nis 0 1 2 3 4 6; if the \ufb01rst character is an A, the NFA could take a match transition to \n3 or 7; then it could take /H9280-transitions from 3 to 2 or 3 to 4, so the set of possible states \nthat could lead to a match transition for the second character is 2 3 4 7. Iterating this \nprocess until all text characters are exhausted leads to one of two outcomes:\n\u25a0 The set of possible states contains the accept state.\n\u25a0 The set of possible states does not contain the accept state.\nThe \ufb01rst of these outcomes indicates that there is some sequence of transitions that \ntakes the NFA to the accept state, so we report success. The second of these outcomes \nindicates that the NFA always stalls on that input, so we report failure. With our SET \n7975.4 \u25a0 Regular Expressions\n Simulation of   ( ( A * B | A C ) D ) NFA  for input   A A B D \n( ( A * B | A C ) D )\n0       1       2       3       4       5       6       7       8       9       10      11\n0 1 2 3 4 6 : set of states reachable via /H9280-transitions from start\n( ( A * B | A C ) D )\n0       1       2       3       4       5       6       7       8       9       10      11\n  3 7 : set of states reachable after matching A\n( ( A * B | A C ", "start": 809, "end": 810}, "1205": {"text": "4       5       6       7       8       9       10      11\n  3 7 : set of states reachable after matching A\n( ( A * B | A C ) D )\n0       1       2       3       4       5       6       7       8       9       10      11\n2 3 4 7 : set of states reachable via /H9280-transitions after matching A\n( ( A * B | A C ) D )\n0       1       2       3       4       5       6       7       8       9       10      11\n    3 : set of states reachable after matching A A\n( ( A * B | A C ) D )\n0       1       2       3       4       5       6       7       8       9       10      11\n  2 3 4 : set of states reachable via /H9280-transitions after matching A A \n( ( A * B | A C ) D )\n0       1       2       3       4       5       6       7       8       9       10      11\n    5 : set of states reachable after matching A A B\n( ( A * B | A C ) D )\n0       1       2       3       4       5       6       7       8       9       10      11\n  5 8 9 : set of states reachable via /H9280-transitions after matching A A B\n( ( A * B | A C ) D )\n0       1       2       3       4       5       6       7 ", "start": 810, "end": 810}, "1206": {"text": "via /H9280-transitions after matching A A B\n( ( A * B | A C ) D )\n0       1       2       3       4       5       6       7       8       9       10      11\n   10 : set of states reachable after matching A A B D\n( ( A * B | A C ) D )\n0       1       2       3       4       5       6       7       8       9       10      11\n  10 11 : set of states reachable via /H9280-transitions after matching A A B D\naccept !\n798 CHAPTER 5 \u25a0 Strings\n data type and the DirectedDFS class just described for computing multiple-source \nreachability in a digraph, the NFA simulation code given below is a straightforward \ntranslation of the English-language description just given. Y ou can check your under-\nstanding of the code by following the trace on the facing page, which illustrates the full \nsimulation for our example.\nProposition Q. Determining whether an N-character text string is recognized by \nthe  NFA corresponding to an M-character RE takes time proportional to NM in \nthe worst case.\nProof: For each of the N text characters, we iterate through a set of states of size \nno more than M and run a DFS on the digraph of /H9280-transitions. The construction \nthat we will consider next establishes that the number of edges in that digraph is no \nmore than 2M, so the worst-case time for each DFS is proportional to M.\nTake a moment to re\ufb02ect on this remar kable result. This worst-case cost, the product \nof the text and pattern lengths, is the same as the worst-case cost of \ufb01nding an exact \nsubstring match using the el-\nementary algorithm that we ", "start": 810, "end": 811}, "1207": {"text": "kable result. This worst-case cost, the product \nof the text and pattern lengths, is the same as the worst-case cost of \ufb01nding an exact \nsubstring match using the el-\nementary algorithm that we \nstarted with at the beginning \nof Section 5.3.\npublic boolean  recognizes(String txt) \n{  // Does the NFA recognize txt?\n   Bag<Integer> pc = new Bag<Integer>();\n   DirectedDFS dfs = new DirectedDFS(G, 0);\n   for (int v = 0; v < G.V(); v++)\n      if (dfs.marked(v)) pc.add(v);\n   for (int i = 0; i < txt.length(); i++)\n   {  // Compute possible NFA states for txt[i+1].\n      Bag<Integer> match = new Bag<Integer>();\n      for (int v : pc)\n         if (v < M)\n            if (re[v] == txt.charAt(i) || re[v] == '.')\n                match.add(v+1);\n      pc = new Bag<Integer>();\n      dfs = new DirectedDFS(G, match);\n      for (int v = 0; v < G.V(); v++)\n         if (dfs.marked(v)) pc.add(v);\n   }\n   for (int v : pc) if (v == M) return true;\n   return false; \n}\n N F A  s i m u l a t i o n  f o r  p a t t e r n  m a t c h i n g\n7995.4 \u25a0 Regular Expressions\n   B u i l d i n g  a n  N F A  c o r r e s p o n d i n g  t o  a n  R E  From the similarity between regu-\nlar expressions and familiar arithmetic expressions, you may not be surprised to \ufb01nd \nthat translating an RE to an NFA is somewhat ", "start": 811, "end": 812}, "1208": {"text": "n g  t o  a n  R E  From the similarity between regu-\nlar expressions and familiar arithmetic expressions, you may not be surprised to \ufb01nd \nthat translating an RE to an NFA is somewhat similar to the process of evaluating an \narithmetic expression using Dijkstra\u2019s two-stack algorithm, which we considered in \nSection 1.3. The process is a bit different because\n\u25a0 REs do not have an explicit operator for concatenation\n\u25a0 REs have a unary operator, for closure (*)\n\u25a0 REs have only one binary operator, for or (|)\nRather than dwell on the differences and similarities, we will consider an implementa-\ntion that is tailored for REs. For example, we need only one stack, not two.\nFrom the discussion of the representation at the beginning of the previous subsec -\ntion, we need only build the digraph G that consists of all the /H9280-transitions. The RE itself \nand the formal de\ufb01nitions that we considered at the beginning of this section provide \nprecisely the information that we need. Taking a cue from Dijkstra\u2019s algorithm, we will \nuse a stack to keep track of the positions of left parentheses and or operators.\nConcatenation. In terms of the NFA, the concatenation operation is the simplest to \nimplement. Match transitions for states corresponding to characters in the alphabet \nexplicitly implement concatenation.\nParentheses. We push the RE index of  each left parenthesis on the stack. Each time we \nencounter a right parenthesis, we eventually pop the corresponding left parentheses \nfrom the stack in the manner described below. As in Dijkstra\u2019s algorithm, the stack en-\nables us to handle nested parentheses in a natural manner.\nClosure. A closure (*) operator must occur either (i ) after a single character, when we \nadd /H9280-transitions to and from the character, or (ii ) after a ", "start": 812, "end": 812}, "1209": {"text": "nested parentheses in a natural manner.\nClosure. A closure (*) operator must occur either (i ) after a single character, when we \nadd /H9280-transitions to and from the character, or (ii ) after a right parenthesis, when we \nadd /H9280-transitions to and from the corresponding left parenthesis, the one at the top of \nthe stack.\nOr expression. We process an RE of  the form (A | B) where A and B are both REs by \nadding two /H9280-transitions: one from the state corresponding to the left parenthesis to the \nstate corresponding to the \ufb01rst character of B and one from the state corresponding to \nthe | operator to the state corresponding to the right parenthesis. We push the RE index \ncorresponding the | operator onto the stack (as well as the index corresponding to the \nleft parenthesis, as described above) so that the information we need is at the top of the \nstack when needed, at the time we reach the right parenthesis. These /H9280-transitions allow \nthe NFA to choose one of the two alternatives. We do not add an /H9280-transition from the \nstate corresponding to the | operator to the state with the next higher index, as we have \nfor all other states\u2014the only way for the NFA to leave such a state is to take a transition \nto the state corresponding to the right parenthesis.\n800 CHAPTER 5 \u25a0 Strings\n These simple rules suffice to build NFAs corresponding to arbitrarily complicated \nREs. Algorithm 5.9 is an implementation whose constructor builds the /H9280-transition \ndigraph corresponding to a given RE, and a trace of the construction for our example \nappears on the following page. Y ou can \ufb01nd other examples at the bottom of this page \nand in the exercises and are encouraged to enhance your understanding of the process \nby working your own examples. For brevity ", "start": 812, "end": 813}, "1210": {"text": "\nappears on the following page. Y ou can \ufb01nd other examples at the bottom of this page \nand in the exercises and are encouraged to enhance your understanding of the process \nby working your own examples. For brevity and for clarity, a few details (handling \nmetacharacters, set-of-character descriptors, closure shortcuts, and multiway or op -\nerations) are left for exercises (see Exercises 5.4.16 through 5.4.21). Otherwise, the \nconstruction requires remarkably little code and represents one of the most ingenious \nalgorithms that we have seen.\nNFA construction rules\n( | )\nA *\niorlp\nG.addEdge(i, i+1);\nG.addEdge(i+1, i);\nG.addEdge(lp, i+1);\nG.addEdge(i+1, lp);\nlp i       i+1\ni       i+1\n( . . .\n... ...\n) *\nsingle-character closure\nclosure expression\nG.addEdge(lp, or+1);\nG.addEdge(or, i);\nor expression\nNFA corresponding to the pattern   (  .  *  A  B  (  (  C  |  D  *  E  )  F  )  *  G  ) \nA*.( B ( ( C | D E ) F ) * G )\n0      1      2      3      4      5      6      7      8      9     10     11     12     13     14     15     16     17  \n*\n8015.4 \u25a0 Regular Expressions\n ALGORITHM 5.9   Regular expression pattern matching (grep).\npublic class  NFA \n{\n   private char[] re;           // match transitions\n   private Digraph G;           // epsilon transitions\n   private int M;               // number of states\n   public NFA(String regexp)\n   { ", "start": 813, "end": 814}, "1211": {"text": "NFA \n{\n   private char[] re;           // match transitions\n   private Digraph G;           // epsilon transitions\n   private int M;               // number of states\n   public NFA(String regexp)\n   {  // Create the NFA for the given regular expression.  \n      Stack<Integer> ops = new Stack<Integer>();\n      re = regexp.toCharArray();\n      M = re.length;\n      G = new Digraph(M+1);\n      for (int i = 0; i < M; i++)\n      {\n         int lp = i;\n         if (re[i] == '(' || re[i] == '|')\n            ops.push(i);\n         else if (re[i] == ')')\n         {\n            int or = ops.pop();\n            if (re[or] == '|')\n            {\n               lp = ops.pop();\n               G.addEdge(lp, or+1);\n               G.addEdge(or, i);\n            }\n            else lp = or;\n         }\n         if (i < M-1 && re[i+1] == '*')  // lookahead \n         {\n            G.addEdge(lp, i+1);\n            G.addEdge(i+1, lp);\n         }\n         if (re[i] == '(' || re[i] == '*' || re[i] == ')')\n            G.addEdge(i, i+1);\n      }\n   }\n   public boolean recognizes(String txt)\n   // Does the NFA recognize txt? (See page 799.) \n}\nThis constructor builds an NFA corresponding to a given RE by creating a digraph of /H9280-transitions. \n802 CHAPTER 5 \u25a0 Strings Building the NFA corresponding to  ( ( A * B | A C ) D ) \n( ( A * B | A C ) D )\n0       1       2       3       4       5       6       7       8       9       10      11\n( ( A * B ", "start": 814, "end": 815}, "1212": {"text": "| A C ) D )\n0       1       2       3       4       5       6       7       8       9       10      11\n( ( A * B | A C ) D )\n0       1       2       3       4       5       6       7       8       9       10\n( ( A * B | A C ) D\n0       1       2       3       4       5       6       7       8       9\n( ( A * B | A C )\n0       1       2       3       4       5       6       7       8\n( ( A * B | A C\n0       1       2       3       4       5       6       7\n( ( A * B | A\n0       1       2       3       4       5       6\n( ( A * B |\n0       1       2       3       4       5\n( ( A * B\n0       1       2       3       4\n( ( A *\n0       1       2       3\n( ( A\n0       1       2\n( (\n0       1\n(\n0\n0\nstack for\nindices of\nleft parentheses\nand ors\n(ops)\ni\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n5\n0\n1\n5     \n0\n1\n5\n0\n0\n8035.4 \u25a0 Regular Expressions\n Proposition R. Building the  NFA corresponding to an M-character RE takes time \nand space proportional to M in the worst case.\nProof. For each of the ", "start": 815, "end": 816}, "1213": {"text": "\n0\n1\n5\n0\n0\n8035.4 \u25a0 Regular Expressions\n Proposition R. Building the  NFA corresponding to an M-character RE takes time \nand space proportional to M in the worst case.\nProof. For each of the M RE characters in the regular expression, we add at most \nthree /H9280-transitions and perhaps execute one or two stack operations.\nThe classic  GREP client for pattern matching, illustrated in the code at left, takes an RE \nas argument and prints the lines from standard input having some substring that is \nin the language described by the RE. \nThis client was a feature in the early \nimplementations of Unix and has \nbeen an indispensable tool for gen-\nerations of programmers.\npublic class  GREP \n{  \n   public static void main(String[] args)\n   {\n      String regexp = \"(.*\" + args[0] + \".*)\";\n      NFA nfa = new NFA(regexp);\n      while (StdIn.hasNextLine())\n      {\n         String txt = StdIn.hasNextLine();\n         if (nfa.recognizes(txt))\n            StdOut.println(txt);\n      }\n   } \n}\n C l a s s i c  Generalized Regular Expression Pattern-matching NFA client\n% more tinyL.txt\nAC \nAD \nAAA \nABD \nADD \nBCD \nABCCBD \nBABAAA \nBABBAAA\n% java GREP \"(A*B|AC)D\" < tinyL.txt \nABD \nABCCBD\n% java GREP StdIn < GREP.java\n     while (StdIn.hasNextLine())\n        String txt = StdIn.hasNextLine();\n804 CHAPTER 5 \u25a0 Strings\n Q&A\nQ. What is the difference between null and /H9280?\nA. The former denotes an empty set; the latter denotes an  empty string. You can have a \nset that contains one element, /H9280, ", "start": 816, "end": 817}, "1214": {"text": "Q&A\nQ. What is the difference between null and /H9280?\nA. The former denotes an empty set; the latter denotes an  empty string. You can have a \nset that contains one element, /H9280, and is therefore not null.\n8055.4 \u25a0 Regular Expressions\n EXERCISES\n5.4.1 Give regular expressions that describe all strings that contain \n\u25a0 Exactly four consecutive As\n\u25a0 No more than four consecutive As\n\u25a0 At least one occurrence of four consecutive As\n5.4.2 Give a brief English description of each of the following REs: \na. .* \nb. A.*A | A \nc. .*ABBABBA.*\nd. .* A.*A.*A.*A.*\n5.4.3 What is the maximum number of different strings that can be described by a \nregular expression with M or operators and no closure operators (parentheses and con-\ncatenation are allowed)?\n5.4.4  Draw the NFA corresponding to the pattern (((A|B)*|CD*|EFG)*)* .\n5.4.5 Draw the digraph of /H9280-transitions for the NFA from Exercise 5.4.4.\n5.4.6 Give the sets of states reachable by your NFA from Exercise 5.4.4 after each \ncharacter match and susbsequent /H9280-transitions for the input ABBACEFGEFGCAAB .\n5.4.7 Modify the GREP client on page 804 to be a client GREPmatch  that encloses the pat-\ntern in parentheses but does not add .* before and after the pattern, so that it prints \nout only those lines that are strings in the language described by the given RE. Give the \nresult of typing each of the following commands: \na. % java GREPmatch \"(A|B)(C|D)\" ", "start": 817, "end": 818}, "1215": {"text": "prints \nout only those lines that are strings in the language described by the given RE. Give the \nresult of typing each of the following commands: \na. % java GREPmatch \"(A|B)(C|D)\" < tinyL.txt\nb. % java GREPmatch \"A(B|C)*D\" < tinyL.txt \nc. % java GREPmatch \"(A*B|AC)D\" < tinyL.txt\n5.4.8 Write a regular expression for each of the following sets of binary strings:\na. Contains at least three consecutive 1s\nb. Contains the substring 110\nc. Contains the substring 1101100\nd. Does not contain the substring 110\n806 CHAPTER 5 \u25a0 Strings\n 5.4.9 Write a regular expression for binary strings with at least two 0s but not con -\nsecutive 0s.\n5.4.10 Write a regular expression for each of the following sets of binary strings: \na. Has at least 3 characters, and the third character is 0\nb. Number of 0s is a multiple of 3\nc. Starts and ends with the same character\nd. Odd length\ne. Starts with 0 and has odd length, or starts with 1 and has even length\nf. Length is at least 1 and at most 3\n5.4.11 For each of the following regular expressions, indicate how many bitstrings of \nlength exactly 1,000 match:\na. 0(0 | 1)*1\nb. 0*101*\nc. (1 | 01)*\n5.4.12 Write a Java regular expression for each of the following:\na. Phone numbers, such as (609) 555-1234\nb. Social Security numbers, such as 123-45-6789\nc. Dates, such as December 31, ", "start": 818, "end": 819}, "1216": {"text": "for each of the following:\na. Phone numbers, such as (609) 555-1234\nb. Social Security numbers, such as 123-45-6789\nc. Dates, such as December 31, 1999 \nd. IP addresses of the form a.b.c.d where each letter can represent one, two, \nor three digits, such as 196.26.155.241\ne. License plates that start with four digits and end with two uppercase letters\n8075.4 \u25a0 Regular Expressions\n CREATIVE PROBLEMS\n5.4.13  Challenging REs. Construct an RE that describes each of the following sets of \nstrings over the binary alphabet:\na. All strings except 11 or 111 \nb. Strings with 1 in every odd-number bit position \nc. Strings with at least two 0s and at most one 1  \nd. Strings with no two consecutive 1s  \n5.4.14  Binary divisibility. Construct an RE that describes all binary strings that when \ninterpreted as a binary number are\na. Divisible by 2 \nb. Divisible by 3 \nc. Divisible by 123 \n5.4.15  One-level REs. Construct a Java RE that describes the set of strings that are \nlegal REs over the binary alphabet, but with no occurrence of parentheses within pa-\nrentheses. For example, (0.*1)* or (1.*0)* is in this language, but (1(0 or 1)1)*\nis not.\n5.4.16    Multiway or. Add multiway or to NFA. Your code should produce the machine \ndrawn below for the pattern (.*AB((C|D|E)F)*G) .\nNFA corresponding to the pattern   (  .  *  A  B  (  (  C ", "start": 819, "end": 820}, "1217": {"text": "the machine \ndrawn below for the pattern (.*AB((C|D|E)F)*G) .\nNFA corresponding to the pattern   (  .  *  A  B  (  (  C  |  D |  E  )  F  )  *  G  ) \nA*.( B ( ( C | D | E ) F ) * G )\n0      1      2      3      4      5      6      7      8      9     10     11     12     13     14     15     16     17  \n808 CHAPTER 5 \u25a0 Strings\n 5.4.17  Wildcard. Add to NFA the capability to handle wildcards.\n5.4.18  One or more. Add to NFA the capability to handle the + closure operator.\n5.4.19  Speci\ufb01ed set. Add to NFA the capability to handle speci\ufb01ed-set descriptors.\n5.4.20  Range. Add to NFA the capability to handle range descriptors.\n5.4.21    Complement. Add to NFA the capability to handle complement descriptors.\n5.4.22    Proof. Develop a version of NFA that prints a proof that a given string is in the \nlanguage recognized by the NFA (a sequence of state transitions that ends in the accept \nstate).\n8095.4 \u25a0 Regular Expressions\n 5.5   DATA COMPRESSION\n \n  \n \nThe world is awash with data, and algorithms designed to represent data ef\ufb01ciently play \nan important role in the modern computational infrastructure.  There are two primary \nreasons to compress data: to save storage when saving information and to save time \nwhen communicating information. Both of these reasons have remained important \nthrough many generations of technology and are familiar today to anyone needing a ", "start": 820, "end": 822}, "1218": {"text": "are two primary \nreasons to compress data: to save storage when saving information and to save time \nwhen communicating information. Both of these reasons have remained important \nthrough many generations of technology and are familiar today to anyone needing a \nnew storage device or waiting for a long download.\nYo u  h ave  ce r t a i n l y  e n co u n t e re d  co m p re s s i o n  w h e n  wo r k i n g  w i t h  d i g i t a l  i m a g e s , \nsound, movies, and all sorts of other data. The algorithms we will examine save space \nby exploiting the fact that most data \ufb01les have a great deal of redundancy: For example, \ntext \ufb01les have certain character sequences that appear much more often than others; \nbitmap \ufb01les that encode pictures have large homogeneous areas; and \ufb01les for the digital \nrepresentation of images, movies, sound, and other analog signals have large repeated \npatterns.\nWe w ill look at an elementar y algor ithm and two advanced  methods that are w idely \nused. The compression achieved by these methods varies depending on characteristics \nof the input. Savings of 20 to 50 percent are typical for text, and savings of 50 to 90 per-\ncent might be achieved in some situations. As you will see, the effectiveness of any data \ncompression method is quite dependent on characteristics of the input. Note : Usually, \nin this book, we are referring to time when we speak of performance; with data com-\npression we normally are referring to the compression they can achieve, although we \nwill also pay attention to the time required to do the job.\nOn the one hand, data-compression techniques are less important than they once \nwere because the cost of computer storage devices has dropped dramatically and far \nmore storage is ", "start": 822, "end": 822}, "1219": {"text": "\nwill also pay attention to the time required to do the job.\nOn the one hand, data-compression techniques are less important than they once \nwere because the cost of computer storage devices has dropped dramatically and far \nmore storage is available to the typical user than in the past. On the other hand, data-\ncompression techniques are more important than ever because, since so much storage \nis in use, the savings they make possible are greater.  Indeed, data compression has come \ninto widespread use with the emergence of the internet, because it is a low-cost way to \nreduce the time required to transmit large amounts of data.\nData compression has a rich history (we will only be providing a brief introduction \nto the topic), and contemplating its role in the future is certainly worthwhile. Every stu-\ndent of algorithms can bene\ufb01t from studying data compression because the algorithms \nare classic, elegant, interesting, and effective. \n810\n  \nRules of the game All of the types of data that we process with modern computer \nsystems have something in common:  they are ultimately represented in binary.  We can \nconsider each of them to be simply a sequence of bits (or bytes). For brevity, we use the \nterm bitstream in this section to refer to a sequence of bits and bytestream when we are \nreferring to the bits being considered as a sequence of \ufb01xed-size bytes. A bitstream or a \nbytestream might be stored as a \ufb01le on your computer, or it might be a message being \ntransmitted on the internet.\nBasic model. Accordingly, our basic model for data compression is quite simple, hav-\ning two primary components, each a black box that reads and writes bitstreams:\n\u25a0 A compress box that transforms a bitstream B into a compressed version C (B )\n\u25a0 An expand box that transforms C (B ) back into B\nUsing the notation | B | to denote the number of ", "start": 822, "end": 823}, "1220": {"text": "bitstreams:\n\u25a0 A compress box that transforms a bitstream B into a compressed version C (B )\n\u25a0 An expand box that transforms C (B ) back into B\nUsing the notation | B | to denote the number of bits in a bitstream, we are interested in \nminimizing the quantity | C (B ) |  / | B |, which is known as the compression ratio. \nBasic model for data compression\nCompress Expand\nbitstream B\n0110110101...\noriginal bitstream B\n0110110101...\ncompressed version C(B)\n1101011111...\n \n \n  \nlossless compression\u2014we insist that no information be lost, in This model is known as    \nthe speci\ufb01c sense that the result of compressing and expanding a bitstream must match \nthe original, bit for bit. Lossless compression is required for many types of \ufb01les, such as \nnumerical data or executable code. For some types of \ufb01les (such as images, videos, or \nmusic), it is reasonable to consider compression methods that are allowed to lose some \ninformation, so the decoder only produces an approximation of the original \ufb01le.   Lossy \nmethods have to be evaluated in terms of a subjective quality standard in addition to \nthe compression ratio.We do not address lossy compression in this book. \nReading and writing  binary data A full description of how information is en-\ncoded on your computer is system-dependent and is beyond our scope, but with a few \nbasic assumptions and two simple APIs, we can separate our implementations from \nthese details. These APIs,   BinaryStdIn and BinaryStdOut, are modeled on the StdIn\nand StdOut APIs that you have been using, but their purpose is to read and write bits, \nwhere StdIn and StdOut are oriented toward character streams encoded in Unicode. \nAn int value on StdOut is a sequence of characters (its decimal representation); an ", "start": 823, "end": 823}, "1221": {"text": "using, but their purpose is to read and write bits, \nwhere StdIn and StdOut are oriented toward character streams encoded in Unicode. \nAn int value on StdOut is a sequence of characters (its decimal representation); an int\nvalue on BinaryStdOut is a sequence of bits (its binary representation).\n8115.5 \u25a0 Data Compression\n Binary input and output. Most systems nowadays, including Java, base their I/O on \n8-bit bytestreams, so we might decide to read and write bytestreams to match I/O for-\nmats with the internal representations of primitive types, encoding an 8-bit char with \n1 byte, a 16-bit short with 2 bytes, a 32-bit int with 4 bytes, and so forth. Since bit-\nstreams are the primary abstraction for data compression, we go a bit further to allow \nclients to read and write individual bits, intermixed with data of primitive types. The \ngoal is to minimize the necessity for type conversion in client programs and also to take \ncare of operating system conventions for representing data. We use the following API \nfor reading a bitstream from standard input:  \npublic class  BinaryStdIn \nboolean readBoolean() read 1 bit of data and return as a boolean value\nchar readChar() read 8 bits of data and return as a char value\nchar readChar(int r) read r (between 1 and 16) bits of data\nand return as a char value\n[similar methods for byte (8 bits); short (16 bits); int (32 bits); long and double (64 bits)]\nboolean isEmpty() is the bitstream empty?\nvoid close() close the bitstream\nAPI for static methods that read from a bitstream on standard input\nA key feature of the abstraction is that, in marked constrast to StdIn, the data on stan-\ndard input is not necessarily aligned on byte boundaries . If the ", "start": 823, "end": 824}, "1222": {"text": "methods that read from a bitstream on standard input\nA key feature of the abstraction is that, in marked constrast to StdIn, the data on stan-\ndard input is not necessarily aligned on byte boundaries . If the input stream is a sin -\ngle byte, a client could read it 1 bit at a time with eight calls to readBoolean(). The   \nclose() method is not essential, but, for clean termination, clients should call close()\nto indicate that no more bits are to be read. As with StdIn/StdOut, we use the follow-\ning complementary API for writing bitstreams to standard output: \npublic class  BinaryStdOut \nvoid write(boolean b) write the specified bit\nvoid write(char c) write the specified 8-bit char\nvoid write(char c, int r) write the r (between 1 and 16) least significant bits \nof the specified char\n[similar methods for byte (8 bits); short (16 bits); int (32 bits); long and double (64 bits)]\nvoid close() close the bitstream\nAPI for static methods that write to a bitstream on standard output\n812 CHAPTER 5 \u25a0 Strings\n  \n \nFor output, the close() method is essential: clients must call close() to ensure that \nall of the bits speci\ufb01ed in prior write() calls make it to the bitstream and that the \ufb01nal \nbyte is padded with 0s to byte-align the output for compatibility with the \ufb01le system.   \nAs with the In and Out APIs associated with StdIn and StdOut, we also have available \nBinaryIn and BinaryOut that allows us to reference binary-encoded \ufb01les directly. \nExample. As a simple example, suppose that you have a data type where a date is rep-\nresented as three int values (month, day, year). Using StdOut to write those values in \nthe format 12/31/1999 ", "start": 824, "end": 825}, "1223": {"text": "simple example, suppose that you have a data type where a date is rep-\nresented as three int values (month, day, year). Using StdOut to write those values in \nthe format 12/31/1999 requires 10 characters, or 80 bits. If you write the values directly \nwith BinaryStdOut, you would produce 96 bits (32 bits for each of the 3 int values); \nif you use a more economical representation that uses byte values for the month and \nday and a short value for the year, you would produce 32 bits. With BinaryStdOut you \ncould also write a 4-bit \ufb01eld, a 5-bit \ufb01eld, and a 12-bit \ufb01eld, for a total of 21 bits (24 \nbits, actually, because \ufb01les must be an integral number of 8-bit bytes, so close() adds \nthree 0 bits at the end). Important note: Such economy, in itself, is a crude form of data \ncompression.\n B i n a r y  d u m p s .  How can we examine the contents of a bitstream or a bytestream while \ndebugging? This question faced early programmers when the only way to \ufb01nd a bug was \nto examine each of the bits in memory, and the term  dump has been used since the early \ndays of computing to describe a human-readable view of a bitstream. If you try to open \nFour ways to put a date onto standard output\n110011111011111001111 000\na 4-bit field, a 5-bit field, and a 12-bit field (BinaryStdOut)\nBinaryStdOut.write(month, 4);\nBinaryStdOut.write(day, 5);\nBinaryStdOut.write(year, 12);\ntwo chars and a short (BinaryStdOut)\nBinaryStdOut.write((char) month);\nBinaryStdOut.write((char) ", "start": 825, "end": 825}, "1224": {"text": "(BinaryStdOut)\nBinaryStdOut.write(month, 4);\nBinaryStdOut.write(day, 5);\nBinaryStdOut.write(year, 12);\ntwo chars and a short (BinaryStdOut)\nBinaryStdOut.write((char) month);\nBinaryStdOut.write((char) day);\nBinaryStdOut.write((short) year);\n00000000000000000000000000001100 00000000000000000000000000011111 00000000000000000000011111001111\nthree ints (BinaryStdOut)\nBinaryStdOut.write(month);\nBinaryStdOut.write(day);\nBinaryStdOut.write(year);\na character stream (StdOut)\nStdOut.print(month + \"/\" + day + \"/\" + year);\n00001100000111110000011111001111\n12 31 1999\n00110001001100100010111100110111001100010010111100110001 001110010011100100111001\n1 2 / 3 1 / 1 9 9 9\n12 31 1999 12 31 1999\n80 bits\n8-bit ASCII representation of '9'\n32-bit integer representation of 31\n32 bits 21 bits ( + 3 bits for byte alignment at close)\n96 bits\n8135.5 \u25a0 Data Compression\n a \ufb01le with an editor or view it in \nthe manner in which you view text \n\ufb01les (or just run a program that \nuses BinaryStdOut), you are like-\nly to see gibberish, depending on \nthe system you use. BinaryStdIn\nallows us to avoid such system de-\npendencies by writing our own \nprograms to convert bitstreams \nsuch that we can see them with \nour standard tools. For example, \nthe program BinaryDump at left is \na BinaryStdIn client that prints \nout the bits from standard in-\nput, encoded with the ", "start": 825, "end": 826}, "1225": {"text": "bitstreams \nsuch that we can see them with \nour standard tools. For example, \nthe program BinaryDump at left is \na BinaryStdIn client that prints \nout the bits from standard in-\nput, encoded with the characters \n0 and 1. This program is useful \nfor debugging when working with \nsmall inputs. The similar client \nHexDump groups the data into 8-bit bytes and prints each as two hexadecimal digits \nthat each represent 4 bits. The client  PictureDump displays the bits in a  Picture with \n0 bits represented as white pixels and 1 bits represented as black pixels. This pictorial \nrepresentation is often useful in identifying patterns in a bitstream. You can download \n \nBinaryDump, HexDump, and PictureDump from the booksite. Typically, we use piping \nand redirection at the command-line level when working with binary \ufb01les: we can pipe \nthe output of an encoder to BinaryDump,  HexDump, or PictureDump, or redirect it to a \n\ufb01le. \npublic class  BinaryDump \n{\n   public static void main(String[] args)\n   {\n      int width = Integer.parseInt(args[0]);\n      int cnt;\n      for (cnt = 0; !BinaryStdIn.isEmpty(); cnt++)\n      {\n         if (width == 0) continue;\n         if (cnt != 0 && cnt % width == 0)\n            StdOut.println();\n         if (BinaryStdIn.readBoolean())\n              StdOut.print(\"1\");\n         else StdOut.print(\"0\");\n      }\n      StdOut.println();\n      StdOut.println(cnt + \" bits\");\n   } \n}\nPrinting a bitstream on standard (character) output\nFour ways to look at a bitstream\nstandard character stream\nbitstream represented as 0 and 1 characters\nbitstream represented with hex digits\nbitstream represented as pixels in a Picture\n16-by-6 pixel\nwindow, magnified\n% ", "start": 826, "end": 826}, "1226": {"text": "a bitstream\nstandard character stream\nbitstream represented as 0 and 1 characters\nbitstream represented with hex digits\nbitstream represented as pixels in a Picture\n16-by-6 pixel\nwindow, magnified\n% more abra.txt\nABRACADABRA!\n% java PictureDump 16 6 < abra.txt\n96 bits\n% java BinaryDump 16 < abra.txt\n0100000101000010\n0101001001000001\n0100001101000001\n0100010001000001\n0100001001010010\n0100000100100001\n96 bits\n%  java HexDump 4 < abra.txt\n41 42 52 41\n43 41 44 41\n42 52 41 21\n96 bits\n814 CHAPTER 5 \u25a0 Strings\n  \n A S C I I  e n c o d i n g .  When you HexDump a bit-\nstream that contains ASCII-encoded charac-\nters, the table at right is useful for reference.   \nGiven a two digit hex number, use the \ufb01rst \nhex digit as a row index and the second hex \ndigit as a column index to \ufb01nd the character \nthat it encodes. For example, 31 encodes the \ndigit 1, 4A encodes the letter J, and so forth. \nThis table is for 7-bit ASCII, so the \ufb01rst hex \ndigit must be 7 or less. Hex numbers starting \nwith 0 and 1 (and the numbers 20 and 7F) \ncorrespond to non-printing control charac -\nters. Many of the control characters are left over from the days when physical devices \nsuch as typewriters were controlled by ASCII input; the table highlights a few that you \nmight see in dumps. For example, SP is the space character, NUL is the null ", "start": 826, "end": 827}, "1227": {"text": "over from the days when physical devices \nsuch as typewriters were controlled by ASCII input; the table highlights a few that you \nmight see in dumps. For example, SP is the space character, NUL is the null character, LF\nis line feed, and CR is carriage return. \nIn summary, working with data compression requires us to reorient our thinking about \nstandard input and standard output to include binary encoding of data. BinaryStdIn \nand BinaryStdOut provide the methods that we need. They provide a way for you to \nmake a clear distinction in your client programs between writing out information in-\ntended for \ufb01le storage and data transmission (that will be read by programs) and print-\ning information (that is likely to be read by humans). \n 0 1 2 3 4 5 6 7 8 9 A B C D E  F  \n0 NUL SOH STX ETX EOT ENQ ACK BEL BS HT LF VT FF CR SO SI\n1 DLE DC1 DC2 DC3 DC4 NAK SYN ETB CAN EM SUB ESC FS GS RS US\n2 SP !\"#$%&\u2018()*+,-.  /\n3 0123456789:;<=>  ?\n4 @ABCDEFGHIJKLMN  O\n5 PQRSTUVWXYZ[\\]^  _\n6 `abcdefghijklmn  o\n7 pqrstuvwxyz{|}~  DEL\n H e x a d e c i m a l - t o - A S C I I  c o n v e r s i o n  t a b l e\n8155.5 \u25a0 Data Compression\n Limitations To  a p p re c i a te  d a t a - co m p re s s i o n  a l g o r i t h m s , yo u  n e e d  to  u n d e r s t a n d    \nfundamental ", "start": 827, "end": 828}, "1228": {"text": "t a - co m p re s s i o n  a l g o r i t h m s , yo u  n e e d  to  u n d e r s t a n d    \nfundamental limitations. Researchers have developed a thorough and important theo-\nretical basis for this purpose, which we will consider brie\ufb02y at the end of this section, \nbut a few ideas will help us get started.\n  U n i v e r s a l  d a t a  c o m p r e s s i o n .  Armed with the algorithmic tools that have proven so \nuseful for so many problems, you might think that our goal should be universal data \ncompression: an algorithm that can make any bitstream smaller. Quite to the contrary, \nwe have to adopt more modest goals because universal data compression is impossible.\n Proposition S. No algorithm can  compress every bitstream.\nProof: We consider two proofs that each prov ide some insig ht. \nThe \ufb01rst is by contradiction: Suppose that you have an algorithm \nthat does compress every bitstream. Then you could use that algo-\nrithm to compress its output to get a still shorter bitstream, and \ncontinue until you have a bistream of length 0! The conclusion \nthat your algorithm compresses every bitstream to 0 bits is absurd, \nand so is the assumption that it can compress every bitstream.\nThe second proof is a counting argument. Suppose that you have \nan algorithm that claims lossless compression for every 1,000-bit \nstream. That is, every such stream must map to a different shorter \none. But there are only 1 + 2 + 4 + ... + 2 998 + 2999 = 21000/H110021 bit-\nstreams with fewer than 1,000 bits and 21000 bitstreams with 1,000 ", "start": 828, "end": 828}, "1229": {"text": "+ 2 + 4 + ... + 2 998 + 2999 = 21000/H110021 bit-\nstreams with fewer than 1,000 bits and 21000 bitstreams with 1,000 \nbits, so your algorithm cannot compress all of them. This argu-\nment becomes more persuasive if we consider stronger claims. Say \nyour goal is to achieve better than a 50 percent compression ratio. \nYo u  h ave  t o  k n ow  t h a t  yo u  w i l l  b e  s u cce s s f u l  f o r  o n l y  a b o u t  1  o u t  \nof 2500 of the 1,000-bit bitstreams!\nPut another way, you have at most a 1 in 2500 chance of being able to \ncompress by half a random 1,000-bit stream with any data-compres -\nsion algorithm.  When you run across a new lossless compression al-\ngorithm, it is a sure bet that it will not achieve signi\ufb01cant compression \nfor a random bitstream. The insight that we cannot hope to compress \nrandom streams is a start to understanding data compression. We \nregularly process strings of millions or billions of bits but will never \nprocess even the tiniest fraction of all possible such strings, so we need \nUniversal \ndata compression?\n.\n.\n.\nU\nU\nU\nU\nU\nU\n816 CHAPTER 5 \u25a0 Strings\n  \nnot be discouraged by this theoretical result. Indeed, the bitstrings that we regularly \nprocess are typically highly structured, a fact that we can exploit for compression.\n  U n d e c i d a b i l i t y .  Consider the million-bit string pictured at the top of this page. This \nstring appears to be random, so you are not likely to \ufb01nd ", "start": 828, "end": 829}, "1230": {"text": "compression.\n  U n d e c i d a b i l i t y .  Consider the million-bit string pictured at the top of this page. This \nstring appears to be random, so you are not likely to \ufb01nd a lossless compression algo-\nrithm that will compress it. But there is a way to represent that string with just a few \nthousand bits, because it was produced by the program below.  (This program is an ex-\nample of a pseudo-random number generator, like Java\u2019s Math.random() method.) A \ncompression algorithm that compresses by writing the program in ASCII and expands \nby reading the program and then running it achieves a .3 percent compression ratio, \nwhich is dif\ufb01cult to beat (and we can drive the ratio arbitrarily low by writing more \nbits). T o compress such a \ufb01le is to discover the program that produced it. This example \nis not so far-fetched as it \ufb01rst appears: when you compress a video or an old book \nthat was digitized with a scanner or any of countless other types of \ufb01les from the web, \nyou are discovering something about the program that produced the \ufb01le. The realiza -\ntion that much of the data that we process is \nproduced by a program leads to deep issues \nin the theory of computation and also gives \ninsight into the challenges of data compres-\nsion. For example, it is possible to prove that \noptimal data compression (\ufb01nd the short-\nest program to produce a given string) is an \nundecidable problem: not only can we not \nhave an algorithm that compresses every bit-\nstream, but also we cannot have a strategy \nfor developing the best algorithm!\nA difficult file to compress: 1 million (pseudo-) random bits\n% java RandomBits | java PictureDump 2000 500\n1000000 bits\npublic class ", "start": 829, "end": 829}, "1231": {"text": "have a strategy \nfor developing the best algorithm!\nA difficult file to compress: 1 million (pseudo-) random bits\n% java RandomBits | java PictureDump 2000 500\n1000000 bits\npublic class RandomBits \n{\n   public static void main(String[] args)\n   {\n      int x = 11111;\n      for (int i = 0; i < 1000000; i++)\n      {\n         x = x * 314159 + 218281;\n         BinaryStdOut.write(x > 0);\n      }\n      BinaryStdOut.close();\n   } \n}\nA \u201ccompressed\u201d million-bit stream\n8175.5 \u25a0 Data Compression\n The practical impact of these limitations is that lossless compression methods must \nbe oriented toward taking advantage of known structure in the bitstreams to be com-\npressed. The four methods that we consider exploit, in turn, the following structural \ncharacteristics:\n\u25a0  Small alphabets\n\u25a0  Long sequences of identical bits/characters\n\u25a0  Frequently used characters\n\u25a0  Long reused bit/character sequences\nIf you know that a given bitstream exhibits one or more of these characteristics, you \ncan compress it with one of the methods that you are about to learn; if not, trying them \neach is probably still worth the effort, since the underlying structure of your data may \nnot be obvious, and these methods are widely applicable. As you will see, each method \nhas parameters and variations that may need to be tuned for best compression of a par-\nticular bitstream. The \ufb01rst and last recourse is to learn something about the structure \nof your data yourself and exploit that knowledge to compress it, perhaps using one of \nthe techniques we are about to consider.\n818 CHAPTER 5 \u25a0 Strings\n  \n  W a r m u p :  g e n o m i c s  As preparation for more complicated data-compression al -\ngorithms, we now consider ", "start": 829, "end": 831}, "1232": {"text": "we are about to consider.\n818 CHAPTER 5 \u25a0 Strings\n  \n  W a r m u p :  g e n o m i c s  As preparation for more complicated data-compression al -\ngorithms, we now consider an elementary (but very important) data-compression task. \nAll of our implementations will use the same conventions that we will now introduce \nin the context of this example. \nGenomic data. As a \ufb01rst example of data compression, consider this string:\nATAGATGCATAGCGCATAGCTAGATGTGCTAGCAT\nUsing standard ASCII encoding (1 byte, or 8 bits per character), this string is a bitstream \nof length 8/H1100335 = 280. Strings of this sort are extremely important in modern biology, \nbecause biologists use the letters A, C, T, and G to represent the four nucleotides in the \nDNA of living organisms. A genome is a sequence of nucleotides. Scientists know that \nunderstanding the properties of genomes is a key to understanding the processes that \nmanifest themselves in living organisms, in-\ncluding life, death, and disease. Genomes for \nmany living things are known, and scientists \nare writing programs to study the structure of \nthese sequences. \n2-bit code compression. One simple prop-\nerty of genomes is that they contain only \nfour different characters, so each can be en-\ncoded with just 2 bits per character, as in the \ncompress() method shown at right. Even \nthough we know the input stream to be \ncharacter-encoded, we use BinaryStdIn to \nread the input, to emphasize adherence to the \nstandard data-compression model (bitstream \nto bitstream). We include the number of encoded characters in the compressed \ufb01le, to \nensure proper decoding if the last bit does not fall at the end of a byte. Since it converts \neach 8-bit character to ", "start": 831, "end": 831}, "1233": {"text": "bitstream). We include the number of encoded characters in the compressed \ufb01le, to \nensure proper decoding if the last bit does not fall at the end of a byte. Since it converts \neach 8-bit character to a 2-bit code  and just adds 32 bits for the length, this program \napproaches a 25 percent compression ratio as the number of characters increases.\n2-bit code expansion. The expand() method at the top of the next page expands a \nbitstream produced by this compress() method. As with compression, this method \nreads a bitstream and writes a bitstream, in accordance with the basic data-compression \nmodel. The bitstream that we produce as output is the original input.\npublic static void  compress() \n{\n   Alphabet DNA = new Alphabet(\"ACTG\");\n   String s = BinaryStdIn.readString();\n   int N = s.length();\n   BinaryStdOut.write(N);\n   for (int i = 0; i < N; i++)\n   {  // Write two-bit code for char.\n      int d = DNA.toIndex(s.charAt(i));\n      BinaryStdOut.write(d, DNA.lgR());\n   }\n   BinaryStdOut.close(); \n}\nCompression  method for genomic data\n8195.5 \u25a0 Data Compression\n  \n \nThe same approach works for other \ufb01xed-\nsize alphabets, but we leave this generaliza-\ntion for an (easy) exercise (see  Exercise \n5.5.25).  \nThese methods do not quite adhere to the \nstandard data-compression model, because \nthe compressed bitstream does not contain \nall the information needed to decode it. The \nfact that the alphabet is one of the letters A, \nC, T, or G is agreed upon by the two meth-\nods. Such a convention is reasonable in an \napplication such as genomics, where the \nsame code is widely reused. Other situations \nmight require including the ", "start": 831, "end": 832}, "1234": {"text": "\nC, T, or G is agreed upon by the two meth-\nods. Such a convention is reasonable in an \napplication such as genomics, where the \nsame code is widely reused. Other situations \nmight require including the alphabet in the encoded message (see  Exercise 5.5.25 ). \nThe norm in data compression is to include such costs when comparing methods.\nIn the early days of genomics, learning a genomic sequence was a long and arduous \ntask, so sequences were relatively short and scientists used standard ASCII encoding to \nstore and exchange them. The experimental process has been vastly streamlined, to the \npoint where known genomes are numerous and lengthy (the human genome is over \n1010 bits), and the 75 percent savings achieved by these simple methods is very signi\ufb01-\ncant. Is there room for further compression? That is a very interesting question to con-\ntemplate, because it is a scienti\ufb01c question: the ability to compress implies the existence \nof some structure in the data, and a prime focus of modern genomics is to discover \nstructure in genomic data. Standard data-compression methods like the ones we will \nconsider are ineffective with (2-bit-encoded) genomic data, as with random data. \nWe package compress() and \nexpand() as static methods in \nthe same class, along with a sim-\nple driver, as shown at right. T o \ntest your understanding of the \nrules of the game and the basic \ntools that we use for data com -\npression, make sure that you \nunderstand the various com-\nmands on the facing page that \ninvoke Genome.compress() and \nGenome.expand() on our sample \ndata (and their consequences).\npublic static void expand() \n{\n   Alphabet DNA = new Alphabet(\"ACTG\");\n   int w = DNA.lgR();\n   int N = BinaryStdIn.readInt();\n   for (int i = 0; i ", "start": 832, "end": 832}, "1235": {"text": "consequences).\npublic static void expand() \n{\n   Alphabet DNA = new Alphabet(\"ACTG\");\n   int w = DNA.lgR();\n   int N = BinaryStdIn.readInt();\n   for (int i = 0; i < N; i++)\n   {   // Read 2 bits; write char.\n       char c = BinaryStdIn.readChar(w);\n       BinaryStdOut.write(DNA.toChar(c));\n   }\n   BinaryStdOut.close(); \n}\nExpansion method for genomic data\npublic class Genome \n{\n   public static void compress()\n   // See text.\n   public static void expand()\n   // See text.\n   public static void main(String[] args)\n   {\n       if (args[0].equals(\"-\")) compress();\n       if (args[0].equals(\"+\")) expand();\n   } \n}\nPackaging convention for data-compression methods\n820 CHAPTER 5 \u25a0 Strings\n Compressing and expanding genomic sequences with 2-bit encoding   \nan actual virus (50000 bits)\ntiny test case (264 bits)\n% java PictureDump 512 100 < genomeVirus.txt\n50000 bits\n% java Genome - < genomeVirus.txt | java PictureDump 512 25\n12536 bits\n% more genomeTiny.txt\nATAGATGCATAGCGCATAGCTAGATGTGCTAGC\njava BinaryDump 64 < genomeTiny.txt\n0100000101010100010000010100011101000001010101000100011101000011\n0100000101010100010000010100011101000011010001110100001101000001\n0101010001000001010001110100001101010100010000010100011101000001\n0101010001000111010101000100011101000011010101000100000101000111\n01000011\n264 bits\n% java Genome - < genomeTiny.txt\n??\n% java ", "start": 832, "end": 833}, "1236": {"text": "genomeTiny.txt\n0100000101010100010000010100011101000001010101000100011101000011\n0100000101010100010000010100011101000011010001110100001101000001\n0101010001000001010001110100001101010100010000010100011101000001\n0101010001000111010101000100011101000011010101000100000101000111\n01000011\n264 bits\n% java Genome - < genomeTiny.txt\n??\n% java Genome - < genomeTiny.txt | java BinaryDump 64\n0000000000000000000000000010000100100011001011010010001101110100\n1000110110001100101110110110001101000000\n104 bits\n% java Genome - < genomeTiny.txt | java HexDump 8\n00 00 00 21 23 2d 23 74\n8d 8c bb 63 40\n104 bits\n% java Genome - < genomeTiny.txt > genomeTiny.2bit\n% java Genome + < genomeTiny.2bit\nATAGATGCATAGCGCATAGCTAGATGTGCTAGC\n% java Genome - < genomeTiny.txt | java Genome +\nATAGATGCATAGCGCATAGCTAGATGTGCTAGC\ncannot see bitstream on standard output\ncompress-expand cycle\nproduces original input\n8215.5 \u25a0 Data Compression\n   R u n - l e n g t h  e n c o d i n g  The simplest type of redundancy in a bitstream is long \nruns of repeated bits. Next, we consider a classic method known as run-length encoding\nfor taking advantage of this redundancy to compress data. For example, consider the \nfollowing 40-bit string:\n0000000000000001111111000000011111111111\nThis ", "start": 833, "end": 834}, "1237": {"text": "Next, we consider a classic method known as run-length encoding\nfor taking advantage of this redundancy to compress data. For example, consider the \nfollowing 40-bit string:\n0000000000000001111111000000011111111111\nThis string consists of 15 0s, then 7 1s, then 7 0s, then 11 1s, so we can encode the bit-\nstring with the numbers 15, 7, 7, and 11. All bitstrings are composed of alternating runs \nof 0s and 1s; we just encode the length of the runs. In our example, if we use 4 bits to \nencode the numbers and start with a run of 0s, we get the 16-bit string\n1111011101111011\n(15 = 1111, then 7 = 0111, then 7 = 0111, then 11 = 1011) for a compression ratio of \n16/40 = 40 percent. In order to turn this description into an effective data compression \nmethod, we have to consider the following issues:\n\u25a0 How many bits do we use to store the counts?\n\u25a0 What do we do when encountering a run that is longer than the maximum \ncount implied by this choice?\n\u25a0 What do we do about runs that are shorter than the number of bits needed to \nstore their length?\nWe are pr imar ily interested in long bitstreams w ith relatively few shor t runs, so we ad-\ndress these questions by making the following choices:\n\u25a0 Counts are between 0 and 255, all encoded with 8 bits.\n\u25a0 We make all run lengths less than 256 by including runs of  length 0 if  needed.\n\u25a0 \n \nWe encode shor t runs, even thoug h doing so mig ht lengthen the output.\nThese choices are very easy to implement and also ", "start": 834, "end": 834}, "1238": {"text": "than 256 by including runs of  length 0 if  needed.\n\u25a0 \n \nWe encode shor t runs, even thoug h doing so mig ht lengthen the output.\nThese choices are very easy to implement and also very effective for several kinds of bit-\nstreams that are commonly encountered in practice. They are not effective when short \nruns are numerous\u2014we save bits on a run only when the length of the run is more than \nthe number of bits needed to represent itself in binary. \n B i t m a p s .  As an example of the effectiveness of run-length encoding, we consider bit-\nmaps, which are widely use to represent pictures and scanned documents. For brev -\nity and simplicity, we consider binary-valued bitmaps organized as bitstreams formed \nby taking the pixels in row-major order. To view the contents of a bitmap, we use \nPictureDump. Writing a program to convert an image from one of the many common \nlossless image formats that have been de\ufb01ned for \u201cscreen shots\u201d or scanned documents \ninto a bitmap is a simple matter (see Exercise 5.5.X ). Our example to demonstrate \nthe effectiveness of run-length encoding comes from screen shots of this book: a let-\nter q (at various resolutions).  We focus on a binary dump of a 32-by-48-pixel screen \n822 CHAPTER 5 \u25a0 Strings\n  \nshot, shown at right along with run lengths for \neach row. Since each row starts and ends with \na 0, there is an odd number of run lengths on \neach row; since the end of one row is followed \nby the beginning of the next, the corresponding \nrun length in the bitstream is the sum of the last \nrun length in each row and the \ufb01rst run length \nin the next (with extra additions corresponding \nto rows that are all 0).\nImplementation. The informal description \njust given leads immediately to the ", "start": 834, "end": 835}, "1239": {"text": "of the last \nrun length in each row and the \ufb01rst run length \nin the next (with extra additions corresponding \nto rows that are all 0).\nImplementation. The informal description \njust given leads immediately to the  compress()\nand expand() implementations on the next \npage. As usual, the expand() implementation is \nthe simpler of the two: read a run length, print \nthat many copies of the current bit, complement \nthe current bit, and continue until the input is \nexhausted. The compress() method is not \nmuch more dif\ufb01cult, consisting of the following \nsteps while there are bits in the input stream: \n\u25a0 Read a bit. \n\u25a0 If it differs from the last bit read, write the \ncurrent count and reset the count to 0. \n\u25a0 If it is the same as the last bit read, and \nthe count is a maximum, write the count, \nwrite a 0 count, and reset the count to 0.\n\u25a0 Increment the count.\nWhen the input stream empties, writing the \ncount (length of the last run) completes the process.\nIncreasing resolution in bitmaps. The primary reason that run-length encoding is \nwidely used for bitmaps is that its effectiveness increases dramatically as resolution in-\ncreases. It is easy to see why this is true. Suppose that we double the resolution for our \nexample. Then the following facts are evident:\n\u25a0 The number of bits increases by a factor of 4.\n\u25a0 The number of runs increases by about a factor of 2.\n\u25a0 The run lengths increase by about a factor of 2.\n\u25a0 The number of bits in the compressed version increases by about a factor of 2.\n\u25a0 Therefore, the compression ratio is halved!\nA typical bitmap, with run lengths for each row \n7 1s\n% java BinaryDump 32 < q32x48.bin\n00000000000000000000000000000000\n00000000000000000000000000000000\n000000000000000 ", "start": 835, "end": 835}, "1240": {"text": "Therefore, the compression ratio is halved!\nA typical bitmap, with run lengths for each row \n7 1s\n% java BinaryDump 32 < q32x48.bin\n00000000000000000000000000000000\n00000000000000000000000000000000\n000000000000000 1111111 0000000000\n00000000000011111111111111100000\n00000000001111000011111111100000\n00000000111100000000011111100000\n00000001110000000000001111100000\n00000011110000000000001111100000\n00000111100000000000001111100000\n00001111000000000000001111100000\n00001111000000000000001111100000\n00011110000000000000001111100000\n00011110000000000000001111100000\n00111110000000000000001111100000\n00111110000000000000001111100000\n00111110000000000000001111100000\n00111110000000000000001111100000\n00111110000000000000001111100000\n00111110000000000000001111100000\n00111110000000000000001111100000\n00111110000000000000001111100000\n00111111000000000000001111100000\n00111111000000000000001111100000\n00011111100000000000001111100000\n00011111100000000000001111100000\n00001111110000000000001111100000\n00001111111000000000001111100000\n00000111111100000000001111100000\n00000011111111000000011111100000\n00000001111111111111111111100000\n00000000011111111111001111100000\n00000000000011111000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000011111110000\n00000000000000000011111111111100\n00000000000000000 ", "start": 835, "end": 835}, "1241": {"text": "0000000000\n00000000000011111111111111100000\n00000000001111000011111111100000\n00000000111100000000011111100000\n00000001110000000000001111100000\n00000011110000000000001111100000\n00000111100000000000001111100000\n00001111000000000000001111100000\n00001111000000000000001111100000\n00011110000000000000001111100000\n00011110000000000000001111100000\n00111110000000000000001111100000\n00111110000000000000001111100000\n00111110000000000000001111100000\n00111110000000000000001111100000\n00111110000000000000001111100000\n00111110000000000000001111100000\n00111110000000000000001111100000\n00111110000000000000001111100000\n00111111000000000000001111100000\n00111111000000000000001111100000\n00011111100000000000001111100000\n00011111100000000000001111100000\n00001111110000000000001111100000\n00001111111000000000001111100000\n00000111111100000000001111100000\n00000011111111000000011111100000\n00000001111111111111111111100000\n00000000011111111111001111100000\n00000000000011111000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000001111100000\n00000000000000000000011111110000\n00000000000000000011111111111100\n00000000000000000 111111111111110\n00000000000000000000000000000000\n00000000000000000000000000000000\n1536 ", "start": 835, "end": 835}, "1242": {"text": "111111111111110\n00000000000000000000000000000000\n00000000000000000000000000000000\n1536 bits\n32\n32\n15  7 10\n12 15  5\n10  4  4  9  5\n 8  4  9  6  5\n 7  3 12  5  5\n 6  4 12  5  5\n 5  4 13  5  5\n 4  4 14  5  5\n 4  4 14  5  5\n 3  4 15  5  5\n 2  5 15  5  5\n 2  5 15  5  5\n 2  5 15  5  5\n 2  5 15  5  5\n 2  5 15  5  5\n 2  5 15  5  5\n 2  5 15  5  5\n 2  5 15  5  5\n 2  5 15  5  5\n 2  6 14  5  5\n 2  6 14  5  5\n 3  6 13  5  5\n 3  6 13  5  5\n 4  6 12  5  5\n 4  7 11  5  5\n 5  7 10  5  5\n 6  8  7  6 ", "start": 835, "end": 835}, "1243": {"text": "12  5  5\n 4  7 11  5  5\n 5  7 10  5  5\n 6  8  7  6  5\n 7 20  5\n 9 11  2  5  5\n22  5  5\n22  5  5\n22  5  5\n22  5  5\n22  5  5\n22  5  5\n22  5  5\n22  5  5\n22  5  5\n22  5  5\n22  5  5\n22  5  5\n21  7  4\n18 12  2\n17 14  1\n32\n32  \n17 0s\n8235.5 \u25a0 Data Compression\n Without run-length encoding, space require-\nments increase by a factor of 4 when the res-\nolution is doubled; with run-length encod-\ning, space requirements for the compressed \nbitstream just double when the resolution is \ndoubled. That is, space grows and the com-\npression ratio drops linearly with resolution. \nFor example, our (low-resolution) letter q\nyields just a 74 percent compression ratio; if \nwe increase the resolution to 64 by 96, the ra-\ntio drops to 37 percent. This change is graph-\nically evident in the PictureDump outputs \nshown in the \ufb01gure on the facing page. The \nhigher-resolution letter takes four times the \nspace of the lower resolution letter (double \nin both dimensions), but the compressed ver-\nsion takes just twice the space (double in one \ndimension). If we further increase the resolu-\ntion to ", "start": 835, "end": 836}, "1244": {"text": "four times the \nspace of the lower resolution letter (double \nin both dimensions), but the compressed ver-\nsion takes just twice the space (double in one \ndimension). If we further increase the resolu-\ntion to 128-by-192 (closer to what is needed \nfor print), the ratio drops to 18 percent (see \nExercise 5.5.5).\nRun-length encoding is very effective\nin many situations, but there are plenty of \ncases where the bitstream we wish to com-\npress (for example, typical English-language \ntext) may have no long runs at all. Next, we \nconsider two methods that are effective for a \nbroad variety of \ufb01les. They are widely used, \nand you likely have used one or both of these \nmethods when downloading from the web.\npublic static void  expand() \n{\n   boolean b = false;\n   while (!BinaryStdIn.isEmpty())\n   {\n      char cnt = BinaryStdIn.readChar();\n      for (int i = 0; i < cnt; i++)\n         BinaryStdOut.write(b);\n      b = !b;\n   }\n   BinaryStdOut.close(); \n}\npublic static void compress() \n{\n   char cnt = 0;\n   boolean b, old = false;\n   while (!BinaryStdIn.isEmpty())\n   {\n      b = BinaryStdIn.readBoolean();\n      if (b != old)\n      {\n         BinaryStdOut.write(cnt);\n         cnt = 0;\n         old = !old;\n      }\n      else\n      {\n         if (cnt == 255)\n         {\n            BinaryStdOut.write(cnt);\n            cnt = 0;\n            BinaryStdOut.write(cnt);\n         }\n      }\n      cnt++;\n   }\n   BinaryStdOut.write(cnt);\n   BinaryStdOut.close(); \n}\nExpand and compress methods for run-length encoding\n824 CHAPTER 5 \u25a0 Strings\n Compressing and expanding bitstreams with run-length ", "start": 836, "end": 837}, "1245": {"text": "cnt++;\n   }\n   BinaryStdOut.write(cnt);\n   BinaryStdOut.close(); \n}\nExpand and compress methods for run-length encoding\n824 CHAPTER 5 \u25a0 Strings\n Compressing and expanding bitstreams with run-length encoding   \na bitmap (1536 bits)\na higher-resolution bitmap (6144 bits)\ntiny test case (40 bits)\nASCII text (96 bits)\n% java RunLength - < q32x48.bin > q32x48.bin.rle\n% java HexDump 16 < q32x48.bin.rle\n4f 07 16 0f 0f 04 04 09 0d 04 09 06 0c 03 0c 05\n0b 04 0c 05 0a 04 0d 05 09 04 0e 05 09 04 0e 05\n08 04 0f 05 08 04 0f 05 07 05 0f 05 07 05 0f 05\n07 05 0f 05 07 05 0f 05 07 05 0f 05 07 05 0f 05\n07 05 0f 05 07 05 0f 05 07 06 0e 05 07 06 0e 05\n08 06 0d 05 08 06 0d 05 09 06 0c 05 09 07 0b 05\n0a 07 0a 05 0b 08 07 06 0c 14 0e 0b 02 05 11 05\n05 05 1b 05 1b 05 1b 05 1b 05 1b 05 1b ", "start": 837, "end": 837}, "1246": {"text": "14 0e 0b 02 05 11 05\n05 05 1b 05 1b 05 1b 05 1b 05 1b 05 1b 05 1b 05\n1b 05 1b 05 1b 05 1b 05 1a 07 16 0c 13 0e 41\n1144 bits\n% java BinaryDump 0 < q64x96.bin\n6144 bits\n% java RunLength - < q64x96.bin | java BinaryDump 0\n2296 bits\n% java BinaryDump 40 < 4runs.bin\n0000000000000001111111000000011111111111\n40 bits\n% java RunLength - < 4runs.bin | java HexDump\n0f 07 07 0b\n32 bits\n% java RunLength - < 4runs.bin | java RunLength + | java BinaryDump 40\n0000000000000001111111000000011111111111\n40 bits\ncompression ratio 416/ = 433% \u2014 do not use run-length encoding for ASCII !\ncompression ratio 2296/6144 = 37%\ncompress-expand produces original input\ncompression ratio 1144/1536 = 74%\ncompression ratio 32/40 = 80%\n% java RunLength - < abra.txt | java HexDump 24\n01 01 05 01 01 01 04 01 02 01 01 01 02 01 02 01 05 01 01 01 04 02 01 01\n05 01 01 01 03 01 03 01 05 01 01 01 04 01 02 01 01 01 02 01 02 01 ", "start": 837, "end": 837}, "1247": {"text": "01\n05 01 01 01 03 01 03 01 05 01 01 01 04 01 02 01 01 01 02 01 02 01 05 01\n02 01 04 01\n416 bits\n% java PictureDump 32 48 < q32x48.bin\n1536 bits\n% java PictureDump 64 96 < q64x96.bin\n6144 bits\n% java PictureDump 64 36 < q64x96.rle.bin\n2296 bits\n% java PictureDump 32 36 < q32x48.rle.bin\n1144 bits\n8255.5 \u25a0 Data Compression\n  \n  \n  H u f f m a n  c o m p r e s s i o n  We now examine a data-compression technique that can \nsave a substantial amount of space in natural language \ufb01les (and many other kinds of \n\ufb01les). The idea is to abandon the way in which text \ufb01les are usually stored: instead of \nusing the usual 7 or 8 bits for each character, we use fewer bits for characters that appear \noften than for those that appear rarely.\nTo  i n t ro d u ce  t h e  b a s i c  i d e a s , w e  s t a r t  w i t h  a  s m a l l  e x a m p l e . Su p p o s e  w e  w i s h  to  e n-\ncode the string ABRACADABRA ! E n c o d i n g  i t  i n  7 - b i t  A S C I I  g i v e s  t h i s  b i t s t r i n g :  \n100000110000101010010100000110000111000001-\n100010010000011000010101001010000010100001. ", "start": 837, "end": 838}, "1248": {"text": "i n  7 - b i t  A S C I I  g i v e s  t h i s  b i t s t r i n g :  \n100000110000101010010100000110000111000001-\n100010010000011000010101001010000010100001. \nTo  d e co d e  t h i s  b i t s t r i n g , w e  s i m p l y  re a d  o f f  7  b i t s  a t  a  t i m e  a n d  co nve r t  a cco rd i n g  to  \nthe ASCII coding table on page 815. In this standard code the D, which appears only once, \nrequires the same number of bits as the A, which appears \ufb01ve times. Huffman compres-\nsion is based on the idea that we can save bits by encoding frequently used characters \nwith fewer bits than rarely used characters, thereby lowering the total number of bits \nused. \n  V a r i a b l e - l e n g t h    p r e \ufb01 x - f r e e  c o d e s .  A code associates each character with a bitstring: a \nsymbol table with characters as keys and bitstrings as values. As a start, we might try \nto assign the shortest bitstrings to the most commonly used letters, encoding A with 0, \nB with 1, R with 00, C with 01, D with 10, and ! with 11, so ABRACADABRA!  w o u l d  \nbe encoded as 0 1 00 0 01 0 10 0 1 00 0 11. This representation uses only 17 bits \ncompared to the 77 ", "start": 838, "end": 838}, "1249": {"text": "u l d  \nbe encoded as 0 1 00 0 01 0 10 0 1 00 0 11. This representation uses only 17 bits \ncompared to the 77 for 7-bit ASCII, but it is not really a code because it depends on the \nblanks to delimit the characters. Without the blanks, the bitstring would be \n01000010100100011  \nand could be decoded as CRRDDCRCB  o r  a s  s e v e r a l  o t h e r  s t r i n g s .  S t i l l ,  t h e  c o u n t  o f  \n17 bits plus 10 delimiters is rather more compact than the standard code, primarily be-\ncause no bits are used to encode letters not appearing in the message. The next step is to \ntake advantage of the fact that delimiters are not needed if no character code is the pre\ufb01x of \nanother. A code with this property is known as a pre\ufb01x-free code. The code just given is \nnot pre\ufb01x-free because 0, the code for A, is a pre\ufb01x of 00, the code for R. For example, if \nwe encode A with 0, B with 11111, C with 110, D with 100, R with 1110, and ! with 101, \nthere is only one way to decode the 30-bit string \n011111110011001000111111100101\nABRACADABRA  !  A l l  p r e \ufb01 x - f r e e  c o d e s  a r e     uniquely decodable (without needing any \ndelimiters) in this way, so pre\ufb01x-free codes are widely used in practice. Note that  \ufb01xed-\nlength ", "start": 838, "end": 838}, "1250": {"text": "e e  c o d e s  a r e     uniquely decodable (without needing any \ndelimiters) in this way, so pre\ufb01x-free codes are widely used in practice. Note that  \ufb01xed-\nlength codes such as 7-bit ASCII are pre\ufb01x-free. \n826 CHAPTER 5 \u25a0 Strings\n  \n \n \n \n  \n  Tr ie representat ion for pre\ufb01x-free codes. One conve-\nnient way to represent a pre\ufb01x-free code is with a trie (see \nSection 5.2). In fact, any trie with M null links de\ufb01nes a \npre\ufb01x-free code for M characters: we replace the null links \nby links to leaves (nodes with two null links), each con-\ntaining a character to be encoded, and de\ufb01ne the code for \neach character with the bitstring de\ufb01ned by the path from \nthe root to the character, in the standard manner for tries \nwhere we associate 0 with moving left and 1 with moving \nright. For example, the \ufb01gure at right shows two pre\ufb01x-\nfree codes for the characters in ABRACADABRA! . On \ntop is the variable-length code just considered; below is a \ncode that produces the string \n11000111101011100110001111101\nwhich is 29 bits, 1 bit shorter. Is there a trie that leads to \neven more compression? How do we \ufb01nd the trie that \nleads to the best pre\ufb01x-free code? It turns out that there is \nan elegant answer to these questions in the form of an al-\ngorithm that computes a trie which leads to a bitstream of \nminimal length for any given string. T o make a fair com-\nparison with other codes, we also need to count the bits in \nthe code itself, since the string cannot be ", "start": 838, "end": 839}, "1251": {"text": "trie which leads to a bitstream of \nminimal length for any given string. T o make a fair com-\nparison with other codes, we also need to count the bits in \nthe code itself, since the string cannot be decoded without \nit, and, as you will see, the code depends on the string. The general method for \ufb01nding \nthe optimal pre\ufb01x-free code was discovered by  D. Huffman (while a student!) in 1952 \nand is called Huffman encoding. \nOverview. Using a pre\ufb01x-free code for data compression involves \ufb01ve major steps. We \nview the bitstream to be encoded as a bytestream and use a pre\ufb01x-free code for the \ncharacters as follows:\n\u25a0 Build an encoding trie.\n\u25a0 Write the trie (encoded as a bitstream) for use in expansion.\n\u25a0 Use the trie to encode the bytestream as a bitstream.\nThen expansion requires that we\n\u25a0 Read the trie (encoded at the beginning of the bitstream)\n\u25a0 Use the trie to decode the bitstream\nTo  h e l p  yo u  b e s t  u n d e r s t a n d  a n d  a p p re c i a te  t h e  p ro ce s s , w e  co n s i d e r  t h e s e  s te p s  i n  \norder of dif\ufb01culty.\nTwo prefix-free codes\n011111110011001000111111100101\nA   B   RA  CA  DA   B   RA  !\n101\n0\n1111\n110\n100\n1110\n!\nA\nB\nC\nD\nR\nkey value\nD !\n00 11\nC\nA\nR B\n00 11\n00 11\n00 11\n00 11\n30 bits\nleaves\n11000111101011100110001111101\n ", "start": 839, "end": 839}, "1252": {"text": "!\n101\n0\n1111\n110\n100\n1110\n!\nA\nB\nC\nD\nR\nkey value\nD !\n00 11\nC\nA\nR B\n00 11\n00 11\n00 11\n00 11\n30 bits\nleaves\n11000111101011100110001111101\n A B  R A  C A  D A B  R A  !\n101\n11\n00\n010\n100\n011\n!\nA\nB\nC\nD\nR\nkey value\nC R\nAB\n00 11\n00 1100 11\n00 11\nD !\n00 11\n29 bits\ntrie representationcodeword table\ntrie representationcodeword table\ncompressed bitstring\ncompressed bitstring\n8275.5 \u25a0 Data Compression\n  \nTr ie nodes. We beg in w ith the \nNode class at left. It is similar to \nthe nested classes that we have \nused before to construct bi-\nnary trees and tries: each Node\nhas left and right references \nto Nodes, which de\ufb01ne the trie \nstructure. Each Node also has \nan instance variable freq that \nis used in construction, and \nan instance variable ch, which \nis used in leaves to represent \ncharacters to be encoded.\nExpansion for pre\ufb01x-free \ncodes. Expanding a bitstream \nthat was encoded with a pre\ufb01x-\nfree code is simple, given the \ntrie that de\ufb01nes the code. The \nexpand() method at left is an implementa-\ntion of this process. After reading the trie from \nstandard input using the readTrie() method \nto be described later, we use it to expand the \nrest of the bitstream as follows: Starting at the \nroot, proceed down the trie as directed by the \nbitstream (read in input bit, move left if it ", "start": 839, "end": 840}, "1253": {"text": "be described later, we use it to expand the \nrest of the bitstream as follows: Starting at the \nroot, proceed down the trie as directed by the \nbitstream (read in input bit, move left if it is 0,  \nand move right if it is 1). When you encounter \na leaf, output the character at that node and re-\nstart at the root. If you study the operation of \nthis method on the small pre\ufb01x code example \non the next page, you will understand and ap-\npreciate this process: For example, to decode \nthe bitstring 011111001011... we start \nat the root, move left because the \ufb01rst bit is 0, \noutput A; go back to the root, move right three times, then output B; go back to the root, \nmove right twice, then left, then output R; and so forth. The simplicity of expansion is \none reason for the popularity of pre\ufb01x-free codes in general and Huffman compression \nin particular.\npublic static void expand() \n{\n   Node root = readTrie();\n   int N = BinaryStdIn.readInt();\n   for (int i = 0; i < N; i++)\n   {  // Expand ith codeword.\n      Node x = root;\n      while (!x.isLeaf())\n         if (BinaryStdIn.readBoolean())\n              x = x.right;\n         else x = x.left;\n      BinaryStdOut.write(x.ch);\n   }\n   BinaryStdOut.close(); \n}\nPrefix-free code expansion (decoding)\nprivate static class  Node implements Comparable<Node> \n{  // Huffman trie node\n   private char ch;   // unused for internal nodes\n   private int freq;  // unused for expand\n   private final Node left, right;\n   Node(char ch, int freq, Node left, Node right)\n   {   \n      this.ch    = ch;\n ", "start": 840, "end": 840}, "1254": {"text": "for internal nodes\n   private int freq;  // unused for expand\n   private final Node left, right;\n   Node(char ch, int freq, Node left, Node right)\n   {   \n      this.ch    = ch;\n      this.freq  = freq;\n      this.left  = left;\n      this.right = right;\n   }\n   public boolean isLeaf()\n   {  return left == null && right == null;  }\n   public int compareTo(Node that)\n   {  return this.freq - that.freq;  }\n}\n T r i e  n o d e  r e p r e s e n t a t i o n\n828 CHAPTER 5 \u25a0 Strings\n  \n C o m p r e s s i o n  f o r  p r e \ufb01 x - f r e e  c o d e s .  For compression, we use \nthe trie that de\ufb01nes the code to build the code table, as shown \nin the buildCode() method at the top of this page. This \nmethod is compact and elegant, but a bit tricky, so it deserves \ncareful study. For any trie, it produces a table giving the bit-\nstring associated with each character in the trie (represented \nas a String of 0s and 1s). The coding table is a symbol table \nthat associates a String with each character: we use a charac-\nter-indexed array st[] instead of a general symbol table for \nef\ufb01ciency, because the number of characters is not large. T o create it, buildCode()\nrecursively walks the tree, maintaining a binary string that corresponds to the path \nfrom the root to each node (0 for left links and 1 for right links), and setting the code-\nword corresponding to each character when the character is found in a leaf. Once the \ncoding table is built, compression is a simple matter: just look up the ", "start": 840, "end": 841}, "1255": {"text": "left links and 1 for right links), and setting the code-\nword corresponding to each character when the character is found in a leaf. Once the \ncoding table is built, compression is a simple matter: just look up the code for each \ncharacter in the input. T o use the encoding at right to compress ABRACADABRA!  w e  \nwrite 0 (the codeword associated with A), \nthen 111 (the codeword associated with \nB), then 110 (the codeword associated \nwith R), and so forth. The code snippet at \nright accomplishes this task: we look up \nthe String associated with each character \nin the input, convert it to 0/1 values in a \nchar array, and write the corresponding \nbitstring to the output.\nprivate static String[] buildCode(Node root) \n{  // Make a lookup table from trie.\n   String[] st = new String[R];\n   buildCode(st, root, \"\");\n   return st; \n}\nprivate static void buildCode(String[] st, Node x, String s) \n{  // Make a lookup table from trie (recursive).\n   if (x.isLeaf())\n   {  st[x.ch] = s; return; }\n   buildCode(st, x.left,  s + '0');\n   buildCode(st, x.right, s + '1'); \n}\nBuilding an encoding table from a (prefix-free) code trie\nfor (int i = 0; i < input.length; i++) \n{\n   String code = st[input[i]];\n   for (int j = 0; j < code.length(); j++)\n      if (code.charAt(j) == '1')\n           BinaryStdOut.write(true);\n      else BinaryStdOut.write(false); \n}\nCompression with an encoding table\nA Huffman code\n1010\n0\n111\n1011\n100\n110\n!\nA\nB\nC\nD\nR\nkey ", "start": 841, "end": 841}, "1256": {"text": "'1')\n           BinaryStdOut.write(true);\n      else BinaryStdOut.write(false); \n}\nCompression with an encoding table\nA Huffman code\n1010\n0\n111\n1011\n100\n110\n!\nA\nB\nC\nD\nR\nkey value\nD R B\nC!\nA\n00 11\n00 11\n00 11\n00 11 00 11\nTrie representationCodeword table\n8295.5 \u25a0 Data Compression\n 830 CHAPTER 5 \u25a0 Strings\n \n \n \nTr ie const r uct ion. For reference as we describe the process, the \ufb01gure on the facing \npage illustrates the process of constructing a Huffman trie for the input \nit was the best of times it was the worst of times \nWe keep the characters to be encoded in leaves and maintain the freq instance variable \nin each node that represents the frequency of occurrence of all characters in the subtree \nrooted at that node. The \ufb01rst step is to create a forest of 1-node trees (leaves), one for \neach character in the input stream, each assigned a freq value equal to its frequency \nof occurrence in the input. In the example, the input has 8 ts, 5 es, 11 spaces, and so \nforth. (Important note: To obtain these frequencies, we need to read the whole input \nstream\u2014 Huffman encoding is a two-pass algorithm because we will need to read the \ninput stream a second time to compress it.) Next, we build the coding trie from the \nbottom up according to the frequencies. When building the trie, we view it as a binary \ntrie with frequencies stored in the nodes; after it has been built, we view it as a trie for \ncoding, as just described. The process works as follows: we \ufb01nd the two nodes with the \nsmallest frequencies and then create a new node with those two nodes as ", "start": 841, "end": 842}, "1257": {"text": "built, we view it as a trie for \ncoding, as just described. The process works as follows: we \ufb01nd the two nodes with the \nsmallest frequencies and then create a new node with those two nodes as children (and \nwith frequency value set to the sum of the values of the children). This operation re -\nduces the number of tries in the forest by one. Then we iterate the process: \ufb01nd the two \nnodes with smallest frequency in that forest and a create a new node created in the same \nway. Implementing the process is straightforward with a  priority queue, as shown in the \nbuildTrie() method on page 830. (For clarity, the tries in the \ufb01gure are kept in sorted \norder.) Continuing, we build up larger and larger tries and at the same time reduce the \nnumber of tries in the forest by one at each step (remove two, add one). Ultimately, all the \nprivate static Node buildTrie(int[] freq) \n{\n    // Initialize priority queue with singleton trees.\n    MinPQ<Node> pq = new MinPQ<Node>();\n    for (char c = 0; c < R; c++)\n       if (freq[c] > 0)\n          pq.insert(new Node(c, freq[c], null, null));\n    while (pq.size() > 1)\n    {  // Merge two smallest trees.\n       Node x = pq.delMin();\n       Node y = pq.delMin();\n       Node parent = new Node('\\0', x.freq + y.freq, x, y);\n       pq.insert(parent);\n    }\n    return pq.delMin(); \n}\n B u i l d i n g  a  H u f f m a n  e n c o d i n g  t r i e\n 8315.5 \u25a0 Data Compression\ne\nw o\nSP\ns\nLF\ni\nb\nf ", "start": 842, "end": 843}, "1258": {"text": "u f f m a n  e n c o d i n g  t r i e\n 8315.5 \u25a0 Data Compression\ne\nw o\nSP\ns\nLF\ni\nb\nf h m a\nt\nr\nConstructing a Huffman encoding trie\n55 8888776666\n33 33 44 44 4433\n2211 22 22 22 22\n11 11\n1111 1313 16161111\n2222 2929\n5151\ne\nw o\nSP\ns\nLF\ni\nb\nf h m a\nt\nr\n55 8888776666\n33 33 44 44 4433\n2211 22 22 22 22\n11 11\n1111 1313 16161111\n2222 2929\ne\nw o\nSPs\nLF\ni\nb\nf h m a\nt\nr\n55\n88887766\n66\n33 33\n44 44 4433\n2211 22 22 22 22\n11 11\n1111\n1313 1616\n1111\n2222\ne\nw o\nSP\ns\nLF\ni\nb\nf h m a\nt\nr\n55 8888776666\n33 33 44 44 4433\n2211 22 22 22 22\n11 11\n1111 1313 16161111\ne\nw o\nSP\ns\nLF\ni\nb\nf h m a\nt\nr\n55\n8888\n776666\n33 33 44\n44 44\n33\n2211\n22 22 22 22\n11 11\n1111 13131111\nSP\ne\nw o\n55 ", "start": 843, "end": 843}, "1259": {"text": "a\nt\nr\n55\n8888\n776666\n33 33 44\n44 44\n33\n2211\n22 22 22 22\n11 11\n1111 13131111\nSP\ne\nw o\n55 66\n33 33\n11111111\nt\n888877\nf h m a\n44 44\n22 22 22 22\nLF\ni\nb\nr\n4433\n2211\n11 11\n77\nLF\ni\nb\nr\n4433\n2211\n11 11\nw o\n66\n33 33\nf h m a\n44 44\n22 22 22 22\nf h\n44\n22 22\nLF b\nr\n33\n2211\n11 11\nLF b\n22\n11 11\nSPs t\n8866\ne\n55\nw o\n33 33\nf h m a\n22 22 22 22\nLF ib r\n441111 11 1111\nSPs t\n8866\ne\n55\nw o\n33 33\nf h m a\n22 22 22 22\nf h m a\n22 22 22 22\nir\n4411 1111\nSPs t\n8866\ne\n55\nw o\n33 33\nw o\n33 33\nLF b\nr\n33\n2211\n11 11\nw o\n33 33\nLF b\nr\n33\n2211\n11 11\nm a\n22 22\ni\n44\ni\n44\ni\n44\n1111\nSPs t\n8866\ne\n55 1111\nSPs t\n8866\ne\n55\nf h m a\n44 44\n22 22 ", "start": 843, "end": 843}, "1260": {"text": "22\ni\n44\ni\n44\ni\n44\n1111\nSPs t\n8866\ne\n55 1111\nSPs t\n8866\ne\n55\nf h m a\n44 44\n22 22 22 22\nLF b\nr\n33\n2211\n11 11\ni\n44\ne\n55\n1111\nSPs t\n8866\nw o\n66\n33 33\nf h m a\n44\n44\n22 22 22 22\ne\n55\ns\n66\nw o\n66\n33 33\ne\n55\ns\n66\nt\n888877\nf h m a\n44 44\n22 22 22 22\nLF\ni\nb\nr\n4433\n2211\n11 11\ns\n66\n1111\nSP\nt\n88\n1111\ntwo tries\nwith smallest\n weights\nnew parent for \nthose two tries\nto top of  right\ncolumn\nfrom bottom of\nleft column\n   \n \n  \nnodes are combined together into a single trie. The leaves in this trie have the characters \nto be encoded and their frequencies in the input; each non-leaf node is the sum of the \nfrequencies of its two children. Nodes with low frequencies end up far down in the trie, \nand nodes with high frequencies end up near the root of the trie. The frequency in the \nroot equals the number of characters in the input. Since it is a binary trie with charac -\nters only in its leaves, it de\ufb01nes a pre\ufb01x-free code for the characters. Using the codeword \ntable created by buildCode() for this example (shown at right in the diagram at the \ntop of this page), we get the output bitstring \n10111110100101101110001111110010000110101100-\n01001110100111100001111101111010000100011011-\n11101001011011100011111100100001001000111010-\n01001110100111100001111101111010000100101010.\nwhich ", "start": 843, "end": 844}, "1261": {"text": "codeword \ntable created by buildCode() for this example (shown at right in the diagram at the \ntop of this page), we get the output bitstring \n10111110100101101110001111110010000110101100-\n01001110100111100001111101111010000100011011-\n11101001011011100011111100100001001000111010-\n01001110100111100001111101111010000100101010.\nwhich is 176 bits, a savings of 57 percent over the 408 bits needed to encode the 51 \ncharacters in standard 8-bit ASCII (not counting the cost of including the code, which \nwe will soon consider). Moreover, since it is a Huffman code, no other pre\ufb01x-free code \ncan encode the input with fewer bits.\nOptimality. We have obser ved that hig h-frequency characters are nearer the root of  \nthe tree than lower-frequency characters and are therefore encoded with fewer bits, so \nthis is a good code, but why is it an optimal pre\ufb01x-free code? T o answer this question, we \nbegin by de\ufb01ning the  weighted  external path length of a tree to be the sum of the weight \n(associated frequency count) times depth (see page 226) of all of the leaves. \ne\nw o\nSP\ns\nLF\ni\nb\nf h m a\nt\nr\nHuffman code for the character stream \u201cit was the best of times it was the worst of times LF\u201d\n00 11\n00 11\n00 11\n00 11\n00 11\n00 11 00 11\n00 11 00 1100 11\n00 11\n00 11\n00 11\nlabels on path\nfrom root are 11010\nso 11010 is code for m\n3 ", "start": 844, "end": 844}, "1262": {"text": "11\n00 11 00 1100 11\n00 11\n00 11\n00 11\nlabels on path\nfrom root are 11010\nso 11010 is code for m\n3 occurrences\nof w in input\ntrie representation codeword table\n101010\n01\n11011\n101011\n000\n11000\n11001\n1011\n11010\n0011\n10100\n100\n111\n0010\nLF\nSP\na\nb\ne\nf\nh\ni\nm\no\nr\ns\nt\nw\nkey value\n55 886666\n33 33 44\n11 22 22 22 22\n11 11\n1111\n832 CHAPTER 5 \u25a0 Strings\n  Proposition T.  For any pre\ufb01x-free code, the length of the encoded bitstring is equal \nto the weighted external path length of the corresponding trie.\nProof: The depth of each leaf is the number of bits used to encode the character in \nthe leaf. Thus, the weighted external path length is the length of encoded bitstring : \nit is equivalent to the sum over all letters of the number of occurrences times the \nnumber of bits per occurrence.\nFor our example, there is one leaf at distance 2 ( SP, with frequency 11), three leaves at \ndistance 3 (e, s, and t, with total frequency 19), three leaves at distance 4 ( w, o, and i, \nwith total frequency 10), \ufb01ve leaves at distance 5 (r, f, h, m, and a, with total frequency \n9) and two leaves at distance 6 (LF and b, with total frequency 2), so the sum total is 2\u00b711 \n 3\u00b719 /H11001 4\u00b710 /H11001 5\u00b79 /H11001 ", "start": 844, "end": 845}, "1263": {"text": "6 (LF and b, with total frequency 2), so the sum total is 2\u00b711 \n 3\u00b719 /H11001 4\u00b710 /H11001 5\u00b79 /H11001 6\u00b72 /H11005176, the length of the output bitstring, as expected.\n \nProposition U. Given a set of r symbols and frequencies, the  Huffman algorithm \nbuilds an    optimal pre\ufb01x-free code.\nProof: By induction on r. Assume that the Huffman code is optimal for any set \nof fewer than r symbols. Let TH be the code computed by Huffman for the set of \nsymbols and associated frequencies (s1, r1), . . . , (sr , fr) and denote the length of the \ncode (weighted external path length of the trie) by W(TH). Suppose that (si , fi) and \n(sj, fj) are the \ufb01rst two symbols chosen. The algorithm then computes the code TH* \nfor the set of n/H110021 symbols with (si , fi) and (si , fj) replaced by (s*, fi + fj) where s* is \na new symbol in a leaf at some depth d. Note that \nW(TH) = W(TH*) /H11002 d( fi + fj) + (d + 1)( fi + fj ) = W(TH*) + (fi + fj )\nNow consider an optimal trie T  for (s1, r1), . . . , (sr , fr), of height h. Note that  that \n(si , fi) and (sj , fj) must be at depth h (else we could make a trie with lower external \npath length by swapping them with nodes at depth h).  Also, assume ( si , fi) and \n(sj, fj) are siblings by swapping (sj, fj) with (si , fi)\u2019s ", "start": 845, "end": 845}, "1264": {"text": "trie with lower external \npath length by swapping them with nodes at depth h).  Also, assume ( si , fi) and \n(sj, fj) are siblings by swapping (sj, fj) with (si , fi)\u2019s sibling. Now consider the tree T* \nobtained by replacing their parent with (s*, fi + fj ). Note that (by the same argument \nas above) W(T ) = W(T*) + (fi + fj).\nBy the inductive hypothesis TH* is optimal: W(TH*) /H11349 W(T*). Therefore, \nW(TH) = W(TH*) + ( fi + fj )  /H11349  W(T*) + ( fi + fj ) = W(T )\nSince T is optimal, equality must hold, and TH is optimal.\n8335.5 \u25a0 Data Compression\n    \n \nWhenever a node is to be picked, it can be the case that there are several nodes with \nthe same weight. Huffman\u2019s method does not specify how such ties are to be broken. It \nalso does not specify the left/right positions of the children. Different choices lead to \ndifferent Huffman codes, but all such \ncodes will encode the message with \nthe optimal number of bits among \npre\ufb01x-free codes. \n  W r i t i n g  a n d  r e a d i n g  t h e  t r i e .  As we \nhave emphasized, the savings \ufb01gure \nquoted above is not entirely accurate, \nbecause the compressed bitstream \ncannot be encoded without the trie, \nso we must account for the cost of \nincluding the trie in the compressed \noutput, along with the bitstring. For \nlong inputs, this cost is relatively \nsmall, but in order for us to have a \nfull data-compression scheme, we must write the trie onto a bitstream when compress-\ning and read it ", "start": 845, "end": 846}, "1265": {"text": "bitstring. For \nlong inputs, this cost is relatively \nsmall, but in order for us to have a \nfull data-compression scheme, we must write the trie onto a bitstream when compress-\ning and read it back when expanding. How can we encode a trie as a bitstream, and then \nexpand it? Remarkably, both tasks can be achieved with simple recursive procedures, \nbased on a  preorder traversal of the trie. The procedure writeTrie() below traverses a \ntrie in preorder: when it visits an internal node, it writes a single 0 bit; when it visits a \nleaf, it writes a 1 bit, followed by the 8-bit ASCII code of the character in the leaf. The \nbitstring encoding of the Huffman trie for our ABRACADABRA!  e x a m p l e  i s  s h o w n  \nabove. The \ufb01rst bit is 0, corresponding to the root; since the leaf containing A is encoun-\ntered next, the next bit is 1, followed by \n0100001, the 8-bit  ASCII code for A; \nthe next two bits are 0 because two in-\nternal nodes are encountered next, and \nso forth. The corresponding method \nreadTrie() on page 835 reconstructs \nthe trie from the bitstring: it reads \na single bit to learn which type of \nnode comes next: if a leaf (the bit is \n1) it reads the next character and cre-\nates a leaf; if an internal node (the bit \nis 0) it creates an internal node and \nUsing preorder traversal to encode a trie as a bitstream\npreorder\ntraversal\nD R B\nLF C\nA\n01010000010010100010001000010101010000110101010010101000010\ninternal nodes\nleaves\nBRCLFDA\n11\n22\n2211 ", "start": 846, "end": 846}, "1266": {"text": "bitstream\npreorder\ntraversal\nD R B\nLF C\nA\n01010000010010100010001000010101010000110101010010101000010\ninternal nodes\nleaves\nBRCLFDA\n11\n22\n2211 33 44 55\n33\n44\n55\nprivate static void writeTrie(Node x) \n{  // Write bitstring-encoded trie.\n   if (x.isLeaf())\n   {\n      BinaryStdOut.write(true);\n      BinaryStdOut.write(x.ch);\n      return;\n   }\n   BinaryStdOut.write(false);\n   writeTrie(x.left);\n   writeTrie(x.right); \n}\nWriting a trie as a bitstring\n834 CHAPTER 5 \u25a0 Strings\n then (recursively) builds its left and right subtrees. Be sure that you understand these \nmethods: their simplicity is somewhat deceiving.\nHuffman compression implementation. Along with the methods buildCode(), \nbuildTrie(), readTrie() and writeTrie() that we have just considered (and \nthe expand() method that we considered \ufb01rst), Algorithm 5.10 is a complete imple-\nmentation of Huffman compression. T o expand the overview that we considered sev-\neral pages earlier, we view the bitstream to be encoded as a stream of 8-bit char values \nand compress it as follows:\n\u25a0 Read the input.\n\u25a0 Tabulate the frequency of  occur rence of  each char value in the input.\n\u25a0 Build the Huffman encoding trie corresponding to those frequencies.\n\u25a0 Build the corresponding codeword table, to associate a bitstring with each char\nvalue in the input.\n\u25a0 Write the trie, encoded as a bitstring.\n\u25a0 Write the count of characters in the input, encoded as a bitstring.\n\u25a0 Use the codeword table to write the codeword for each input character.\nTo  e x p a n d  a  b i ", "start": 846, "end": 847}, "1267": {"text": "Write the count of characters in the input, encoded as a bitstring.\n\u25a0 Use the codeword table to write the codeword for each input character.\nTo  e x p a n d  a  b i t s t re a m  e n co d e d  i n  t h i s  w ay, w e\n\u25a0 Read the trie (encoded at the beginning of the bitstream)\n\u25a0 Read the count of characters to be decoded\n\u25a0 Use the trie to decode the bitstream\nWith four recursive trie-processing methods and a seven-step compression process, \nHuffman compression is one of the more involved algorithms that we have considered, \nbut it is also one of the most widel-used, because of its effectiveness.\nprivate static Node readTrie() \n{\n   if (BinaryStdIn.readBoolean())\n      return new Node(BinaryStdIn.readChar(), 0, null, null);\n   return new Node('\\0', 0, readTrie(), readTrie()); \n}\n R e c o n s t r u c t i n g  a  t r i e  f r o m  t h e  p r e o r d e r  b i t s t r i n g  r e p r e s e n t a t i o n\n8355.5 \u25a0 Data Compression\n ALGORITHM 5.10   Huffman compression\npublic class  Huffman \n{\n   private static int R = 256;   // ASCII alphabet\n   // See page 828 for inner Node class.\n   // See text for helper methods and expand().\n   public static void compress()\n   {\n      // Read input.\n      String s = BinaryStdIn.readString();\n      char[] input = s.toCharArray();\n      // Tabulate frequency counts.\n      int[] freq = new int[R];\n      for (int i = 0; i < input.length; i++)\n         freq[input[i]]++;\n ", "start": 847, "end": 848}, "1268": {"text": "BinaryStdIn.readString();\n      char[] input = s.toCharArray();\n      // Tabulate frequency counts.\n      int[] freq = new int[R];\n      for (int i = 0; i < input.length; i++)\n         freq[input[i]]++;\n      // Build Huffman code trie.\n      Node root = buildTrie(freq);\n      // Build code table (recursive).\n      String[] st = new String[R];\n      buildCode(st, root, \"\");\n      // Print trie for decoder (recursive).\n      writeTrie(root);\n      // Print number of chars.\n      BinaryStdOut.write(input.length);\n      // Use Huffman code to encode input.\n      for (int i = 0; i < input.length; i++)\n      {\n         String code = st[input[i]];\n         for (int j = 0; j < code.length(); j++)\n         if (code.charAt(j) == '1')\n              BinaryStdOut.write(true);\n         else BinaryStdOut.write(false);\n      }\n      BinaryStdOut.close();\n   } \n}\nThis implementation of Huffman encoding builds an explicit coding trie, using various helper meth-\nods that are presented and explained in the last several pages of text.\n836 CHAPTER 5 \u25a0 Strings Compressing and expanding bytestreams with Huffman encoding   \nfirst chapter of Tale of Two Cities\ntest case (96 bits)\nexample from text (408 bits)\nentire text of Tale of Two Cities\ncompression ratio 23912/45056 = 53%\ncompression ratio 120/96 = 125% due to 59 bits for trie and 32 bits for count\ncompression ratio 352/408 = 86% even with 137 bits for trie and 32 bits for count\ncompression ratio 3043928/5812552 = 52%\n45056 bits\n23912 bits\n% java PictureDump 512 90 < medTale.txt\n% java Huffman - < medTale.txt | java PictureDump ", "start": 848, "end": 849}, "1269": {"text": "3043928/5812552 = 52%\n45056 bits\n23912 bits\n% java PictureDump 512 90 < medTale.txt\n% java Huffman - < medTale.txt | java PictureDump 512 47 \n% java Huffman - < tale.txt > tale.txt.huf\n% java BinaryDump 0 < tale.txt.huf\n3043928 bits\n% java BinaryDump 0 < tale.txt\n5812552 bits\n% more abra.txt\nABRACADABRA!\n% java Huffman - < abra.txt | java BinaryDump 60\n010100000100101000100010000101010100001101010100101010000100\n000000000000000000000000000110001111100101101000111110010100\n120 bits\n% more tinytinyTale.txt\nit was the best of times it was the worst of times\n% java Huffman - < tinytinyTale.txt | java BinaryDump 64         \n0001011001010101110111101101111100100000001011100110010111001001\n0000101010110001010110100100010110011010110100001011011011011000\n0110111010000000000000000000000000000110011101111101001011011100\n0111111001000011010110001001110100111100001111101111010000100011\n0111110100101101110001111110010000100100011101001001110100111100\n00111110111101000010010101000000\n352 bits\n% java Huffman - < tinytinyTale.txt | java Huffman +\nit was the best of times it was the worst of times\n8375.5 \u25a0 Data Compression\n  \nOne reason for the popularity  of Huffman compression is that it is effective for \nvarious types of \ufb01les, not just natural ", "start": 849, "end": 850}, "1270": {"text": "the best of times it was the worst of times\n8375.5 \u25a0 Data Compression\n  \nOne reason for the popularity  of Huffman compression is that it is effective for \nvarious types of \ufb01les, not just natural language text. We have been careful to code the \nmethod so that it can work properly for any 8-bit value in each 8-bit character. In \nother words, we can apply it to any bytestream whatsoever. Several examples, for \ufb01le \ntypes that we have considered earlier in this section, are shown in the \ufb01gure at the bot-\ntom of this page.  These examples show that Huffman compression is competitive with \nboth \ufb01xed-length encoding and run-length encoding, even though those methods are \ndesigned to perform well for certain types of \ufb01les. Understanding the reason Huff-\nman encoding performs well in these domains is instructive. In the case of genomic \ndata, Huffman compression essentially discovers a 2-bit code, as the four letters appear \nwith approximately equal frequency so that the Huffman trie is balanced, with each \ncharacter assigned a 2-bit code. In the case of run-length encoding, 00000000  a n d  \n11111111  a r e  l i k e l y  t o  b e  t h e  m o s t  f r e q u e n t l y  o c c u r r i n g  c h a r a c t e r s ,  s o  t h e y  a r e  l i k e l y  \nto be encoded with 2 or 3 bits, leading to substantial compression.\nCompressing and expanding genomic data and bitmaps with Huffman encoding   \nbitmap (1536 bits)\nvirus (50000 bits)\nHuffman compression uses 29%  fewer bits than customized method\n% java Genome  - < genomeVirus.txt | ", "start": 850, "end": 850}, "1271": {"text": "data and bitmaps with Huffman encoding   \nbitmap (1536 bits)\nvirus (50000 bits)\nHuffman compression uses 29%  fewer bits than customized method\n% java Genome  - < genomeVirus.txt | java PictureDump 512 25\n12536 bits\n% java Huffman - < genomeVirus.txt | java PictureDump 512 25\n12576 bits\n Huffman compression needs just 40 more bits than custom 2-bit code\n% java RunLength - < q32x48.bin | java BinaryDump 0\n1144 bits\n% java Huffman   - < q32x48.bin | java BinaryDump 0\n816 bits\nhigher-resolution bitmap (6144 bits)\ngap narrows to 11%  for higher resolution\n% java RunLength - < q64x96.bin | java BinaryDump 0\n2296 bits\n% java Huffman   - < q64x96.bin | java BinaryDump 0\n2032 bits\n838 CHAPTER 5 \u25a0 Strings\n  \n   A remarkable alternative to Huffman compression that was developed in the late \n1970s and the early 1980s by A. Lempel, J. Ziv, and T. Welch has emerged as one of the \nmost widely used compression methods because it is easy to implement and works well \nfor a variety of \ufb01le types.\nThe basic plan complements the basic plan for Huffman coding. Rather than main-\ntain a table of variable-length codewords for \ufb01xed-length patterns in the input, we \nmaintain a table of \ufb01xed-length codewords for variable-length patterns in the input. A \nsurprising added feature of the method is that, by contrast with Huffman encoding, we \ndo not have to encode the table. \nLZW compression. To  \ufb01 x  i d e a s , w e  w i l l  co n s i d e r  a ", "start": 850, "end": 851}, "1272": {"text": "encoding, we \ndo not have to encode the table. \nLZW compression. To  \ufb01 x  i d e a s , w e  w i l l  co n s i d e r  a  co m p re s s i o n  e x a m p l e  w h e re  w e  \nread the input as a stream of 7-bit ASCII characters and write the output as a stream of \n8-bit bytes. (In practice, we typically use larger values for these parameters\u2014our imple-\nmentations use 8-bit inputs and 12-bit outputs.) We refer to input bytes as characters, \nsequences of input bytes as strings, and output bytes as codewords, even though these \nterms have slightly different meanings in other contexts. The LZW compression algo -\nrithm is based on maintaining a symbol table that associates string keys with (\ufb01xed-\nlength) codeword values. We initialize the symbol table with the 128 possible single-\ncharacter string keys and associate them with 8-bit codewords obtained by prepending \n0 to the 7-bit value de\ufb01ning each character.  For economy and clarity, we use hexadeci-\nmal to refer to codeword values, so 41 is the codeword for ASCII A, 52 for R, and so \nforth. We reserve the codeword 80 to signify end of \ufb01le. We will assign the rest of the \ncodeword values (81 through FF) to various substrings of the input that we encounter, \nby starting at 81 and incrementing the value for each new key added. T o compress, we \nperform the following steps as long as there are unscanned input characters:\n\u25a0  Find the longest string s in the symbol table that is a pre\ufb01x of the unscanned \ninput.\n\u25a0 Write the 8-bit ", "start": 851, "end": 851}, "1273": {"text": "the following steps as long as there are unscanned input characters:\n\u25a0  Find the longest string s in the symbol table that is a pre\ufb01x of the unscanned \ninput.\n\u25a0 Write the 8-bit value (codeword) associated with s.\n\u25a0 Scan one character past s in the input.\n\u25a0 Associate the next codeword value with s + c (c appended to s) in the symbol \ntable, where c is the next character in the input.\nIn the last of these steps, we look ahead to see the next character in the input to build \nthe next dictionary entry, so we refer to that character c as the lookahead character. \nFor the moment, we simply stop adding entries to the symbol table when we run out \nof codeword values (after assigning the value FF to some string)\u2014we will later discuss \nalternate strategies.\n8395.5 \u25a0 Data Compression\n  \nLZW compression example. The \ufb01gure below gives details of the operation of LZW \ncompression for the example input ABRACADABRABRABRA . For the \ufb01rst seven \ncharacters, the longest pre\ufb01x match is just one character, so we output the codeword \nassociated with the character and associate the codewords from 81 through 87 to two-\ncharacter strings. Then we \ufb01nd pre\ufb01x matches with AB (so we output 81 and add ABR to \nthe table), RA (so we output 83 and add RAC to the table), BR (so we output 82 and add \nBRA to the table), and ABR (so we output 88 and add ABRA to the table), leaving the last \nA (so we output its codeword, 41).   \nLZW compression for  ABRACADABRABRABRA\nAB\nB R8 2\nA B8 1  AB\nBR\nR ", "start": 851, "end": 852}, "1274": {"text": "\nA (so we output its codeword, 41).   \nLZW compression for  ABRACADABRABRABRA\nAB\nB R8 2\nA B8 1  AB\nBR\nR A8 3\nAB\nBR\nRA\nA C8 4\nAB\nBR\nRA\nAC\nC A8 5\nAB\nBR\nRA\nAC\nCA\nA D8 6\nAB\nBR\nRA\nAC\nCA\nAD\nD A8 7\nAB\nBR\nRA\nAC\nCA\nAD\nDA\nAB R8 8\nAB\nBR\nRA\nAC\nCA\nAD\nDA\nABR\nRA B8 9\nAB \nBR\nRA\nAC\nCA\nAD\nDA\nABR\nRAB\nBRA\nABR A8 B\nAB \nBR\nRA\nAC\nCA\nAD\nDA\nABR\nRAB\nBR A8 A\nA     B     R     A     C     A     D     A     B     R     A     B     R     A     B     R     A\nA     B     R     A     C     A     D     A B         R A         B R         A B R             A\n41    42    52    41    43    41    44    81          83          82          88                41    80\nEOF\nAB \nBR\nRA\nAC\nCA\nAD\nDA\nABR\nRAB\nBRA\nABRA\n81\n82\n83\n84\n85\n86\n87\n88\n89\n8A\n8B\nvaluekey\noutput\ninput\nmatches\nlookahead\ncharacter\ncodeword table\nLZW\ncodeword\ninput\nsubstring\nThe input is 17 ", "start": 852, "end": 852}, "1275": {"text": "\nBR\nRA\nAC\nCA\nAD\nDA\nABR\nRAB\nBRA\nABRA\n81\n82\n83\n84\n85\n86\n87\n88\n89\n8A\n8B\nvaluekey\noutput\ninput\nmatches\nlookahead\ncharacter\ncodeword table\nLZW\ncodeword\ninput\nsubstring\nThe input is 17 ASCII characters of 7 bits each for a total of 119 bits; the output is 12 \ncodewords of 8 bits each for a total of 96 bits\u2014a compression ratio of 82 percent even \nfor this tiny example. \n  L Z W  t r i e  r e p r e s e n t a t i o n .  LZW compression involves two symbol-table operations:\n\u25a0  Find a longest-pre\ufb01x match of the input with a symbol-table key.\n\u25a0 Add an entry associating the next code-\nword with the key formed by appending \nthe lookahead character to that key.\nOur trie data structures of Section 5.2 are tai-\nlor-made for these operations. The trie repre -\nsentation for our example is shown at right. T o \n\ufb01nd a longest pre\ufb01x match, we traverse the trie \nfrom the root, matching node labels with input \ncharacters; to add a new codeword, we connect \na new node labeled with the next codeword and \nthe lookahead character to the node where the \nsearch terminated. In practice, we use a TST for Trie representation of LZW code table\nAA\n81\n41\n84 86 82 85 87 83\n8A88\n8B\n89\n42 43 44 52BB CC DD RR\nRR\nBB CC DD RR AA AA AA\nAA BB\nAA\n840 CHAPTER 5 \u25a0 Strings\n  \n \nspace ef\ufb01ciency, ", "start": 852, "end": 853}, "1276": {"text": "83\n8A88\n8B\n89\n42 43 44 52BB CC DD RR\nRR\nBB CC DD RR AA AA AA\nAA BB\nAA\n840 CHAPTER 5 \u25a0 Strings\n  \n \nspace ef\ufb01ciency, as described in Section 5.2. The contrast with the use of tries in Huff-\nman encoding is worth noting: for Huffman encoding, tries are useful because no pre\ufb01x \nof a codeword is also a codeword; for LZW tries are useful because every pre\ufb01x of an \ninput-substring key is also a key.\nLZW  expansion. The input for LZW expansion in our example is a sequence of 8-bit \ncodewords; the output is a string of 7-bit ASCII characters. To implement expansion, \nwe maintain a symbol table that associates strings of characters with codeword values \n(the inverse of the table used for compression). We \ufb01ll the table entries from 00 to 7F\nwith one-character strings, one for each ASCII character, set the \ufb01rst unassigned code-\nword value to 81 (reserving 80 for end of \ufb01le), set the current string val to the one-\ncharacter string consisting of the \ufb01rst character, and perform the following steps until \nreading codeword 80 (end of \ufb01le):\n\u25a0 Write the current string val.\n\u25a0 Read a codeword x from the input.\n\u25a0 Set s to the value associated with x in the symbol table.\n\u25a0 \n \nAssociate the next unasssigned codeword value to val + c in the symbol table, \nwhere c is the \ufb01rst character of s.\n\u25a0 \n \n \nSet the current string val to s.\nThis process is more complicated than compression because of the lookahead charac-\nter: we need to read the next codeword to get the \ufb01rst character in the string associated ", "start": 853, "end": 853}, "1277": {"text": "\n \n \nSet the current string val to s.\nThis process is more complicated than compression because of the lookahead charac-\nter: we need to read the next codeword to get the \ufb01rst character in the string associated \nwith it, which puts the process one step out of synch. For the \ufb01rst seven codewords, we \njust look up and write the appropriate character, then look ahead one character and \nadd a two-character entry to the symbol table, as before. Then we read 81 (so we write \nAB  a n d  a d d  ABR  t o  t h e  t a b l e ) ,  83 (so we write RA  a n d  a d d  RAB  t o  t h e  t a b l e ) ,  82 (so \nwe write BR  a n d  a d d  BRA  t o  t h e  t a b l e ) ,  a n d  88 (so we write ABR  a n d  a d d  ABRA  t o  \nthe table), leaving 41. Finally we read the end-of-\ufb01le character 80 (so we write A ). At \nLZW expansion for  41 42 52 41 43 41 44 81 83 82 88 41 80\nAB\nB R82\nA B81 AB\nBR\nR A83\nAB\nBR\nRA\nA C84\nAB\nBR\nRA\nAC\nC A85\nAB\nBR\nRA\nAC\nCA\nA D86\nAB\nBR\nRA\nAC\nCA\nAD\nD A87\nAB\nBR\nRA\nAC\nCA\nAD\nDA\nAB R88\nAB\nBR\nRA\nAC\nCA\nAD\nDA\nABR\nRA ", "start": 853, "end": 853}, "1278": {"text": "D86\nAB\nBR\nRA\nAC\nCA\nAD\nD A87\nAB\nBR\nRA\nAC\nCA\nAD\nDA\nAB R88\nAB\nBR\nRA\nAC\nCA\nAD\nDA\nABR\nRA B89\nAB\nBR\nRA\nAC\nCA\nAD\nDA\nABR\nRAB\nBR A8A\nAB\nBR\nRA\nAC\nCA\nAD\nDA\nABR\nRAB\nBRA\nABR A8B\n A     B     R     A     C     A     D     A B         R A         B R         A B R       A\n41    42    52    41    43    41    44    81          83          82          88          41     80\nAB\nBR\nRA\nAC\nCA\nAD\nDA\nABR\nRAB\nBRA\nABR A\n81\n82\n83\n84\n85\n86\n87\n88\n89\n8A\n8B\nkey value\noutput\ninput\ninverse codeword table\nLZW\ncodeword input\nsubstring\n8415.5 \u25a0 Data Compression\n ALGORITHM 5.11   LZW compression\npublic class  LZW \n{\n   private static final int R = 256;        // number of input chars\n   private static final int L = 4096;       // number of codewords = 2^12\n   private static final int W = 12;         // codeword width\n   public static void compress()\n   {\n      String input = BinaryStdIn.readString();\n      TST<Integer> st = new TST<Integer>();\n      for (int i = 0; i < R; i++)\n          st.put(\"\" + (char) i, i);\n      int code = R+1; ", "start": 853, "end": 854}, "1279": {"text": "BinaryStdIn.readString();\n      TST<Integer> st = new TST<Integer>();\n      for (int i = 0; i < R; i++)\n          st.put(\"\" + (char) i, i);\n      int code = R+1;  // R is codeword for EOF.\n      while (input.length() > 0)\n      {\nlongestPrefixOf(input); // Find max prefix match.         String s = st.   \n         BinaryStdOut.write(st.get(s), W);     // Print s's encoding.\n         int t = s.length();\n         if (t < input.length() && code < L)   // Add s to symbol table.\n             st.put(input.substring(0, t + 1), code++);\n         input = input.substring(t);           // Scan past s in input.\n      }\n     BinaryStdOut.write(R, W);                 // Write EOF.\n     BinaryStdOut.close();\n   }\n   public static void expand()\n   // See page 844. \n}\nThis implementation of Lempel-Ziv-Welch data compression uses 8-bit input bytes and 12-bit code-\nwords and is appropriate for arbitrary large \ufb01les. Its codewords for the small example are similar to \nthose discussed in the text: the single-character codewords have a leading 0; the others start at 100. \n% more abraLZW.txt \nABRACADABRABRABRA\n% java LZW - < abraLZW.txt | java HexDump 20 \n04 10 42 05 20 41 04 30 41 04 41 01 10 31 02 10 80 41 10 00 \n160 bits\n842 CHAPTER 5 \u25a0 Strings  \n \n \n \nthe end of the process, we have written the original input, as expected, and also built \nthe same code table as for compression, ", "start": 854, "end": 855}, "1280": {"text": "41 10 00 \n160 bits\n842 CHAPTER 5 \u25a0 Strings  \n \n \n \nthe end of the process, we have written the original input, as expected, and also built \nthe same code table as for compression, but with the key-value roles inverted. Note that \nwe can use a simple array-of-strings representation for the table, indexed by codeword.\nTr icky situation. There is a subtle bug in the process just described, one that is of-\nten discovered by students (and experienced programmers!) only after developing an \nimplementation based on the description above. \nThe problem, illustrated in the example at right, is \nthat it is possible for the lookahead process to get \none character ahead of itself. In the example, the \ninput string \nABABABA\nis compressed to \ufb01ve output codewords \n41 42 81 83 80\nas shown in the top part of the \ufb01gure. T o expand, \nwe read the codeword 41, output A, read the code-\nword 42 to get the lookahead character, add AB as \ntable entry 81, output the B associated with 42, \nread the codeword 81 to get the lookahead char -\nacter, add BA as table entry 82, and output the AB \nassociated with 81. So far, so good. But when we \nread the codeword 83 to get the lookahead character, we are stuck, because the reason \nthat we are reading that codeword is to complete table entry 83! Fortunately, it is easy \nto test for that condition (it happens precisely when the codeword is the same as the \ntable entry to be completed) and to correct it (the lookahead character must be the \ufb01rst \ncharacter in that table entry, since that will be the next character to be output). In this \nexample, this logic tells us that the ", "start": 855, "end": 855}, "1281": {"text": "be completed) and to correct it (the lookahead character must be the \ufb01rst \ncharacter in that table entry, since that will be the next character to be output). In this \nexample, this logic tells us that the lookahead character must be A (the \ufb01rst character in \nABA). Thus, both the next output string and table entry 83 should be ABA.\nImplementation. With these descriptions, implementing LZW encoding is straight -\nforward, given in Algorithm 5.11 on the facing page (the implementation of expand() \nis on the next page). These implementations take 8-bit bytes as input (so we can com-\npress any \ufb01le, not just strings) and produce 12-bit codewords as output (so that we can \nget better compression by having a much larger dictionary). These values are speci\ufb01ed \nin the (\ufb01nal) instance variables R, L, and W in the code. We use a TST (see section 5.2) \nfor the code table in compress() (taking advantage of the ability of trie data structures \nto support ef\ufb01cient implementations of longestPrefixOf()) and an array of strings \nLZW expansion: tricky situation\nAB\nB A82\nAB\nBA\nAB?83\nA B81\nA     B     A B         ?          \n41    42    81          83    80   \nA     B     A     B     A     B     A\nA     B     A B         A B A        \n41    42    81          83               80\nexpansion\ncompression\nAB\nB A8 2\nA B8 1  AB\nBA\nAB A8 3\nAB\nBA\nABA\n81\n82\n83\nneed lookahead character\nto complete entry\nmust be ABA\n(see below)\nnext character in output\u2014the lookahead character!\noutput\ninput\noutput\ninput\nmatches\nvaluekey\ncodeword ", "start": 855, "end": 855}, "1282": {"text": "AB\nBA\nAB A8 3\nAB\nBA\nABA\n81\n82\n83\nneed lookahead character\nto complete entry\nmust be ABA\n(see below)\nnext character in output\u2014the lookahead character!\noutput\ninput\noutput\ninput\nmatches\nvaluekey\ncodeword table\n8435.5 \u25a0 Data Compression\n  A L G O R I T H M  5 . 1 1 (continued) LZW expansion\npublic static void  expand() \n{\n   String[] st = new String[L];\n   int i; // next available codeword value\n   for (i = 0; i < R; i++)            // Initialize table for chars.\n      st[i] = \"\" + (char) i;\n   st[i++] = \" \";  // (unused) lookahead for EOF  \n   int codeword = BinaryStdIn.readInt(W);\n   String val = st[codeword];\n   while (true)\n   {\n      BinaryStdOut.write(val);        // Write current substring.\n      codeword = BinaryStdIn.readInt(W);\n      if (codeword == R) break;       \n      String s = st[codeword];        // Get next codeword.\n      if (i == codeword)              // If lookahead is invalid,\n         s = val + val.charAt(0);     //    make codeword from last one.\n      if (i < L)\n         st[i++] = val + s.charAt(0); // Add new entry to code table.\n      val = s;                        // Update current codeword.\n   }\n   BinaryStdOut.close();\n}\nThis implementation of expansion for the Lempel-Ziv-Welch algorithm is a bit more complicated \nthan compression because of the need to extract the lookahead character from the next codeword \nand because of a tricky situation where lookahead is invalid (see text). \n% ", "start": 855, "end": 856}, "1283": {"text": "Lempel-Ziv-Welch algorithm is a bit more complicated \nthan compression because of the need to extract the lookahead character from the next codeword \nand because of a tricky situation where lookahead is invalid (see text). \n% java LZW - < abraLZW.txt | java LZW + \nABRACADABRABRABRA\n% more ababLZW.txt\nABABABA\n% java LZW - < ababLZW.txt | java LZW + \nABABABA\n844 CHAPTER 5 \u25a0 Strings  \nfor the inverse code table in  expand(). With these choices, the code for compress()\nand expand() is little more than a line-by-line translation of the descriptions in the \ntext. These methods are very effective as they stand. For certain \ufb01les, they can be further \nimproved by emptying the codeword table and starting over each time all the codeword \nvalues are used. These improvements, along with experiments to evaluate their effec-\ntiveness, are addressed in the exercises at the end of this section. \nAs usual, it is worth your while to study carefully the examples given with the pro -\ngrams and at the bottom of this page of LZW compression in action. Over the several \ndecades since its invention, it has proven to be a versatile and effective data-compres-\nsion method.\nCompressing and expanding various files with LZW 12-bit encoding\nvirus (50000 bits)\n% java Genome - < genomeVirus.txt | java PictureDump 512 25\n12536 bits\n% java LZW - < genomeVirus.txt | java PictureDump 512 36\n18232 bits not as good as 2-bit code because repetitive data is rare\nbitmap (6144 bits)\n% java RunLength - < q64x96.bin | java BinaryDump 0\n2296 bits\n% java LZW ", "start": 856, "end": 857}, "1284": {"text": "not as good as 2-bit code because repetitive data is rare\nbitmap (6144 bits)\n% java RunLength - < q64x96.bin | java BinaryDump 0\n2296 bits\n% java LZW - < q64x96.bin | java BinaryDump 0\n2824 bits\nentire text of Tale of Two Cities  (5812552 bits)\ncompression ratio 2667952/5812552 = 46% (best yet)\n% java Huffman - < tale.txt | java BinaryDump 0\n3043928 bits\n% java LZW - < tale.txt | java BinaryDump 0\n2667952 bits\n% java BinaryDump 0 < tale.txt\n5812552 bits\nnot as good as run-length code because file size is too small\n8455.5 \u25a0 Data Compression\n Q&A\n \nQ. Why BinaryStdIn and BinaryStdOut?\nA. It\u2019s a tradeoff between ef\ufb01ciency and convenience. StdIn can handle 8 bits at a time; \nBinaryStdIn has to handle each bit. Most applications are bytestream-oriented; data \ncompression is a special case.\nQ. Why close() ?\nA. This requirement stems from the fact that standard output is actually a bytestream, \nso BinaryStdOut needs to know when to write the last byte.\nQ. Can we mix StdIn and BinaryStdIn ?\nA. That is not a good idea. Because of system and implementation dependencies, there \nis no guarantee of what might happen.  Our implementations will raise an exception. \nOn the other hand, there is no problem with mixing StdOut and BinaryStdOut (we do \nit in our code).\nQ. Why is the Node class static in Huffman?\nA. Our data-compression algorithms are organized as collections of static methods, \nnot data-type implementations.\nQ. Can I at least guarantee that my compression algorithm will not increase the ", "start": 857, "end": 858}, "1285": {"text": "code).\nQ. Why is the Node class static in Huffman?\nA. Our data-compression algorithms are organized as collections of static methods, \nnot data-type implementations.\nQ. Can I at least guarantee that my compression algorithm will not increase the length \nof a bitstream?\nA. Yo u  c a n  j u s t  co p y  i t  f ro m  i n p u t  t o  o u t p u t , b u t  yo u  s t i l l  n e e d  t o  s i g n i f y  n o t  t o  u s e  \na standard compression scheme. Commercial implementations sometimes make this \nguarantee, but it is quite weak and far from universal compression. Indeed, typical \ncompression algorithms do not even make it past the second step of our \ufb01rst proof of \nProposition S: few algorithms will further compress a bitstring produced by that same \nalgorithm.\n846 CHAPTER 5 \u25a0 Strings\n EXERCISES\n \n \n \n5.5.1 Consider the four variable-length codes shown \nin the table at right. Which of the codes are pre\ufb01x-free? \nUniquely decodable? For those that are uniquely decod-\nable, give the encoding of 1000000000000 .\n5.5.2 Given a example of a uniquely decodable code \nthat is not pre\ufb01x-free.\nAnswer : Any   suf\ufb01x-free code is uniquely decodable.\n5.5.3 Give an example of a uniquely decodable code that is not pre\ufb01x free or suf\ufb01x free.\nAnswer : {0011, 011, 11, 1110} or {01, 10, 011, 110}\n5.5.4 Are  { 01, 1001, 1011, 111, 1110 } ", "start": 858, "end": 859}, "1286": {"text": "11, 1110} or {01, 10, 011, 110}\n5.5.4 Are  { 01, 1001, 1011, 111, 1110 } and  { 01, 1001, 1011, 111, 1110 } unique-\nly decodable? If not, \ufb01nd a string with two encodings. \n5.5.5  Use RunLength on the \ufb01le q128x192.bin from the booksite. How many bits are \nthere in the compressed \ufb01le?\n5.5.6 How many bits are needed to encode N copies of the symbol a (as a function \nof N)? N copies of the sequence abc?\n5.5.7 Give the result of encoding the strings a, aa, aaa, aaaa, ... (strings consisting of \nN a\u2019s) with run-length, Huffman, and LZW encoding. What is the compression ratio \nas a function of N?\n5.5.8 Give the result of encoding the strings ab, abab, ababab, abababab, ... (strings \nconsisting of N repetitions of ab) with run-length, Huffman, and LZW encoding. What \nis the compression ratio as a function of N?\n5.5.9  Estimate the compression ratio achieved by run-length, Huffman, and LZW en-\ncoding for a random ASCII string of length N (all characters equally likely at each posi-\ntion, independently). \n5.5.10 In the style of the \ufb01gure in the text, show the Huffman coding tree construction \nprocess when you use Huffman for the string \"it was the age of foolishness\u201d .  \nHow many  bits does the compressed bitstream require?\nsymbol code 1 code 2 code 3 code 4\nA 0 0 1 1\nB 100 1 ", "start": 859, "end": 859}, "1287": {"text": "age of foolishness\u201d .  \nHow many  bits does the compressed bitstream require?\nsymbol code 1 code 2 code 3 code 4\nA 0 0 1 1\nB 100 1 01 01\nC 10 00 001 001\nD 11 11 0001 000\n8475.5 \u25a0 Data Compression\n 5.5.11 What is the Huffman code for a string whose characters are all from a two-\ncharacter alphabet? Give an example showing the maximum number of bits that could \nbe used in a Huffman code for an N-character string whose characters are all from a \ntwo-character alphabet.\n5.5.12 Suppose that all of the symbol probabilities are negative powers of 2. Describe \nthe Huffman code.\n5.5.13 Suppose that all of the symbol frequencies are equal. Describe the Huffman code. \n5.5.14 Suppose that the frequencies of the occurrence of all the characters to be encoded \nare different. Is the Huffman encoding tree unique?\n5.5.15 Huffman coding could be extended in a straightforward way to encode in 2-bit \ncharacters (using 4-way trees). What would be the main advantage and the main dis -\nadvantage of doing so?\n5.5.16 What is the LZW encoding of the following inputs?\na. T O B E O R N O T T O B E\nb. Y A B B A D A B B A D A B B A D O O\nc. A A A A A A A A A A A A A A A A A A A A A\n5.5.17 Characterize the tricky situation in LZW coding.\nSolution : Whenever it encounters cScSc, where c is a symbol and S is a string, cS is in \nthe dictionary already but cSc is not.\n5.5.18 Let Fk be the k ", "start": 859, "end": 860}, "1288": {"text": "coding.\nSolution : Whenever it encounters cScSc, where c is a symbol and S is a string, cS is in \nthe dictionary already but cSc is not.\n5.5.18 Let Fk be the k th Fibonacci number. Consider N symbols, where the k th symbol \nhas frequency Fk. Note that F1 + F1 + ... + FN = FN+2 /H11002 1. Describe the Huffman code.\nHint : The longest codeword has length N /H11002 1.\n5.5.19 Show that there are at least 2 N/H110021 different Huffman codes corresponding to a \ngiven set of N symbols.\n5.5.20 Give a Huffman code where the frequency of 0s in the output is much, much \nhigher than the frequency of 1s.\nAnswer : If the character A occurs 1 million times and the character B occurs once, the \ncodeword for A will be 0 and the codeword for B will be 1.\nEXERCISES  (continued)\n848 CHAPTER 5 \u25a0 Strings\n  \n5.5.21 Prove that the two longest codewords in a Huffman code have the same length.\n5.5.22 Prove the following fact about Huffman codes: If the frequency of symbol i is \nstrictly larger than the frequency of symbol j, then the length of the codeword for sym-\nbol i is less than or equal to the length of the codeword for symbol j. \n5.5.23 What would be the result of breaking up a Huffman-encoded string into \ufb01ve-bit \ncharacters and Huffman-encoding that string?\n5.5.24 In the style of the \ufb01gures in the text, show the encoding trie and the compression \nand expansion processes when LZW is used for the string\nit was the best of times it was the worst of times\n8495.5 ", "start": 860, "end": 861}, "1289": {"text": "style of the \ufb01gures in the text, show the encoding trie and the compression \nand expansion processes when LZW is used for the string\nit was the best of times it was the worst of times\n8495.5 \u25a0 Data Compression\n CREATIVE PROBLEMS\n5.5.25    Fixed length width code. Implement a class RLE that uses \ufb01xed-length encod-\ning, to compress ASCII bytestreams using relatively few different characters, including \nthe code as part of the encoded bitstream. Add code to compress() to make a string \nalpha with all the distinct characters in the message and use it to make an Alphabet for \nuse in compress(), prepend alpha (8-bit encoding plus its length) to the compressed \nbitstream, then add code to expand() to read the alphabet before expansion.\n5.5.26    Rebuilding the LZW dictionary. Modify LZW to empty the dictionary and start \nover when it is full. This approach is recommended in some applications because it bet-\nter adapts to changes in the general character of the input.\n5.5.27  Long repeats. Estimate the compression ratio achieved by run-length, Huff-\nman, and LZW encoding for a string of length 2 N formed by concatenating two copies \nof a random ASCII string of length N (see Exercise 5.5.9), under any assumptions that \nyou think are reasonable.\n850 CHAPTER 5 \u25a0 Strings\n This page intentionally left blank \n SIX\nContext 853\nC\nomputing devices are ubiquitous in the modern world. In the last several \ndecades, we have evolved from a world where computing devices were virtually \nunknown to a world where billions of people use them regularly. Moreover, to-\nday\u2019s cellphones are orders of magnitude more powerful than the supercomputers that \nwere available only to the privileged few as little as 30 years ago. But many of the under-\nlying ", "start": 861, "end": 865}, "1290": {"text": "people use them regularly. Moreover, to-\nday\u2019s cellphones are orders of magnitude more powerful than the supercomputers that \nwere available only to the privileged few as little as 30 years ago. But many of the under-\nlying algorithms that enable these devices to work effectively are the same ones that we \nhave studied in this book. Why? Survival of the \ufb01ttest. Scalable (linear and linearithmic) \nalgorithms have played a central role in the process and validate the idea that ef\ufb01cient \nalgorithms are important. Researchers of the 1960s and 1970s built the basic infrastruc-\nture that we now enjoy with such algorithms. They knew that scalable algorithms are \nthe key to the future; the developments of the past several decades have validated that \nvision.  Now that the infrastructure is built, people are beginning to use it, for all sorts \nof purposes. As  B. Chazelle has famously observed, the 20th century was the century of \nthe equation, but the 21st century is the century of the  algorithm.\nOur treatment of fundamental algorithms in this book is only a starting point. The \nday is soon coming (if it is not already here) when one could build a college major \naround the study of algorithms. In commerical applications, scienti\ufb01c computing, en-\ngineering, operations research (OR), and countless other areas of inquiry too diverse \nto even mention, ef\ufb01cient algorithms make the difference between being able to solve \nproblems in the modern world and not being able to address them at all. Our empha-\nsis throughout this book has been to study important and useful algorithms. In this \nchapter, we reinforce this orientation by considering examples that illustrate the role of \nthe algorithms that we have studied (and our approach to the study of algorithms) in \n 854 CONTEXT\n \n \nseveral advanced contexts. ", "start": 865, "end": 866}, "1291": {"text": "In this \nchapter, we reinforce this orientation by considering examples that illustrate the role of \nthe algorithms that we have studied (and our approach to the study of algorithms) in \n 854 CONTEXT\n \n \nseveral advanced contexts. T o indicate the scope of the impact of the algorithms, we be-\ngin with a very brief description of several important areas of application. T o indicate \nthe depth, we later consider speci\ufb01c representative examples in detail and introduce the \ntheory of algorithms.  In both cases, this brief treatment at the end of a long book can \nonly be indicative, not inclusive. For every area of application that we mention, there \nare dozens of others, equally broad in scope; for every point that we describe within \nan application, there are scores of others, equally important; and for every detailed \nexample we consider, there are hundreds if not thousands of others, equally impactful.\nCommercial applications. The emergence of the internet has underscored the central \nrole of algorithms in commercial applications. All of the applications that you use regu-\nlarly bene\ufb01t from the classic algorithms that we have studied:\n\u25a0 Infrastructure (operating systems, databases, communications)\n\u25a0 Applications (email, document processing, digital photography)\n\u25a0 Publishing (books, magazines, web content)\n\u25a0 Networks (wireless networks, social networks, the internet)\n\u25a0  Transaction processing (\ufb01nancial, retail, web search)\nAs a prominent example, we consider in this chapter B-trees, a venerable data structure \nthat was developed for mainstream computers of the 1960s but still serve as the basis \nfor modern database systems. We will also discuss suf\ufb01x arrays, for text indexing.\nScienti\ufb01c computing. Since von Neumann developed mergesort in 1950, algorithms \nhave played a central role in scienti\ufb01c computing . Today\u2019s scientists are awash in ex -\nperimental data and are using ", "start": 866, "end": 866}, "1292": {"text": "indexing.\nScienti\ufb01c computing. Since von Neumann developed mergesort in 1950, algorithms \nhave played a central role in scienti\ufb01c computing . Today\u2019s scientists are awash in ex -\nperimental data and are using both mathematical and computational models to under-\nstand the natural world for: \n\u25a0 Mathematical calculations (polynomials, matrices, differential equations)\n\u25a0 Data processing (experimental results and observations, especially genomics)\n\u25a0 Computational models and simulation\nAll of these can require complex and extensive computing with huge amounts of data.\nAs a detailed example of an application in scienti\ufb01c computing, we consider in this \nchapter a classic example of event-driven simulation. The idea is to maintain a model of \na complicated real-world system, controlling changes in the model over time. There are \na vast number of applications of this basic approach.  We also consider a fundamental \ndata-processing problem in computational genomics.\nEngineering. Almost by de\ufb01nition, modern engineering is based on technology.  Mod-\nern technology is computer-based, so algorithms play a central role for\n\u25a0 Mathematical calculations and data processing\n\u25a0 Computer-aided design and manufacturing\n 855CONTEXT\n\u25a0 Algorithm-based engineering (networks, control systems)\n\u25a0 Imaging and other medical systems\nEngineers and scientists use many of the same tools and approaches. For example, sci-\nentists develop computational models and simulations for the purpose of understand-\ning the natural world; engineers develop computational models and simulations for the \npurpose of designing, building, and controlling they artifacts they create.\nOperations research. Researchers and practitioners in OR develop and apply math -\nematical models for problem solving, including\n\u25a0 Scheduling\n\u25a0 Decision making\n\u25a0 Assignment of resources\nThe shortest-paths problem of Section 4.4  is a classic OR problem. We revisit this \nproblem and consider the max\ufb02ow problem, illustrate the importance of reduction, and \ndiscuss ", "start": 866, "end": 867}, "1293": {"text": "Assignment of resources\nThe shortest-paths problem of Section 4.4  is a classic OR problem. We revisit this \nproblem and consider the max\ufb02ow problem, illustrate the importance of reduction, and \ndiscuss implications for general problem-solving models, in particular the linear pro-\ngramming model that is central in OR.\nAlgorithms play an imporTant role  in numerous sub\ufb01elds of computer science \nwith applications in all of these areas, including, but certainly not limited to\n\u25a0 Computational geometry\n\u25a0 Cryptography\n\u25a0 Databases\n\u25a0 Programming languages and systems\n\u25a0 \n  \n \n \nArti\ufb01cial intelligence\nIn each \ufb01eld, articulating problems and \ufb01nding ef\ufb01cient algorithms and data structures \nfor solving them play an essential role. Some of the algorithms we have studied apply \ndirectly; more important, the general approach of designing, implementing, and ana-\nlyzing algorithms that lies at the core of this book has proven successful in all of these \n\ufb01elds. This effect is spreading beyond computer science to many other areas of inquiry, \nfrom games to music to linguistics to \ufb01nance to neuroscience.\nSo many important and useful algorithms have been developed that learning and \nunderstanding relationships among them are essential. We \ufb01nish this section (and this \nbook!) with an introduction to the theory of algorithms, with particular focus on intrac-\ntability and the P=NP? question that still stands as the key to understanding the practi-\ncal problems that we aspire to solve.\n 856 CONTEXT\n E v e n t - d r i v e n  s i m u l a t i o n  Our \ufb01rst example is a fundamental scienti\ufb01c applica-\ntion: simulate the motion of a system of moving particles that behave according to the \nlaws of elastic collision. Scientists use such systems to understand and predict proper -\nties ", "start": 867, "end": 868}, "1294": {"text": "\ufb01rst example is a fundamental scienti\ufb01c applica-\ntion: simulate the motion of a system of moving particles that behave according to the \nlaws of elastic collision. Scientists use such systems to understand and predict proper -\nties of physical systems. This paradigm embraces the motion of molecules in a gas, the \ndynamics of chemical reactions, atomic diffusion, sphere packing, the stability of the \nrings around planets, the phase transitions of certain elements, one-dimensional self-\ngravitating systems, front propagation, and many other situations. Applications range \nfrom molecular dynamics, where the objects are tiny subatomic particles, to astrophys-\nics, where the objects are huge celestial bodies.\nAddressing this problem requires a bit of high-school physics, a bit of software engi-\nneering, and a bit of algorithmics. We leave most of the physics for the exercises at the \nend of this section so that we can concentrate on the topic at hand: using a fundamental \nalgorithmic tool (heap-based priority queues) to address an application, enabling cal-\nculations that would not otherwise be possible.\n H a r d - d i s c  m o d e l .  We beg in w ith an idealized model of  the motion of  atoms or mol-\necules in a container that has the following salient features:\n\u25a0 Moving particles interact via elastic collisions with each other and with walls.\n\u25a0 Each particle is a disc with known position, velocity, mass, and radius. \n\u25a0 \n \n \nNo other forces are exerted.\nThis simple model plays a central role in statistical mechanics , a \n\ufb01eld that relates macroscopic observables (such as temperature and \npressure) to microscopic dynamics (such as the motion of individ-\nual atoms and molecules). Maxwell and Boltzmann used the model \nto derive the distribution of speeds of interacting molecules as a \nfunction of temperature; Einstein used the model to explain the \nBrownian ", "start": 868, "end": 868}, "1295": {"text": "as the motion of individ-\nual atoms and molecules). Maxwell and Boltzmann used the model \nto derive the distribution of speeds of interacting molecules as a \nfunction of temperature; Einstein used the model to explain the \nBrownian motion of pollen grains immersed in water. The assump-\ntion that no other forces are exerted implies that particles travel in \nstraight lines at constant speed between collisions. We could also \nextend the model to add other forces. For example, if we add fric -\ntion and spin, we can more accurately model the motion of familiar \nphysical objects such as billiard balls on a pool table. \n T i m e - d r i v e n  s i m u l a t i o n .  Our primary goal is simply to maintain \nthe model: that is, we want to be able to keep track of the positions \nand velocities of all the particles as time passes. The basic calcula-\ntion that we have to do is the following: given the positions and \nvelocities for a speci\ufb01c time t, update them to re\ufb02ect the situation at \nadvance time to t + dt\nadvance time to t + 2dt\nroll back time to moment of collision\nTime-driven simulation\n 857 E v e n t - d r i v e n  s i m u l a t i o n  \n a future time t+dt for a speci\ufb01c amount of time dt. Now, if the particles are suf\ufb01ciently \nfar from one another and from the walls that no collision will occur before t+dt, then \nthe calculation is easy: since particles travel in a straight-line trajectory, we use each \nparticle\u2019s velocity to update its position. The challenge is to take the collisions into \naccount. One approach, known as time-driven simulation, is based \non using a \ufb01xed value of dt. To do each update, ", "start": 868, "end": 869}, "1296": {"text": "\nparticle\u2019s velocity to update its position. The challenge is to take the collisions into \naccount. One approach, known as time-driven simulation, is based \non using a \ufb01xed value of dt. To do each update, we need to check \nall pairs of particles, determine whether or not any two occupy the \nsame position, and then back up to the moment of the \ufb01rst such \ncollision. At that point, we are able to properly update the velocities \nof the two particles to re\ufb02ect the collision (using calculations that \nwe will discuss later). This approach is computationally intensive \nwhen simulating a large number of particles: if dt is measured in \nseconds (fractions of a second, usually), it takes time proportional \nto N 2/dt to simulate an N-particle system for 1 second. This cost is \nprohibitive (even worse than usual for quadratic algorithms)\u2014in \nthe applications of interest, N is very large and dt is very small. The \nchallenge is that if we make dt too small, the computational cost is \nhigh, and if we make dt too large, we may miss collisions.\nEvent-driven simulation. We pursue an alternative approach that \nfocuses only on those times at which collisions occur. In particular, we are always in-\nterested in the next collision (because the simple update of all of the particle positions \nusing their velocities is valid until that time). Therefore, we maintain a priority queue \nof events, where an event is a potential collision sometime in the future, either between \ntwo particles or between a particle and a wall. The priority associated with each event \nis its time, so when we remove the minimum from the priority queue, we get the next \npotential collision.\nCollision prediction. How do we identify potential collisions? The particle velocities \nprovide precisely the information that we need. For example, suppose that we have, at \ntime t, a ", "start": 869, "end": 869}, "1297": {"text": "queue, we get the next \npotential collision.\nCollision prediction. How do we identify potential collisions? The particle velocities \nprovide precisely the information that we need. For example, suppose that we have, at \ntime t, a particle of radius s at position (rx  , ry ) moving with velocity (vx , vy ) in the unit \nbox. Consider the vertical wall at x = 1 with y between 0 and 1. Our interest is in the \nhorizontal component of the motion, so we can concentrate on the x-component of the \nposition rx and the x-component of the velocity vx. If vx is negative, the particle is not \non a collision course with the wall, but if vx is positive, there is a potential collision with \nthe wall. Dividing the horizontal distance to the wall (1 /H11002 s /H11002 rx ) by the magnitude \nof the horizontal component of the velocity  ( vx ) we \ufb01nd that the particle will hit the \nwall after dt = (1 /H11002 s /H11002 rx )/vx time units, when the particle will be at ( 1 /H11002 s, ry /H11001 vy  dt), \nunless it hits some other particle or a horizontal wall before that time. Accordingly, we \ndt too small: excessive computation\ndt too large: may miss collisions\nFundamental challenge for\ntime-driven simulation\n 858 CONTEXT\n \nput an entry on the priority queue with priority t /H11001 dt (and appropriate information \ndescribing the particle-wall collision event). The collision-prediction calculations for \nother walls are similar (see Exercise 6.1). The calculation for two particles colliding \nis also similar, but more complicated. Note that it is often the case that the calculation \nleads to a prediction that the collision will not happen (if the particle is moving away \nfrom the wall, or if two particles ", "start": 869, "end": 870}, "1298": {"text": "\nis also similar, but more complicated. Note that it is often the case that the calculation \nleads to a prediction that the collision will not happen (if the particle is moving away \nfrom the wall, or if two particles are moving away from one another)\u2014we do not need \nto put anything on the priority queue in such cases. To handle another typical situation \nwhere the predicted collision might be too far in the future to be of interest, we include \na parameter limit that speci\ufb01es the time period of interest, so we can also ignore any \nevents that are predicted to happen at a time later than limit.\n C o l l i s i o n  r e s o l u t i o n .  When a collision does occur, we need to resolve it by applying \nthe physical formulas that specify the behavior of a particle after an elastic collision \nwith a re\ufb02ecting boundary or with another particle.  In our example where the particle \nhits the vertical wall, if the collision does occur, the velocity of the particle will change \nfrom (vx  , vy ) to (\u2013 v x  , vy ) at that time. The collision-resolution calculations for other \nwalls are similar, as are the calculations for two particles colliding, but these are more \ncomplicated (see Exercise 6.1).\nprediction (at time t)\n    particles hit unless one passes\n    intersection point before the other\n    arrives\nresolution (at time t + dt)\n     velocities of both particles\n     change after collision\nPredicting and resolving a particle-particle collision\nPredicting and resolving a particle-wall collision\nprediction (at time t)\n    dt  /H11013 time to hit wall\n          = distance/velocity\nresolution (at time t + dt)\n     velocity after collision   = ( \u2212 vx , vy) \n     position after collision  = ( 1 \u2212 s , ry + vydt)\n ", "start": 870, "end": 870}, "1299": {"text": "hit wall\n          = distance/velocity\nresolution (at time t + dt)\n     velocity after collision   = ( \u2212 vx , vy) \n     position after collision  = ( 1 \u2212 s , ry + vydt)\n = (1 \u2212 s \u2212 rx )/vx\n1 \u2212 s \u2212 rx \n(rx , ry )\ns\nwall at\nx = 1\nvx\nvy\n 859 E v e n t - d r i v e n  s i m u l a t i o n  \nInvalidated events. Many of the collisions that we predict do not actu -\nally happen because some other collision intervenes. T o handle this situ-\nation, we maintain an instance variable for each particle that counts the \nnumber of collisions in which it has been involved. When we remove an \nevent from the priority queue for processing, we check whether the counts \ncorresponding to its particle(s) have changed since the event was creat -\ned. This approach to handling invalidated collisions is the so-called lazy \napproach: when a particle is involved in a colli-\nsion, we leave the now-invalid events associated \nwith it on the priority queue and essentially ig -\nnore them when they come off. An alternative approach, the \nso-called eager approach, is to remove from the priority queue \nall events involving any colliding particle before calculating all \nof the new potential collisions for that particle. This approach \nrequires a more sophisticated priority queue (that implements \nthe remove operation).\nThis discussion sets the stage for a full event-driven simula -\ntion of particles in motion, interacting according to the physi-\ncal laws of elastic collisions. The software architecture is to en-\ncapsulate the implementation in three classes: a Particle data \ntype that encapsulates calculations that involve particles, an \nEvent data type for predicted events, and a CollisionSystem\nclient that does the simulation. The cen-\nterpiece of the simulation is a MinPQ ", "start": 870, "end": 871}, "1300": {"text": "classes: a Particle data \ntype that encapsulates calculations that involve particles, an \nEvent data type for predicted events, and a CollisionSystem\nclient that does the simulation. The cen-\nterpiece of the simulation is a MinPQ that \ncontains events, ordered by time. Next, we \nconsider implementations of Particle, \nEvent, and CollisionSystem. \nAn invalidated event\ntwo particles on a collision course\nthird particle interferes: no collision\nPredictable events\nparticle moving\ntowards a wall\nparticles moving\non a collision course\nPredictable non-events\nparticle moving\naway from a wall\nparticles moving\naway from each other\ncollision too far\ninto the future\none particle reaching\n potential collision point\nbefore the other\n 860 CONTEXT\nParticles. Exercise 6.1 outlines the implementation of a data type particles, based on \na direct application of Newton\u2019s laws of motion. A simulation client needs to be able to \nmove particles, draw them, and perform a number of calculations related to collisions, \nas detailed in the following API:\npublic class    Particle\nParticle() create a new random particle in unit square\nParticle(\n  double rx, double ry,\n  double vx, double vy,\n  double s,\n  double mass)\ncreate a particle with the given\n     position,\n     velocity,\n     radius,\n     and mass\nvoid draw() draw the particle\nvoid move(double dt) change position to reflect passage of time dt \nint count() number of collisions involving this particle\ndouble timeToHit(Particle b) time until this particle hits particle b\ndouble timeToHitHorizontalWall() time until this particle hits a horizontal wall\ndouble timeToHitVericaltWall() time until this particle hits a vertical wall\nvoid bounceOff(Particle b) change particle velocities to reflect collision\nvoid bounceOffHorizontalWall() change velocity to reflect hitting horizontal wall\nvoid bounceOffVerticalWall() change velocity to reflect ", "start": 871, "end": 872}, "1301": {"text": "until this particle hits a vertical wall\nvoid bounceOff(Particle b) change particle velocities to reflect collision\nvoid bounceOffHorizontalWall() change velocity to reflect hitting horizontal wall\nvoid bounceOffVerticalWall() change velocity to reflect hitting vertical wall\nAPI for moving-particle objects\n \nThe three timeToHit*() methods all return Double.POSITIVE_INFINITY for the \n(rather common) case when there is no collision course. These methods allow us to \npredict all future collisions that are associated with a given particle, putting an event on \na priority queue corresponding to each one that happens before a given time limit. We \nuse the bounce() method each time that we process an event that corresponds to two \nparticles colliding to change the velocities (of both particles) to re\ufb02ect the collision, and \nthe bounceOff*() methods for events corresponding to collisions between a particle \nand a wall. \n 861 E v e n t - d r i v e n  s i m u l a t i o n  \nEvents. We encapsulate in a pr ivate class the descr iption of  the objects to be placed on \nthe priority queue (events). The instance variable time holds the time when the event \nis predicted to happen, and the instance variables a and b hold the particles associated \nwith the event. W e have three different types of events: a particle may hit a vertical wall, \na horizontal wall, or another particle. T o develop a smooth dynamic display of the par-\nticles in motion, we add a fourth event type, a redraw event that is a command to draw \nall the particles at their current positions. A slight twist in the implementation of Event\nis that we use the fact that particle values may be null to encode these four different \ntypes of events, as follows:\n\u25a0 Neither a nor b null: particle-particle collision\n\u25a0 a not null and b null: collision between a and ", "start": 872, "end": 873}, "1302": {"text": "use the fact that particle values may be null to encode these four different \ntypes of events, as follows:\n\u25a0 Neither a nor b null: particle-particle collision\n\u25a0 a not null and b null: collision between a and a vertical wall\n\u25a0 a null and b not null: collision between b and a horizontal wall\n\u25a0 Both a and b null: redraw event (draw all particles)\nWhile not the \ufb01nest object-oriented programming, this convention is a natural one \nthat enables straightforward client code and leads to the implementation shown below.\nprivate class    Event implements Comparable<Event> \n{\n   private final double time;\n   private final Particle a, b;\n   private final int countA, countB;\n   public Event(double t, Particle a, Particle b)\n   {  // Create a new event to occur at time t involving a and b.\n      this.time = t;\n      this.a    = a;\n      this.b    = b;\n      if (a != null) countA = a.count(); else countA = -1;\n      if (b != null) countB = b.count(); else countB = -1;\n   }\n   public int compareTo(Event that)\n   { \n      if      (this.time < that.time) return -1;\n      else if (this.time > that.time) return +1;\n      else return 0;\n   }\n   public boolean isValid()\n   {\n      if (a != null && a.count() != countA) return false;\n      if (b != null && b.count() != countB) return false;\n      return true;\n   } \n}\nEvent class for particle simulation\n 862 CONTEXT\nA second twist in the implementation of Event is that we maintain the instance vari-\nables countA and countB to record the number of collisions involving each of the par-\nticles at the time the event is created . If these counts are unchanged when the event is \nremoved from the priority ", "start": 873, "end": 874}, "1303": {"text": "the instance vari-\nables countA and countB to record the number of collisions involving each of the par-\nticles at the time the event is created . If these counts are unchanged when the event is \nremoved from the priority queue, we can go ahead and simulate the occurrence of the \nevent, but if one of the counts changes between the time an event goes on the priority \nqueue and the time it leaves, we know that the event has been invalidated and can ig-\nnore it.  The method isValid() allows client code to test this condition.\nSimulation code. With the computational details encapsulated in Particle and Event, \nthe simulation itself requires remarkably little code, as you can see in the implemen-\ntation in the class CollisionSystem (see page 863 and page 864). Most of the calculations\nare encapsulated in the predictCollisions() method shown on this page. This meth-\nod calculates all potential \nfuture collisions involv-\ning particle a (either with \nanother particle or with \na wall) and puts an event \ncorresponding to each onto \nthe priority queue. \nThe heart of the simu-\nlation is the simulate()\nmethod shown on  page \n864. We initialize by calling \npredictCollisions()\nfor each particle to \ufb01ll the \npriority queue with the \npotential collisions involv-\ning all particle-wall and all \nparticle-particle pairs. Then we enter the main event-driven simulation loop, which \nworks as follows:\n\u25a0 Delete the impending event (the one with minimum priority t).\n\u25a0 If the event is invalid, ignore it.\n\u25a0 Advance all particles to time t on a straight-line trajectory.\n\u25a0 Update the velocities of the colliding particle(s).\n\u25a0 Use predictCollisions() to predict future collisions involving the colliding \nparticle(s) and insert onto the priority queue an event corresponding to each.\nThis simulation can serve as the basis for computing all manner of interesting proper-\nties of the ", "start": 874, "end": 874}, "1304": {"text": "predictCollisions() to predict future collisions involving the colliding \nparticle(s) and insert onto the priority queue an event corresponding to each.\nThis simulation can serve as the basis for computing all manner of interesting proper-\nties of the system, as explored in the exercises. For example, one fundamental property \nprivate void predictCollisions(Particle a, double limit) \n{\n   if (a == null) return;\n   for (int i = 0; i < particles.length; i++)\n   {  // Put collision with particles[i] on pq.\n      double dt = a.timeToHit(particles[i]);\n      if (t + dt <= limit)\n         pq.insert(new Event(t + dt, a, particles[i]));\n   }\n   double dtX = a.timeToHitVerticalWall();\n   if (t + dtX <= limit)\n      pq.insert(new Event(t + dtX, a, null));\n   double dtY = a.timeToHitHorizontalWall();\n   if (t + dtY <= limit)\n      pq.insert(new Event(t + dtY, null, a)); \n}\nPredicting collisions with other particles\n 863 E v e n t - d r i v e n  s i m u l a t i o n  \n E v e n t - b a s e d  s i m u l a t i o n  o f  c o l l i d i n g  p a r t i c l e s  ( s c a f f o l d i n g )\npublic class  CollisionSystem \n{\n   private class Event implements Comparable<Event>\n   {  /* See text. */  }\n   private MinPQ<Event> pq;        // the priority queue\n   private double t  = 0.0;        // simulation clock time\n   private Particle[] particles;   // the array of particles\n   public CollisionSystem(Particle[] particles)\n   {  this.particles ", "start": 874, "end": 875}, "1305": {"text": "priority queue\n   private double t  = 0.0;        // simulation clock time\n   private Particle[] particles;   // the array of particles\n   public CollisionSystem(Particle[] particles)\n   {  this.particles  = particles;  }\n   private void predictCollisions(Particle a, double limit)\n   {  /* See text. */  }\n   public void redraw(double limit, double Hz)\n   { // Redraw event: redraw all particles.\n     StdDraw.clear();\n     for(int i = 0; i < particles.length; i++) particles[i].draw();\n     StdDraw.show(20);\n     if (t < limit)\n        pq.insert(new Event(t + 1.0 / Hz, null, null));\n   }\n   public void simulate(double limit, double Hz)\n   {  /* See next page. */  }\n   public static void main(String[] args)\n   {\n      StdDraw.show(0);\n      int N = Integer.parseInt(args[0]);\n      Particle[] particles = new Particle[N];\n      for (int i = 0; i < N; i++) \n         particles[i] = new Particle();\n      CollisionSystem system = new CollisionSystem(particles);\n      system.simulate(10000, 0.5);\n   } \n}\nThis class is a priority-queue client that simulates the motion of a system of particles over time. \nThe main() test client takes a command-line argument N, creates N random particles, creates a \nCollisionSystem consisting of the particles, and calls simulate() to do the simulation. The instance \nvariables are a priority queue for the simulation, the time, and the particles.  864 CONTEXT\n E v e n t - b a s e d  s i m u l a t i o n  o f  c o l l i d i n g  p a r t i c l e s  ( p r i ", "start": 875, "end": 876}, "1306": {"text": "t - b a s e d  s i m u l a t i o n  o f  c o l l i d i n g  p a r t i c l e s  ( p r i m a r y  l o o p )  \npublic void simulate(double limit, double Hz) \n{\n   pq = new MinPQ<Event>();\n   for (int i = 0; i < particles.length; i++)\n      predictCollisions(particles[i], limit);\n   pq.insert(new Event(0, null, null));  // Add redraw event.\n   while (!pq.isEmpty())\n   {  // Process one event to drive simulation.\n      Event event = pq.delMin();\n      if (!event.isValid()) continue;\n      for (int i = 0; i < particles.length; i++)\n         particles[i].move(event.time - t); // Update particle positions\n      t = event.time;                       //   and time.\n      Particle a = event.a, b = event.b;\n      if      (a != null && b != null) a.bounceOff(b);\n      else if (a != null && b == null) a.bounceOffHorizontalWall();\n      else if (a == null && b != null) b.bounceOffVerticalWall();\n      else if (a == null && b == null) redraw(limit, Hz);\n      predictCollisions(a, limit);\n      predictCollisions(b, limit);\n   } \n}\nThis method represents the main event-driven simulation. First, the priority queue is initialized with \nevents representing all predicted future collisions involving each particle. Then the main loop takes \nan event from the queue, updates time and particle positions, and adds new events to re\ufb02ect changes. \na collision\n% java CollisonSystem 5 865 E v e n t - d r i v e n  s i m u l a t i o n  \nof interest ", "start": 876, "end": 877}, "1307": {"text": "re\ufb02ect changes. \na collision\n% java CollisonSystem 5 865 E v e n t - d r i v e n  s i m u l a t i o n  \nof interest is the amount of pressure exerted by the particles against the walls. One way \nto calculate the pressure is to keep track of the number and magnitude of wall colli -\nsions (an easy computation based on particle mass and velocity) so that we can easily \ncompute the total . Temperature involves a similar calculation.\n P e r f o r m a n c e .  As described at the outset, our interest in event-driven simulation is \nto avoid the computationally intensive inner loop intrinsic in time-driven simulation.\nProposition A.  An  event-based simulation of N colliding particles requires at most \nN 2 priority queue operations for initialization, and at most N priority queue op -\nerations per collision (with one extra priority queue operation for each invalid \ncollision).  \nProof Immediate from the code.\n \n \nUsing our standard guaranteed-logarithmic-time-per operation priority-queue imple-\nmentation from Section 2.4, the time needed per collision is linearithmic.  Simulations \nwith large numbers of particles are therefore quite feasible. \nEvent-driven simulation applies to countless other domains that involve physical \nmodeling of moving objects, from molecular modeling to astrophysics to robotics. \nSuch applications may involve extending the model to add other kinds of bodies, to \noperate in three dimensions, to include other forces, and in many other ways.  Each ex-\ntension involves its own computational challenges. This event-driven approach results \nin a more robust, accurate, and ef\ufb01cient simulation than many other alternatives that \nwe might consider, and the ef\ufb01ciency of the heap-based priority queue enables calcula-\ntions that might not otherwise be possible. \nSimulation plays a vital role in helping researchers to ", "start": 877, "end": 877}, "1308": {"text": "than many other alternatives that \nwe might consider, and the ef\ufb01ciency of the heap-based priority queue enables calcula-\ntions that might not otherwise be possible. \nSimulation plays a vital role in helping researchers to understand properties of the \nnatural world in all \ufb01elds of science and engineering. Applications ranging from manu-\nfacturing processes to biological systems to \ufb01nancial systems to complex engineered \nstructures are too numerous to even list here. For a great many of these applications, \nthe extra ef\ufb01ciency afforded by the heap-based priority queue data type or an ef\ufb01cient \nsorting algorithm can make a substantial difference in the quality and extent that are \npossible in the simulation. \n 866 CONTEXT\n \n \n \n   B - t r e e s  In Chapter 3, we saw that algorithms that are appropriate for accessing \nitems from huge collections of data are of immense practical importance. Searching is \na fundamental operation on huge data sets, and such searching consumes a signi\ufb01cant \nfraction of the resources used in many computing environments. With the advent of \nthe web, we have the ability to access a vast amount of information that might be rele-\nvant to a task\u2014our challenge is to be able to search through it ef\ufb01ciently. In this section, \nwe describe a further extension of the balanced-tree algorithms from Section 3.3 that \ncan support external search in symbol tables that are kept on a disk or on the web and \nare thus potentially far larger than those we have been considering (which have to \ufb01t in \naddressable memory). Modern software systems are blurring the distinction between \nlocal \ufb01les and web pages, which may be stored on a remote computer, so the amount \nof data that we might wish to search is virtually unlimited. Remarkably, the methods \nthat we shall study can support search and insert operations on symbol tables contain-\ning trillions ", "start": 877, "end": 878}, "1309": {"text": "stored on a remote computer, so the amount \nof data that we might wish to search is virtually unlimited. Remarkably, the methods \nthat we shall study can support search and insert operations on symbol tables contain-\ning trillions of items or more using only four or \ufb01ve references to small blocks of data.\nCost model. Data storage mechanisms vary widely and continue to evolve, so we use \na simple model to capture the essentials. We use the term page to refer to a contiguous \nblock of data and the term probe to refer to the \ufb01rst access to a page. We assume that \naccessing a page involves reading its contents into local memory, so that subsequent ac-\ncesses are relatively inexpensive. A page could be a \ufb01le on your local computer or a web \npage on a distant computer or part of a \ufb01le on \na server, or whatever. Our goal is to develop \nsearch  implementations that use a small num-\nber of probes to \ufb01nd any given key.  We avoid \nmaking speci\ufb01c assumptions about the page \nsize and about the ratio of the time required \nfor a probe (which presumably requires com-\nmunicating with a distant device) to the time \nrequired, subsequently, to access items within \nthe block (which presumably happens in a local processor). In typical situations, these \nvalues are likely to be on the order of 100 or 1,000 or 10,000; we do not need to be more \nprecise because the algorithms are not highly sensitive to differences in the values in the \nranges of interest.\nB-trees. The approach is to extend the 2-3 tree data structure described in Section \n3.3, with a crucial difference: rather than store the data in the tree, we build a tree with \ncopies of the keys, each key copy associated with a link. This approach enables us to \nmore easily separate the ", "start": 878, "end": 878}, "1310": {"text": "\n3.3, with a crucial difference: rather than store the data in the tree, we build a tree with \ncopies of the keys, each key copy associated with a link. This approach enables us to \nmore easily separate the index from the table itself, much like the index in a book. As \nwith 2-3 trees, we enforce upper and lower bounds on the number of key-link pairs \nB-tree  cost model. When study -\ning algorithms for external \nsearching, we count page accesses\n(the number of times a page is ac-\ncessed, for read or write). \n 867 B - t r e e s  \nthat can be in each node: we choose a parameter M (an even number, by convention)   \nand build multiway trees where every node must have at most M /H11002 1 key-link pairs (we \nassume that M is suf\ufb01ciently small that an M-way node will \ufb01t on a page) and at least\nM/2 key-link pairs (to provide the branching that we need to keep search paths short), \nexcept possibly the root, which can have fewer than M/2 key-link pairs but must have \nat least 2.  Such trees were named B-trees by Bayer and McCreight, who, in 1970, were \nthe \ufb01rst researchers to consider the use of multiway balanced trees for external search-\ning. Some people reserve the term B-tree to describe the exact data structure built by \nthe algorithm suggested by Bayer and McCreight; we use it as a generic term for data \nstructures based on multiway balanced search trees with a \ufb01xed page size. We specify \nthe value of M by using the terminology \u201cB-tree of order M.\u201d In  a B-t re e of  order 4, each  \nnode has at most 3 and at least 2 key-link pairs; in ", "start": 878, "end": 879}, "1311": {"text": "\nthe value of M by using the terminology \u201cB-tree of order M.\u201d In  a B-t re e of  order 4, each  \nnode has at most 3 and at least 2 key-link pairs; in a B-tree of order 6, each node has at \nmost 5 and at least 3 link pairs (except possibly the root, which could have 2 key-link \npairs), and so forth. The reason for the exception at the root for larger M will become \nclear when we consider the construction algorithm in detail.\nConventions. To  i l l u s t r a te  t h e  b a s i c  m e c h a n i s m s , w e  co n s i d e r  a n  ( o rd e re d )  SET imple-\nmentation (with keys and no values). Extending to provide an ordered ST to associate \nkeys with values is an instructive exercise (see Exercise 6.16). Our goal is to support \nadd() and contains() for a set of keys that could be huge. We use ordered keys be -\ncause we are generalizing search trees, which are based on ordered keys. Extending our \nimplementation to support other ordered operations is also an instructive exercise. In \nexternal searching applications, it is common to keep the index separate from the data. \nFor B-trees, we do so by using two different kinds of nodes: \n\u25a0 Internal nodes, which associate copies of keys with pages\n\u25a0 External nodes, which have references to the actual data\nEvery key in an internal node is associated with another node that is the root of a tree \ncontaining all keys greater than or equal to that key and less than the next largest key, if \nAnatomy of a B-tree set (M = 6)\n2-node\nexternal\n3-node external 5-node (full)\n internal 3-node\n ", "start": 879, "end": 879}, "1312": {"text": "than or equal to that key and less than the next largest key, if \nAnatomy of a B-tree set (M = 6)\n2-node\nexternal\n3-node external 5-node (full)\n internal 3-node\n external 4-node\nall nodes except the root are 3-, 4- or 5-nodes\n* B C\n sentinel key\nD E F H I J K M N O P Q R T\n* D H\n* K\nK Q U\nU W X Y\neach red key is a copy\nof min key in subtree\nclient keys (black)\nare in external nodes\n 868 CONTEXT\nany. It is convenient to use a special key, known as a sentinel, that is de\ufb01ned to be less \nthan all other keys, and to start with a root node containing that key, associated with \nthe tree containing all the keys. The symbol table does not contain duplicate keys, but \nwe use copies of keys (in internal nodes) to guide the search. (In our examples, we use \nsingle-letter keys and the character * as the sentinel that is less than all other keys.) \nThese conventions simplify the code somewhat and thus represent a convenient (and \nwidely used) alternative to mixing all the data with links in the internal nodes, as we \nhave done for other search trees. \n  S e a r c h  a n d  i n s e r t .  Search in a B-tree is based on recursively searching in the unique \nsubtree that could contain the search key. Every search ends in an external node that \ncontains the key if and only if it is in the set. We might also terminate a search hit when \nencountering a copy of the search key in an internal node, but we always search to an \nexternal node because doing so simpli\ufb01es extending the code to an ordered symbol-\ntable implementation (also, ", "start": 879, "end": 880}, "1313": {"text": "search hit when \nencountering a copy of the search key in an internal node, but we always search to an \nexternal node because doing so simpli\ufb01es extending the code to an ordered symbol-\ntable implementation (also, this event rarely happens when M is large).  T o be speci\ufb01c, \nconsider searching in a B-tree of order 6: it consists of 3-nodes with 3 key-link pairs, \n4-nodes with 4 key-link pairs, and 5-nodes with 5 key-link pairs, with possibly a 2-node \nat the root. T o search, we start at the root and move from one node to the next by \ufb01nd-\ning the proper interval for the search key in the current node and then exiting through \nthe corresponding link to get to the next node. Eventually, the search process leads us to \na page containing keys at the bottom of the tree. We terminate the search with a search \nhit if the search key is in that page; we terminate with a search miss if it is not.  As with \n2-3 trees, we can use recursive code to insert a new key at the bottom of the tree. If there \nis no room for the key, we allow the node at the bottom to temporarily over\ufb02ow (be -\ncome a 6-node) and then split 6-nodes on the way up the tree, after the recursive call. If \nthe root is an 6-node, we split it into a 2-node connected to two 3-nodes; elsewhere in \nthe tree, we replace any k-node attached to a 6-node by a ( k+1)-node attached to two \n3-nodes. Replacing 3 by M/2 and 6 by M in this description converts it into a descrip-\ntion of search and insert for B-trees of order M and leads ", "start": 880, "end": 880}, "1314": {"text": "k+1)-node attached to two \n3-nodes. Replacing 3 by M/2 and 6 by M in this description converts it into a descrip-\ntion of search and insert for B-trees of order M and leads to the following de\ufb01nition:\nDefinition. A B-tree of order M (where M is an even positive integer) is a tree that \neither is an external k-node (with k keys and associated information) or comprises \ninternal k-nodes (each with k keys and k links to B-trees representing each of the k\nintervals delimited by the keys), having the following structural properties: every \npath from the root to an external node must be the same length ( perfect balance); \nand k must be between 2 and M /H11002 1 at the root and between M/2 and M /H11002 1 at \nevery other node. \n 869 B - t r e e s  \n* A B C E F H I J K M N O P Q R T\n* C H\n* K\nK Q U\nU W X\n* A B C E F H I J K M N O P Q R T U W X\n* C H K Q U\n* A B C E F H I J K M N O P Q R T U W X\n* H K Q U\n* B C E F H I J K M N O P Q R T U W X\n* H K Q U\nnew key (A) causes\noverflow and split\nroot split causes\na new root to be created\nnew key (C) causes\noverflow and split\nInserting a new key into a B-tree set\ninserting A\n* B C\nsearching for E\nD E F H I J K M N O P Q R T\n* D H\n* K\nK Q U\nU ", "start": 880, "end": 881}, "1315": {"text": "into a B-tree set\ninserting A\n* B C\nsearching for E\nD E F H I J K M N O P Q R T\n* D H\n* K\nK Q U\nU W X\nsearch for E in\nthis external node\nfollow this link because\nE is between * and K\nfollow this link because\nE is between D and H\nSearching in a B-tree set (M = 6)\n 870 CONTEXT\n \nRepresentation. As just discussed, we have a great deal of freedom in choosing con -\ncrete representations for nodes in B-trees. We encapsulate these choices in a Page API \nthat associates keys with links to Page objects and supports the operations that we \nneed to test for overfull pages, split them, and distinguish between internal and exter -\nnal pages. Y ou can think of a Page as a symbol table, kept externally (in a \ufb01le on your \ncomputer or on the web). The terms open and close in the API refer to the process of \nbringing an external page into internal memory and writing its contents back out (if \nnecessary). The put() method for internal pages is a symbol-table operation that asso-\nciates the given page with the minimum key in the tree rooted at that page. The put()\nand contains() methods for external pages are like their corresponding SET opera-\ntions. The workhorse of any implementation is the split() method, which splits a full \npage by moving the M/2 key-value pairs of rank greater than M/2 to a new Page and \nreturns a reference to that page. Exercise 6.15 discusses an implementation of Page \nusing BinarySearchST, which implements B-trees in memory, like our other search \nimplementations. On some systems, this might suf\ufb01ce as an external searching imple-\nmentation because a virtual-memory system might take care of disk references. More \ntypical practical implementations ", "start": 881, "end": 882}, "1316": {"text": "memory, like our other search \nimplementations. On some systems, this might suf\ufb01ce as an external searching imple-\nmentation because a virtual-memory system might take care of disk references. More \ntypical practical implementations might involve hardware-speci\ufb01c code that reads and \nPage<Key> public class    \nPage(boolean bottom) create and open a page\nvoid close() close a page\nvoid add(Key key) put key into the (external) page\nvoid add(Page p)\nopen p and put an entry into this\n(internal) page that associates the \nsmallest key in p with p\nboolean isExternal() is this page external?\nboolean contains(Key key) is key in the page?\nPage next(Key key) the subtree that could contain the key\nboolean isFull() has the page overflowed?\nPage split() move the highest-ranking half of the \nkeys in the page to a new page\nIterable<Key> keys() iterator for the keys on the page\nAPI for a B-tree page\n 871 B - t r e e s  \nwrites pages. Exercise 6.19 encourages you to think about implementing Page using \nweb pages. We ignore such details here in the text to emphasize the utility of the B-tree \nconcept in a broad variety of settings.\nWith these preparations, the code for BTreeSET on page 872  is remarkably simple. For \ncontains(), we use a recursive method that takes a Page as argument and handles \nthree cases:\n\u25a0 If the page is external and the key is in the page, return true.\n\u25a0 If the page is external and the key is not in the page, return false.\n\u25a0 Otherwise, do a recursive call for the subtree that could contain the key.\nFor put() we use the same recursive structure, but insert the key at the bottom if it is \nnot found during the search and then split any full nodes on the way up the tree.\nPerformance. The most important property ", "start": 882, "end": 883}, "1317": {"text": "key.\nFor put() we use the same recursive structure, but insert the key at the bottom if it is \nnot found during the search and then split any full nodes on the way up the tree.\nPerformance. The most important property of B-trees is that for reasonable values of \nthe parameter M the search cost is constant, for all practical purposes:\nProposition B.  A   search or an insertion in a B-tree of order M with N items requires \nbetween logM N and logM/2 N probes\u2014a constant number, for practical purposes.  \nProof This property follows from the observation that all the nodes in the interior \nof the tree (nodes that are not the root and are not external) have between M/2 \nand M /H11002 1 links, since they are formed from a split of a full node with M keys and \ncan only grow in size (when a child is split). In the best case, these nodes form a \ncomplete tree of branching factor M /H11002 1, which leads immediately to the stated \nbound. In the worst case, we have a root with two entries each of which refers to a \ncomplete tree of degree M/2. Taking the logarithm to the base M results in a very \nsmall number\u2014for example, when M is 1,000, the height of the tree is less than 4 \nfor N less than 62.5 billion. \nIn typical situations, we can reduce the cost by one probe by keeping the root in in -\nternal memory. For searching on disk or on the web, we might take this step explicitly \nbefore embarking on any application involving a huge number of searches; in a virtual \nmemory with caching, the root node will be the one most likely to be in fast memory, \nbecause it is the most frequently accessed node.\nSpace. The space usage of B-trees is also of interest in practical applications. By con-\nstruction, ", "start": 883, "end": 883}, "1318": {"text": "caching, the root node will be the one most likely to be in fast memory, \nbecause it is the most frequently accessed node.\nSpace. The space usage of B-trees is also of interest in practical applications. By con-\nstruction, the pages are at least half full, so, in the worst case, B-trees use about double \nthe space that is absolutely necessary for keys, plus extra space for links. For random \nkeys, A. Yao proved in 1979 (using mathematical analysis that is beyond the scope of \n 872 CONTEXT\nALGORITHM 6.12   B-tree set implementation\npublic class  BTreeSET<Key extends Comparable<Key>> \n{\n   private Page root = new Page(true);\n   public BTreeSET(Key sentinel)\n   {  put(sentinel);  }\n   public boolean contains(Key key)\n   {  return contains(root, key);  }\n   private boolean contains(Page h, Key key)\n   {\n      if (h.isExternal()) return h.contains(key);\n      return contains(h.next(key), key);\n   }\n   public void add(Key key)\n   {\n      put (root, key);\n      if (root.isFull())\n      {\n         Page lefthalf = root;\n         Page righthalf = root.split();\n         root = new Page(false);\n         root.put(lefthalf);\n         root.put(righthalf);\n      }\n   }\n   public void add(Page h, Key key)\n   {\n      if (h.isExternal()) {  h.put(key); return;  }\n      Page next = h.next(key);\n      put(next, key);\n      if (next.isFull())\n         h.put(next.split());\n      next.close();\n   } \n}\n \nThis B-tree implementation implements multiway balanced search trees as described in the text, us-\ning a Page data type that supports search by associating keys with subtrees that could contain the key \nand supports insertion by including a test ", "start": 883, "end": 884}, "1319": {"text": "\nThis B-tree implementation implements multiway balanced search trees as described in the text, us-\ning a Page data type that supports search by associating keys with subtrees that could contain the key \nand supports insertion by including a test for over\ufb02ow and a page split method. 873 B - t r e e s  \nfull page, about to split\nBuilding a large B-tree\n 874 CONTEXT\n \n \nthis book) that the average number of keys in a node is about M ln 2, so about 44 per-\ncent of the space is unused.  As with many other search algorithms, this random model \nreasonably predicts results for key distributions that we observe in practice.\nThe implications of Proposition B are profound and worth contemplating. Would \nyou have guessed that you can develop a search implementation that can guarantee a \ncost of four or \ufb01ve probes for search and insert in \ufb01les as large as you can reason -\nably contemplate needing to process? B-trees are widely used because they allow us to \nachieve this ideal. In practice, the primary challenge to developing an implementation \nis ensuring that space is available for the B-tree nodes, but even that challenge becomes \neasier to address as available storage space increases on typical devices.\n   Many variations on the basic B-tree abstraction suggest themselves immediately .   \nOne class of variations saves time by packing as many page references as possible in \ninternal nodes, thereby increasing the branching factor and \ufb02attening the tree.  Another \nclass of variations improves storage ef\ufb01ciency by combining nodes with siblings before \nsplitting.  The precise choice of variant and algorithm parameter can be engineered to \nsuit particular devices and applications. Although we are limited to getting a small con-\nstant factor improvement, such an improvement can be of signi\ufb01cant importance for \napplications where the table is huge and/or huge numbers of transactions are involved, \nprecisely the applications ", "start": 884, "end": 886}, "1320": {"text": "limited to getting a small con-\nstant factor improvement, such an improvement can be of signi\ufb01cant importance for \napplications where the table is huge and/or huge numbers of transactions are involved, \nprecisely the applications for which B-trees are so effective.\n 875 S u f f i x  a r r a y s  \n \n \n \n \n \n   S u f \ufb01 x  a r r a y s  Ef\ufb01cient algorithms for string processing play a critical role in com-\nmercial applications and in scienti\ufb01c computing. From the countless strings that de\ufb01ne \nweb pages that are searched by billions of users to the extensive genomic databases that \nscientists are studying to unlock the secret of life, computing applications of the 21st \ncentury are increasingly string-based. As usual, some classic algorithms are effective, \nbut remarkable new algorithms are being developed. Next, we describe a data structure \nand an API that support some of these algorithms. We begin by describing a typical \n(and a classic) string-processing problem.\nLongest repeated substring. What is the longest substring that appears at least \ntwice in a given string? For example, the longest repeated substring in the string \n\"to be or not to be\" is the string \"to be\". Think brie\ufb02y about how you might solve \nit. Could you \ufb01nd the longest repeated substring in a string that has millions of char -\nacters? This problem is simple to state and has many important applications, including \ndata compression, cryptography, and computer-assisted music analysis. For example, \na standard technique used in the development of large software systems is refactoring \ncode. Programmers often put together new programs by cutting and pasting code from \nold programs. In a large program built over a long period of time, replacing duplicate \ncode by function calls to a single copy of the code can make the program much easier \nto ", "start": 886, "end": 887}, "1321": {"text": "new programs by cutting and pasting code from \nold programs. In a large program built over a long period of time, replacing duplicate \ncode by function calls to a single copy of the code can make the program much easier \nto understand and maintain. This improvement can be accomplished by \ufb01nding long \nrepeated substrings in the program. Another application is found in computational \nbiology. Are substantial identical fragments to be found within a given genome? Again, \nthe basic computational problem underlying this question is to \ufb01nd the longest repeat-\ned substring in a string. Scientists are typically interested in more detailed questions \n(indeed, the nature of the repeated substrings is precisely what scientists seek to under-\nstand), but such questions are certainly no easier to answer than the basic question of \n\ufb01nding the longest repeated substring.\nBrute-force solution. As a warmup, consider the following simple task: given two \nstrings, \ufb01nd their  longest common pre\ufb01x (the longest substring that is a pre\ufb01x of both \nstrings). For example, the longest \ncommon pre\ufb01x of acctgttaac and \naccgttaa is acc. The code at right \nis a useful starting point for address-\ning more complicated tasks: it takes \ntime proportional to the length of \nthe match. Now, how do we \ufb01nd the \nlongest repeated substring in a given \nstring? With lcp(), the following \nprivate static int lcp(String s, String t) \n{\n   int N = Math.min(s.length(), t.length());\n   for (int i = 0; i < N; i++)\n      if (s.charAt(i) != t.charAt(i)) return i;\n   return N; \n}\n L o n g e s t  c o m m o n  p r e f i x  o f  t w o  s t r i n g s\n ", "start": 887, "end": 887}, "1322": {"text": "t.charAt(i)) return i;\n   return N; \n}\n L o n g e s t  c o m m o n  p r e f i x  o f  t w o  s t r i n g s\n 876 CONTEXT\n \n \n \nbrute-force solution immediately suggests itself: we compare the substring starting at \neach string position i with the substring starting at each other starting position j, keep-\ning track of the longest match found. This code is not useful for long strings, because \nits running time is at least quadratic in the length of the string: as usual, the number of \ndifferent pairs i and j is N (N/H110021)/H11408 2, so the number of calls on lcp() for this approach \nwould be ~N 2/2. Using this solution for a genomic sequence with millions of characters \nwould require trillions of lcp() calls, which is infeasible.\nSuf\ufb01x sort solution. The following clever approach, which takes advantage of sorting \nin an unexpected way, is an effective way to \ufb01nd the longest repeated substring, even in \na huge string: we use Java\u2019s substring() meth-\nod to make an array of strings that consists of \nthe suf\ufb01xes of s (the substrings starting at each \nposition and going to the end), and then we sort \nthis array. The key to the algorithm is that ev-\nery substring appears somewhere as a pre\ufb01x of \none of the suf\ufb01xes in the array. After sorting, the \nlongest repeated substrings will appear in adja-\ncent positions in the array. Thus, we can make \na single pass through the sorted array, keeping \ntrack of the longest matching pre\ufb01xes between \nadjacent strings. This approach is signi\ufb01cantly \nmore ef\ufb01cient than the brute-force method, but \nbefore implementing ", "start": 887, "end": 888}, "1323": {"text": "the sorted array, keeping \ntrack of the longest matching pre\ufb01xes between \nadjacent strings. This approach is signi\ufb01cantly \nmore ef\ufb01cient than the brute-force method, but \nbefore implementing and analyzing it, we con-\nsider another application of suf\ufb01x sorting.\nComputing the LRS by sorting suffixes\naacaagtttacaagc\nacaagtttacaagc\ncaagtttacaagc\naagtttacaagc\nagtttacaagc\ngtttacaagc\ntttacaagc\nttacaagc\ntacaagc\nacaagc\ncaagc\naagc\nagc\ngc\nc\naacaagtttacaagc\naagc\naagtttacaagc\nacaag  c\nacaag  tttacaagc\nagc\nagtttacaagc\nc\ncaagc\ncaagtttacaagc\ngc\ngtttacaagc\ntacaagc\nttacaagc\ntttacaagc\nsuffixes\ninput string\nsorted suffixes\n0 1 2 3 4 5 6 7 8 9 \naacaagtttacaagc\n10 11 12 13\nlongest repeated substring\na acaag  ttt  acaag  c\n91\n14\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n0\n11\n3\n9\n1\n12\n4\n14\n10\n2\n13\n5\n8\n7\n6\n 877 S u f f i x  a r r a y s  \nIndexing a string. When you are trying to \ufb01nd a particular ", "start": 888, "end": 889}, "1324": {"text": "c\n91\n14\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n0\n11\n3\n9\n1\n12\n4\n14\n10\n2\n13\n5\n8\n7\n6\n 877 S u f f i x  a r r a y s  \nIndexing a string. When you are trying to \ufb01nd a particular substring within a large \ntext\u2014for example, while working in a text editor or within a page you are viewing with \na browser\u2014you are doing a substring search , the problem we considered in Section \n5.3. For that problem, we assume the text to be relatively large and focus on preprocess-\ning the substring, with the goal of being able to ef\ufb01ciently \ufb01nd that substring in any \ngiven text. When you type search keys into your web browser, you are doing a search \nwith string keys , the subject of Section 5.2. Y our search engine must precompute an \nindex, since it cannot afford to scan all the pages in the web for your keys. As we dis-\ncussed in Section 3.5 (see FileIndex on page 501), this would ideally be an inverted in-\ndex associating each possible search \nstring with all web pages that con-\ntain it\u2014a symbol table where each \nentry is a string key and each value \nis a set of pointers (each pointer giv-\ning the information necessary to lo-\ncate an occurrence of the key on the \nweb\u2014perhaps a URL that names \na web page and an integer offset \nwithin that page). In practice, such \na symbol table would be far too big, \nso your search engine uses various \nsophisticated algorithms to reduce \nits size. One approach is to rank web \npages by importance (perhaps using \nan algorithm like ", "start": 889, "end": 889}, "1325": {"text": "\na symbol table would be far too big, \nso your search engine uses various \nsophisticated algorithms to reduce \nits size. One approach is to rank web \npages by importance (perhaps using \nan algorithm like the PageRank al-\ngorithm that we discussed on page \n507) and work only with highly-ranked \npages, not all pages. Another ap-\nproach to cutting down on the size \nof a symbol table to support search with string keys is to associate URLs with words\n(substrings delimited by whitespace) as keys in the precomputed index. Then, when \nyou search for a word, the search engine can use the index to \ufb01nd the (important) \npages containing your search keys (words) and then use substring search within each \npage to \ufb01nd them.  But with this approach, if the text were to contain \"everything\"\nand you were to search for \"thing\", you would not \ufb01nd it. For some applications, it \nis worthwhile to build an index to help \ufb01nd any substring within a given text. Doing \nso might be justi\ufb01ed for a linguistic study of an important piece of literature, for a ge-\nnomic sequence that might be an object of study for many scientists, or just for a widely \nIdealized view of a typical web search\nkey\nsymbol-table search with string keys:\nfind the pages containing the key\nsubstring search:\nfind the key in the page\nvalue    \n       ... it \nwas the best \ndeal I could \nget ...\n   \n... it was the \nbest kiss I\u2019ve \never had . .\n   \n   ... it was \nthe best of \ntimes, it was \nthe worst of \ntimes ...\nit was the best\n 878 CONTEXT\n \n \n \naccessed web page. Again, ideally, the index \nwould associate all possible substrings of the \ntext string with each position where it occurs \nin the text string, as ", "start": 889, "end": 890}, "1326": {"text": "...\nit was the best\n 878 CONTEXT\n \n \n \naccessed web page. Again, ideally, the index \nwould associate all possible substrings of the \ntext string with each position where it occurs \nin the text string, as depicted at right. The ba-\nsic problem with this ideal is that the number \nof possible substrings is too large to have a \nsymbol-table entry for each of them (an N-\ncharacter text has N (N/H110021)/H11408 2 substrings). \nThe table for the example at right would need \nentries for b, be, bes, best, best o, best of, \ne, es, est, est o, est of, s, st, st o, st of, \nt, t o, t of,  o, of, and many, many other substrings. Again, we can use a suf\ufb01x  sort to \naddress this problem in a manner analogous to our \ufb01rst symbol-table implementation \nusing binary search, in Section 3.1. We consider each of the N suf\ufb01xes to be keys, create \na sorted array of our keys (the suf\ufb01xes), and use binary search to search in that array, \ncomparing the search key with each suf\ufb01x.  \nit was the best of times it was the\nt was the best of times it was the\n was the best of times it was the\nwas the best of times it was the\nas the best of times it was the\ns the best of times it was the\n the best of times it was the\nthe best of times it was the\nhe best of times it was the\ne best of times it was the\n best of times it was the\nbest of times it was the\nest of times it was the\nst of times it was the\nt of times it was the\n of times it was the\nof ", "start": 890, "end": 890}, "1327": {"text": "times it was the\n best of times it was the\nbest of times it was the\nest of times it was the\nst of times it was the\nt of times it was the\n of times it was the\nof times it was the\nf times it was the\n times it was the\ntimes it was the\nimes it was the\nmes it was the\nes it was the\ns it was the\n it was the\nit was the\nt was the\n was the\nwas the\nas the\ns the\n the\nthe\nhe\ne\n   0   10    0        best of times it was the\n   1   24    1        it was the\n   2   15    1        of times it was the\n   3   31    1        the\n   4    6    4        the best of times it was the\n   5   18    2        times it was the\n   6   27    1        was the\n   7    2    8        was the best of times it was the\n   8   29    0       as the\n   9    4    6       as the best of times it was the\n  10   11    0       best of times it was the\n  11   34    0       e\n  12    9    1       e best of times it was the\n  13   22    1       es it was the\n  14   12    2       est of times it was the\n  15   17    0       f times it was the\n  16   33    0       he\n  17    8    2       he best of times it was ", "start": 890, "end": 890}, "1328": {"text": "the\n  15   17    0       f times it was the\n  16   33    0       he\n  17    8    2       he best of times it was the\n  18   20    0       imes it was the\n  19   25    1       it was the\n  20    0   10       it was the best of times it was the\n  21   21    0       mes it was the\n  22   16    0       of times it was the\n  23   23    0       s it was the\n  24   30    2       s the\n  25    5    5       s the best of times it was the\n  26   13    1       st of times it was the\n  27   14    0       t of times it was the\n  28   26    2       t was the\n  29    1    9       t was the best of times it was the\n  30   32    1       the\n  31    7    3       the best of times it was the\n  32   19    1       times it was the\n  33   28    0       was the\n  34    3    7       was the best of times it was the\nBinary search in a suffix array\nsuffixes sorted suffix array\n 0\n 1\n 2\n 3\n4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\nselect(9)\nindex(9)\nlcp(20)\nrank(\"th\")\ni ", "start": 890, "end": 890}, "1329": {"text": "6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\nselect(9)\nindex(9)\nlcp(20)\nrank(\"th\")\ni index(i) lcp(i)\nintervals containing\n\"th\" found by  rank()\nduring binary search\nIdealized view of a text-string index\nkey value    \n         ... \nit was the \nbest of times, \nit was the \nworst of times \nit was the age \nof wisdom    \nit was the age \nof foolishness \nit was the \nepoch of \nbelief\n ...\nbest of times\nit was\n 879 S u f f i x  a r r a y s  \n \n \n \n \nAPI and client code. To  s u p p o r t  c l i e n t  co d e  to  s o lve  t h e s e  t wo  p ro b l e m s , w e  a r t i c-\nulate the API shown below. It includes a constructor; a length() method; methods \nselect() and index(), which give the string and index of the suf\ufb01x of a given rank in \nthe sorted list of suf\ufb01xes; a method lcp() that gives the length of the longest common \npre\ufb01x of each suf\ufb01x and the one preceding it in the sorted list; and a method rank()\nthat gives the number of suf\ufb01xes less than the given key (just as we have been using \nsince we \ufb01rst examined binary search in Chapter 1). We use the term suf\ufb01x array to \ndescribe the abstraction of a sorted list of suf\ufb01x strings, without necessarily committing \nto ", "start": 890, "end": 891}, "1330": {"text": "have been using \nsince we \ufb01rst examined binary search in Chapter 1). We use the term suf\ufb01x array to \ndescribe the abstraction of a sorted list of suf\ufb01x strings, without necessarily committing \nto use an array of strings as the underlying data structure.\npublic class    SuffixArray \nSuffixArray(String text) build suffix array for text \nint length() length of text \nString select(int i)  ith in the suffix array \n(i between 0 and N-1)\nint index(int i) index of  select(i)  (i between 0 and N-1)\nint lcp(int i) length of longest common prefix of select(i)\nand select(i-1) (i between 1 and N-1)\nint    rank(String key)  number of suffixes less than  key \nSuffix array API\n   \n \n  \n \nIn the example on the facing page, select(9) is \"as the best of times...\", index(9)\nis 4, lcp(20) is 10 because \"it was the best of times...\" and \"it was the\" have \nthe common pre\ufb01x \"it was the\" which is of length 10, and rank(\"th\") is 30.  Note \nalso that the select(rank(key)) is the \ufb01rst possible suf\ufb01x in the sorted suf\ufb01x list that \nhas key as pre\ufb01x and that all other occurrences of key in the text immediately follow \n(see the \ufb01gure on the opposite page). With this API, the client code on the next two \npages is immediate. LRS (page 880) \ufb01nds the longest repeated substring in the text on stan-\ndard input by building a suf\ufb01x array and then scanning through the sorted suf\ufb01xes to \n\ufb01nd the maximum lcp() value.  KWIC (page 881) builds ", "start": 891, "end": 891}, "1331": {"text": "text on stan-\ndard input by building a suf\ufb01x array and then scanning through the sorted suf\ufb01xes to \n\ufb01nd the maximum lcp() value.  KWIC (page 881) builds a suf\ufb01x array  for the text named \nas command-line argument, takes queries from standard input, and prints all occur -\nrences of each query in the text (including a speci\ufb01ed number of characters before and \nafter to give context). The name KWIC stands for  keyword-in-context search, a term \ndating at least to the 1960s. The simplicity and ef\ufb01ciency of this client code for these \ntypical string-processing applications is remarkable, and testimony to the importance \nof careful API design (and the power of a simple but ingenious idea).\n 880 CONTEXT\n% more tinyTale.txt \nit was the best of times it was the worst of times \nit was the age of wisdom it was the age of foolishness \nit was the epoch of belief it was the epoch of incredulity \nit was the season of light it was the season of darkness \nit was the spring of hope it was the winter of despair\n% java LRS < tinyTale.txt \nst of times it was the  \npublic class  LRS \n{\n   public static void main(String[] args)\n   {\n      String text = StdIn.readAll();\n      int N = text.length();\n      SuffixArray sa = new SuffixArray(text);\n      String lrs = \"\";\n      for (int i = 1; i < N; i++)\n      {\n         int length = sa.lcp(i);\n         if (length > substring.length())\n            lrs = sa.select(i).substring(0, length);\n      }\n      StdOut.println(lrs);\n   } \n} \n L o n g e s t  r e p e a t e d  s u ", "start": 891, "end": 892}, "1332": {"text": "lrs = sa.select(i).substring(0, length);\n      }\n      StdOut.println(lrs);\n   } \n} \n L o n g e s t  r e p e a t e d  s u b s t r i n g  c l i e n t\n 881 S u f f i x  a r r a y s  \npublic class  KWIC \n{\n   public static void main(String[] args)\n   { \n      In in = new In(args[0]);\n      int context = Integer.parseInt(args[1]);\n      String text = in.readAll().replaceAll(\"\\\\s+\", \" \");;\n      int N = text.length();\n      SuffixArray sa = new SuffixArray(text);\n      while (StdIn.hasNextLine())\n      {\n         String q = StdIn.readLine();\n         for (int i = sa.rank(q); i < N && sa.select(i).startsWith(q); i++)\n         {\n            int from = Math.max(0, sa.index(i) - context);\n            int to   = Math.min(N-1, from + q.length() + 2*context);\n            StdOut.println(text.substring(from, to));\n         }\n         StdOut.println();\n      }\n   } \n} \n K e y w o r d - i n - c o n t e x t  i n d e x i n g  c l i e n t\n% java KWIC tale.txt 15 \nsearch \no st giless to search for contraband \nher unavailing search for your fathe \nle and gone in search of her husband \nt provinces in search of impoverishe\n dispersing in search of other carri \nn that bed and search the straw hold\nbetter thing \nt is a far far better thing that i do than\n some sense of better things else forgotte \nwas capable of better things mr carton ent\n 882 CONTEXT\n \n  \n \n \nImplementation. ", "start": 892, "end": 894}, "1333": {"text": "search the straw hold\nbetter thing \nt is a far far better thing that i do than\n some sense of better things else forgotte \nwas capable of better things mr carton ent\n 882 CONTEXT\n \n  \n \n \nImplementation. The code on the facing page is a straightforward implementation of \nthe SuffixArray API. Its instance variables are an array of strings and (for economy \nin code) a variable N that holds the length of the array (the length of the string and its \nnumber of suf\ufb01xes). The constructor builds the suf\ufb01x array and sorts it, so select(i)\njust returns suffixes[i]. The implementation of index() is also a one-liner, but it is a \nbit tricky, based on the observation that the length of the suf\ufb01x string uniquely determines \nits starting point. The suf\ufb01x of length N starts at position 0, the suf\ufb01x of length N-1 starts \nat position 1, the suu\ufb01x of length N-2 starts at position 2, and so forth,  so index(i)\njust returns N - suffixes[i].length(). The implementation of lcp() is immedi-\nate, given the static method lcp() on page 875, and rank() is virtually the same as our \nimplementation of binary search for symbol tables, on page 381. Again, the simplicity and \nelegance of this implementation should not mask the fact that it is a sophisticated algo-\nrithm that enables solution of important problems like the longest repeated substring \nproblem that would otherwise seem to be infeasible.\nPerformance. The ef\ufb01ciency of suf\ufb01x sorting depends on the fact that Java substring \nextraction uses a constant amount of space\u2014each substring is composed of standard \nobject overhead, a pointer into the original, and a length. Thus, the size of the index \nis linear in the size of ", "start": 894, "end": 894}, "1334": {"text": "Java substring \nextraction uses a constant amount of space\u2014each substring is composed of standard \nobject overhead, a pointer into the original, and a length. Thus, the size of the index \nis linear in the size of the string. This point is a bit counterintuitive because the total \nnumber of characters in the suf\ufb01xes is ~ N 2/2, a quadratic function of the size of the \nstring. Moreover, that quadratic factor gives one pause when considering the cost of \nsorting the suf\ufb01x array. It is very important to bear in mind that this approach is effec-\ntive for long strings because of the Java representation for strings: when we exchange \ntwo strings, we are exchanging only references, not the whole string. Now, the cost of \ncomparing two strings may be proportional to the length of the strings in the case when \ntheir common pre\ufb01x is very long, but most comparisons in typical applications involve \nonly a few characters. If so, the running time of the suf\ufb01x sort is linearithmic. For ex-\nample, in many applications, it is reasonable to use a random string model:\n \nProposition C. Using 3-way string quicksort, we can build a  suf\ufb01x array from a \nrandom string of length N with space proportional to N and  ~ 2N ln N character \ncompares, on the average.\nDiscussion: The space bound is immediate, but the time bound is follows from \na a detailed and dif\ufb01cult research result by  P . Jacquet and  W. Szpankowski, which \nimplies  that the cost of sorting the suf\ufb01xes is asymptotically the same as the cost of \nsorting N random strings (see Proposition E on page 723).\n 883 S u f f i x  a r r a y s  \nALGORITHM 6.13  Suffix array ", "start": 894, "end": 895}, "1335": {"text": "the same as the cost of \nsorting N random strings (see Proposition E on page 723).\n 883 S u f f i x  a r r a y s  \nALGORITHM 6.13  Suffix array (elementary implementation)\npublic class  SuffixArray \n{\n   private final String[] suffixes;  // suffix array\n   private final int N;              // length of string (and array)\n   public SuffixArray(String s)\n   {\n      N = s.length();\n      suffixes = new String[N];\n      for (int i = 0; i < N; i++)\n         suffixes[i] = s.substring(i);\n      Quick3way.sort(suffixes);\n   }\n   public int length()         { return N; }\n   public String select(int i) { return suffixes[i]; }\n   public int index(int i)     { return N - suffixes[i].length(); }\n   private static int lcp(String s, String t)\n   // See page page 875.\n   public int lcp(int i)\n   {  return lcp(suffixes[i], suffixes[i-1]); }\n   public int rank(String key)\n   {  // binary search\n      int lo = 0, hi = N - 1;\n      while (lo <= hi)\n      {\n         int mid = lo + (hi - lo) / 2;\n         int cmp = key.compareTo(suffixes[mid]);\n         if      (cmp < 0) hi = mid - 1;\n         else if (cmp > 0) lo = mid + 1;\n         else return mid;\n      }\n      return lo;\n   } \n}\n This implementation of our SuffixArray API depends for its ef\ufb01ciency on the fact that Java String\nvalues are immutable, so that substrings are constant-size references and substring extraction takes \nconstant time (see text). 884 CONTEXT\n \n \n  \n \nImproved implementations. ", "start": 895, "end": 896}, "1336": {"text": "for its ef\ufb01ciency on the fact that Java String\nvalues are immutable, so that substrings are constant-size references and substring extraction takes \nconstant time (see text). 884 CONTEXT\n \n \n  \n \nImproved implementations. Our elementary implementation of SuffixArray has \npoor worst-case performance. For example, if all the characters are equal, the sort ex-\namines every character in each substring and \nthus takes quadratic time. For strings of the \ntype we have been using as examples, such as \ngenomic sequences or natural-language text, \nthis is not likely to be problematic, but the al-\ngorithm can be slow for texts with long runs of \nidentical characters. Another way of looking at \nthe problem is to observe that the cost of \ufb01nd -\ning the longest repeated substring is quadratic \nin the length of the substring  because all of the \npre\ufb01xes of the repeat need to be checked (see \nthe diagram at right). This is not a problem for \na text such as A T ale of Two Cities, where the \nlongest repeat\n\"s dropped because it would have\n been a bad thing for me in a \n worldly point of view i\" \nhas just 84 characters, but it is a serious prob-\nlem for genomic data, where long repeated sub-\nstrings are not unusual. How can this quadratic \nbehavior for repeat searching be avoided? Re-\nmarkably, research by  P . Weiner in 1973 showed \nthat it is possible to solve the longest repeated substring problem in guaranteed linear time.  \nWeiner\u2019s algor ithm was based on building a suf\ufb01x tree data structure (essentially a \ntrie for suf\ufb01xes). With multiple pointers per character, suf\ufb01x trees consume too much \nspace for many practical problems, which led to the development of suf\ufb01x arrays. In the \n1990s,  U. Manber and  E. ", "start": 896, "end": 896}, "1337": {"text": "pointers per character, suf\ufb01x trees consume too much \nspace for many practical problems, which led to the development of suf\ufb01x arrays. In the \n1990s,  U. Manber and  E. Myers presented a linearithmic algorithm for building suf\ufb01x \narrays directly and a method that does preprocessing at the same time as the suf\ufb01x sort \nto support constant-time lcp(). Several linear-time suf\ufb01x sorting algorithms have been \ndeveloped since. With a bit more work, the Manber-Myers implementation can also \nsupport a two-argument lcp() that \ufb01nds the longest common pre\ufb01x of two given suf-\n\ufb01xes that are not necessarily adjacent in guaranteed constant time, again a remarkable \nimprovement over the straightforard implementation. These results are quite surpris-\ning, as they achieve ef\ufb01ciencies quite beyond what you might have expected. \nLRS cost is quadratic in repeat length\nacaag\ncaag\naag\n ag\n  g\ninput string\nsuffixes of longest repeat (M = 5)\naacaagtttacaagc\naag  c\naag  tttacaagc\nacaag  c\nacaag  tttacaagc\nag  c\nag  tttacaagc\nc\ncaag  c\ncaag  tttacaagc\ng c\ng tttacaagc\ntacaagc\nttacaagc\ntttacaagc\nsorted suffixes of input\na acaag  ttt  acaag  c\nall appear at least\ntwice as a prefix of\na suffix string\ncomparison cost is at least\n1 + 2 + . . . + M ~ M2/2\n3\n5\n2\n4\n1\n 885 S u f ", "start": 896, "end": 897}, "1338": {"text": "as a prefix of\na suffix string\ncomparison cost is at least\n1 + 2 + . . . + M ~ M2/2\n3\n5\n2\n4\n1\n 885 S u f f i x  a r r a y s  \n Proposition D. With suf\ufb01x arrays, we can solve both the suf\ufb01x sorting and longest \nrepeated substring problems in linear time.\nProof: The remarkable algorithms for these tasks are just beyond our scope, but \nyou can \ufb01nd on the booksite code that implements the SuffixArray constructor \nin linear time and lcp() queries in constant time.\nA SuffixArray implementation based on these ideas supports ef\ufb01cient solutions of \nnumerous string-processing problems, with simple client code, as in our LRS and KWIC\nexamples.\nSuffix arrays are the culmination of decades of research that began with the de-\nvelopment of tries for KWIC indices in the 1960s. The algorithms that we have dis -\ncussed were worked out by many researchers over several decades in the context of solv-\ning practical problems ranging from putting the Oxford English Dictionary online to the \ndevelopment of the \ufb01rst web search engines to sequencing the human genome. This \nstory certainly helps put the importance of algorithm design and analysis in context. \n 886 CONTEXT\n  \n \n \n  \n  N e t w o r k - \ufb02 o w  a l g o r i t h m s  Next, we consider a \ngraph model that has been successful not just because it \nprovides us with a simply stated problem-solving model \nthat is useful in many practical applications but also be-\ncause we have ef\ufb01cient algorithms for solving problems \nwithin the model. The solution that we consider illustrates \nthe tension between our quest for implementations of gen-\neral applicability and our quest for ef\ufb01cient solutions to \nspeci\ufb01c problems. The ", "start": 897, "end": 898}, "1339": {"text": "solving problems \nwithin the model. The solution that we consider illustrates \nthe tension between our quest for implementations of gen-\neral applicability and our quest for ef\ufb01cient solutions to \nspeci\ufb01c problems. The study of network-\ufb02ow algorithms is \nfascinating because it brings us tantalizingly close to com-\npact and elegant implementations that achieve both goals.   \nAs you will see, we have straightforward implementations \nthat are guaranteed to run in time proportional to a poly-\nnomial in the size of the network.\nThe classical solutions to network-\ufb02ow problems are \nclosely related to other graph algorithms that we studied \nin Chapter 4, and we can write surprisingly concise pro -\ngrams that solve them, using the algorithmic tools we have \ndeveloped. As we have seen in many other situations, good \nalgorithms and data structures can lead to substantial re-\nductions in running times. Development of better imple-\nmentations and better algorithms is still an area of active \nresearch, and new approaches continue to be discovered.\nA physical model. We beg in w ith an idealized physical \nmodel in which several of the basic concepts are intui-\ntive. Speci\ufb01cally, imagine a collection of interconnected oil \npipes of varying sizes, with switches controlling the direc-\ntion of \ufb02ow at junctions, as in the example illustrated at \nright. Suppose further that the network has a single source\n(say, an oil \ufb01eld) and a single sink (say, a large re\ufb01nery) to \nwhich all the pipes ultimately connect. At each vertex, the \n\ufb02owing oil reaches an equilibrium where the amount of oil \n\ufb02owing in is equal to the amount \ufb02owing out. We measure \nboth \ufb02ow and pipe capacity in the same units (say, gallons \nper second). ", "start": 898, "end": 898}, "1340": {"text": "an equilibrium where the amount of oil \n\ufb02owing in is equal to the amount \ufb02owing out. We measure \nboth \ufb02ow and pipe capacity in the same units (say, gallons \nper second). If every switch has the property that the total \ncapacity of the ingoing pipes is equal to the total capacity \nof the outgoing pipes, then there is no problem to solve: we Adding flow to a network\nadd 2 units of flow\nalong 0->1->3->5\nadd 1 unit of flow\nalong 0->2->4->5\nredirect 1 unit of flow\nfrom 1->3->5 \nto 1->4->5\nadd 1 unit of flow\nalong 0->2->3->5\nsource\nsink\n 887 N e t w o r k - f l o w  a l g o r i t h m s  \n \n \n \n \n  \n \nsimply \ufb01ll all pipes to full capacity. Otherwise, not all pipes are \nfull, but oil \ufb02ows through the network, controlled by switch \nsettings at the junctions, satisfying a local equilibrium con-\ndition at the junctions: the amount of oil \ufb02owing into each \njunction is equal to the amount of oil \ufb02owing out. For ex-\nample, consider the network in the diagram on the opposite \npage. Operators might start the \ufb02ow by opening the switches \nalong the path 0->1->3->5, which can handle 2 units of \ufb02ow, \nthen open switches along the path 0->2->4->5 to get another \nunit of \ufb02ow in the network. Since 0->1, 2->4, and 3->5 are full, there is no direct way \nto get more \ufb02ow from 0 to 5, but ", "start": 898, "end": 899}, "1341": {"text": "\ufb02ow in the network. Since 0->1, 2->4, and 3->5 are full, there is no direct way \nto get more \ufb02ow from 0 to 5, but if we change the switch at 1 to redirect enough \ufb02ow \nto \ufb01ll 1->4, we open up enough capacity in 3->5 to allow us to add a unit of \ufb02ow on \n0->2->3->5. Even for this simple network, \ufb01nding switch settings that increase the \ufb02ow \nis not an easy task; for a complicated network, we are clearly interested in the following \nquestion: What switch settings will maximize the amount of oil \ufb02owing from source \nto sink? We can model this situation directly with an edge-weighted digraph that has a \nsingle source and a single sink.  The edges in the network correspond to the oil pipes, \nthe vertices correspond to the junctions with switches that control how much oil goes \ninto each outgoing edge, and the weights on the edges correspond to the capacity of the \npipes. We assume that the edges are directed, specifying that oil can \ufb02ow in only one di-\nrection in each pipe. Each pipe has a certain amount of \ufb02ow, which is less than or equal \nto its capacity, and every vertex satis\ufb01es the equilibrium condition that the \ufb02ow in is \nequal to the \ufb02ow out. This \ufb02ow-network abstraction is a useful problem-solving model \nthat applies directly to a variety of applications and indirectly to still more. We some-\ntimes appeal to the idea of oil \ufb02owing through pipes for intuitive support of basic ideas, \nAnatomy of a network-flow problem\nflow value\nassociated\nwith\neach edgecapacities\n6 \n8\n0 1  2.0\n0 ", "start": 899, "end": 899}, "1342": {"text": "oil \ufb02owing through pipes for intuitive support of basic ideas, \nAnatomy of a network-flow problem\nflow value\nassociated\nwith\neach edgecapacities\n6 \n8\n0 1  2.0\n0 2  3.0\n1 3  3.0\n1 4  1.0\n2 3  1.0\n2 4  1.0\n3 5  2.0\n4 5  3.0\n0 1  2.0  2.0\n0 2  3.0  1.0\n1 3  3.0  2.0\n1 4  1.0  0.0\n2 3  1.0  0.0\n2 4  1.0  1.0\n3 5  2.0  2.0\n4 5  3.0  1.0\ntinyFN.txt standard drawing\nV source\nsink\nE\ndrawing with capacities drawing with flow flow representation\nLocal equilibrium in a flow network\ninflow equals outflow\nat every vertex\n(except the source\n and the sink)\n 888 CONTEXT\nbut our discussion applies equally well to goods moving through distribution chan-\nnels and to numerous other situations. As with our use of distance in shortest-paths \nalgorithms, we are free to abandon any physical intuition when convenient because \nall the de\ufb01nitions, properties, and algorithms that we consider are based entirely on \nan abstract model that does not necessarily obey physical laws. Indeed, a prime reason \nfor our interest in the network-\ufb02ow model is that it allows us to solve numerous other \nproblems through reduction, as we see in the next section.  \nDe\ufb01nitions. ", "start": 899, "end": 900}, "1343": {"text": "Indeed, a prime reason \nfor our interest in the network-\ufb02ow model is that it allows us to solve numerous other \nproblems through reduction, as we see in the next section.  \nDe\ufb01nitions. Because of this broad applicability, it is worthwhile to consider precise \nstatements of the terms and concepts that we have just informally introduced:\nDefinition. A  \ufb02ow network is an edge-weighted digraph with positive edge weights \n(which we refer to as capacities). An    st-\ufb02ow network  has two identi\ufb01ed vertices, a \nsource s and a sink t.\nWe sometimes refer to edges as hav ing in\ufb01nite capacit y or, equivalently, as being un -\ncapacitated. That might mean that we do not compare \ufb02ow against capacity for such \nedges, or we might use a sentinel value that is guaranteed to be larger than any \ufb02ow \nvalue. We refer to the total \ufb02ow into a vertex (the sum of the \ufb02ows on its incoming \nedges) as the vertex\u2019s  in\ufb02ow, the total \ufb02ow out of a vertex (the sum of the \ufb02ows on its \noutgoing edges) as the vertex\u2019s out\ufb02ow, and the difference between the two (in\ufb02ow \nminus out\ufb02ow) as the vertex\u2019s net\ufb02ow. To simplify the discussion, we also assume that \nthere are no edges leaving t or entering s.\n  \n \nst- \ufb02ow in an st-\ufb02ow network is a set of nonnegative values associ-Definition. An    \nated with each edge, which we refer to as edge \ufb02ows. We say that a \ufb02ow is feasible if it \nsatis\ufb01es the condition that no edge\u2019s \ufb02ow is greater ", "start": 900, "end": 900}, "1344": {"text": "\nated with each edge, which we refer to as edge \ufb02ows. We say that a \ufb02ow is feasible if it \nsatis\ufb01es the condition that no edge\u2019s \ufb02ow is greater than that edge\u2019s capacity and the \nlocal equilibrium condition that the every vertex\u2019s net\ufb02ow is zero (except s and t).\n \n  \n  \nWe refer to the sink\u2019s in\ufb02ow as the st-\ufb02ow  value. We will see in Proposition C that the \nvalue is also equal to the source\u2019s out\ufb02ow. With these de\ufb01nitions, the formal statement \nof our basic problem is straightforward:\n M a x i m u m   s t - \ufb02 o w .  Given an st-\ufb02ow network, \ufb01nd an st-\ufb02ow such that no other \n\ufb02ow from s to t has a larger value. \nFor brevity, we refer to such a \ufb02ow as a max\ufb02ow and the problem of \ufb01nding one in a \nnetwork as the max\ufb02ow problem. In some applications, we might be content to know \n 889 N e t w o r k - f l o w  a l g o r i t h m s  \n  \n \njust the max\ufb02ow value, but we generally want to know a \ufb02ow (edge \ufb02ow values) that \nachieves that value.\nAPIs. The FlowEdge and FlowNetwork APIs shown on page 890 are straightforward \nextensions of APIs from CHAPTER 3. We will consider on page 896 an implementation \nof FlowEdge that is based on adding an instance variable containing the \ufb02ow to our \nWeightedEdge class from page 610. Flows have a direction, but we do not base FlowEdge\non WeightedDirectedEdge ", "start": 900, "end": 901}, "1345": {"text": "FlowEdge that is based on adding an instance variable containing the \ufb02ow to our \nWeightedEdge class from page 610. Flows have a direction, but we do not base FlowEdge\non WeightedDirectedEdge because we work with a more general abstraction known as \nthe residual network that is described below, and we need each edge to appear in the adja-\ncency lists of both its vertices to implement the residual network. The residual network \nallows us to both add and subtract \ufb02ow and to test whether an edge is full to capacity \n(no more \ufb02ow can be added) or empty (no \ufb02ow can be subtracted). This abstraction is \nimplemented via the the methods residualCapacity() and addResidualFlow() that \nwe will consider later. The implementation of FlowNetwork is virtually identical to our \nEdgeWeightedGraph implementation on page 611, so we omit it. T o simplify the \ufb01le for-\nmat, we adopt the convention that the source is 0 and the sink is V/H110021. These APIs leave \na straightforward goal for max\ufb02ow \nalgorithms: build a network, then \nassign values to the \ufb02ow instance \nvariables in the client\u2019s edges that \nmaximize \ufb02ow through the net-\nwork.  Shown at right are client \nmethods for certifying whether a \n\ufb02ow is feasible. Typically, we might \ndo such a check as the \ufb01nal action \nof a max\ufb02ow algorithm. \nprivate boolean localEq(FlowNetwork G, int v) \n{  // Check local equilibrium at each vertex. \n   double EPSILON = 1E-11;\n   double netflow = 0.0;\n   for (FlowEdge e : G.adj(v))\n      if (v == e.from()) netflow -= e.flow();\n      else               netflow += e.flow();\n ", "start": 901, "end": 901}, "1346": {"text": "1E-11;\n   double netflow = 0.0;\n   for (FlowEdge e : G.adj(v))\n      if (v == e.from()) netflow -= e.flow();\n      else               netflow += e.flow();\n   return Math.abs(netflow) < EPSILON; \n}\nprivate boolean isFeasible(FlowNetwork G) \n{\n   // Check that flow on each edge is nonnegative \n   //   and not greater than capacity.\n   for (int v = 0; v < G.V(); v++)\n      for (FlowEdge e : G.adj(v))\n         if (e.flow() < 0 || e.flow() > e.cap())\n            return false;\n   // Check local equilibrium at each vertex. \n   for (int v = 0; v < G.V(); v++)\n      if (v !=s && v != t && !localEq(v))\n         return false;\n   return true; \n}\nChecking that a flow is feasible in a flow network\n 890 CONTEXT\npublic class  FlowEdge \nFlowEdge(int v, int w, double cap) \nint from() vertex this edge points from\nint to() vertex this edge points to\nint other(int v) other endpoint\ndouble capacity() capacity of this edge\ndouble flow() flow in this edge\ndouble residualCapacityTo(int v) residual capicity toward v \ndouble addFlowTo(int v, double delta) add delta flow toward v\nString toString() string representation\n A P I  f o r  e d g e s  i n  a  f l o w  n e t w o r k\npublic class  FlowNetwork \nFlowNetwork(int V) empty V-vertex flow network\nFlowNetwork(In in) construct from input stream\nint V() number of vertices\nint E() number of edges\nvoid addEdge(FlowEdge e) add e to this flow network\nIterable<FlowEdge> ", "start": 901, "end": 902}, "1347": {"text": "flow network\nFlowNetwork(In in) construct from input stream\nint V() number of vertices\nint E() number of edges\nvoid addEdge(FlowEdge e) add e to this flow network\nIterable<FlowEdge> adj(int v) edges pointing from v\nIterable<FlowEdge> edges() all edges in this flow network\nString toString() string representation\nFlow network API\nFlow network representation\nadj[]\n0\n1\n2\n3\n4\n5\n02 1.03.0 0 1 2.0 2.0\nBag\nobjects\n45 1.03.0 3 5 2.0 2.0\n45 1.03.0 2 4 1.0 1.0 1 4 1.0 0.0\n35 2.02.0 2 3 1.0 0.0 1 3 3.0 2.0\n24 1.01.0 2 3 1.0 0.0 0 2 3.0 1.0\n14 0.01.0 1 3 3.0 2.0 0 1 2.0 2.0\nreferences to the \nsame Edge object\n6 \n8\n0 1  2.0\n0 2  3.0\n1 3  3.0\n1 4  1.0\n2 3  1.0\n2 4  1.0\n3 5  2.0\n4 5  3.0\ntinyFN.txt\nV\nE\n 891 N e t w o r k - f l o w  a l g o r i t h m s  \n \n \n \n  \n  \n \n   F o r d - F ", "start": 902, "end": 903}, "1348": {"text": "3.0\ntinyFN.txt\nV\nE\n 891 N e t w o r k - f l o w  a l g o r i t h m s  \n \n \n \n  \n  \n \n   F o r d - F u l k e r s o n  a l g o r i t h m .  An effective approach to solving max-\n\ufb02ow problems was developed by L. R. Ford and D. R. Fulkerson in \n1962. It is a generic method for increasing \ufb02ows incrementally along \npaths from source to sink that serves as the basis for a family of algo-\nrithms. It is known as the Ford-Fulkerson algorithm in the classical lit-\nerature; the more descriptive term augmenting-path algorithm is also \nwidely used. Consider any directed path from source to sink through \nan st-\ufb02ow network. Let x be the minimum of the unused capacities of \nthe edges on the path. We can increase the network\u2019s \ufb02ow value by at \nleast x by increasing the \ufb02ow in all edges on the path by that amount. \nIterating this action, we get a \ufb01rst attempt at computing \ufb02ow in a \nnetwork: \ufb01nd another path, increase the \ufb02ow along that path, and \ncontinue until all paths from source to sink have at least one full edge \n(so that we can no longer increase \ufb02ow in this way). This algorithm \nwill compute the max\ufb02ow in some cases but will fall short in other \ncases. Our introductory example on page 886 is such an example. T o im-\nprove the algorithm such that it always \ufb01nds a max\ufb02ow, we consider \na more general way to increase the \ufb02ow, along a path from source to \nsink through the network\u2019s underlying undirected graph. ", "start": 903, "end": 903}, "1349": {"text": "such that it always \ufb01nds a max\ufb02ow, we consider \na more general way to increase the \ufb02ow, along a path from source to \nsink through the network\u2019s underlying undirected graph. The edges \non any such path are either    forward edges, which go with the \ufb02ow \n(when we traverse the path from source to sink, we traverse the edge \nfrom its source vertex to its destination vertex), or  backward edges, \nwhich go against the \ufb02ow (when we traverse the path from source \nto sink, we traverse the edge from its destination vertex to its source \nvertex). Now, for any path with no full forward edges and no empty \nbackward edges, we can increase the amount of \ufb02ow in the network \nby increasing \ufb02ow in forward edges and decreasing \ufb02ow in backward \nedges. The amount by which the \ufb02ow can be increased is limited by \nthe minimum of the unused capacities in the forward edges and the \n\ufb02ows in the backward edges. Such a path is called an  augmenting path. \nAn example is shown at right. In the new \ufb02ow, at least one of the \nforward edges along the path becomes full or at least one of the back-\nward edges along the path becomes empty. The process just sketched \nis the basis for the classical Ford-Fulkerson max\ufb02ow algorithm (aug-\nmenting-path method). We summarize it as follows:  \nAn augmenting path\n(0->2->3->1->4->5)\nno path from 0 to 5\nwithout a full edge\nadd 1 unit of flow\nalong 0->2->3\nsubtract 1 unit of flow\nfrom 1->3 \n(traverse 3->1)\nadd 1 unit of flow\nalong 1->4->5\nout of\nequlibrium\nout ", "start": 903, "end": 903}, "1350": {"text": "flow\nalong 0->2->3\nsubtract 1 unit of flow\nfrom 1->3 \n(traverse 3->1)\nadd 1 unit of flow\nalong 1->4->5\nout of\nequlibrium\nout of\nequlibrium\n 892 CONTEXT\n \n \nFord-Fulkerson  maxflow algorithm. Start with zero \ufb02ow everywhere. Increase the \n\ufb02ow along any augmenting path from source to sink (with no full forward edges or \nempty backward edges), continuing until there are no such paths in the network.\n \n \n \n \n   \nRemarkably (under certain technical conditions about numeric properties of the \ufb02ow), \nthis method always \ufb01nds a max\ufb02ow, no matter how we choose the paths. Like the \ngreedy MST algorithm discussed in Section 4.3 and the generic shortest-paths method \ndiscussed in Section 4.4, it is a generic algorithm that is useful because it establishes \nthe correctness of a whole family of more speci\ufb01c algorithms. We are free to use any \nmethod whatever to choose the path. Several algorithms that compute sequences of \naugmenting paths have been developed, all of which lead to a max\ufb02ow. The algorithms \ndiffer in the number of augmenting paths they compute and the costs of \ufb01nding each \npath, but they all implement the Ford-Fulkerson algorithm and \ufb01nd a max\ufb02ow.\n M a x \ufb02 o w - m i n c u t  t h e o r e m .  To  s h ow  t h a t  a ny  \ufb02 ow  co m p u t e d  by  a ny  i m p l e m e n t a t i o n  \nof the Ford-Fulkerson algorithm is indeed a max\ufb02ow, ", "start": 903, "end": 904}, "1351": {"text": "ow  co m p u t e d  by  a ny  i m p l e m e n t a t i o n  \nof the Ford-Fulkerson algorithm is indeed a max\ufb02ow, we prove a key fact known as the \nmax\ufb02ow-mincut theorem. Understanding this theorem is a crucial step in understand -\ning network-\ufb02ow algorithms. As suggested by its name, the theorem is based on a direct \nrelationship between \ufb02ows and cuts in networks, so we begin by de\ufb01ning terms that \nrelate to cuts. Recall from Section 4.3 that a cut in a graph is a partition of the vertices \ninto two disjoint sets, and a crossing edge is an edge that connects a vertex in one set to \na vertex in the other set. For \ufb02ow networks, we re\ufb01ne these de\ufb01nitions as follows:  \nst-cut is a cut that places vertex s in one of its sets and vertex t in Definition. An    \nthe other.\n \n \nEach crossing edge corresponding to an st-cut is either an st-edge that goes from a ver-\ntex in the set containing s to a vertex in the set containing t, or a ts-edge that goes in \nthe other direction. We sometimes refer to the set of crossing st-edges as a cut set. The \ncapacity of an st-cut in a \ufb02ow network is the sum of the capacities of that cut\u2019s  st-edges, \nand the \ufb02ow across an st-cut is the difference between the sum of the \ufb02ows in that cut\u2019s \nst-edges and the sum of the \ufb02ows in that cut\u2019s ts-edges. Removing all the st-edges (the \ncut set) in an st-cut of a network leaves no path from s to t, but adding any one of them \nback could create such a path. ", "start": 904, "end": 904}, "1352": {"text": "cut\u2019s ts-edges. Removing all the st-edges (the \ncut set) in an st-cut of a network leaves no path from s to t, but adding any one of them \nback could create such a path. Cuts are the appropriate abstraction for many applica-\ntions. For our oil-\ufb02ow model, a cut provides a way to completely stop the \ufb02ow of oil \n 893 N e t w o r k - f l o w  a l g o r i t h m s  \n \n \n \n \nfrom the source to the sink. If we view the capacity of the cut as the cost of doing so, to \nstop the \ufb02ow in the most economical manner is to solve the following problem:\n M i n i m u m  s t - c u t .  Given an st-network, \ufb01nd an st-cut such that the capacity of \nno other cut is smaller. For brevity, we refer to such a cut as a mincut and to the \nproblem of \ufb01nding one in a network as the mincut problem.\nThe statement of the mincut problem includes no mention of \ufb02ows, and these de\ufb01ni -\ntions might seem to digress from our discussion of the augmenting-path algorithm. \nOn the surface, computing a mincut (a set of edges) seems easier than computing a \nmax\ufb02ow (an assignment of weights to all the edges). On the contrary, the max\ufb02ow and \nmincut problems are intimately related. The augmenting-path method itself provides \na proof.   That proof rests on the following basic relationship between \ufb02ows and cuts, \nwhich immediately gives a proof that local equilibrium in an st-\ufb02ow implies global \nequilibrium as well (the \ufb01rst corollary) and an upper bound on the value of any st-\ufb02ow ", "start": 904, "end": 905}, "1353": {"text": "cuts, \nwhich immediately gives a proof that local equilibrium in an st-\ufb02ow implies global \nequilibrium as well (the \ufb01rst corollary) and an upper bound on the value of any st-\ufb02ow \n(the second corollary):\n   \n \nProposition E. For any  st-\ufb02ow, the \ufb02ow across each st-cut \nis equal to the value of the \ufb02ow.\nProof: Let Cs be the vertex set containing s and Ct the \nvertex set containing t. This fact follows immediately by \ninduction on the size of Ct.  The property is true by de\ufb01-\nnition when Ct is t and when a vertex is moved from Cs to \nCt , local equilibrium at that vertex implies that the stated \nproperty is preserved. Any st-cut can be created by mov-\ning vertices in this way.\nCorollary. The out\ufb02ow from s is equal to the in\ufb02ow to t (the value of the  st-\ufb02ow).\nProof: Let Cs be {s }.\n Corollary. No st-\ufb02ow\u2019s value can exceed the capacity of any st-cut.\nCs\nCt\ninflow to t is\nvalue of the flow\ndifference between\ninflow and outflow\nis flow across cut\ns\nt\n 894 CONTEXT\n  \nProposition F.   (  Maxflow-mincut theorem) Let f  be an st-\ufb02ow.  The following three \nconditions are equivalent:\ni. There exists an st-cut whose capacity equals the value of the \ufb02ow f.\nii. f is a max\ufb02ow.\niii. There is no augmenting path with respect to f.\nProof:  Condition i. implies condition ii. by the corollary to Proposition E. Con-\ndition ii. implies condition iii. because the ", "start": 905, "end": 906}, "1354": {"text": "max\ufb02ow.\niii. There is no augmenting path with respect to f.\nProof:  Condition i. implies condition ii. by the corollary to Proposition E. Con-\ndition ii. implies condition iii. because the existence of an augmenting path implies \nthe existence of a \ufb02ow with a larger \ufb02ow value, contradicting the maximality of f.\nIt remains to prove that condition iii. implies condition i. Let Cs be the set of all \nvertices that can be reached from s with an undirected path that does not contain a \nfull forward or empty backward edge, and let Ct be the remaining vertices. Then, t \nmust be in Ct , so (Cs , Ct) is an st-cut, whose cut set consists entirely of full forward \nor empty backward edges. The \ufb02ow across this cut is equal to the cut\u2019s capacity \n(since forward edges are full and the backward edges are empty) and also to the \nvalue of the network \ufb02ow (by Proposition E).\n \nCorollary. ( Integrality property) When capacities are integers, there exists an inte-\nger-valued max\ufb02ow, and the Ford-Fulkerson algorithm \ufb01nds it.\nProof:  Each augmenting path increases the \ufb02ow by a positive integer (the mini-\nmum of the unused capacities in the forward edges and the \ufb02ows in the backward \nedges, all of which are always positive integers). \n \n \n \nIt is possible to design a max\ufb02ow with noninteger \ufb02ows, even when capacities are all \nintegers, but we do not need to consider such \ufb02ows. From a theoretical standpoint, this \nobservation is important: allowing capacities and \ufb02ows that are real numbers, as we \nhave done and as is common in practice, can lead to unpleasant anomalous situations. \nFor example, it ", "start": 906, "end": 906}, "1355": {"text": "theoretical standpoint, this \nobservation is important: allowing capacities and \ufb02ows that are real numbers, as we \nhave done and as is common in practice, can lead to unpleasant anomalous situations. \nFor example, it is known that the Ford-Fulkerson algorithm could, in principle, lead to \nan in\ufb01nite sequence of augmenting paths that does not even converge to the max\ufb02ow \nvalue.  The version of the algorithm that we consider is known to always converge, even \nwhen capacities and \ufb02ows are real-valued. No matter what method we choose to \ufb01nd an \naugmenting path and no matter what paths we \ufb01nd, we always end up with a \ufb02ow that \ndoes not admit an augmenting path, which therefore must be a max\ufb02ow.\n 895 N e t w o r k - f l o w  a l g o r i t h m s  \n  R e s i d u a l  n e t w o r k .  The generic Ford-Fulkerson algorithm does not specify any par -\nticular method for \ufb01nding an augmenting path. How can we \ufb01nd a path with no full \nforward edges and no empty backward edges? T o this end, we begin with the following \nde\ufb01nition:  \n \nDefinition. Given a st-\ufb02ow network and an st-\ufb02ow, the  residual network  for the \n\ufb02ow has the same vertices as the original and one or two edges in the residual net -\nwork for each edge in the original, de\ufb01ned as follows: For each edge e from v to w in \nthe original, let fe  be its \ufb02ow and ce its capacity. If fe is positive, include an edge w->v \nin the residual with capacity fe ; and if fe is ", "start": 906, "end": 907}, "1356": {"text": "from v to w in \nthe original, let fe  be its \ufb02ow and ce its capacity. If fe is positive, include an edge w->v \nin the residual with capacity fe ; and if fe is less than ce, include an edge v->w in the \nresidual with capacity ce /H11002 fe .\n \n \n \nIf an edge e from v to w is empty (fe is equal to 0), there is a single corresponding edge \nv->w with capacity ce in the residual; if it is full ( fe is equal to ce), there is a single cor -\nresponding edge w->v with capacity fe in the residual; and if it is neither empty nor full, \nboth v->w and w->v are in the residual with their respective capacities. An example is \nshown at the bottom of this page. At \ufb01rst, the residual network representation is a bit \nconfusing because the edges corresponding to \ufb02ow go in the opposite direction of the \n\ufb02ow itself. The forward edges represent the remaining capacity (the amount of \ufb02ow we \ncan add if traversing that edge); the backward edges represent the \ufb02ow (the amount of \n\ufb02ow we can remove if traversing that edge).  The code on page 896 gives the methods in the \nFlowEdge class that we need to implement the residual network abstraction. With these \nimplementations, our algorithms work with the residual network, but they are actually \nexamining capacities and changing \ufb02ow (through edge references) in the client\u2019s edg -\nes. The methods from() and other() allow us to process edges in either orientation: \nAnatomy of a network-flow problem (revisited)\nflow\nforward edge\n(remaining capacity)\ncapacity\nbackward edge\n(flow)\n0 1  2.0  2.0\n0 2  3.0 ", "start": 907, "end": 907}, "1357": {"text": "of a network-flow problem (revisited)\nflow\nforward edge\n(remaining capacity)\ncapacity\nbackward edge\n(flow)\n0 1  2.0  2.0\n0 2  3.0  1.0\n1 3  3.0  2.0\n1 4  1.0  0.0\n2 3  1.0  0.0\n2 4  1.0  1.0\n3 5  2.0  2.0\n4 5  3.0  1.0\nresidual networkdrawing with flow flow representation\n2.0\n1.0\n2.0\n1.0\n1.02.0 \n1.0\n2.0\n2.0 1.0\n1.0\n 896 CONTEXT\n F l o w  e d g e  d a t a  t y p e  ( r e s i d u a l  n e t w o r k )\npublic class  FlowEdge \n{\n   private final int v;                       // edge source\n   private final int w;                       // edge target\n   private final double capacity;             // capacity\n   private double flow;                       // flow\n   public FlowEdge(int v, int w, double capacity)\n   {\n      this.v = v;\n      this.w = w;\n      this.capacity = capacity;\n      this.flow = 0.0;\n   }\n   public int from()         {  return v;          }\n   public int to()           {  return w;          }\n   public double capacity()  {  return capacity;   }\n   public double flow()      {  return flow;       }\n   public int other(int vertex)\n   // same as for Edge\n   public double residualCapacityTo(int vertex)\n   {\n ", "start": 907, "end": 908}, "1358": {"text": "{  return capacity;   }\n   public double flow()      {  return flow;       }\n   public int other(int vertex)\n   // same as for Edge\n   public double residualCapacityTo(int vertex)\n   {\n      if      (vertex == v) return flow;\n      else if (vertex == w) return cap - flow;\n      else throw new RuntimeException(\"Inconsistent edge\");\n   }\n   public void addResidualFlowTo(int vertex, double delta)\n   {\n      if      (vertex == v) flow -= delta;\n      else if (vertex == w) flow += delta;\n      else throw new RuntimeException(\"Inconsistent edge\");\n   }\n   public String toString()\n   {  return String.format(\"%d->%d %.2f %.2f\", v, w, capacity, flow);  } \n}\nThis FlowEdge implementation adds to the weighted DirectedEdge implementation of Section 4.4 \n(see page 642) a flow instance variable and two methods to implement the residual \ufb02ow network. 897\n \n N e t w o r k - f l o w  a l g o r i t h m s  \ne.other(v) returns the endpoint of e that is not v. The methods residualCapTo()\nand addResidualFlowTo() implement the residual network. Residual networks allow \nus to use graph search to \ufb01nd an augmenting path, since any path from source to sink \nin the residual network corresponds directly to an augmenting path in the original \nnetwork. Increasing the \ufb02ow along the path implies making changes in the residual \nnetwork: for example, at least one edge on the path becomes full or empty, so at least \none edge in the residual network changes direction or disappears (but our use of an ab-\nstract residual network means that we just check for positive capacity and do not need \nto actually insert and delete edges). \n  S h o r t e s ", "start": 908, "end": 909}, "1359": {"text": "residual network changes direction or disappears (but our use of an ab-\nstract residual network means that we just check for positive capacity and do not need \nto actually insert and delete edges). \n  S h o r t e s t - a u g m e n t i n g - p a t h  m e t h o d .  Perhaps the simplest Ford-Fulkerson implemen-\ntation is to use a shortest augmenting path (as measured by the number of edges on the \npath, not \ufb02ow or capacity). This method was suggested by J. Edmonds and R. Karp in 1972. \nIn this case, the search for an augmenting path amounts to breadth-\ufb01rst search (BFS) \nin the residual network, precisely as described in Section 4.1, as you can see by com -\nparing the hasAugmentingPath() implementation below to our breadth-\ufb01rst search \nimplemention in Algorithm 4.2 on page 540 (the residual graph is a digraph, and this is \nfundamentally a digraph processing algorithm, as mentioned on page 685). This method \nforms the basis   \nfor the full im-\nplementation  \nin Algorithm \n6.14 on the next \npage, a remark-\nably concise\nimplementation \nbased on the tools \nwe have devel -\noped. For brevity, \nwe refer to this \nmethod as the \nshortest-augment-\ning-path max\ufb02ow \nalgorithm. A trace \nfor our example \nis shown in detail \non page 899.\nprivate boolean hasAugmentingPath(FlowNetwork G, int s, int t) \n{\n   marked = new boolean[G.V()];  // Is path to this vertex known?\n   edgeTo = new FlowEdge[G.V()]; // last edge on path\n   Queue<Integer> q = new Queue<Integer>();\n ", "start": 909, "end": 909}, "1360": {"text": "\n{\n   marked = new boolean[G.V()];  // Is path to this vertex known?\n   edgeTo = new FlowEdge[G.V()]; // last edge on path\n   Queue<Integer> q = new Queue<Integer>();\n   marked[s] = true;             // Mark the source\n   q.enqueue(s);                 //   and put it on the queue.\n   while (!q.isEmpty()) \n   {\n      int v = q.dequeue();\n      for (FlowEdge e : G.adj(v))\n      {\n         int w = e.other(v);\n         if (e.residualCapacityTo(w) > 0 && !marked[w])\n         {  // For every edge to an unmarked vertex (in residual)\n            edgeTo[w] = e;      // Save the last edge on a path.\n            marked[w] = true;   // Mark w because a path is known\n            q.enqueue(w);       //   and add it to the queue.\n         }\n      }\n   }\n   return marked[t]; \n}\n F i n d i n g  a n  a u g m e n t i n g  p a t h  i n  t h e  r e s i d u a l  n e t w o r k  v i a  b r e a d t h - f i r s t  s e a r c h\n 898 CONTEXT\nALGORITHM 6.14   Ford-Fulkerson shortest-augmenting path maxflow algorithm\npublic class  FordFulkerson \n{\n   private boolean[] marked;    // Is s->v path in residual graph?\n   private FlowEdge[] edgeTo;   // last edge on shortest s->v path\n   private double value;        // current value of maxflow\n   public FordFulkerson(FlowNetwork G, int s, int t)\n   {  // Find maxflow in flow network G from s to ", "start": 909, "end": 910}, "1361": {"text": "path\n   private double value;        // current value of maxflow\n   public FordFulkerson(FlowNetwork G, int s, int t)\n   {  // Find maxflow in flow network G from s to t.\n      while (hasAugmentingPath(G, s, t))\n      {  // While there exists an augmenting path, use it.\n         // Compute bottleneck capacity.\n         double bottle = Double.POSITIVE_INFINITY;\n         for (int v = t; v != s; v = edgeTo[v].other(v))\n            bottle = Math.min(bottle, edgeTo[v].residualCapacityTo(v));\n         // Augment flow.\n         for (int v = t; v != s; v = edgeTo[v].other(v))\n            edgeTo[v].addResidualFlowTo(v, bottle);\n         value += bottle;\n      }\n   }\n   public double value()        {  return value;      }\n   public boolean inCut(int v)  {  return marked[v];  }\n   public static void main(String[] args)\n   {\n      FlowNetwork G = new FlowNetwork(new In(args[0]));\n      int s = 0, t = G.V() - 1;\n      FordFulkerson maxflow = new FordFulkerson(G, s, t);\n      StdOut.println(\"Max flow from \" + s + \" to \" + t);\n      for (int v = 0; v < G.V(); v++)\n         for (FlowEdge e : G.adj(v))\n            if ((v == e.from()) && e.flow() > 0)\n               StdOut.println(\"   \" + e);\n      StdOut.println(\"Max flow value = \" +  maxflow.value());\n   } \n}\nThis implementation of the Ford-Fulkerson algorithm \ufb01nds the shortest augmenting path in the \nresidual network, \ufb01nds the bottneck capacity in that path, and augments ", "start": 910, "end": 910}, "1362": {"text": "maxflow.value());\n   } \n}\nThis implementation of the Ford-Fulkerson algorithm \ufb01nds the shortest augmenting path in the \nresidual network, \ufb01nds the bottneck capacity in that path, and augments the \ufb02ow along that path, \ncontinuing until no path from source to sink exists. 899 N e t w o r k - f l o w  a l g o r i t h m s  \n% java FordFulkerson tinyFN.txt \nMax flow from 0 to 5\n  0->2 3.0 2.0\n  0->1 2.0 2.0\n  1->4 1.0 1.0\n  1->3 3.0 1.0\n  2->3 1.0 1.0\n  2->4 1.0 1.0\n  3->5 2.0 2.0\n  4->5 3.0 2.0 \nMax flow value = 4.0\nTrace of augmenting-path Ford-Fulkerson algorithm\nadd 2 units of flow\nalong 0->1->3->5\nadd 1 unit of flow\nalong 0->2->4->5\nadd 1 unit of flow\nalong 0->2->3->1->4->5\ninitial empty network residual network\nst-cut\n1.0\n1.02.0 \n2.0\n2.0\n1.0\n3.0\n1.0\n3.0\n3.0\n1.0\n3.0\n1.0\n1.0\n2.0\n2.0\n3.0\n2.0\n1.0\n2.0\n1.0\n1.02.0 \n1.0\n2.0\n2.0 ", "start": 910, "end": 911}, "1363": {"text": "\n2.0\n2.0\n1.0\n3.0\n1.0\n3.0\n3.0\n1.0\n3.0\n1.0\n1.0\n2.0\n2.0\n3.0\n2.0\n1.0\n2.0\n1.0\n1.02.0 \n1.0\n2.0\n2.0 1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.01.0 \n2.0\n2.0\n2.0 2.0\n2.0\n 900 CONTEXT\n \nPerformance. A larger example is shown in the \ufb01gure above. As is evident from the \n\ufb01gure, the lengths of the augmenting paths form a nondecreasing sequence.  This fact \nis a \ufb01rst key to analyzing the performance of the algorithm.  \n \n \nProposition G.  The  number of augmenting paths needed in the shortest-augment-\ning-path implementation of the Ford-Fulkerson max\ufb02ow algorithm for a \ufb02ow net-\nwork with V vertices and E edges is at most EV /2.\nProof sketch: Every augmenting path has a    critical edge \u2014an edge that is deleted \nfrom the residual network because it corresponds either to a forward edge that be-\ncomes \ufb01lled to capacity or a backward edge that is emptied. Each time an edge is a \ncritical edge, the length of the augmenting path through it must increase by 2 (see \nExercise 6.39). Since an augmenting path is of length at most V each edge can be \non at most V/2 augmenting paths, and the total number of augmenting paths is at \nmost EV/2.\nShortest augmenting paths in a larger flow network\n 901 ", "start": 911, "end": 913}, "1364": {"text": "most V each edge can be \non at most V/2 augmenting paths, and the total number of augmenting paths is at \nmost EV/2.\nShortest augmenting paths in a larger flow network\n 901 N e t w o r k - f l o w  a l g o r i t h m s  \n \n \nCorollary. The shortest-augmenting-path implementation of the Ford-Fulkerson \nmax\ufb02ow algorithm takes time proportional to VE 2/2 in the worst case.\nProof: Breadth-\ufb01rst search examines at most E edges.\n \nThe upper bound of Proposition G is very conservative. For example, the graph shown \nin the \ufb01gure at the top of page 900 has 11 vertices and 20 vertices, so the bound says that \nthe algorithm uses no more than 110 augmenting paths. In fact, it uses 14.\nOther implementations. Another Ford-Fulkerson implementation, suggested by  Ed-\nmonds and  Karp, is the following: Augment along the path that increases the \ufb02ow by \nthe largest amount. For brevity, we refer to this method as the  maximum-capacity-\naugmenting-path max\ufb02ow algorithm. We can implement this (and other approaches) \nby using a priority queue and slightly modifying our implementation of Dijkstra\u2019s \nshortest-paths algorithm, choosing edges from the priority queue to give the maximum \namount of \ufb02ow that can be pushed through a forward edge or diverted from a back -\nward edge. Or, we might look for a longest augmenting path, or make a random choice. \nA complete analysis establishing which method is best is a complex task, because their \nrunning times depend on\n\u25a0   The number of augmenting paths needed to \ufb01nd a max\ufb02ow\n\u25a0   The time needed to \ufb01nd each augmenting ", "start": 913, "end": 913}, "1365": {"text": "best is a complex task, because their \nrunning times depend on\n\u25a0   The number of augmenting paths needed to \ufb01nd a max\ufb02ow\n\u25a0   The time needed to \ufb01nd each augmenting path\nThese quantities can vary widely, depending on the network being processed and on \nthe graph-search strategy. Several other approaches to solving the max\ufb02ow problem \nhave also been devised, some of which compete well with the Ford-Fulkerson algo-\nrithm in practice. Developing a mathematical model of max\ufb02ow algorithms that can \nvalidate such hypotheses, however, is a signi\ufb01cant challenge. The analysis of max\ufb02ow \nalgorithms remains an interesting and active area of research. From a theoretical stand-\npoint, worst-case performance bounds for numerous max\ufb02ow algorithms have been \ndeveloped, but the bounds are generally substantially higher than the actual costs ob-\nserved in applications and also quite a bit higher than the trivial (linear-time) lower \nbound. This gap between what is known and what is possible is larger than for any \nother problem that we have considered (so far) in this book.  \n 902 CONTEXT\n \n \nThe practical application of max\ufb02ow algorithms remains both an art and a science. \nThe art lies in picking the strategy that is most effective for a given practical situation; \nthe science lies in understanding the essential nature of the problem. Are there new data \nstructures and algorithms that can solve the max\ufb02ow problem in linear time, or can we \nprove that none exist? \nalgorithm\nworst-case order of growth\nof running time\n for V vertices and E edges\nwith integral capacities (max C)\nFord-Fulkerson\nshortest augmenting path VE 2\nFord-Fulkerson\nmaximal augmenting path E 2 log C\n p r e fl o w - p u s h  E ", "start": 913, "end": 914}, "1366": {"text": "(max C)\nFord-Fulkerson\nshortest augmenting path VE 2\nFord-Fulkerson\nmaximal augmenting path E 2 log C\n p r e fl o w - p u s h  E V log (E / V 2)\npossible ? V  + E ?\n P e r f o r m a n c e  c h a r a c t e r i s t i c s  o f  m a x f l o w  a l g o r i t h m s\n 903 R e d u c t i o n  \n R e d u c t i o n  Throughout this book, we have focused on articulating speci\ufb01c prob -\nlems, then developing algorithms and data structures to solve them. In several cases \n(many of which are listed below), we have found it convenient to solve a problem by \nformulating it as an instance of another problem that we have already solved. Formal-\nizing this notion is a worthwhile starting point for studying relationships among the   \ndiverse problems and algorithms that we have studied. \n D e f i n i t i o n .  We say that a problem A  reduces to another problem B if we can use an \nalgorithm that solves B to develop an algorithm that solves A.\n  \n \n \nThis concept is certainly a familiar one in software development: when you use a library \nmethod to solve a problem, you are reducing your problem to the one solved by the li-\nbrary method. In this book, we have informally referred to problems that we can reduce \nto a given problem as applications.\n  S o r t i n g  r e d u c t i o n s .  We \ufb01rst encountered reduction in Chapter 2, to express the idea \nthat an ef\ufb01cient sorting algorithm is useful for ef\ufb01ciently solving many other problems, \nthat may not seem ", "start": 914, "end": 915}, "1367": {"text": "n s .  We \ufb01rst encountered reduction in Chapter 2, to express the idea \nthat an ef\ufb01cient sorting algorithm is useful for ef\ufb01ciently solving many other problems, \nthat may not seem to be at all related to sorting. For example, we considered the follow-\ning problems, among many others:\nFinding the median. Given a set of numbers, \ufb01nd the median value.\nDistinct values. Determine the number of distinct values in a set of numbers.\nScheduling to minimize average completion time. Given a set of jobs of speci-\n\ufb01ed duration to be completed, how can we schedule the jobs on a single processor \nso as to minimize their average completion time?\nProposition H. The following problems  reduce to sorting:\n\u25a0 Finding the median\n\u25a0 Counting distinct values\n\u25a0 Scheduling to minimize average completion time\nProof: See page 345 and Exercise 2.5.12.\nNow, we have to pay attention to cost when doing a reduction. For example, we can \ufb01nd \nthe median of a set of numbers in linear time, but using the reduction to sorting will \n 904 CONTEXT\nend up costing linearithmic time. Even so, such extra cost might be acceptable, since we \ncan use an exisiting sort implementation. Sorting is valuable for three reasons:\n\u25a0 It is useful in its own right.\n\u25a0  We have an ef\ufb01cient algor ithms for solv ing it.\n\u25a0 \n \nMany problems reduce to it.\nGenerally, we refer to a problem with these properties as a problem-solving model. Like \nwell-engineered software libraries, well-designed problem-solving models can greatly \nexpand the universe of problems that we can ef\ufb01ciently address. One pitfall in focusing \non problem-solving models is known as  Maslow\u2019s hammer, an idea widely attributed \nto  A. Maslow in the 1960s: If all you ", "start": 915, "end": 916}, "1368": {"text": "ef\ufb01ciently address. One pitfall in focusing \non problem-solving models is known as  Maslow\u2019s hammer, an idea widely attributed \nto  A. Maslow in the 1960s: If all you have is a hammer, everything seems to be a nail . By \nfocusing on a few problem-solving models, we may use them like Maslow\u2019s hammer \nto solve every problem that comes along, depriving ourselves of the opportunity to \ndiscover better algorithms to solve the problem, or even new problem-solving models. \nWhile the models we consider are important, powerful, and broadly useful, it is also \nwise to consider other possibilities.\n  S h o r t e s t - p a t h s  r e d u c t i o n s .  In Section 4.4, we revisited the idea of reduction in the \ncontext of shortest-paths algorithms. We considered the following problems, among \nmany  others:\n S i n g l e - s o u r c e  s h o r t e s t  p a t h s  i n  u n d i r e c t e d  g r a p h s .  Given an edge-weighted un-\ndirected graph with nonnegative weights and a source vertex s, support queries of \nthe form Is there a path from s to a given target vertex v? If so, \ufb01nd a shortest such \npath (one whose total weight is minimal).\n P a r a l l e l  p r e c e d e n c e - c o n s t r a i n e d  s c h e d u l i n g .  Given a set of jobs of speci\ufb01ed du-\nration to be completed, with precedence constraints that specify that certain jobs \nhave to be completed before certain other jobs are begun, how can we schedule \nthe jobs on ", "start": 916, "end": 916}, "1369": {"text": "Given a set of jobs of speci\ufb01ed du-\nration to be completed, with precedence constraints that specify that certain jobs \nhave to be completed before certain other jobs are begun, how can we schedule \nthe jobs on identical processors (as many as needed) such that they are all com-\npleted in the minimum amount of time while still respecting the constraints?\nArbitrage. Find an arbitrage opportunity in a given table of currency-conversion \nrates.\nAgain, the latter two problems do not seem to be directly related to shortest-paths \nproblems, but we saw that shortest paths is an effective way to address them. These ex-\namples, while important, are merely indicative. A large number of important problems, \ntoo many to survey here, are known to reduce to shortest paths\u2014it is an effective and \nimportant problem-solving model.\n 905 R e d u c t i o n  \nProposition I. The following problems reduce to  shortest paths in weighted \ndigraphs:\n\u25a0 Single-source shortest paths in undirected graphs with nonnegative weights\n\u25a0 Parallel precedence-constrained scheduling\n\u25a0 Arbitrage\n\u25a0 [many other problems]\nProof examples: See page 654, page 665, and page 680.\n  \n  M a x \ufb02 o w  r e d u c t i o n s .  Max\ufb02ow algorithms are also important in a broad context. We \ncan remove various restrictions on the \ufb02ow network and solve related \ufb02ow problems; we \ncan solve other network- and graph-processing problems; and we can solve problems \nthat are not network problems at all.  For example, consider the following problems.\nJob placement. A college\u2019s job-placement of\ufb01ce arranges interviews for a set of \nstudents with a set of companies; these interviews result in a set of job offers. As-\nsuming that an interview followed by a job offer represents mutual interest in the \nstudent ", "start": 916, "end": 917}, "1370": {"text": "of\ufb01ce arranges interviews for a set of \nstudents with a set of companies; these interviews result in a set of job offers. As-\nsuming that an interview followed by a job offer represents mutual interest in the \nstudent taking a job at the company, it is in everyone\u2019s best interests to maximize \nthe number of job placements. Is it possible to match every student with a job?  \nWhat is the maximum number of jobs that can be \ufb01lled?\nProduct distribution. A company that manufactures a single product has fac-\ntories, where the product is produced; distribution centers, where the product is \nstored temporarily; and retail outlets, where the product is sold.  The company \nmust distribute the product from factories through distribution centers to retail \noutlets on a regular basis, using distribution channels that have varying capacities. \nIs it possible to get the product from the warehouses to the retail outlets such that \nsupply meets demand everywhere? \nNetwork reliability. A simpli\ufb01ed model considers a computer network as con-\nsisting of a set of trunk lines that connect computers through switches such that \nthere is the possibility of a switched path through trunk lines connecting any two \ngiven computers.  What is the minimum number of trunk lines that can be cut to \ndisconnect some pair of computers? \nAgain, these problems seem to be unrelated to one another and to \ufb02ow networks, but \nthey all reduce to max\ufb02ow.\n 906 CONTEXT\nProposition J.  The following problems reduce to the  max\ufb02ow problem:\n\u25a0 Job placement\n\u25a0 Product distribution\n\u25a0 Network reliability\n\u25a0 \n \n \n \n \n[many other problems]\nProof example: We prove the \ufb01rst (which is know n as the    maximum bipartite \nmatching problem) and leave the others for exercises. Given a job-placement prob-\nlem, construct an instance of the max\ufb02ow problem by directing all edges from ", "start": 917, "end": 918}, "1371": {"text": "(which is know n as the    maximum bipartite \nmatching problem) and leave the others for exercises. Given a job-placement prob-\nlem, construct an instance of the max\ufb02ow problem by directing all edges from \nstudents to companies, adding a source vertex with edges directed to all the stu-\ndents and adding a sink vertex with edges directed from all the companies. Assign \neach edge capacity 1. Now, any integral solution to the max\ufb02ow problem for this \nnetwork provides a solution to the corresponding bipartite matching problem (see \nthe corollary to Proposition F). The matching corresponds exactly to those edges \nbetween vertices in the two sets that are \ufb01lled to capacity by the max\ufb02ow algorithm. \nFirst, the network \ufb02ow always gives a legal matching: since each vertex has an edge \nof capacity 1 either coming in (from the sink) or going out (to the source), at most \n1 unit of \ufb02ow can go through each vertex, implying in turn that each vertex will be \nincluded at most once in the matching. Second, no matching can have more edges, \nsince any such matching would lead directly to a better \ufb02ow than that produced by \nthe max\ufb02ow algorithm. \nExample of reducing maximum bipartite matching to network flow\nmaximum flow\nbipartite matching problem matching (solution)\n77 88 99 1010 1111 1212\n11\nss\nss\n22 33 44 55 66\nnetwork-flow formulation\n77 88 99 1010 1111 1212\n11\nss\nss\n22 33 44 55 66\nAlice\n     Adobe\n     Amazon\n     Facebook\nBob\n     Adobe\n     Amazon\n     Yahoo\nCarol\n     Facebook\n     Google\n     IBM\nDave\n     Adobe\n     Amazon\nEliza\n ", "start": 918, "end": 918}, "1372": {"text": "44 55 66\nAlice\n     Adobe\n     Amazon\n     Facebook\nBob\n     Adobe\n     Amazon\n     Yahoo\nCarol\n     Facebook\n     Google\n     IBM\nDave\n     Adobe\n     Amazon\nEliza\n     Google\n     IBM\n     Yahoo\nFrank\n     IBM\n     Yahoo\nAlice\nBob\nCarol\nDave\nEliza\nFrank\nAmazon\nYahoo\nFacebook\nAdobe\nGoogle\nIBM\nAdobe\n     Alice\n     Bob\n     Dave\nAmazon\n     Alice\n     Bob\n     Dave\nFacebook\n     Alice\n     Carol\nGoogle\n     Carol\n     Eliza\nIBM\n     Carol\n     Eliza\n     Frank\nYahoo\n     Bob\n     Eliza\n     Frank\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n 907 R e d u c t i o n  \n \n \nFor example, as illustrated in the \ufb01gure at right, an augmenting-path max-\n\ufb02ow algorithm might use the paths s->1->7->t, s->2->8->t, s->3->9->t, \ns->5->10->t, s->6->11->t, and s->4->7->1->8->2->12->t to compute \nthe matching 1-8, 2-12, 3-9, 4-7, 5-10, and 6-11. Thus, there is a way to \nmatch all the students to jobs in our example. Each augmenting path \ufb01lls \none edge from the source and one edge into the sink. Note that these edges \nare never used as back edges, so there are at most V augmenting paths. and \na total running time proportional to VE. \nShortest paths and maxflow are important problem-solving models \nbecause ", "start": 918, "end": 919}, "1373": {"text": "Note that these edges \nare never used as back edges, so there are at most V augmenting paths. and \na total running time proportional to VE. \nShortest paths and maxflow are important problem-solving models \nbecause they have the same properties that we articulated for sorting:\n\u25a0 They are useful in their own right.\n\u25a0  We have ef\ufb01cient algor ithms for solv ing them.\n\u25a0 Many problems reduce to them.\nThis short discussion serves only to introduce the idea. If you take a course \nin operations research, you will learn many other problems that reduce to \nthese and many other problem-solving models.\n   L i n e a r  p r o g r a m m i n g .  One of the cornerstones of operations research is \nlinear programming (LP). It refers to the idea of reducing a given problem \nto the following mathematical formulation: \nLinear programming. Given a set of M linear inequalities and linear \nequations involving N variables, and a linear objective function of the N\nvariables, \ufb01nd an assignment of values to the variables that maximizes \nthe objective function, or report that no feasible assignment exists.\nLinear programming is an extremely important prob-\nlem-solving model because\n        \u25a0 A great many important problems reduce to\n        linear programming\n        \u25a0  We have ef\ufb01cient algor ithms for solv ing linear-\n        programming problems\nThe \u201cuseful in its own right\u201d phrase is not needed \nin this litany that we have stated for other problem-\nsolving models because so many  practical problems \nreduce to linear programming. \npath with \nback edges\nAugmenting paths for\nbipartite matching\nLP example\n0 /H11349  a /H11349 2\n0 /H11349  b /H11349 3\n0 /H11349  c /H11349 3\n0 /H11349 ", "start": 919, "end": 919}, "1374": {"text": "/H11349  a /H11349 2\n0 /H11349  b /H11349 3\n0 /H11349  c /H11349 3\n0 /H11349  d /H11349 1\n0 /H11349  e /H11349 1\n0 /H11349  f /H11349 1\n0 /H11349  g /H11349 2\n0 /H11349  h /H11349 3\na = c + d \nb = e + f \nc+ e = g \nd+ f = h \nMaximize f  +  h\nsubject to the constraints\n 908 CONTEXT\nProposition K.  The following problems reduce to  linear programming\n\u25a0 Max\ufb02ow\n\u25a0 Shortest paths\n\u25a0 \n  \n \n[many, many other problems]\nProof example: We prove the \ufb01rst and leave the second to Exercise 6.49. We con-\nsider a system of inequalities and equations that involve one variable correspond-\ning to each edge, two inequalities corresponding to each edge, and one equation \ncorresponding to each vertex (except the source and the sink). The value of the \nvariable is the edge \ufb02ow, the inequalities specify that the edge \ufb02ow must be between \n0 and the edge\u2019s capacity, and the equations specify that the total \ufb02ow on the edges \nthat go into each vertex must be equal to the total \ufb02ow on the edges that go out of \nthat vertex. Any max \ufb02ow problem can be converted into an instance of a linear \nprogramming problem in this way, and the solution is easily converted to a solution \nof the max\ufb02ow problem. The illustration below gives the details for our example.\nExample of reducing network flow to linear programming\nLP solution\nmaxflow problem maxflow ", "start": 919, "end": 920}, "1375": {"text": "and the solution is easily converted to a solution \nof the max\ufb02ow problem. The illustration below gives the details for our example.\nExample of reducing network flow to linear programming\nLP solution\nmaxflow problem maxflow solution\nLP formulation\ncapacities\n6 \n8\n0 1  2.0\n0 2  3.0\n1 3  3.0\n1 4  1.0\n2 3  1.0\n2 4  1.0\n3 5  2.0\n4 5  3.0\nV\nE\n0  /H11349 x 01  2\n0  /H11349 x 02  3\n0  /H11349 x 13  3\n0  /H11349 x 14  1\n0  /H11349 x 23  1\n0  /H11349 x 24  1\n0  /H11349 x 35  2\n0  /H11349 x 45  3\nx 01 =x 13 +x 14 \nx 02 =x 23 +x 24 \nx 13 +x 23 =x 35 \nx 14 +x 24 =x 45 \nMaximize x 35 +x 45\nsubject to the constraints\nx 01 =  2\nx 02 =  2\nx 13 =  1\nx 14 =  1\nx 23 =  1\nx 24 =  1\nx 35 =  2\nx 45 =  2\nMax flow from 0 to 5\n  0->2 3.0 2.0\n  0->1 2.0 2.0\n ", "start": 920, "end": 920}, "1376": {"text": "=  2\nx 45 =  2\nMax flow from 0 to 5\n  0->2 3.0 2.0\n  0->1 2.0 2.0\n  1->4 1.0 1.0\n  1->3 3.0 1.0\n  2->3 1.0 1.0\n  2->4 1.0 1.0\n  3->5 2.0 2.0\n  4->5 3.0 2.0\nMax flow value: 4.0\n 909 R e d u c t i o n  \n \n \nThe  \u201cmany, many other problems\u201d in the statement of Proposition K refers to three \nideas. First, it is very easy to extend a model and to add constraints . Second, reduction \nis transitive, so all the problems that reduce to shortest paths and maximum \ufb02ow also \nreduce to linear programming. Third, and more generally, optimization problems of all \nsorts can be directly formulated as linear programming problems . Indeed, the term lin-\near programming means \u201cformulate an optimization problem as a linear programming \nproblem.\u201d This use predates the use of the word programming for computers. Equally \nimportant as the idea that a great many problems reduce to linear programming is the \nfact that ef\ufb01cient algorithms have been known for linear programming for many de-\ncades. The most famous, developed by  G. Dantzig in the 1940s, is known as the  simplex \nalgorithm.  Simplex is not dif\ufb01cult to understand (see the bare-bones implementation \non the booksite). More recently, the    ellipsoid algorithm presented by  L. G. Khachian in \n1979 led to the development of   interior ", "start": 920, "end": 921}, "1377": {"text": "dif\ufb01cult to understand (see the bare-bones implementation \non the booksite). More recently, the    ellipsoid algorithm presented by  L. G. Khachian in \n1979 led to the development of   interior point methods in the 1980s that have proven to \nbe an effective complement to the simplex algorithm for the huge linear programming \nproblems that people are solving in modern applications. Nowadays, linear program-\nming solvers are robust, extensively tested, ef\ufb01cient, and critical to the basic operation \nof modern corporations. Uses in scienti\ufb01c contexts and even in applications program-\nming are also greatly expanding. If you can model your problem as a linear program-\nming problem, you are likely to be able to solve it.  \nIn a very real sense, linear programming is the parent  of problem-solving \nmodels, since so many problems reduce to it. Naturally, this idea leads to the question \nof whether there is an even more powerful problem-solving model than linear pro-\ngramming. What sorts of problems do not reduce to linear programming?  Here is an \nexample of such a problem:\n L o a d  b a l a n c i n g .  Given a set of jobs of speci\ufb01ed duration to be completed, how \ncan we schedule the jobs on two identical processors so as to minimize the com-\npletion time of all the jobs?\nCan we articulate a more general problem-solving model and solve instances of prob -\nlems within that model ef\ufb01ciently? This line of thinking leads to the idea of intractabil-\nity, our last topic.\n 910 CONTEXT\n \n  I n t r a c t a b i l i t y  The algorithms that we have studied in this book generally are used \nto solve practical problems and therefore consume reasonable amounts of resources.   \nThe practical utility of most of the algorithms is obvious, and for many problems we \nhave ", "start": 921, "end": 922}, "1378": {"text": "y  The algorithms that we have studied in this book generally are used \nto solve practical problems and therefore consume reasonable amounts of resources.   \nThe practical utility of most of the algorithms is obvious, and for many problems we \nhave the luxury of several ef\ufb01cient algorithms to choose from. Unfortunately, many \nother problems arise in practice that do not admit such ef\ufb01cient solutions. What\u2019s \nworse, for a large class of such problems we cannot even tell whether or not an ef\ufb01-\ncient solution exists. This state of affairs has been a source of extreme frustration for \nprogrammers and algorithm designers, who cannot \ufb01nd any ef\ufb01cient algorithm for a \nwide range of practical problems, and for theoreticians, who have been unable to \ufb01nd \nany proof that these problems are dif\ufb01cult. A great deal of research has been done in \nthis area and has led to the development of mechanisms by which new problems can \nbe classi\ufb01ed as being \u201chard to solve\u201d in a particular technical sense. Though much of \nthis work is beyond the scope of this book, the central ideas are not dif\ufb01cult to learn. \nWe introduce them here because ever y prog rammer, when faced w ith a new problem, \nshould have some understanding of the possibility that there exist problems for which \nno one knows any algorithm that is guaranteed to be ef\ufb01cient.\nGroundwork. One of the most beautiful and intriguing intellectual discoveries of \nthe 20th century, developed by  A. Turing in the 1930s, is the  Tur ing machine, a simple \nmodel of computation that is general enough to embody any computer program or \ncomputing device. A Turing machine is a \ufb01nite-state machine that can read inputs, \nmove from state to state, and write outputs. Turing machines form the foundation of \ntheoretical computer science, starting with the following two ideas:\n\u25a0 ", "start": 922, "end": 922}, "1379": {"text": "device. A Turing machine is a \ufb01nite-state machine that can read inputs, \nmove from state to state, and write outputs. Turing machines form the foundation of \ntheoretical computer science, starting with the following two ideas:\n\u25a0     Universality .  All physically realizable computing devices can be simulated by a \nTuring machine. This idea is know n as the  Church-Turing thesis. This is a state-\nment about the natural world and cannot be proven (but it can be falsi\ufb01ed). The \nevidence in favor of the thesis is that mathematicians and computer scientists \nhave developed numerous models of computation, but they all have been proven \nequivalent to the Turing machine.\n\u25a0 Computability . There exist problems that cannot be solved by a Turing machine \n(or by any other computing device, by universality). This is a mathematical \ntruth. The  halting problem (no program can guarantee to determine whether a \ngiven program will halt) is a famous example of such a problem.\nIn the present context, we are interested in a third idea, which speaks to the ef\ufb01ciency \nof computing devices: \n\u25a0 Extended Church-Turing thesis . The order of growth of the running time of a \nprogram to solve a problem on any computing device is within a polynomial \nfactor of some program to solve the problem on a Turing machine (or any other \ncomputing device). \n 911 I n t r a c t a b i l i t y  \nAgain, this is a statement about the natural world, buttressed by the idea that all known \ncomputing devices can be simulated by a Turing machine, with at most a polynomial \nfactor increase in cost. In recent years, the idea of  quantum computing has given some \nresearchers reason to doubt the extended Church-Turing thesis. Most agree that, from a \npractical point of view, it is probably safe for some time, but many researchers ", "start": 922, "end": 923}, "1380": {"text": "the idea of  quantum computing has given some \nresearchers reason to doubt the extended Church-Turing thesis. Most agree that, from a \npractical point of view, it is probably safe for some time, but many researchers are hard \nat work on trying to falsify the thesis.\n  E x p o n e n t i a l  r u n n i n g  t i m e .  The purpose of the theory of intractability is to separate \nproblems that can be solved in polynomial time from problems that (probably) require \nexponential time to solve in the worst case. It is useful to think of an exponential-time \nalgorithm as one that, for some input of size N, takes time proportional to 2N (at least). \nThe substance of the argument does not change if we replace 2 by any number /H9251 > 1. \nWe generally take as g ranted that an exponential-time algor ithm cannot be guaranteed \nto solve a problem of size 100 (say) in a reasonable amount of time, because no one \ncan wait for an algorithm to take 2 100 steps, regardless of the speed of the computer. \nExponential growth dwarfs technological changes: a supercomputer may be a trillion \ntimes faster than an abacus, but neither can come close to solving a problem that re-\nquires 2100 steps. Sometimes the \nline between \u201ceasy\u201d and \u201chard\u201d \nproblems is a \ufb01ne one. For ex -\nample, we studied an algorithm   \nin Section 4.1 that can solve the \nfollowing problem: \nShortest-path length. What \nis the length of the shortest \npath from a given vertex s to a \ngiven vertex t in a given graph?\nBut we did not study algorithms \nfor the following problem, which \nseems to be virtually the same: \nLongest-path length. What \nis the length of ", "start": 923, "end": 923}, "1381": {"text": "s to a \ngiven vertex t in a given graph?\nBut we did not study algorithms \nfor the following problem, which \nseems to be virtually the same: \nLongest-path length. What \nis the length of the longest \nsimple path from a given ver -\ntex s to a given vertex t in a \ngiven graph?\npublic class  LongestPath \n{\n   private boolean[] marked;\n   private int max;\n  public LongestPath(Graph G, int s, int t)\n   {  \n      marked = new boolean[G.V()];\n      dfs(G, s, t, 0);\n   }\n   private void dfs(Graph G, int v, int t, int i)\n   {\n      if (v == t && i > max) max = i;\n      if (v == t) return;\n      marked[v] = true;\n      for (int w : G.adj(v))\n         if (!marked[w]) dfs(G, w, t, i+1);\n      marked[v] = false;\n   }\n   public int maxLength()\n   {  return max;  }\n}\n F i n d i n g  t h e  l e n g t h  o f  t h e  l o n g e s t  p a t h  i n  a  g r a p h\n 912 CONTEXT\n \n \n \n \nThe crux of the matter is this: as far as we know, these problems are nearly at opposite \nends of the spectrum with respect to dif\ufb01culty. Breadth-\ufb01rst search yields a solution for \nthe \ufb01rst problem in linear time, but all known algorithms for the second problem take \nexponential time in the worst case. The code at the bottom of the previous page shows a \nvariant of    depth-\ufb01rst search that accomplishes the task. It is quite similar to depth-\ufb01rst \nsearch, but it examines all ", "start": 923, "end": 924}, "1382": {"text": "case. The code at the bottom of the previous page shows a \nvariant of    depth-\ufb01rst search that accomplishes the task. It is quite similar to depth-\ufb01rst \nsearch, but it examines all simple paths from s to t in the digraph to \ufb01nd the longest one.\n S e a r c h  p r o b l e m s .  The great disparity between problems that can be solved with \u201cef-\n\ufb01cient\u201d algorithms of the type we have been studying in this book and problems where \nwe need to look for a solution among a potentially huge number of possibilities makes \nit possible to study the interface between them with a simple formal model. The \ufb01rst \nstep is to characterize the type of problem that we study:\nDefinition. A search problem is a problem having solutions with the property that \nthe time needed to  certify that any solution is correct is bounded by a polynomial in \nthe size of the input. We say that an algorithm solves a search problem if, given any \ninput, it either produces a solution or reports that none exists.\nFour particular problems that are of interest in our discussion of intractability are \nshown at the top of the facing page. These problems are known as satis\ufb01ability prob-\nlems. Now, all that is required to establish that a problem is a search problem is to show \nthat any solution is suf\ufb01ciently well-characterized that you can ef\ufb01ciently  certify that \nit is correct. Solving a search problem is like searching for a \u201cneedle in a haystack\u201d with \nthe sole proviso that you can recognize the needle when you see it. For example, if you \nare given an assignment of values to variables in each of the satis\ufb01ability problems at \nthe top of page 913, you easily can certify that each equality or inequality is satis\ufb01ed, but \nsearching for ", "start": 924, "end": 924}, "1383": {"text": "an assignment of values to variables in each of the satis\ufb01ability problems at \nthe top of page 913, you easily can certify that each equality or inequality is satis\ufb01ed, but \nsearching for such an assignment is a totally different task. The name NP is commonly \nused to describe search problems\u2014we will describe the reason for the name on page 914: \nDefinition.   NP is the set of all search problems.\n \nNP is nothing more than a precise characterization of all the problems that scientists, \nengineers, and applications programmers  aspire to solve  with programs that are guar -\nanteed to \ufb01nish in a feasible amount of time. \n 913 I n t r a c t a b i l i t y  \n \nLinear equation satis\ufb01ability. Given a set of M linear equations involving N\nvariables, \ufb01nd an assignment of values to the variables that satis\ufb01es all of the \nequations, or report that none exists.\nLinear inequality satis\ufb01ability (search formulation of linear program -\nming). Given a set of M linear inequalities involving N variables, \ufb01nd an assign-\nment of values to the variables that satis\ufb01es all of the inequalities, or report that \nnone exists.\n0-1 integer linear inequality satis\ufb01ability (search formulation of 0-1 integer \nlinear programming). Given a set of M linear inequalities involving N integer\nvariables, \ufb01nd an assignment of the values 0 or 1 to the variables that satis\ufb01es all \nof the inequalities, or report that none exists.\n B o o l e a n  s a t i s \ufb01 a b i l i t y .  Given a set of M equations involving and and or opera-\ntions on N boolean variables, \ufb01nd an assignment of values to the variables that \nsatis\ufb01es all of the equations, ", "start": 924, "end": 925}, "1384": {"text": "i t y .  Given a set of M equations involving and and or opera-\ntions on N boolean variables, \ufb01nd an assignment of values to the variables that \nsatis\ufb01es all of the equations, or report that none exists.\n S e l e c t e d  s e a r c h  p r o b l e m s\n \nOther types of problems. The concept of search problems is one of many ways to char-\nacterize the set of problems that form the basis of the study of intractability. Other \npossibilities are  decision problems (does a solution exist?) and  optimization problems \n(what is the best solution)? For example, the longest-paths length problem on page 911 \nis an optimization problem, not a search problem (given a solution, we have no way \nto verify that it is a longest-path length). A search version of this problem is to \ufb01nd a \nsimple path connecting all the vertices (this problem is known as the   Hamiltonian path \nproblem). A decision version of the problem is to ask whether there exists a simple path \nconnecting all the vertices. Arbitrage, boolean satis\ufb01ability, and Hamiltonian path are \nsearch problems; to ask whether a solution exists to any of these problems is a decision \nproblem; and shortest/longest paths, max\ufb02ow, and linear programming are all optimi-\nzation problems. While not technically equivalent, search, decision, and optimization \nproblems typically reduce to one another (see Exercise 6.58 and 6.59) and the main \nconclusions we draw apply to all three types of problems. \n 914 CONTEXT\nEasy search problems. The de\ufb01nition of NP says nothing about the dif\ufb01culty of \ufb01nding\nthe solution, just certifying that it is a solution. The second of the two sets of problems \nthat form ", "start": 925, "end": 926}, "1385": {"text": "The de\ufb01nition of NP says nothing about the dif\ufb01culty of \ufb01nding\nthe solution, just certifying that it is a solution. The second of the two sets of problems \nthat form the basis of the study of intractability, which is known as P, is concerned with \nthe dif\ufb01culty of \ufb01nding the solution. In this model, the ef\ufb01ciency of an algorithm is a \nfunction of the number of bits used to encode the input.\n D e f i n i t i o n .    P is the set of all search problems that can be solved in polynomial time.\n \n \nImplicit in the de\ufb01nition is the idea that the polynomial time bound is a worst-case \nbound. For a problem to be in P, there must exist an algorithm that can guarantee to \nsolve it in polynomial time. Note that the polynomial is not speci\ufb01ed at all. Linear, lin-\nearithmic, quadratic, and cubic are all polynomial time bounds, so this de\ufb01nition cer -\ntainly covers the standard algorithms we have studied so far. The time taken by an algo-\nrithm depends on the computer used, but the extended Church-Turing thesis renders \nthat point moot\u2014it says that a polynomial-time solution on any computing device \nimplies the existence of a polynomial-time solution on any other computing device. \nSorting belongs to P because (for example) insertion sort runs in time proportional to\nN 2 (the existence of linearithmic sorting algorithms is not relevant in this context), as \ndoes shortest paths, linear equation satis\ufb01ability, and many others. Having an ef\ufb01cient \nalgorithm to solve a problem is a proof that the problem is in P. In other words, P is \nnothing more than a precise characterization of all the problems that scientists, engi-\nneers, and applications programmers  do ", "start": 926, "end": 926}, "1386": {"text": "\nalgorithm to solve a problem is a proof that the problem is in P. In other words, P is \nnothing more than a precise characterization of all the problems that scientists, engi-\nneers, and applications programmers  do solve  with programs that are guaranteed to \n\ufb01nish in a feasible amount of time.\n  N o n d e t e r m i n i s m .  The N in NP stands for  nondeterminism. It represents the idea that \none way (in theory) to extend the power of a computer is to endow it with the power \nof nondeterminism: to assert that when an algorithm is faced with a choice of several \noptions, it has the power to \u201cguess\u201d the right one.  For the purposes of our discus-\nsion, we can think of an algorithm for a nondeterministic machine as \u201cguessing\u201d the \nsolution to a problem, then certifying that the solution is valid. In a Turing machine, \nnondeterminism is as simple as de\ufb01ning two different successor states for a given state \nand a given input and characterizing solutions as all legal paths to the desired result. \nNondeterminism may be a mathematical \ufb01ction, but it is a useful idea. For example, \nin Section 5.4, we used nondeterminism as a tool for algorithm design\u2014our regular \nexpression pattern-matching algorithm is based on ef\ufb01ciently simulating a nondeter -\nministic machine. \n 915 I n t r a c t a b i l i t y  \nproblem input description poly-time\nalgorithm instance solution\nHamiltonian \npath graph G  \ufb01nd a simple path \nthat visits every vertex \n? 0-2-1-3\nfactoring  integer x \ufb01nd a nontrivial \nfactor of x ? 97605257271 8784561\n0-1 ", "start": 926, "end": 927}, "1387": {"text": "path \nthat visits every vertex \n? 0-2-1-3\nfactoring  integer x \ufb01nd a nontrivial \nfactor of x ? 97605257271 8784561\n0-1 linear \ninequality\nsatisfiability\nM 0-1\nvariables\nN inequalities\nassign values to the \nvariables that\nsatisfy the inequalities\n?\nx /H11002 y /H11349 1\n2x /H11002 z /H11349 2 \nx + y /H11350 2\nz /H11350 0\nx  = 1\ny = 1\nz = 0\nall problems \nin P see table below\nExamples of problems in NP\nproblem input description poly-time\nalgorithm instance solution\nshortest st-path graph G \nvertices s, t\n \ufb01nd the shortest path\nfrom s to t \nBFS\ns\nt\n0-3\nsorting array a\n \ufb01nd a permutation \nthat puts a in \nascending order\nmergesort 2.8 8.5 4.1 1.3 3 0 2 1\nlinear equation\nsatisfiability\nM variables\nN equations\nassign values to the \nvariables that\nsatisfy the equations\nGaussian\nelimination\nx + y = 1.5\n2x /H11002 y = 0\nx  = 0.5\ny = 1\nlinear \ninequality\nsatisfiability\nM variables\nN inequalities\nassign values to the \nvariables that\nsatisfy the inequalities\nellipsoid\nx /H11002 y /H11349 1.5\n2x /H11002 z /H11349 0 \nx + y /H11350 3.5\nz /H11350 4.0\nx ", "start": 927, "end": 927}, "1388": {"text": "/H11349 1.5\n2x /H11002 z /H11349 0 \nx + y /H11350 3.5\nz /H11350 4.0\nx  = 2.0\ny = 1.5\nz = 4.0\nExamples of problems in P\n 916 CONTEXT\n \nThe main question. Nondeterminism is such a powerful notion that it seems almost \nabsurd to consider it seriously. Why bother considering an imaginary tool that makes \ndif\ufb01cult problems seem trivial?  The answer is that, powerful as nondeterminism may \nseem, no one has been able to prove that it helps for any particular problem!  Put an -\nother way, no one has been able to \ufb01nd a single problem that can be proven to be in NP\nbut not in P (or even prove that one exists), leaving the following question open:\nDoes   P = NP ? \nThis question was \ufb01rst posed in a famous letter from K. G\u00f6del to J. von Neumann in \n1950 and has completely stumped mathematicians and computer scientists ever since. \nOther ways of posing the question shed light on its fundamental nature:\n\u25a0 Are there any hard-to-solve search problems?\n\u25a0  \n \nWould we be able to solve some search problems more ef\ufb01ciently if  we could \nbuild a nondeterministic computing device?\nNot knowing the answers to these questions is extremely frustrating because many im-\nportant practical problems belong to NP but may or may not belong to P (the best \nknown deterministic algorithms could take exponential time). If we could prove that \na problem does not belong to P, then we could abandon the search for an ef\ufb01cient so -\nlution to it. In the absence of such a proof, there is the possibility that some ef\ufb01cient \nalgorithm has gone undiscovered. In fact, ", "start": 927, "end": 928}, "1389": {"text": "we could abandon the search for an ef\ufb01cient so -\nlution to it. In the absence of such a proof, there is the possibility that some ef\ufb01cient \nalgorithm has gone undiscovered. In fact, given the current state of our knowledge, \nthere could be some ef\ufb01cient algorithm for every problem in NP, which would imply \nthat many ef\ufb01cient algorithms have gone undiscovered.  Virtually no one believes that \nP = NP, and a considerable amount of effort has gone into proving the contrary, but this \nremains the outstanding open research problem in computer science. \n P o l y - t i m e   r e d u c t i o n s .  Recall from page 903 that we show that a problem A reduces to \nanother problem B by demonstrating that we can solve any instance of A in three steps:\n\u25a0 Transform it to an instance of  B. \n\u25a0 Solve that instance of B. \n\u25a0 \n \n \nTr ansform the solution of  B to be a solution of A. \nAs long as we can perform the transformations (and solve B) ef\ufb01ciently, we can solve A\nef\ufb01ciently. In the present context, for ef\ufb01cient we use the weakest conceivable de\ufb01nition: \nto solve A we solve at most a polynomial number of instances of B, using transforma-\ntions that require at most polynomial time. In this case, we say that A poly-time reduces\nto B. Before, we used reduction to introduce the idea of problem-solving models that \ncan signi\ufb01cantly expand the range of problems that we can solve with ef\ufb01cient algo -\nrithms. Now, we use reduction in another sense: to prove a problem to be hard to solve. If \na problem A is known to be hard to solve, and A poly-time reduces to B, then B must be ", "start": 928, "end": 928}, "1390": {"text": "-\nrithms. Now, we use reduction in another sense: to prove a problem to be hard to solve. If \na problem A is known to be hard to solve, and A poly-time reduces to B, then B must be \nhard to solve, too. Otherwise, a guaranteed polynomial-time solution to B would give a \nguaranteed polynomial-time solution to A.\n 917 I n t r a c t a b i l i t y  \n Proposition L.   Boolean satis\ufb01ability poly-time reduces \nto 0-1 integer linear inequality satis\ufb01ability.\nProof:  Given an instance of boolean satis\ufb01ability, de -\n\ufb01ne a set of inequalities with one 0-1 variable corre-\nsponding to each boolean variable and one 0-1 vari-\nable corresponding to each clause, as illustrated in \nthe example at right. With this construction, we can \ntranform a solution to the integer 0-1 linear inequality \nsatis\ufb01ability problem to a solution to the boolean sat-\nis\ufb01ability problem by assigning each boolean variable \nto be true if the corresponding integer variable is 1 and \nfalse if it is 0.\n Corollary. If satis\ufb01ability is hard to solve, then so is in-\nteger linear programming.\n \nThis statement is a meaningful statement about the rela-\ntive dif\ufb01culty of solving these two problems even in the \nabsence of a precise de\ufb01nition of hard to solve. In the pres-\nent context, by \u201chard to solve, \u201d we mean \u201cnot in P.\u201d We  \ngenerally use the word intractable to refer to problems that \nare not in P. Starting with the seminal work of R. Karp in \n1972, researchers have shown literally tens of thousands \nof problems from a wide variety of applications areas to \nbe related by reduction ", "start": 928, "end": 929}, "1391": {"text": "problems that \nare not in P. Starting with the seminal work of R. Karp in \n1972, researchers have shown literally tens of thousands \nof problems from a wide variety of applications areas to \nbe related by reduction relationships of this sort. More-\nover, these relationships imply much more than just rela-\ntionships between the individual problems, a concept that \nwe now address.\n  NP-completeness. Many, many problems are known to belong to NP but probably do   \nnot belong to P. That is, we can easily certify that any given solution is valid, but, despite \nconsiderable effort, no one has been able to develop an ef\ufb01cient algorithm to \ufb01nd a so-\nlution. Remarkably, all of these many, many problems have an additional property that \nprovides convincing evidence that P /HS11005 NP :\nExample of reducing boolean satisfiability to\n0-1 integer linear inequality satisfiability\nc1 /H113501 /H5009 x1\nc1 /H11350x2\nc1 /H11350x3\nc1 /H11349(1 /H5009 x1) + x2 + x3\nc2 /H11350x1\nc2 /H113501 /H5009 x2\nc2 /H11350x3\nc2/H11349 x1 + (1 /H5009 x2) + x3\nc3 /H113501 /H5009 x1\nc3 /H113501 /H5009 x2\nc3 /H113501 /H5009 x3\nc3 /H11349(1 /H5009 x1) + 1 /H5009 x2 + (1 /H5009 x3 )\nc4 /H113501 /H5009 x1\nc4 /H113501 /H5009 x2\nc4 ", "start": 929, "end": 929}, "1392": {"text": "x1) + 1 /H5009 x2 + (1 /H5009 x3 )\nc4 /H113501 /H5009 x1\nc4 /H113501 /H5009 x2\nc4 /H11350x3\nc4 /H11349(1 /H5009 x1) + (1 /H5009 x2) + x3\ns /H11349  c1\ns /H11349  c2\ns /H11349  c3\ns /H11349  c4\ns  /H11350  c1 + c2 + c3 + c4  /H5009 3\ns is 1\nif and only if\nc\u2019s are all 1 \nc1is 1\nif and only if\nfirst clause is \nsatisfiable \nboolean satisfiability problem\n0-1 integer linear inequality satisfiability formulation\n(x'1 o r  x2 or x3) and \n        (x1 or x'2 o r  x3) and\n               (x'1 o r  x'2 o r  x'3) and\n                       (x'1 o r  x'2 o r  x3) \n 918 CONTEXT\nDefinition. A search problem A is said to be NP-complete if all problems in NP poly-\ntime reduce to A.\n  \n \nThis de\ufb01nition enables us to upgrade our de\ufb01nition of \u201chard to solve\u201d to mean \u201cintrac-\ntable unless P = NP.\u201d  If  any NP-complete problem can be solved in polynomial time on \na deterministic machine, then so can all problems in NP (i.e., P = NP). That is, the collec-\ntive failure of all researchers to \ufb01nd ef\ufb01cient algorithms for all of these problems might \nbe viewed as a collective ", "start": 929, "end": 930}, "1393": {"text": "all problems in NP (i.e., P = NP). That is, the collec-\ntive failure of all researchers to \ufb01nd ef\ufb01cient algorithms for all of these problems might \nbe viewed as a collective failure to prove that P = NP .  NP-complete problems, meaning \nthat we do not expect to \ufb01nd guaranteed polynomial-time algorithms. Mosr practical \nsearch problems are known to be either in P or NP-complete.\n  C o o k - L e v i n  t h e o r e m .  Reduction uses the NP-completeness of one problem to imply \nthe NP-completeness of another.  But reduction cannot be used in one case: how was \nthe \ufb01rst problem proven to be NP-complete?  This was done independently by  S. Cook \nand  L. Levin in the early 1970s.\n \nProposition M. ( Cook-Levin theorem) Boolean satis\ufb01ability is NP-complete.\nExtremely brief proof sketch:  The goal is to show that if there is a polynomial \ntime algorithm for boolean satis\ufb01ability, then all problems in NP can be solved in \npolynomial time. Now, a nondeterministic Turing machine can solve any problem \nin NP, so the \ufb01rst step in the proof is to describe each feature of the machine in \nterms of logical formulas such as appear in the boolean satis\ufb01ability problem. This \nconstruction establishes a correspondence between every problem in NP (which \ncan be expressed as a program on the nondeterministic Turing machine) and some \ninstance of satis\ufb01ability (the translation of that program into a logical formula). \nNow, the solution to the satis\ufb01ability problem essentially corresponds to a simula-\ntion of the machine running the given program on the given input, so it produces a \nsolution to an instance of the given problem.  Further details of this ", "start": 930, "end": 930}, "1394": {"text": "the satis\ufb01ability problem essentially corresponds to a simula-\ntion of the machine running the given program on the given input, so it produces a \nsolution to an instance of the given problem.  Further details of this proof are well \nbeyond the scope of this book.  Fortunately, only one such proof is really necessary: \nit is much easier to use reduction to prove NP-completeness.\nThe Cook-Levin theorem, in conjunction with the thousands and thousands of poly-\ntime reductions from NP-complete problems that have followed it, leaves us with two \npossible universes: either P = NP  and no intractable search problems exist (all search \nproblems can be solved in polynomial time); or P \u2260 NP, there do exist intractable search \nproblems (some search problems cannot be solved in polynomial time). NP-complete \n 919 I n t r a c t a b i l i t y  \nproblems arise frequently in important natural practical ap-\nplications, so there has been strong motivation to \ufb01nd good \nalgorithms to solve them.  The fact that no good algorithm has \nbeen found for any of these problems is surely strong evidence \nthat P \u2260 NP , and most researchers certainly believe this to be \nthe case. On the other hand, the fact that no one has been able \nto prove that any of these problems do not belong to P could \nbe construed to comprise a similar body of circumstantial evi-\ndence on the other side. Whether or not P = NP, the practical \nfact is that the best known algorithm for any of the NP-com-\nplete problems takes exponential time in the worst case.\nClassifying problems. To  p rove  t h a t  a  s e a rc h  p ro b l e m  i s  i n  P, we need to exhibit a \npolynomial-time algorithm for solving it, perhaps by reducing it to a problem ", "start": 930, "end": 931}, "1395": {"text": "t  a  s e a rc h  p ro b l e m  i s  i n  P, we need to exhibit a \npolynomial-time algorithm for solving it, perhaps by reducing it to a problem known \nto be in P. To prove that a problem in NP is NP-complete, we need to show that some \nknown NP-complete problem is poly-time reducible to it: that is, that a polynomial-\ntime algorithm for the new problem could be used to solve the NP-complete problem, \nand then could, in turn, be used to solve all problems in NP. Thousands and thousands \nof problems have been shown to be NP-complete in this way, as we did for integer lin-\near programming in Proposition L. The list on page 920, which includes several of the \nproblems addressed by Karp, is representative, but contains only a tiny fraction of the \nknown NP-complete problems. Classifying problems as being easy to solve (in P) or \nhard to solve (NP-complete) can be: \n\u25a0 \n \nStraightforward. For example, the venerable  Gaussian elimination algorithm \nproves that linear equation satis\ufb01ability is in P.\n\u25a0 Tricky but not dif\ufb01cult. For example, developing a proof like the proof of Propo-\nsition A takes some experience and practice, but it is easy to understand.\n\u25a0 Extremely challenging. For example, linear programming was long unclassi\ufb01ed, \nbut Khachian\u2019s ellipsoid algorithm proves that linear programming is in P.\n\u25a0  \n \nOpen. For example,  graph isomorphism (given two graphs, \ufb01nd a way to rename \nthe vertices of one to make it identical to the other) and  factor (given an integer, \n\ufb01nd a nontrivial factor) are still unclassi\ufb01ed.\nThis is a rich and active area of current research, still involving thousands of ", "start": 931, "end": 931}, "1396": {"text": "to the other) and  factor (given an integer, \n\ufb01nd a nontrivial factor) are still unclassi\ufb01ed.\nThis is a rich and active area of current research, still involving thousands of research \npapers per year. As indicated by the last few entries on the list on page 920,  all areas of sci-\nenti\ufb01c inquiry are affected. Recall that our de\ufb01nition of NP encompasses the problems \nthat scientists, engineers, and applications programmers  aspire to solve feasibly\u2014all \nsuch problems certainly need to be classi\ufb01ed! \nTwo possible universes\nP\nNP\nP \u2260 NP\nP = NP\nP = NP\nNPC\n 920 CONTEXT\n \n \nBoolean satis\ufb01ability. Given a set of M equations involving N boolean variables, \n\ufb01nd an assignment of values to the variables that satis\ufb01es all of the equations, or \nreport that none exists. \nInteger linear programming. Given a set of M linear inequalities involving N\ninteger variables, \ufb01nd an assignment of values to the variables that satis\ufb01es all of \nthe inequalities, or report that none exists.\nLoad balancing. Given a set of jobs of speci\ufb01ed duration  to be completed and a \ntime bound T, how can we schedule the jobs on two identical processors so as to \ncomplete them all by time T?\n V e r t e x  c o v e r .  Given a graph and a integer C, \ufb01nd a set of C vertices such that each \nedge of the graph is incident to at least one vertex of the set.\n   H a m i l t o n i a n  p a t h .  Given a graph, \ufb01nd a simple path that visits each vertex ex-\nactly once, or report that none exists.\n P r o t e i n  f o l d i n ", "start": 931, "end": 932}, "1397": {"text": "p a t h .  Given a graph, \ufb01nd a simple path that visits each vertex ex-\nactly once, or report that none exists.\n P r o t e i n  f o l d i n g .  Given energy level M, \ufb01nd a folded three-dimensional confor -\nmation of a protein having potential energy less than M.\nIsing model. Given an Ising model on a lattice of dimension three and an energy \nthreshhold E, is there a subgraph with free energy less than E ?\nRisk portfolio of a given return. Given an investment portfolio with a given total \ncost, a given return, risk values assigned to each investment, and a threshold M, \n\ufb01nd a way to allocate the investments such that the risk is less than M.\n S o m e  f a m o u s  N P - c o m p l e t e  p r o b l e m s\n 921 I n t r a c t a b i l i t y  \n \n  \n \n \n \n \nCoping with NP-completeness. Some sort of solution to this vast panopoly of prob-\nlems must be found in practice, so there is intense interest in \ufb01nding ways to address \nthem. It is impossible to do justice to this vast \ufb01eld of study in one paragraph, but we \ncan brie\ufb02y describe various approaches that have been tried. One approach is to change \nthe problem and \ufb01nd an \u201capproximation\u201d algorithm that \ufb01nds not the best solution but \na solution guaranteed to be close to the best. For example, it is easy to \ufb01nd a solution \nto the Euclidean traveling salesperson problem that is within a factor of 2 of the opti -\nmal. Unfortunately, this approach is often not suf\ufb01cient to ward off NP-completeness, \nwhen seeking improved approximations. Another approach is to ", "start": 932, "end": 933}, "1398": {"text": "problem that is within a factor of 2 of the opti -\nmal. Unfortunately, this approach is often not suf\ufb01cient to ward off NP-completeness, \nwhen seeking improved approximations. Another approach is to develop an algorithm \nthat solves ef\ufb01ciently virtually all of the instances that do arise in practice, even though \nthere exist worst-case inputs for which \ufb01nding a solution is infeasible. The most famous \nexample of this approach are the integer linear programming solvers, which have been \nworkhorses for many decades in solving huge optimizaiton problems in countless in-\ndustrial applications. Even though they could require exponential time, the inputs that \narise in practice evidently are not worst-case inputs. A third approach is to work with \n\u201cef\ufb01cient\u201d exponential algorithms, using a technique known as  backtracking to avoid \nhaving to check all possible solutions. Finally, there is quite a large gap between polyno-\nmial and exponential time that is not addressed by the theory. What about an algorithm \nthat runs in time proportional to N log N or 2/H20906 N ?\nAll the applications areas we have studied in this book are touched by NP-com-\npleteness: NP-complete problems arise in elementary programming, in sorting and \nsearching, in graph processing, in string processing, in scienti\ufb01c computing, in systems \nprogramming, in operations research, and in any conceivable area where computing \nplays a role.  The most important practical contribution of the theory of NP-complete-\nness is that it provides a mechanism to discover whether a new problem from any of \nthese diverse areas is \u201ceasy\u201d or \u201chard. \u201d  If one can \ufb01nd an ef\ufb01cient algorithm to solve a \nnew problem, then there is no dif\ufb01culty. If not, a proof that the problem is NP-complete \ntells us that developing an ef\ufb01cient ", "start": 933, "end": 933}, "1399": {"text": "can \ufb01nd an ef\ufb01cient algorithm to solve a \nnew problem, then there is no dif\ufb01culty. If not, a proof that the problem is NP-complete \ntells us that developing an ef\ufb01cient algorithm would be a stunning achievement (and \nsuggests that a different approach should perhaps be tried). The scores of ef\ufb01cient al-\ngorithms that we have examined in this book are testimony that we have learned a \ngreat deal about ef\ufb01cient computational methods since Euclid, but the theory of NP-\ncompleteness shows that, indeed, we still have a great deal to learn.\n 922 CONTEXT\nEXERCISES on collision simulation\n6.1  Complete the implementation predictCollisions() and Particle as described \nin the text. There are three equations governing the elastic collision between a pair of \nhard discs: (a) conservation of linear momentum, ( b) conservation of kinetic energy, \nand (c) upon collision, the normal force acts perpendicular to the surface at the colli-\nsion point (assuming no friction or spin). See the booksite for more details.\n6.2  Develop a version of CollisionSystem, Particle, and Event that handles multi-\nparticle collisions. Such collisions are important when simulating the break in a game \nof billiards. (This is a dif\ufb01cult exercise!)\n6.3  Develop a version of CollisionSystem, Particle, and Event that works in three \ndimensions.\n6.4 Explore the idea of improving the performance of simulate() in CollisionSystem\nby dividing the region into rectangular cells and adding a new event type so that you \nonly need to predict collisions with particles in one of nine adjacent cells in any time \nquantum. This approach reduces the number of predictions to calculate at the cost of \nmonitoring the movement of particles from cell to cell.\n6.5 Introduce the concept of entropy to CollisionSystem and use it to ", "start": 933, "end": 934}, "1400": {"text": "time \nquantum. This approach reduces the number of predictions to calculate at the cost of \nmonitoring the movement of particles from cell to cell.\n6.5 Introduce the concept of entropy to CollisionSystem and use it to con\ufb01rm clas -\nsical results. \n6.6  Brownian motion. In 1827, the botanist Robert Brown observed the motion \nof wild\ufb02ower pollen grains immersed in water using a microscope. He observed that \nthe pollen grains were in a random motion, following what would become known as \nBrownian motion. This phenomenon was discussed, but no convincing explanation \nwas provided until Einstein provided a mathematical one in 1905. Einstein\u2019s explana-\ntion: the motion of the pollen grain particles was caused by millions of tiny molecules \ncolliding with the larger particles. Run a simulation that illustrates this phenomenon.\n6.7  Temperature. Add a method temperature() to Particle that returns the prod-\nuct of its mass and the square of the magitude of its velocity divided by dkB where d =2 is \nthe dimension and kB =1.3806503 \u00d7 10/H1100223 is Boltzmann\u2019s constant. The temperature of \nthe system is the average value of these quantities. Then add a method temperature() \nto CollisionSystem and write a driver that plots the temperature periodically, to check \nthat it is constant.\n 923\n \n \n6.8  Maxwell-Boltzmann. The distribution of velocity of particles in the hard disc \nmodel obeys the Maxwell-Boltzmann distribution (assuming that the system has ther -\nmalized and particles are suf\ufb01ciently heavy that we can discount quantum-mechanical \neffects), which is known as the Rayleigh distribution in two dimensions. The distribu-\ntion shape depends on temperature. Write a driver that computes a histogram of the \nparticle velocities and test it for various temperatures.\n6.9  Arbitrary shape. ", "start": 934, "end": 935}, "1401": {"text": "is known as the Rayleigh distribution in two dimensions. The distribu-\ntion shape depends on temperature. Write a driver that computes a histogram of the \nparticle velocities and test it for various temperatures.\n6.9  Arbitrary shape. Molecules travel very quickly (faster than a speeding jet) but dif-\nfuse slowly because they collide with other molecules, thereby changing their direction. \nExtend the model to have a boundary shape where two vessels are connected by a pipe \ncontaining two different types of particles. Run a simulation and measure the fraction \nof particles of each type in each vessel as a function of time.\n6.10  Rewind. After running a simulation, negate all velocities and then run the system \nbackward. It should return to its original state! Measure roundoff error by measuring \nthe difference between the \ufb01nal and original states of the system.\n6.11  Pressure. Add a method pressure() to Particle that measures pressure by \naccumulating the number and magnitude of collisions against walls. The pressure \nof the system is the sucm of these quantities. Then add a method pressure() to \nCollisionSystem and write a client that validates the equation pv = nRT.\n6.12  Index priority queue implementation. Develop a version of CollisionSystem\nthat uses an index priority queue to guarantee that the size of the priority queue is at \nmost linear in the number of particles (instead of quadratic or worse).\n6.13  Priority queue performance. Instrument the priority queue and test Pressure\nat various temperatures to identify the computational bottleneck. If warranted, try \nswitching to a different priority-queue implementation for better performance at high \ntemperatures.\n   \n 924 CONTEXT\nEXERCISES on B-Trees\n6.14 Suppose that, in a three-level tree, we can afford to keep a links in internal mem-\nory, between b and 2b links in pages representing internal nodes, and between c and 2c\nitems ", "start": 935, "end": 936}, "1402": {"text": "B-Trees\n6.14 Suppose that, in a three-level tree, we can afford to keep a links in internal mem-\nory, between b and 2b links in pages representing internal nodes, and between c and 2c\nitems in pages representing external nodes. What is the maximum number of items that \nwe can hold in such a tree, as a function of a, b, and c?\n6.15  Develop an implementation of Page that represents each B-tree node as a \nBinarySearchST object.\n6.16  Extend BTreeSET to develop a BTreeST implementation that associates keys with \nvalues and supports our full ordered symbol table API that includes min(), max(), \nfloor(), ceiling(), deleteMin(), deleteMax(), select(), rank(), and the two-\nargument versions of size() and get().\n6.17 Write a program that uses StdDraw to visualize B-trees as they grow, as in the text.\n6.18 Estimate the average number of probes per search in a B-tree for S random \nsearches, in a typical cache system, where the T most-recently-accessed pages are kept in \nmemory (and therefore add 0 to the probe count). Assume that S is much larger than T.\n6.19  Web search.  Develop an implementation of Page that represents B-tree nodes as \ntext \ufb01les on web pages, for the purposes of indexing (building a concordance for) the \nweb. Use a \ufb01le of search terms. Take web pages to be indexed from standard input. T o \nkeep control, take a command-line parameter m, and set an upper limit of 10m internal \nnodes (check with your system administrator before running for large m). Use an m-\ndigit number to name your internal nodes. For example, when m is 4, your nodes names \nmight be BTreeNode0000, BTreeNode0001, BTreeNode0002, ", "start": 936, "end": 936}, "1403": {"text": "running for large m). Use an m-\ndigit number to name your internal nodes. For example, when m is 4, your nodes names \nmight be BTreeNode0000, BTreeNode0001, BTreeNode0002, and so forth. Keep pairs \nof strings on pages. Add a close() operation to the API, to sort and write. T o test your \nimplementation, ook for yourself  and your friends on your university\u2019s website.\n6.20  B* trees. Consider the sibling split (or B*-tree) heuristic for B-trees: When it \ncomes time to split a node because it contains M entries, we combine the node with \nits sibling.  If the sibling has k entries with k < M/H110021, we reallocate the items giving the \nsibling and the full node each about (M+k)/2 entries.  Otherwise, we create a new node \nand give each of the three nodes about 2 M/3 entries. Also, we allow the root to grow \nto hold about 4 M/3 items, splitting it and creating a new root node with two entries \nwhen it reaches that bound. State bounds on the number of probes used for a search \nor an insertion in a B*-tree of order M with N items. Compare your bounds with the \n 925\ncorresponding bounds for B-trees (see Proposition B).  Develop an insert implementa-\ntion for B*-trees.\n6.21 Write a program to compute the average number of external pages for a B-tree of \norder M built from N random insertions into an initially empty tree. Run your program \nfor reasonable values of M and N.\n6.22 If your system supports virtual memory, design and conduct experiments to com-\npare the performance of B-trees with that of binary search, for random searches in a \nhuge symbol table.\n6.23 For your internal-memory implementation of Page in ", "start": 936, "end": 937}, "1404": {"text": "system supports virtual memory, design and conduct experiments to com-\npare the performance of B-trees with that of binary search, for random searches in a \nhuge symbol table.\n6.23 For your internal-memory implementation of Page in EXERCISE 6.15, run experi-\nments to determine the value of M that leads to the fastest search times for a B-tree \nimplementation supporting random search operations in a huge symbol table. Restrict \nyour attention to values of M that are multiples of 100.\n6.24 Run experiments to compare search times for internal B-trees (using the value of \nM determined in the previous exercise), linear probing hashing, and red-black trees for \nrandom search operations in a huge symbol table. \n 926 CONTEXT\nEXERCISES on suffix arrays\n  6.25 Give, in the style of the \ufb01gure on page 882, the suf\ufb01xes, sorted suf\ufb01xes, index() and \nlcp() tables for the following strings: \na. abacadaba \nb. mississippi \nc. abcdefghij\nd. aaaaaaaaaa\n6.26 Identify the problem with the following code fragment to compute all the suf\ufb01xes \nfor suf\ufb01x sort:\nsuffix = \"\"; \nfor (int i = s.length() - 1; i >= 0; i--) \n{\n   suffix = s.charAt(i) + suffix;\n   suffixes[i] = suffix; \n}\nAnswer : It uses quadratic time and quadratic space.\n6.27  Some applications require a sort of cyclic rotations of a text, which all contain all \nthe characters of the text. For i from 0 to N /H11002 1, the i th cyclic rotation of a text of length \nN is the last N /H11002 i characters followed by the \ufb01rst i characters. Identify the problem with \nthe following code fragment to compute all ", "start": 937, "end": 938}, "1405": {"text": "1, the i th cyclic rotation of a text of length \nN is the last N /H11002 i characters followed by the \ufb01rst i characters. Identify the problem with \nthe following code fragment to compute all the cyclic rotations:\nint N = s.length(); \nfor (int i = 0; i < N; i++)\n    rotation[i] = s.substring(i, N) + s.substring(0, i);\nAnswer : It uses quadratic time and quadratic space.\n6.28  Design a linear-time algorithm to compute all the cyclic rotations of a text string.\nAnswer : \nString t = s + s; \nint N = s.length(); \nfor (int i = 0; i < N; i++)\n   rotation[i] = r.substring(i, i + N);\n 927\n \n \n \n \n \n6.29 Under the assumptions described in Section 1.4.  give the memory usage of a \nSuffixArray object with a string of length N .\n6.30  Longest common substring. Write a SuffixArray client LCS that take two \ufb01le-\nnames as command-line arguments, reads the two text \ufb01les, and \ufb01nds the longest sub-\nstring that appears in both in linear time.  (In 1970, D. Knuth conjectured that this \ntask was impossible.) Hint : Create a suf\ufb01x array for s#t where s and t are the two text \nstrings and # is a character that does not appear in either.\n6.31  Burrows-Wheeler transform. The Burrows-Wheeler transform (BWT) is a trans -\nformation that is used in data compression algorithms, including bzip2 and in high-\nthroughput sequencing in genomics. Write a SuffixArray client that computes the \nBWT in linear time, as follows: Given a string of length N (terminated by a special end-\nof-\ufb01le character $ ", "start": 938, "end": 939}, "1406": {"text": "high-\nthroughput sequencing in genomics. Write a SuffixArray client that computes the \nBWT in linear time, as follows: Given a string of length N (terminated by a special end-\nof-\ufb01le character $ that is smaller than any other character), consider the N-by-N matrix \nin which each row contains a different cyclic rotation of the original text string. Sort \nthe rows lexicographically. The Burrows-Wheeler transform is the rightmost column \nin the sorted matrix. For example, the BWT of mississippi$ is ipssm$pissii. The \nBurrows-Wheeler inverse transform  (BWI) inverts the BWT. For example, the BWI of \nipssm$pissii is mississippi$.  Also write a client that, given the BWT of a text string, \ncomputes the BWI in linear time.\n6.32  Circular string linearization. Write a SuffixArray client that, given a string, \ufb01nds \nthe cyclic rotation that is the smallest lexicographically in linear time. This problem \narises in chemical databases for circular molecules, where each molecule is represented \nas a circular string, and a canonical representation (smallest cyclic rotation) is used to \nsupport search with any rotation as key. (See Exercise 6.27 and Exercise 6.28.)\n6.33  Longest k-repeated substring. Write a SuffixArray client that, given a string and \nan integer k, \ufb01nd the longest substring that is repeated k or more times.\n6.34  Long repeated substrings. Write a SuffixArray client that, given a string and an \ninteger L, \ufb01nds all repeated substrings of length L or more.\n6.35  k-gram frequency counts. Develop and implement an ADT for preprocessing a \nstring to support ef\ufb01ciently answering queries of the form How ", "start": 939, "end": 939}, "1407": {"text": "\ufb01nds all repeated substrings of length L or more.\n6.35  k-gram frequency counts. Develop and implement an ADT for preprocessing a \nstring to support ef\ufb01ciently answering queries of the form How many times does a given \nk-gram appear ? Each query should take time proportional to k log N in the worst case, \nwhere N is the length of the string.\n 928 CONTEXT\nEXERCISES on maxflow\n \n \n \n6.36 If capacities are positive integers less than M, what is the maximum possible \ufb02ow \nvalue for any st-network with V vertices and E edges? Give two answers, depending on \nwhether or not parallel edges are allowed.\n6.37 Give an algorithm to solve the max\ufb02ow problem for the case that the network \nforms a tree if the sink is removed.\n6.38  True or false. If true provide a short proof, if false give a counterexample: \na. In any max \ufb02ow, there is no directed cycle on which every edge carries \npositive \ufb02ow \nb. There exists a max \ufb02ow for which there is no directed cycle on which every \nedge carries positive \ufb02ow \nc. If all edge capacities are distinct, the max \ufb02ow is unique\nd. If all edge capacities are increased by an additive constant, the min cut \nremains unchanged\ne. If all edge capacities are multiplied by a positive integer, the min cut re-\nmains unchanged\n6.39  Complete the proof of Proposition G: Show that each time an edge is a critical \nedge, the length of the augmenting path through it must increase by 2.\n6.40 Find a large network online that you can use as a vehicle for testing \ufb02ow algo -\nrithms on realistic data. Possibilities include transportation networks (road, rail, or \nair), communications ", "start": 939, "end": 940}, "1408": {"text": "2.\n6.40 Find a large network online that you can use as a vehicle for testing \ufb02ow algo -\nrithms on realistic data. Possibilities include transportation networks (road, rail, or \nair), communications networks (telephone or computer connections), or distribution \nnetworks. If capacities are not available, devise a reasonable model to add them. Write \na program that uses the interface to implement \ufb02ow networks from your data. If war -\nranted, develop additional private methods to clean up the data.\n6.41 Write a random-network generator for sparse networks with integer capacities \nbetween 0 and 2 20. Use a separate class for capacities and develop two implementa -\ntions: one that generates uniformly distributed capacities and another that generates \ncapacities according to a Gaussian distribution. Implement client programs that gener-\nate random networks for both weight distributions with a well-chosen set of values of \nV and E so that you can use them to run empirical tests on graphs drawn from various \ndistributions of edge weights.\n 929\n6.42 Write a program that generates V random points in the plane, then builds a \ufb02ow \nnetwork with edges (in both directions) connecting all pairs of points within a given \ndistance d of each other, setting each edge\u2019s capacity using one of the random models \ndescribed in the previous exercise.\n6.43  Basic reductions. Develop FordFulkerson clients for \ufb01nding a max\ufb02ow in each \nof the following types of \ufb02ow networks:\n\u25a0 Undirected \n\u25a0 No constraint on the number of sources or sinks or on edges entering the source \nor leaving the sink\n\u25a0 Lower bounds on capacities \n\u25a0 \n \n \n \nCapacity constraints on vertices\n6.44  Product distribution. Suppose that a \ufb02ow represents products to be transferred by \ntrucks between cities, with the \ufb02ow on edge u-v representing ", "start": 940, "end": 941}, "1409": {"text": "\n\u25a0 \n \n \n \nCapacity constraints on vertices\n6.44  Product distribution. Suppose that a \ufb02ow represents products to be transferred by \ntrucks between cities, with the \ufb02ow on edge u-v representing the amount to be taken \nfrom city u to city v in a given day. Write a client that prints out daily orders for truck-\ners, telling them how much and where to pick up and how much and where to drop off. \nAssume that there are no limits on the supply of truckers and that nothing leaves a given \ndistribution point until everything has arrived.\n6.45  Job placement. Develop a  FordFulkerson client that solves the job-placement \nproblem, using the reduction in Proposition J. Use a symbol table to convert symbolic \nnames into integers for use in the \ufb02ow network.\n6.46 Construct a family of bipartite matching problems where the average length of \nthe augmenting paths used by any augmenting-path algorithm to solve the correspond-\ning max\ufb02ow problem is proportional to E.\n6.47  st-connectivity. Develop a FordFulkerson client that, given an undirected graph \nG and vertices s and t, \ufb01nds the minimum number of edges in G whose removal will \ndisconnect t from s.\n6.48  Disjoint paths. Develop a FordFulkerson client that, given an undirected graph \nG and vertices s and t, \ufb01nds the maximum number of edge-disjoint paths from s to t.\n 930 CONTEXT\nEXERCISES on reductions and intractability\n \n \n  \n \n6.49  Find a nontrivial factor of 37703491.\n6.50 Prove that the shortest-paths problem reduces to linear programming.\n6.51 Could there be an algorithm that solves an NP-complete problem in an average \ntime of N log N, if P \u2260 NP? Explain your answer.\n6.52 ", "start": 941, "end": 942}, "1410": {"text": "the shortest-paths problem reduces to linear programming.\n6.51 Could there be an algorithm that solves an NP-complete problem in an average \ntime of N log N, if P \u2260 NP? Explain your answer.\n6.52 Suppose that someone discovers an algorithm that is guranteed to solve the bool-\nean satis\ufb01ability problem in time proportional to 1.1N  Does this imply that we can solve \nother NP-complete problems in time proportional to 1.1N?\n6.53 What would be the signi\ufb01cance of a program that could solve the integer linear \nprogramming problem in time proportional to 1.1N ?\n6.54 Give a poly-time reduction from vertex cover to 0-1 integer linear inequality \nsatis\ufb01ability. \n6.55 Prove that the problem of \ufb01nding a Hamiltonian path in a directed graph is NP-\ncomplete, using the NP-completeness of the Hamiltonian-path problem for undirected \ngraphs.\n6.56 Suppose that two problems are known to be NP-complete. Does this imply that \nthere is a poly-time reduction from one to the other?\n6.57 Suppose that X is NP-complete, X poly-time reduces to Y, and Y poly-time reduces \nto X. Is Y necessarily NP-complete?\nAnswer : No, since Y may not be in NP. \n6.58  Suppose that we have an algorithm to solve the decision version of boolean satis\ufb01-\nability, which indicates that there exists and assignment of truth values to the variables \nthat sait\ufb01es the boolean expression. Show how to \ufb01nd the assignment.\n6.59  Suppose that we have an algorithm to solve the decision version of the vertex \ncover problem, which indicates that there exists a vertex cover of a given size. Show how \nto solve the optimization version of \ufb01nding the vertex cover of minimum cardinality.\n6.60 ", "start": 942, "end": 942}, "1411": {"text": "decision version of the vertex \ncover problem, which indicates that there exists a vertex cover of a given size. Show how \nto solve the optimization version of \ufb01nding the vertex cover of minimum cardinality.\n6.60 Explain why the optimization version of the vertex cover problem is not neces-\nsarily a search problem.\n 931\n \nAnswer : There does not appear to be an ef\ufb01cient way to certify that a purported solu -\ntion is the best possible (even though we could use binary search on the search version \nof the problem to \ufb01nd the best solution).\n6.61 Suppose that X and Y are two search problems an that X poly-time reduces to Y. \nWhich of the following can we infer?\na. If Y is NP-complete then so is X.\nb. If X is NP-complete then so is Y.\nc. If X is in P, then Y is in P.\nd. If Y is in P, then X is in P.\n6.62 Suppose that P \u2260 NP. Which of the following can we infer?\ne. If X is NP-complete, then X cannot be solved in polynomial time.\nf. If X is in NP, then X cannot be solved in polynomial time.\ng. If X is in NP but not NP-complete, then X can be solved in polynomial time.\nh. If X is in P, then X is not NP-complete.\n Index 933\nSymbols\n2-3-4 search tree 441, 451\n2-3 search tree 424\u2013431\n2-nodes and 3-nodes 424\nanalysis of 429\nde\ufb01ned 424\nheight 429\ninsertion 425\u2013427\norder 424\nperfect balance 424\nand red-black BST 432\nsearch 425\n2-3 tree. See 2-3 search tree\n2-colorability problem 546\n2-dimensional array 19\n2-satis\ufb01ability ", "start": 942, "end": 945}, "1412": {"text": "424\nperfect balance 424\nand red-black BST 432\nsearch 425\n2-3 tree. See 2-3 search tree\n2-colorability problem 546\n2-dimensional array 19\n2-satis\ufb01ability problem 599\n2-sum problem 189\n3-collinear problem 211\n3-sum problem 173, 190\n3-way partitioning 298\n3-way quicksort 298\u2013301\n3-way string quicksort 719\u2013723\n8-puzzle problem 358\n32-bit architecture 13, 201, 212\n64-bit architecture 13, 201\nA\nA* algorithm 350\nAbstract data type 64\nAPI 65\nclient 88\u201389\ndesign 96\u201397\nimplementing an 84\u201387\nmultiple implementations 90\nAbstract in-place merge 270\nAccumulator data type 92\u201393\nActual type 134 , 328\nAcyclic digraph.\nSee Directed acyclic graph\nAcyclic edge-weighted digraph. \nSee Edge-weighted DAG\nAcyclic graph 520, 547, 576\nAdjacency list\ndirected graph 568\u2013569\nedge-weighted digraph 644\nedge-weighted graph 609\nundirected graph 524\u2013525\nAdjacency matrix 524, 527\nAdjacency set 527\nAdjacent vertex 519\nADT. See Abstract data type\nAlgorithm\ncentury of 853\nde\ufb01ned 4\ndeterministic 4\nnondeterministic 914\nrandomized 198\nAliasing\nof arrays 19\nof objects 69\nof substrings 202\nAll-pairs reachability 590\nAll-pairs shortest paths 656\nAlphabet data type 698\u2013700\nAmortized analysis\nbinary heap 320\nde\ufb01ned 198\u2013199\nhash table 475\nresizing ", "start": 945, "end": 945}, "1413": {"text": "202\nAll-pairs reachability 590\nAll-pairs shortest paths 656\nAlphabet data type 698\u2013700\nAmortized analysis\nbinary heap 320\nde\ufb01ned 198\u2013199\nhash table 475\nresizing array 199\nunion-\ufb01nd 231 , 237\nweighted quick-union with \npath compression 231\nAnalysis of algorithms 172\u2013215.\nSee also Propositions;\nSee also Properties\namortized analysis 198\u2013199\nbig-Oh notation 206\u2013207\n INDEX934\ncost model 182\ndivide-and-conquer 272\ndoubling ratio 192\u2013193\ndoubling test 176\u2013177\ninput models 197\nlog-log plot 176\nmathematical models 178\nmemory usage 200\u2013204\nmultiple parameters 196\nobservations 173\u2013175\norder-of-growth 179\norder-of-growth classi\ufb01ca-\ntions 186\u2013188\norder-of-growth hypoth-\nesis 180\nproblem size 173\nrandomized algorithm 198\nscienti\ufb01c method 172\ntilde approximation 178\nworst-case guarantee 197\nAntisymmetric relation 247\nAPIs\nAccumulator 93\nAlphabet 698\nBag 121\nBinaryStdIn 812\nBinaryStdOut 812\nBuffer 170\nCC 543\nCounter 65\nDate 79\nDegrees 596\nDeque 167\nDigraph 568\nDirectedCycle 576\nDirectedDFS 570\nDirectedEdge 641\nDraw 83\nEdge 608\nEdgeWeightedDigraph 641\nEdgeWeightedGraph 608\nFixedCapacityStack 135\nFixedCapacityStackOf-\nStrings 133\nFlowEdge 890\nFlowNetwork 890\nGeneralizedQueue 169\nGraph 522\nGraphProperties 559\nIn 41 , 83\nIndexMaxPQ ", "start": 945, "end": 946}, "1414": {"text": "608\nFixedCapacityStack 135\nFixedCapacityStackOf-\nStrings 133\nFlowEdge 890\nFlowNetwork 890\nGeneralizedQueue 169\nGraph 522\nGraphProperties 559\nIn 41 , 83\nIndexMaxPQ 320\nIndexMinPQ 320\nInterval1D 77\nInterval2D 77\njava.lang.Double 34\njava.lang.Integer 34\njava.lang.Math 28\njava.lang.String 80\njava.util.Arrays 29\nKMP 769\nList 511\nMathSET 509\nMatrix 60\nMaxPQ 309\nMinPQ 309\nMST 613\nOut 41 , 83\nPage 870\nParticle 860\nPaths 535\nPoint2D 77\nQueue 121\nRandomBag 167\nRandomQueue 168\nRational 117\nSCC 586\nSearch 528\nSET 489\nSP 644 , 677\nST 363 , 366, 860, 870, 879\nStack 121\nStaticSETofInts 99\nStdDraw 43\nStdIn 39\nStdOut 37\nStdRandom 30\nStdStats 30\nStopwatch 175\nStringSET 754\nStringST 730\nSuffixArray 879\nSymbolDigraph 581\nSymbolGraph 548\nTopological 578\nTransaction 79\nTransitiveClosure 592\nUF 219\nVisualAccumulator 95\nApplication programming inter-\nface. See also APIs\nclient 28\ncontract 33\ndata type de\ufb01nition 65\nimplementation 28\nlibrary of static methods 28\nArbitrage detection 679\u2013681\nArithmetic expression evalua-\ntion 128\u2013131\nArray 18\u201321\n2-dimensional 19\naliasing 19\nas object 72\nbounds checking 19\nmemory ", "start": 946, "end": 946}, "1415": {"text": "28\nArbitrage detection 679\u2013681\nArithmetic expression evalua-\ntion 128\u2013131\nArray 18\u201321\n2-dimensional 19\naliasing 19\nas object 72\nbounds checking 19\nmemory usage of 202\nof objects 72\nragged 19\nArray resizing. See Resizing array\nArrays.sort() 29 , 306\nArticulation point 562\nASCII encoding 696, 815\nAssertion 107\nassert statement 107\nAssignment statement 14\nAssociative array 363\nAugmenting path 891\nAutoboxing 122, 214\nAV L  t re e  4 5 2\n INDEX 935\nB\nBacktracking 921\nBag data type 124 , 154\u2013156\nBalanced search tree 424\u2013457\n2-3 search tree 424\u2013431\nAV L  t re e  4 5 2\nB-tree 866\u2013874\nred-black BST 432\u2013447\nBase case 25\nBellman-Ford 671\u2013678\nBellman, R. 683\nBentley, J. 298, 306\nBFS. See Breadth-\ufb01rst search\nBiconnectivity 562\nBig-Oh notation 206\u2013207\nBig-Omega notation 207\nBig-Theta notation 207\nBinary data 811\u2013815\nBinary dump 813\u2013814\nBinary heap 313\u2013322\namortized analysis of 320\nanalysis of 319\nchange priority 321\nde\ufb01ned 314\ndeletion 321\nheapsort 323\u2013327\ninsertion 317\nremove the maximum 317\nremove the minimum 321\nrepresentation 313\nsink and swim 315\u2013316\nBinary logarithm function 185\nBinary search 8\nanalysis of 383 , 391\nbitonic ", "start": 946, "end": 947}, "1416": {"text": "323\u2013327\ninsertion 317\nremove the maximum 317\nremove the minimum 321\nrepresentation 313\nsink and swim 315\u2013316\nBinary logarithm function 185\nBinary search 8\nanalysis of 383 , 391\nbitonic search 210\nfor a fraction 211\nin a sorted array 46\u201347, 98\u201399\nlocal minimum 210\nsymbol table 378\u2013384\nBinary search tree 396\u2013423\nanalysis of 403\nanatomy of 396\n \nAV L  t re e  4 5 2\ncerti\ufb01cation 419\nde\ufb01ned 396\ndelete the min/max 408\n\ufb02oor and ceiling 406\nheight 412\nHibbard deletion 410, 422\ninsertion 400\u2013401\nminimum and maximum 406\nnonrecursive 417\nperfectly balanced 403\nrange query 412\nrank and select 415\nrecursion 415\nrepresentation 397\nrotation 433\u2013434\nsearch 397\u2013401\nselection and rank 406, 408\nsymmetric order 396\nthreading 420\nBinaryStdIn library 811\u2013815\nBinaryStdOut library 811\u2013815\nBinary tree\nanatomy of 396\nbinary heap 313\ncomplete 313 , 314\ndecision tree 280\nexternal path length 418\nheap-ordered 313\nheight 314\ninorder traversal 412\ninternal path length 412\nlevel-order traversal 420\npreorder traversal 834\nweighted external path \nlength 832\nBinomial coef\ufb01cient 185\nBinomial distribution 59, 466\nBinomial tree 237\nBipartite graph 521, 546\u2013547\nBirthday problem 215\nBitmap 822\nBitonic array 210\nBitonic search 210\nBitonic shortest paths ", "start": 947, "end": 947}, "1417": {"text": "466\nBinomial tree 237\nBipartite graph 521, 546\u2013547\nBirthday problem 215\nBitmap 822\nBitonic array 210\nBitonic search 210\nBitonic shortest paths 689\nBlacklist \ufb01lter 491\nBoerner\u2019s theorem 357\nboolean primitive data type 12\nBoolean satis\ufb01ability 913, 920\nBoruvka, O. 628\nBoruvka\u2019s algorithm 629, 636\nBottleneck shortest paths  690\nBottom-up 2-3-4 tree 451\nBottom-up mergesort 277\nBoyer-Moore 770\u2013773\nBoyer, R. S. 759\nBreadth-\ufb01rst search\nin a digraph 573\nin a graph 538\u2013542\nbreak statement 15\nBridge in a graph 562\nB-tree 448 , 866\u2013874\nanalysis of 871\ninsertion 868\nperfect balance 868\nsearch 868\nBuffer data type 170\nByte (8 bits) 200\nbyte primitive data type 13\nC\nCache 195 , 307, 327, 343, 394, \n419, 423\nCall a method 22\nCallback 339 . See also Interface\nCast 13 , 328, 346\nCatenable queue 171\nCeiling function\nbinary search tree 406\nmathematical function 185\nordered array 380\nsymbol table 367\nCell-probe model 234\n INDEX936\nCenter of a graph 559\nCerti\ufb01cation\nbinary heap 330\nbinary search 392\nbinary search tree 419\nminimum spanning tree 634\nNP complexity class 912\nred-black BST 452\nsearch problem 912\nshortest paths 651\nsorting 246 , 265\nchar primitive data ", "start": 947, "end": 948}, "1418": {"text": "392\nbinary search tree 419\nminimum spanning tree 634\nNP complexity class 912\nred-black BST 452\nsearch problem 912\nshortest paths 651\nsorting 246 , 265\nchar primitive data type 12, 696\nChazelle, B. 629, 853\nChebyshev\u2019s inequality 303\nChurch-Turing thesis 910\nCircular linked list 165\nCircular queue 169\nCircular rotation 114\nClasspath 66\nClient 28\nClosest pair 210\nCollections 120\nbag 124\u2013125\ncatenable 171\ndeque 167\ngeneralized queue 169\npriority queue 308\u2013334\npushdown stack 127\nqueue 126\nrandom bag 167\nrandom queue 168\nring buffer 169\nstack 127\nsteque 167\nsymbol table 360\u2013513\ntrie 730\u2013757 .\nCollision resolution 458\nCombinatorial search 912\nCommand-line argument 36\nCommand-line interface\ncommand-line argument 36\ncompile a Java program 10\npiping 40\nredirection 40\nrun a Java program 10\nstandard input 39\nstandard output 37\u201338\nterminal window 36\nComma-separated-value 493\nComparable interface\ncompareTo() method 246\u2013247\nDate 247\nnatural order 337\nsorting 244 , 246\u2013247\nString 353\nsymbol table 368\u2013369\nTransaction 266\nComparator interface 338\u2013340\ncompare() method 338\u2013339\npriority queue 340\nTransaction 339\ncompare() method. \nSee Comparator interface\ncompareTo() method.\nSee Comparable interface\nCompile a program 10\nCompiler 492 , 498\nComplete binary tree 314\nComplete graph 681\nCompression. \nSee Data compression\nComputability 910\nComputational complexity\nCook-Levin theorem 918\nintractability ", "start": 948, "end": 948}, "1419": {"text": "interface\nCompile a program 10\nCompiler 492 , 498\nComplete binary tree 314\nComplete graph 681\nCompression. \nSee Data compression\nComputability 910\nComputational complexity\nCook-Levin theorem 918\nintractability 910\u2013921\nNP-complete 917\u2013918\nNP 912\nP 914\nP= NP question 916\npoly-time reduction 916\u2013917\nsorting 279\u2013282\nComputational geometry 76\nConcatenation of strings 34\nConcordance 510\nConcrete type 122, 134\nConditional statement 15\nConnected components\ncomputing 543\u2013546\nde\ufb01ned 519\nunion-\ufb01nd 217\nConnected graph 519\nConnectivity\narticulation point 562\nbiconnectivity 562\nbridge 562\ncomponents 543\u2013546\ndynamic 216\nedge-connected graph 562\nstrong connectivity 584\u2013591\nundirected graph 530\nunion-\ufb01nd 216\u2013241\nConstant running time 186\nConstructor 65 , 84\u201385\ncontinue statement 15\nContract 33\nCook-Levin theorem 918\nCook, S. 759, 918\nCost model 182 .\narray accesses 182, 220, 369\nbinary search 184\nB-tree 866\ncompares 369\nequality tests 369\nsearching 369\nsorting 246\nsymbol table 369\n3-sum 182\nunion-\ufb01nd 220\nCoupon collector problem 215\nCovariant arrays 158\nCPM. See Critical-path method\nC language 104\nC++ language 104\nCritical edge 633, 690, 900\nCritical path 663\nCritical-path method 663, 664\n INDEX 937\nCrossing edge 606\nCubic running time 186\nCuckoo hashing 484\nCut ", "start": 948, "end": 949}, "1420": {"text": "633, 690, 900\nCritical path 663\nCritical-path method 663, 664\n INDEX 937\nCrossing edge 606\nCubic running time 186\nCuckoo hashing 484\nCut 606 .\nSee also Mincut problem\ncapacity of 892\noptimality conditions 634\nproperty for MST 606\nst-cut 892\nCycle\nEulerian 562 , 598\nHamiltonian 562\nin a digraph 567\nin a graph 519\nodd length 562\nsimple 519 , 567\nCycle detection 546\u2013547\nCyclic rotation of a string 784\nD\nDAG. See Directed acyclic graph\nDangling else 52\nDantzig, G. 909\nData abstraction 64\u2013119\nData compression 810\u2013851\n\ufb01xed-length code 819\u2013821\nHuffman 826\u2013838\nlossless 811\nlossy 811\nLZW algorithm 839\u2013845\npre\ufb01x-free code 826\u2013827\nrun-length encoding 822\u2013825\n2-bit genomics code 819\u2013821\nundecidability 817\nuniquely decodable code 826\nuniversal 816\nvariable-length code 826\nData structure\nadjacency lists 525\nadjacency matrix 524\nbinary heap 313\nbinary search tree 396\nbinary tree 396\ncircular linked list 165\nde\ufb01ned 4\ndoubly-linked list 146\nlinked list 142\u2013146\nmultiway trie 732\nordered array 312\nordered list 312\nparallel arrays 378\nparent-link 225\nresizing array 136\nternary search trie 746\nunordered array 310\nunordered list 312\nData type\nabstract 64\ndesign of 96\u201397\nencapsulation 96\nDate data type 78\u201379\ncompareTo() ", "start": 949, "end": 949}, "1421": {"text": "array 136\nternary search trie 746\nunordered array 310\nunordered list 312\nData type\nabstract 64\ndesign of 96\u201397\nencapsulation 96\nDate data type 78\u201379\ncompareTo() method 247\nequals() method 103\nimplementation 91\ntoString() method 103\nDecision problem 913\nDecision tree 280\nDeclaration statement 14\nDedup 490\nDefault initialization 18, 86\nDefensive copy 112\nDegree of a vertex 519\nDegrees of separation 553\u2013554\nDenial-of-service attacks 197\nDense graph 520\nDeprecated method 113\nDepth-\ufb01rst search 530\u2013534\nbipartiteness 547\nconnected components 543\ncycle detection 547\ndirected cycle 574\u2013581\nlongest path 912\nmaze exploration 530\npath \ufb01nding 535\u2013537\n \nreachability 570\u2013573\nstrong components 584\u2013591\ntopological order 574\u2013581\ntransitive closure 592\nTremaux explor ation 530\n2-colorability 547\nunion-\ufb01nd 546\nDepth of a node 226\nDeque data type 167 , 212\nDesign by contract 107\nDeterministic \ufb01nite state au-\ntomaton 764\nDevroye, L. 412\nDFA. See Deterministic \ufb01nite \nstate automaton\nDiameter of a graph 559, 685\nDictionary 361 .\nSee also Symbol table\nDigraph. See Directed graph\nDigraph data type 568\u2013569\nDijkstra, E. W. 128, 298, 628, \n682\nDijkstra\u2019s 2-stack algo-\nrithm 128\u2013131\nDijkstra\u2019s algorithm 652\u2013657\nbidirectional search 690\nnegative weights 668\nDirected acyclic ", "start": 949, "end": 949}, "1422": {"text": "298, 628, \n682\nDijkstra\u2019s 2-stack algo-\nrithm 128\u2013131\nDijkstra\u2019s algorithm 652\u2013657\nbidirectional search 690\nnegative weights 668\nDirected acyclic graph 574\u2013583\ndepth-\ufb01rst orders 578\nedge-weighted 658\u2013667\nHamiltonian path 598\nlowest common ancestor 598\nshortest ancestral path 598\ntopological order 575\ntopological sort 575\nDirected cycle 567\nDirected cycle detection 576\nDirected edge 566\nDirected graph 566\u2013603.\nSee also Edge-weighted \ndigraph\nacyclic 574\u2013583\n INDEX938\nadjacency-lists representa-\ntion 568 , 568\u2013569\nall-pairs reachability 590\nanatomy of 567\nbreadth-\ufb01rst search 573\ncycle 567\ncycle detection 576\nde\ufb01ned 566\ndirected paths 573\nedge 566\nEuler cycle 598\nindegree and outdegree 566\nKosaraju\u2019s algorithm 586\u2013590\npath 567\npostorder traversal 578\npreorder traversal 578\nreachability 570\u2013572\nreachable vertex 567\nreverse 568\nreverse postorder 578\nshortest ancestral path 598\nshortest directed paths 573\nsimple 567\nstrong component 584\nstrong connectivity 584\u2013591\nstrongly-connected 584\ntopological order 575\u2013583\ntransitive closure 592\nDirected path 567\nDisjoint set union.\nSee Union \ufb01nd\nDivide-and-conquer paradigm\nmergesort 270\nquicksort 288 , 293\nDivision by zero 51\nDocumentation 28\nDouble hashing 483\ndouble primitive data type 12\nDouble probing 483\nDoubling array.\nSee Resizing array \nDoubling ratio ", "start": 949, "end": 950}, "1423": {"text": "270\nquicksort 288 , 293\nDivision by zero 51\nDocumentation 28\nDouble hashing 483\ndouble primitive data type 12\nDouble probing 483\nDoubling array.\nSee Resizing array \nDoubling ratio experiment 192\nDoubling test 176\u2013177\nDoubly-linked list 146\nDraw data type 82 , 83\nDump 813\nDuplicate keys\n3-way quicksort 301\nhash table 488\nin a symbol table 363\nMSD string sort 715\npriority queue 309\nquicksort 292\nsorting 344\nstability 341\nDutch National Flag 298\nDynamic connectivity 216\nDynamic memory allocation 104\nDynamic programming 671\nDynamic resizing array.\nSee Resizing array\nE\nEccentricity of a vertex 559\nEdge\nbackward 891\ncritical 633 , 900\ncrossing 606\ndata type 608\ndirected 566 , 638\neligible 646\nforward 891\nincident 519\nineligible 616 , 646\nparallel 518\nself-loop 518\nundirected 518\nweighted 608 , 638\nEdge-connected graph 562\nEdge relaxation 646\u2013647\nEdge-weighted DAG 658\u2013667\ncritical path method 663\u2013667\nlongest paths 661\nshortest paths 658\u2013660\nEdge-weighted digraph\nadjacency-lists 644\ncomplete 679\ndata type 641\ndiameter of 685\nshortest paths 638\u2013693\nEdge-weighted graph\nadjacency-lists 609\ndata type 608\nmin spanning forest 605\nmin spanning tree 604\u2013637\nEdmonds, J. 901\nEligible edge 616, 646\nEllipsoid algorithm 909\nEmpty string epsilon 789, 805\nEncapsulation 96\nEntropy ", "start": 950, "end": 950}, "1424": {"text": "spanning tree 604\u2013637\nEdmonds, J. 901\nEligible edge 616, 646\nEllipsoid algorithm 909\nEmpty string epsilon 789, 805\nEncapsulation 96\nEntropy 300\u2013301\nEpsilon-transition 795\nEqual keys. See Duplicate keys\nequals() method 102\u2013103\nsymbol table 365\nEquivalence class 216\nEquivalence relation\nconnectivity 216 , 543\nequals() method 102\nstrong connectivity 584\nErd\u00f6s number 554\nErd\u00f6s, P . 554\nErd\u00f6s-Renyi model 239\nError. See also Exception\nOutOfMemoryError 107\nStackOverflowError 57 , 107\nEuclid\u2019s algorithm 4, 58\nEulerian cycle 562, 598\nEvent-driven simulation 349, \n856\u2013865\nException. See also Error\nArithmetic 107\nArrayIndexOutOfBounds 107\nClassCast 387\nNoSuchElement 139\nNullPointer 159\n INDEX 939\nRuntime 107\nUnsupportedOperation 139\nConcurrentModification 160\nexch() method 245 , 315\nExhaustive search 912\nExponential inequality 185\nExponential running time 186, \n661, 911\nExtended Church-Turing the-\nsis 910\nExtensible library 101\nExternal path length 418, 832\nF\nFactor an integer 919\nFactorial function 185\nFail-fast design 107\nFail-fast iterator 160, 171\nFarthest pair 210\nFibonacci heap 628, 682\nFibonacci numbers 57\nFIFO. See First-in \ufb01rst-out policy\nFIFO queue.\nSee Queue data type\nFile system 493\nFilter 60\nblacklist 491\ndedup 490\nwhitelist 8 , 491\nFinal access ", "start": 950, "end": 951}, "1425": {"text": "First-in \ufb01rst-out policy\nFIFO queue.\nSee Queue data type\nFile system 493\nFilter 60\nblacklist 491\ndedup 490\nwhitelist 8 , 491\nFinal access modi\ufb01er 105\u2013106\nFingerprint search 774\u2013778\nFinite state automaton.\nSee Deterministic \ufb01nite \nstate automaton\nFirst-in-\ufb01rst-out policy 126\nFixed-capacity stack 132, \n134\u2013135\nFixed-length code 826\nFloat primitive data type 13\nFlood \ufb01ll 563\nFloor function\nbinary search tree 406\n \nmathematical function 185\nordered array 380\nsymbol table 367 , 383\nFlow 888 .\nSee also Max\ufb02ow problem\n\ufb02ow network 888\nin\ufb02ow and out\ufb02ow 888\nresidual network 895\nst-\ufb02ow 888\nst-\ufb02ow network 888\nvalue 888\nFloyd, R. W. 326\nFloyd\u2019s method 327\nfor loop 16\nFord-Fulkerson 891\u2013893\nanalysis of 900\nmaximum-capacity path 901\nshortest augmenting path 897\nFord, L. 683\nForeach loop 138\narrays 160\nstrings 160\nForest\ngraph 520\nspanning 520\nForest-of-trees 225\nFormatted output 37\nFortran language 217\nFragile base class problem 112\nFrazer, W. 306\nFredman, M. L. 628\nFunction-call stack 246, 415\nG\nGarbage collection 104, 195\nloitering 137\nmark-and-sweep 573\nGaussian elimination 919\nGenerics 122\u2013123 , 134\u2013135\nand covariant ", "start": 951, "end": 951}, "1426": {"text": "415\nG\nGarbage collection 104, 195\nloitering 137\nmark-and-sweep 573\nGaussian elimination 919\nGenerics 122\u2013123 , 134\u2013135\nand covariant arrays 158\nand type erasure 158\narray creation 134, 158\nparameterized type 122\npriority queues 309\nstacks and queues 134\u2013135\nsymbol tables 363\ntype parameter 122 , 134\nGenomics 492 , 498\nGeometric data types 76\u201377\nGeometric sum 185\ngetClass() method 101 , 103\nGirth of a graph 559\nGlobal variable 113\nGosper, R. W. 759\nGraph data type 522\u2013527\nGraph isomorphism 561, 919\nGraph processing 514\u2013693. \nSee also Directed graph; \nSee also Edge-weighted \ndigraph; See also Edge-\nweighted graph; See \nalso Undirected graph; \nSee also Directed acyclic \ngraph\nBellman-Ford 668\u2013681\nbreadth-\ufb01rst search 538\u2013541\ncomponents 543\u2013546\ncritical-path method 664\u2013666\ndepth-\ufb01rst search 530\u2013537\nDijkstra\u2019s algorithm 652\nKosaraju\u2019s algorithm 586\u2013590\nKruskal\u2019s algorithm 624\u2013627\nlongest paths 911\u2013912\nmax bipartite matching 906\nmin spanning tree 604\u2013637\nPrim\u2019s algorithm 616\u2013623\nreachability 570\u2013573\nshortest paths 638\u2013693\nstrong components 584\u2013591\nsymbol graphs 548\ntransitive closure 592\u2013593\nunion-\ufb01nd 216\u2013241\nGreatest common divisor 4\nGreedy algorithm\n INDEX940\nHuffman encoding 830\nminimum spanning tree 607\nGrep 804\nH\nHalting ", "start": 951, "end": 952}, "1427": {"text": "548\ntransitive closure 592\u2013593\nunion-\ufb01nd 216\u2013241\nGreatest common divisor 4\nGreedy algorithm\n INDEX940\nHuffman encoding 830\nminimum spanning tree 607\nGrep 804\nH\nHalting problem 910\nHamiltonian cycle 562, 920\nHamiltonian path 598, 913, 920\nHandle 112\nHard-disc model 856\nHarmonic number 23, 185\nHarmonic sum 185\nhashCode() method 101 , 102, \n461\u2013462\nHash function 458, 459\u2013463\nmodular 459\nperfect 480\nRabin-Karp algorithm 774\nHashing. See Hash function;\nSee also Hash table\nhash function 459\u2013463\ntime-space tradeoff 458\nHash table 458\u2013485\narray resizing 474\u2013475\nclustering 472\ncollision resolution 458\ncuckoo hashing 484\ndeletion 468\ndouble hashing 483\ndouble probing 483\nduplicate keys 488\nhashCode() method 461\u2013462\nhash function 458\nJava library 489\nlinear probing 469\u2013474\nload factor 471\nmemory usage of 476\nprimitive types 488\nseparate chaining 464\u2013468\nuniform hashing assump-\ntion 463\nHead vertex 566\nHeap. See Binary heap\nmultiway 319\nHeap order 313\nHeapsort 323\u2013327\nHeight\n2-3 search tree 429\nbinary search tree 412\ncomplete binary tree 314\nred-black BST 444\ntree 226\nHibbard deletion 422\nHibbard, T. 410\nHoare, C. A. R. 205\nHorner\u2019s method 460\nh-sorted array 258\nHuffman compression 350, \n826\u2013838\nanalysis of 833\noptimality ", "start": 952, "end": 952}, "1428": {"text": "T. 410\nHoare, C. A. R. 205\nHorner\u2019s method 460\nh-sorted array 258\nHuffman compression 350, \n826\u2013838\nanalysis of 833\noptimality of 833\nHuffman, D. 827\nI\nif statement 15\nif-else statement 15\nImmutability 105\u2013106\ndefensive copy 112\nof strings 114 , 202, 696\npriority queue keys 320\nsymbol table keys 365\nImplementation 28, 88\nImplementation inheritance 101\nimport statement 27 , 29, 66\nIncident edge 519\nIncrement sequence 258\nIn data type 41 , 83\nIndegree of a vertex 566\nIndex 361 , 496\u2013501\na string 877\n\ufb01les 500\u2013501\ninverted 498\u2013501\nIndex priority queue 320\u2013322\nDijkstra\u2019s algorithm 652\n \nPrim\u2019s algorithm 620\nIndirect sort 286\nIneligible edge\nminimum spanning tree 616\nshortest paths 646\nIn\ufb01x notation 13 , 128, 162\nInherited methods 66, 100\u2013101\ncompare() 338\u2013339\ncompareTo() 246\u2013247\nequals() 102\u2013103\ngetClass() 101\nhashCode() 101 , 461\u2013462\nhasNext() 138\niterator() 138\nnext() 138\ntoString() 66 , 101\nInner loop 180 , 184, 195\nInorder tree traversal 412\nIn-place merge 270\nInput and output 82\u201383\nbinary data 812\u2013815\nfrom a \ufb01le 41\npiping 40\nredirection 40\nInput model 197\nInput size 173\nInsertion sort 250\u2013252\nInstance method 65, ", "start": 952, "end": 952}, "1429": {"text": "data 812\u2013815\nfrom a \ufb01le 41\npiping 40\nredirection 40\nInput model 197\nInput size 173\nInsertion sort 250\u2013252\nInstance method 65, 84\nInstance variable 84\nint primitive data type 12\nInteger linear inequality satis\ufb01-\nability problem 913\nInteger linear programming 920\nInteger over\ufb02ow 51\nInterface 100\nComparable 246\u2013247\nComparator 338\u2013340\nIterable 138\nIterator 139\nInterface inheritance 100\nInterior point method 909\nInternal path length 412\n INDEX 941\nInternet DNS 493\nInternet Movie Database 497\nInterpreter 130\nInterval graph 564\nIntractability 910\u2013921\nInversion 252, 286\nInverted index 498\u2013501\nIsing model 920\nIsomorphic graph 561\nItem\ncontains a key 244\nsorting 244\nsymbol table 387\nwith multiple keys 339\nItem type parameter 134\nIteration 123 , 138\u2013141\nfail-fast 171\nforeach loop 123\nJ\nJacquet, P . 882\nJarnik\u2019s algorithm 628.\nSee also Prim\u2019s algorithm\nJarnik, V. 628\nJava programming\narray 18\u201321\narrays as objects 72\narrays of objects 72\nassertion 107\nassert statement 107\nassignment statement 14\nautoboxing 122\nautounboxing 122\nbase class 101\nbitwise operators 52\nblock statement 15\nboolean expression 13\nbreak statement 15\nbytecode 10\ncast 13\nclass 10 , 64\nclasspath 66\ncomparison operator 13\nconditional statement 15\nconstructor 65 , 84\ncontinue statement 15\ncovariant arrays 158\ncreate an object 67\ndeclaration ", "start": 952, "end": 953}, "1430": {"text": "13\nclass 10 , 64\nclasspath 66\ncomparison operator 13\nconditional statement 15\nconstructor 65 , 84\ncontinue statement 15\ncovariant arrays 158\ncreate an object 67\ndeclaration statement 14\ndefault initialization 18, 86\ndeprecated method 113\nderived class 101\nError 107\nException 107\nexpression 11 , 13\nfinal modi\ufb01er 84 , 105\u2013106\nfor loop 16\nforeach loop 123\ngarbage collection 104\ngeneric array creation 158\ngenerics 122\u2013123 , 134\u2013135\nidenti\ufb01er 11\nif statement 15\nif-else statement 15\nimplicit assignment 16\nimport statement 29\nimported system libraries 27\nin\ufb01x expression 13\ninheritance 100\u2013101\ninherited method 66\ninitializing declarations 16\ninner class 159\ninstance method 65, 84, 86\ninstance method signature 86\ninstance variable 84\ninvoke instance method 68\niterable collections 123\njust-in-time compiler 195\nliteral 11\nloitering 137\nloop statement 15\nmemory management 104\nmodular programming 26\nnested class 159\nnew() 67\nobjects 67\u201374\nobjects as arguments 71\nobjects as return values 71\noperator 11\noperator precedence 13\norphan 137\norphaned object 104\noverloading 12, 24\noverride a method 101\nparameterized type 122, 134\npass by reference 71\npass by value 24, 71\nprimitive data type 11\u201312\nprivate class 159\nprivate modi\ufb01er 84\nprotected modi\ufb01er 110\npublic modi\ufb01er 84 , 110\nragged ", "start": 953, "end": 953}, "1431": {"text": "24, 71\nprimitive data type 11\u201312\nprivate class 159\nprivate modi\ufb01er 84\nprotected modi\ufb01er 110\npublic modi\ufb01er 84 , 110\nragged array 19\nrecursion 25\nreference 67\nreference type 64\nreturn statement 86\nscope 14 , 87\nshort-circuit operator 52\nside effects 24\nsingle-statement blocks 16\nstandard libraries 27\nstandard system libraries 27\nstatement 14\nstatic method 22\u201325\nstatic variable 113\nstrong typing 14\nsubclass 101\nsuperclass 101\nthis reference 87\nthrow an error/exception 107\ntwo-dimensional array 19\ntype conversion 13, 35\ntype erasure 158\ntype parameter 122\nunit testing 26\n INDEX942\nusing objects 69\nvariable 11\nvisibility modi\ufb01er 84\nwhile loop 15\nwrapper type 122\nJava system sort 306\nJava virtual machine 51\njava.awt \nColor 75\nFont 75\njava.io \nFile 75\njava.lang \nArithmeticException 107\nArrayIndexOutOfBounds 107\nBoolean 102\nByte 102\nCharacter 102\nClassCastException 387\nComparable 100\nDouble 34 , 102\nFloat 102\nInteger 102\nIterable 100 , 123, 138, 154\nLong 102\nMath 28\nNullPointer 107 , 113, 159\nObject 101\nOutOfMemoryError 107\nRuntimeException 107\nShort 102\nStackOverflowError 57 , 107\nStringBuilder 27, 105 , 697\nUnsupportedOperation 139\njava.net \nURL 75\njava.util \nArrayList 160\nArrays 29\nComparator 100 , 339\nConcurrentModification 160\nDate ", "start": 953, "end": 954}, "1432": {"text": "107\nStringBuilder 27, 105 , 697\nUnsupportedOperation 139\njava.net \nURL 75\njava.util \nArrayList 160\nArrays 29\nComparator 100 , 339\nConcurrentModification 160\nDate 113\nHashMap 489\nIterator 100 , 138\u2013141, 154\nLinkedList 160\nNoSuchElementException 139\nPriorityQueue 352\nStack 159\nTreeMap 489\nJob-scheduling problem. \nSee Scheduling\nJosephus problem 168\nJust-in-time compiler 195\nK\nKarp, R. 901\nKarp, R. M. 759\nKendall tau distance 286, 345, \n356\nKevin Bacon number 553\u2013554\nKey 244\nKey equality\nordered symbol table 368\nsymbol table 365\nKey-indexed counting 703\u2013705\nKey type parameter\npriority queue 309\nsymbol table 361\nKeyword in context 879\nKhachian, L. G. 909\nKleene\u2019s theorem 794\nKnuth, D. E. 178, 205, 759\nKnuth-Morris-Pratt 762\u2013769\nKnuth shuf\ufb02e 32\nKosaraju\u2019s algorithm 586\u2013590\nKruskal, J. 628\nKruskal\u2019s algorithm 624\u2013627\nKWIC. See Keyword-in-context\nL\nLast-in-\ufb01rst-out policy 127\nLas V egas algorithm 778\nLeading-term approximation. \nSee Tilde notation\nLeast-signi\ufb01cant digit.\nSee LSD string sort\nLeipzig Corpora Collection 371\nLempel, A. 839\nless() method 245 , 315\nLevel-order traversal\nbinary heap 313\nbinary search tree 420\nLevin, L. 918\nLIFO. See Last-in \ufb01rst-out ", "start": 954, "end": 954}, "1433": {"text": "371\nLempel, A. 839\nless() method 245 , 315\nLevel-order traversal\nbinary heap 313\nbinary search tree 420\nLevin, L. 918\nLIFO. See Last-in \ufb01rst-out policy\nLIFO stack. See Stack data type\nLinear equation satis\ufb01ability 913\nLinear inequality satis\ufb01abil-\nity 913\nLinear probing 469\u2013474\nLinear programming 907\u2013909\nellipsoid algorithm 909\ninterior point method 909\nreductions 907\u2013909\nsimplex algorithm 909\nLinear running time 186\nLinearithmic running time 186\nLinked allocation 156\nLinked list 142\u2013146\nbuilding 143\ncircular 165\nde\ufb01ned 142\ndeletion 145\ndeletion from beginning 145\ngarbage collection 145\ninsertion 145\ninsertion at beginning 144\ninsertion at end 145\niterator 154\u2013155\nmemory usage of 201\nNode data type 142\nqueue 150\nreverse a 165\nsequential search 374\nshuf\ufb02e a 286\nsort a 286\n INDEX 943\nstack 147\u2013149\ntraversal 146\nLiteral\nnull 112\u2013113\nprimitive type 11\nstring 80\nLoad-balancing 349, 909\nLoad factor 471\nLocal minimum 210\nLogarithm function\nbinary 185\ninteger binary 185\nnatural 185\nLogarithmic running time 186\nLog-log plot 176\nLoitering 137\nLongest common pre\ufb01x 875\nLongest paths 661, 911\nLongest pre\ufb01x match 842\nLongest-processing-time \ufb01rst \nrule 349\nLongest repeated substring  875\nlong primitive data type 13\nLoop\nfor ", "start": 954, "end": 955}, "1434": {"text": "875\nLongest paths 661, 911\nLongest pre\ufb01x match 842\nLongest-processing-time \ufb01rst \nrule 349\nLongest repeated substring  875\nlong primitive data type 13\nLoop\nfor 16\nforeach 138\ninner 180\nwhile 15\nLossless data compression 811\nLossy data compression 811\nLower bound\npriority queue 332\nsorting 279\u2013282\n3-sum problem 190\nunion-\ufb01nd 231\nLowest common ancestor 598\nLoyd, S. 358\nLSD string sort 706\u2013709\nLZW algorithm 839\u2013845\ncompression 840\nexpansion 841\ntrie representation 840\nM\nManber, U. 884\nMark-and-sweep garbage collec-\ntion 573\nMaslow, A. 904\nMaslow\u2019s hammer 904\nMatrix data type 60\nMax\ufb02ow-mincut theorem 894\nMax\ufb02ow problem 886\u2013902.\nSee also Mincut problem\nFord-Fulkerson 891\u2013893\nintegrality property 894\nmax\ufb02ow-mincut theo-\nrem 892\u2013894\nmax bipartite matching 906\npre\ufb02ow-push algorithm 902\nreductions 905\u2013907\nresidual network 895\u2013897\nMaximum\nin array 30\nin binary heap 313\nin binary search tree 406\nin ordered symbol table 367\nMaximum st-\ufb02ow problem. \nSee Max\ufb02ow problem\nMax bipartite matching 906\nMaze 530\nMcIlroy, D. 298, 306\nMcKellar, A. 306\nMedian 332 , 345\u2013347\nMedian-of-3 partitioning 305\nMemory management 104\nlinked allocation 156\nloitering ", "start": 955, "end": 955}, "1435": {"text": "530\nMcIlroy, D. 298, 306\nMcKellar, A. 306\nMedian 332 , 345\u2013347\nMedian-of-3 partitioning 305\nMemory management 104\nlinked allocation 156\nloitering 137\norphan 137\nSequential allocation 156\nMemory usage 200\u2013204\narray 202\nhash table 476\nlinked list 201\nnested class 201\nobject 67 , 201\nprimitive types 200\nR-way trie 744\nstack 213\nstring 202\nsubstring 202\u2013204\nMergesort 270\u2013288\nabstract in-place merge 270\nanalysis of 272\nbottom-up 277\nlinked list 279, 286\nmultiway 287\nnatural 285\noptimality 282\nstability 341\ntop-down 272\nMerging 270\u2013271\nMethod\ninherited 100\u2013101\ninstance 68\u201369 , 86\u201387\nstatic 22\u201325\nMincut problem 893. See \nalso Max\ufb02ow problem\nMinimum\nin array 30\nin binary search tree 406\nin ordered symbol table 367\nMin spanning forest 605\nMin spanning tree 604\u2013637\nBoruvka\u2019s algorithm 636\nbottleneck shortest paths 690\ncritical edge 633\ncrossing edge 606\ncut 606\ncut optimality conditions 634\ncut property 606\nde\ufb01ned 604\ngreedy algorithm 607\nKruskal\u2019s algorithm 624\u2013627\nPrim\u2019s algorithm 616\u2013623\nreverse-delete algorithm 633\nVyssotsky\u2019s algorithm 633\nMinimum st-cut problem. \n INDEX944\nSee Mincut problem\nMinotaur 530\nMismatched character rule 770\nM. L. Fredman 628\nModular hash function ", "start": 955, "end": 956}, "1436": {"text": "633\nVyssotsky\u2019s algorithm 633\nMinimum st-cut problem. \n INDEX944\nSee Mincut problem\nMinotaur 530\nMismatched character rule 770\nM. L. Fredman 628\nModular hash function 459, 774\nModular programming 26\nMonte Carlo algorithm 776\nMoore, J. S. 759\nMoore\u2019s law 194\u2013195\nMorris, J. H. 759\nMost-signi\ufb01cant-digit sort. \nSee MSD string sort\nMove-to-front 169\nMSD string sort 710\u2013718\nMultidimensional sort 356\nMultigraph 518\nMultiple-source reachability \nproblem 570 , 797\nMultiset 509\nMultiway mergesort 287\nMultiway trie. See R-way trie\nMyers, E. 884\nN\nNatural logarithm function 185\nNatural mergesort 285\nNatural order 337\nNegative cost cycle.\nSee Negative cycle\nNegative cycle 668\u2013670, \n677\u2013681\nNested class 159\nNetwork \ufb02ow.\nSee Max\ufb02ow problem\nnew() 67\nNewton\u2019s method 23\nNFA. See Nondeterministic \n\ufb01nite-state automata\nNode data type 159\nbag 155\nbinary search tree 398\nHuffman trie 828\nlinked list 142\nqueue 151\nred-black BST 433\nR-way trie 734\nstack 149\nternary search trie 747\nNondeterminism 794\nTuring machine 914\nNondeterministic \ufb01nite-state \nautomata 794\u2013799\nNP 912\nNP-complete 917\u2013918\nNull link 396\nnull literal 112\u2013113\nO\nObject 67\u201374 . See also Object-\noriented programming\nbehavior 67 , 73\nidentity 67 ", "start": 956, "end": 956}, "1437": {"text": "912\nNP-complete 917\u2013918\nNull link 396\nnull literal 112\u2013113\nO\nObject 67\u201374 . See also Object-\noriented programming\nbehavior 67 , 73\nidentity 67 , 73\nmemory usage of 201\nstate 67 , 73\nObject-oriented program-\nming 64\u2013119\narrays of objects 72\ncreating an object 67\ndeclaring an object 67\nencapsulation 96\ninheritance 100\ninstance 73\ninstantiate an object 67\ninvoke instance method 68\nobjects 67\u201374\nobjects as arguments 71\nobjects as return values 71\nreference 67\nsubtyping 100\nusing objects 69\nOdd-length cycle in a graph 562\nOOP. See Object-oriented pro-\ngramming\nOperations research 349\n \nOptimization problem 913\nOrdered symbol table 366\u2013369\n\ufb02oor and ceiling 367\nminimum and maximum 367\nordered array 378\nrange query 368\nrank and selection 367\nred-black BST 446\nOrder of growth 179\nOrder-of-growth classi\ufb01ca-\ntions 186\u2013188\nOrder-of-growth hypothesis 180\nOrder statistic 345\nbinary search tree 406\nordered symbol table 367\nquickselect 345\u2013347\nOrphaned object 104, 137\nOut data type 41 , 83\nOutdegree of a vertex 566\nOutput. See Input and output\nOver\ufb02ow 51\nOverloading\nconstructor 84\nstatic method 24\nOverriding a method 66, 101\nP\n \nP complexity class 914\nP= NP question 916\nPage data type 870\nPalindrome 81, 783\nParallel arrays\nlinear probing 471\nordered symbol table 378\nsorting 357\nParallel edge 518, 566, ", "start": 956, "end": 956}, "1438": {"text": "914\nP= NP question 916\nPage data type 870\nPalindrome 81, 783\nParallel arrays\nlinear probing 471\nordered symbol table 378\nsorting 357\nParallel edge 518, 566, 612, 640\nParallel job scheduling 663\u2013667\nParallel precedence-constrained \nscheduling 663 , 904\nParameterized type. See Generics\nParent-link representation\nbreadth-\ufb01rst search tree 539\n INDEX 945\n depth-\ufb01rst search tree 535\nminimum spanning tree 620\nshortest-paths tree 640\nunion-\ufb01nd 225\nParsing\nan arithmetic expression 128\na regular expression 800\u2013804\nParticle data type 860\nPartitioning algorithm 290\n2-way 288\n3-way (Bentley-McIlroy) 306\n3-way (Dijkstra) 298\nmedian-of-3 296 , 305\nmedian-of-5 305\nselection 346\u2013347\nPartitioning item 290\nPass by reference 71\nPass by value 24, 71\nPath.  See Longest paths; \nSee also Shortest paths\naugmenting 891\nHamiltonian 913, 920\nin a digraph 567\nin a graph 519\nlength of 519 , 567\nsimple 519 , 567\nPath compression 231\nPattern matching.\nSee Regular expression\nPerfect hash function 480\nPerformance. See Propositions\nPermutation\nKendall-tau distance 356\nrandom 168\nranking 345\nsorting 354\nPhone book 492\nPicture data type 814\nPiping 40\nPoint data type 77\nPointer 111. See also Reference\nsafe 112\nPointer sort 338\nPoisson approximation 466\nPoisson distribution 466\nPolar angle 356\nPolar ", "start": 956, "end": 957}, "1439": {"text": "40\nPoint data type 77\nPointer 111. See also Reference\nsafe 112\nPointer sort 338\nPoisson approximation 466\nPoisson distribution 466\nPolar angle 356\nPolar coordinate 77\nPolar sort 356\nPoly-time reduction 916\nPop operation 127\nPost\ufb01x notation 162\nPostorder traversal\nof a digraph 578\nreverse 578\nPower law 178\nPratt, V. R. 759\nPrecedence-constrainted sched-\nuling 574\u2013575\nPrecedence order\narithmetic expressions 13\nregular expressions 789\nPre\ufb01x-free code 826\u2013827\ncompression 829\nexpansion 828\nHuffman 833\noptimal 833\nreading and writing 834\u2013835\ntrie representation 827\nPreorder traversal\nof a digraph 578\nof a trie 834\nPrime number 23, 774, 785\nPrimitive data type 11\u201312\nmemory usage of 200\nreason for 51\nwrapper type 102\nPrimitive type\nversus reference type 110\nPrim, R. 628\nPrim\u2019s algorithm 350, 616\u2013623\neager 620\u2013623\nlazy 616\u2013619\nPriority queue 308\u2013335\nbinary heap 313\u2013322\nchange priority 321\ndelete 321\nDijkstra\u2019s algorithm 652\nFibonacci heap 628\nHuffman compression 830\nindex priority queue 320\u2013321\nlinked-list 312\nmultiway heap 319\nordered array 312\nPrim's algorithm 616\nreductions 345\nremove the minimum 321\nsoft heap 629\nstability 356\nunordered array 310\nprivate access modi\ufb01er 84\nProbabilistic algorithm. See Ran-\ndomized algorithm\nProbe 471\nProblem ", "start": 957, "end": 957}, "1440": {"text": "minimum 321\nsoft heap 629\nstability 356\nunordered array 310\nprivate access modi\ufb01er 84\nProbabilistic algorithm. See Ran-\ndomized algorithm\nProbe 471\nProblem size 173\nPrograms\nAccumulator 93\nAcyclicLP 661\nAcyclicSP 660\nArbitrage 680\nAverage 39\nBag 155\nBellmanFordSP 674\nBinaryDump 814\nBinarySearch 47\nBinarySearchST 379 , 381, 382\nBlackFilter 491\nBoyerMoore 772\nBreadthFirstPaths 540\nBST 398 , 399, 407, 409, 411\nBTreeSET 872\nCat 82\nCC 544\nCollisionSystem 863\u2013864\nCount 699\nCounter 89\nCPM 665\n INDEX946\nCycle 547\nDate 91 , 103, 247\nDeDup 490\nDegreesOfSeparation 555\nDepthFirstOrder 580\nDepthFirstPaths 536\nDepthFirstSearch 531\nDigraph 569\nDijkstraAllPairsSP 656\nDijkstraSP 655\nDirectedCycle 577\nDirectedDFS 571\nDirectedEdge 642\nDoublingTest 177\nEdge 610\nEdgeWeightedDigraph 643\nEdgeWeightedGraph 611\nEvaluate 129\nEvent 861\nExample 245\nFileIndex 501\nFixedCapacityStack 135\nFixedCapacityStackOf-\nStrings 133\nFlips 70\nFlipsMax 71\nFlowEdge 896\nFordFulkerson 898\nFrequencyCounter 372\nGenome 819\u2013820\nGraph 526\nGREP 804\nHeap 324\nHexDump 814\nHuffman 836\nInsertion 251\nKMP 768\nKosarajuSCC 587\nKruskalMST ", "start": 957, "end": 958}, "1441": {"text": "819\u2013820\nGraph 526\nGREP 804\nHeap 324\nHexDump 814\nHuffman 836\nInsertion 251\nKMP 768\nKosarajuSCC 587\nKruskalMST 627\nKWIC 881\nLazyPrimMST 619\nLinearProbingHashST 470\nLookupCSV 495\nLookupIndex 499\nLRS 880\nLSD 707\nLZW 842, 844\nMaxPQ 318\nMerge 271 , 273\nMergeBU 278\nMSD 712\nMultiway 322\nNFA 799 , 802\nPictureDump 814\nPrimMST 622\nQueue 151\nQuick 289 , 291\nQuick3string 720\nQuick3way 299\nRabinKarp 777\nRedBlackBST 439\nResizingArrayQueue 140\nResizingArrayStack 141\nReverse 127\nRLE 824\nRolls 72\nSelection 249\nSeparateChainingHashST 465\nSequentialSearchST 375\nSET 489\nShell 259\nSortCompare 256\nSparseVector 503\nStack 149\nStaticSETofInts 99\nStats 125\nStopwatch 175\nSuffixArray 883\nSymbolGraph 552\nThreeSum 173\nThreeSumFast 190\nTopM 311\nTopological 581\nTransaction 340\nTransitiveClosure 593\nTrieST 737\u2013741\nTST 747\nTwoColor 547\nTwoSumFast 189\nUF 221\nVisualAccumulator 95\nWeightedQuickUnionUF 228\nWhiteFilter 491\nWhitelist 99\nProperties 180\n3-sum 180\nBoyer-Moore algorithm 773\ninsertion sort 255\nquicksort 343\nRabin-Karp ", "start": 958, "end": 958}, "1442": {"text": "228\nWhiteFilter 491\nWhitelist 99\nProperties 180\n3-sum 180\nBoyer-Moore algorithm 773\ninsertion sort 255\nquicksort 343\nRabin-Karp algorithm 778\nred-black BST 445\nselection sort 255\nseparate-chaining 467\nshellsort 262\nversus proposition 183\nPropositions 182\n2-3 search tree 429\n3-sum 182\n3-way quicksort 301\n3-way string quicksort 723\narbitrage 681\nB-tree 871\nBellman-Ford 671, 673\nbinary heap 319\nbinary search 383\nBST 403\u2013404 , 412\nbreadth-\ufb01rst search 541\nbrute substring search 761\ncomplete binary tree 314\nconnected components 546\nCook-Levin theorem 918\ncritical path method 666\ncut property 606\nDFS 531 , 537, 570\nDijkstra\u2019s algorithm 652, 654\n\ufb02ow conservation 893\nFord-Fulkerson 900\u2013901\ngeneric shortest-paths 651\n INDEX 947\ngreedy MST algorithm 607\nheapsort 323 , 326\nHuffman algorithm 833\nindex priority queue 321\ninsertion sort 250 , 252\ninteger programming 917\nkey-indexed counting 705\nKnuth-Morris-Pratt 769\nKosaraju\u2019s algorithm 588, 590\nKruskal\u2019s algorithm 624, 625\nlinear-probing hash table 475\nlinear programming 908\nlongest paths in DAG 661\nlongest repeated substring 885\nLSD string sort 706, 709\nmax\ufb02ow-mincut theorem 894\nmax\ufb02ow reductions 906\nmergesort 272 , 279, 282\nMSD ", "start": 958, "end": 959}, "1443": {"text": "repeated substring 885\nLSD string sort 706, 709\nmax\ufb02ow-mincut theorem 894\nmax\ufb02ow reductions 906\nmergesort 272 , 279, 282\nMSD string sort 717, 718\nnegative cycles 669\nparallel job scheduling with \nrelative deadlines 667\nparticle collision 865\nPrim\u2019s algorithm 616, 618, 623\nquick-\ufb01nd algorithm 223\nquickselect 347\nquicksort 293\u2013295\nquick-union algorithm 226\nred-black BST 444, 447\nregular expression 799, 804\nresizing-array stack 199\nR-way trie 742, 743, 744\nselection sort 248\nseparate-chaining 466, 475\nsequential search 376\nshortest paths in DAG 658\nshortest-paths optimality 650\nshortest paths reductions 905\nsorting lower bound 280, 300\nsorting reductions 903\nsuf\ufb01x array 882\nternary search trie 749, 751\ntopological order 578, 582\nuniversal compression 816\nweighted quick-union 229\nprotected modi\ufb01er 110\nProtein folding 920\npublic access modi\ufb01er 110\nPushdown stack 127.\nSee also Stack data type\nPush operation 127\nQ\nQuadratic running time 186\nQuantum computer 911\nQueue data type\nanalysis of 198\nAPI 126\ncircular linked list 165\nlinked-list 150\u2013151\nresizing-array 140\nQuick-\ufb01nd algorithm 222\u2013223\nQuickselect 345\u2013347\nQuicksort 288\u2013307\n2-way partitioning 290\n3-way partitioning 298\u2013301\n3-way string 719\nanalysis of 293\u2013295\nand binary search ", "start": 959, "end": 959}, "1444": {"text": "222\u2013223\nQuickselect 345\u2013347\nQuicksort 288\u2013307\n2-way partitioning 290\n3-way partitioning 298\u2013301\n3-way string 719\nanalysis of 293\u2013295\nand binary search trees 403\nduplicate keys 292\nfunction-call stack size 304\nmedian-of-2 296, 305\nmedian-of-5 305\nnonrecursive 306\nrandom shuf\ufb02e 292\nQuick-union 224\u2013227\npath compression 231\nweighted 227\u2013230\nR\nRabin-Karp algorithm 774\u2013778\nRabin, M. O. 759\nRadius of a graph 559\nRadix 700\nRadix sorting. See String sorting\nRandom bag data type 167\nRandomized algorithm 198\nLas V egas 778\nMonte Carlo 776\nquickselect 345\u2013347\nquicksort 290 , 307\nRabin-Karp algorithm 776\n3-way string quicksort 722\nRandom number 30\u201332\nRandom queue data type 168\nRandom string model 716\u2013717\nRange query\nbinary search tree 412\nordered symbol table 368\nRank\nbinary search 25, 378\u2013381\nbinary search tree 408, 415\nordered symbol table 367\nsuf\ufb01x array 879\nReachability 570\u2013572, 590\nReachable vertex 567\nRecurrence relation\nbinary search 383\nmergesort 272\nquicksort 293\nRecursion 25 .\nSee also Base case;\nSee also Recursion\nbinary search 25, 380\nbinary search tree 401\ndepth-\ufb01rst search 531\nEuclid\u2019s algorithm 4\nFibonacci numbers 57\nmergesort 272\nquicksort 289\nRed-black BST 432\u2013447\nand ", "start": 959, "end": 959}, "1445": {"text": "tree 401\ndepth-\ufb01rst search 531\nEuclid\u2019s algorithm 4\nFibonacci numbers 57\nmergesort 272\nquicksort 289\nRed-black BST 432\u2013447\nand 2-3 search tree 432\nanalysis of 444\u2013447\ncolor \ufb02ip 436\ncolor representation 433\n INDEX948\nde\ufb01ned 432\ndelete the maximum 454\ndelete the minimum 453\ndeletion 441\u2013443 , 455\nimplementation 439\ninsertion 437\u2013439\nleft-leaning 432\nperfect black balance 432\nrotation 433\u2013434\nsearch 432\nRedirection 40\nReduction 903\u2013909\nde\ufb01ned 903\npolynomial-time 916\nlinear programming 907\u2013909\nmax\ufb02ow 905\u2013907\npriority queue 345\nshortest-paths 904\u2013905\nsorting 344\u2013347 , 903\u2013904\nReference 67\nReference type 64\nRe\ufb02exive relation 102, 216, 247, \n584\nRegular expression 82, 788\nbuilding an NFA 800\u2013804\nclosure operation 789\nconcatenation operation 789\nde\ufb01ned 790\nepsilon-transition 795\nmatch transition 795\nnondeterministic \ufb01nite-state \nautomaton 794\u2013799\nor operation 789\nparentheses 789\n\\\\s+ 82\nshortcuts 791\nsimulating an NFA 797\u2013799\nRehashing 474\nRelation\nantisymmetric 247\nequivalence 102, 216, 584\nre\ufb02exive 102 , 216, 247, 584\nsymmetric 102 , 216, 584\ntotal order 247\ntransitive 102 , 216, 247, 584\nResidual ", "start": 959, "end": 960}, "1446": {"text": "584\nre\ufb02exive 102 , 216, 247, 584\nsymmetric 102 , 216, 584\ntotal order 247\ntransitive 102 , 216, 247, 584\nResidual network 895\u2013897\nResizing array 136\u2013137\nbinary heap 320\nhash table 474\u2013475\nqueue 140\nstack 136\nReturn value 22\nReverese postorder traversal 578\nReverse\na linked list 165\u2013166\nan array 21\narray iterator 139\nwith a stack 127\nReverse-delete algorithm 633\nReverse graph 586\nReverse Polish notation. \nSee Post\ufb01x notation\nReverse postorder 578\nRing buffer data type 169\nRLE. See Run-length encoding\nRobson, J. 412\nRooted tree 640\nRotation in a BST 433\u2013434, 452\nRun-length encoding 822\u2013825\nRunning time 172\u2013173\nanalysis of 176\nconstant 186\ncubic 186\ndoubling ratio 192\nexponential 186\ninner loop 180\nlinear 186\nlogarithmic 186\nmeasuring 174\norder of growth 179\nquadratic 186\ntilde approximation 178\u2013179\nRun-time error.See Error; See \nalso Exception\nR-way trie 730\u2013744\nAlphabet 741\nanalysis of 742\u2013743\ncollecting keys 738\ndeletion 740\ninsertion 734\nlongest pre\ufb01x 739\nmemory usage of 744\none-way branching 744\u2013745,\nrepresentation 734\nsearch 732\u2013733\nwildcard match 739\nS\nSafe pointer 112\nSample mean 30\nSamplesort 306\nSample standard deviation 30\nSample variance 30\nScheduling\ncritical-path method 664\u2013666\nload-balancing ", "start": 960, "end": 960}, "1447": {"text": "732\u2013733\nwildcard match 739\nS\nSafe pointer 112\nSample mean 30\nSamplesort 306\nSample standard deviation 30\nSample variance 30\nScheduling\ncritical-path method 664\u2013666\nload-balancing problem 349\nLPT \ufb01rst 349\nparallel precedence-con-\nstrained 663\u2013667\nprecedence constraint 574\u2013\n575\nrelative deadlines 666\nSPT \ufb01rst 349\nScienti\ufb01c method 172\nScope of a variable 14, 87\nSearch hit 376\nSearching 360\u2013513. See \nalso Symbol table\nSearch miss 376\nSearch problem 912\nSedgewick, R. 298\nSelection 345\nbinary search tree 406\nordered symbol table 367\nquickselect 346\u2013347\nsuf\ufb01x array 879\n INDEX 949\nSelection client 249\nSelection sort 248\u2013249\nSelf-loop 518 , 566, 612, 640\nSeparate-chaining 464\u2013468\nSequential allocation 156\nSequential search 374\u2013377\nSet data type 489\u2013491\nShannon entropy 300\u2013301\nShellsort 258\u2013262\nShortest ancestral path 598\nShortest augmenting path 897\nShortest path 638\nShortest paths problem 638\u2013693\nall-pairs 656\narbitrage detection 679\u2013681\nBellman-Ford 668\u2013678\nbitonic 689\nbottleneck 690\ncerti\ufb01cation 651\ncritical edge 690\nDijkstra\u2019s algorithm 652\u2013657\nedge relaxation 646\u2013647\nedge-weighted DAG 658\u2013667\ngeneric algorithm 651\nineligible edge 646\nin Euclidean graphs 656\nmonotonic 689\nnegative cycle 669\nNegative cycle detection 670\nnegative ", "start": 960, "end": 961}, "1448": {"text": "646\u2013647\nedge-weighted DAG 658\u2013667\ngeneric algorithm 651\nineligible edge 646\nin Euclidean graphs 656\nmonotonic 689\nnegative cycle 669\nNegative cycle detection 670\nnegative weights 668\u2013681\noptimality conditions 650\nparent-link 640\nreduction 904\u2013905\nshortest-paths tree 640\nsingle-source 639, 654\nsource-sink 656\nundirected graph 654\nvertex relaxation 648\nShortest-processing-time-\ufb01rst \nrule 349 , 355\nshort primitive data type 13\nShuf\ufb02ing\na linked list 286\nan array 32\nquicksort 292\nSide effect 22 , 108\nSignature\ninstance method 86\nstatic method 22\nSimple digraph 567\nSimple graph 518\nSimplex algorithm 909\nSingle-source problems\nconnectivity 556\ndirected paths 573\nlongest paths in DAG 661\npaths 534\nreachability 570\nshortest directed paths 573\nshortest paths in undirected \ngraphs 654 , 904\nshortest paths 538, 639\nSocial network 517\nSoft heap 629\nSoftware cache 391, 451, 462\nSollin, M. 628\nSorting 242\u2013359 .\nSee also String sorting\n3-way quicksort 298\u2013301\nbinary search tree 412\ncerti\ufb01cation 246 , 265\nComparable 246\u2013247\ncompare-based 279\ncomplexity of 279\u2013282\ncost model 246\nentropy-optimal 296\u2013301\nextra memory 246\nheapsort 323\u2013327\nindirect 286\nin-place 246\ninsertion sort 250\u2013252\ninversion 252\nlower bound 279\u2013282, 306\nmergesort ", "start": 961, "end": 961}, "1449": {"text": "memory 246\nheapsort 323\u2013327\nindirect 286\nin-place 246\ninsertion sort 250\u2013252\ninversion 252\nlower bound 279\u2013282, 306\nmergesort 270\u2013288\npartially-sorted array 252\npointer 338\nprimitive types 343\nquicksort 288\u2013307\nreduction 903\u2013904\nreductions 344\u2013347\nselection sort 248\u2013250\nshellsort 258\u2013262\nstability 341\nsuf\ufb01x array 875\u2013885\nsystem sort 343\nSource-sink shortest paths 656\nSpanning forest 520\nSpanning tree 520, 604\nSparse graph 520\nSparse matrix 510\nSparse vector 502\u2013505\nSpeci\ufb01cation problem 97\nSPT. See Shortest paths tree;\nSee also Shortest-process-\ning-time-\ufb01rst rule\nst-cut 892\nst-\ufb02ow 888\nst-\ufb02ow network 888\nStability 341 , 355\ninsertion sort 341\nkey-indexed counting 705\nLSD string sort 706\nmergesort 341\npriority queue 356\nStack data type 127\nanalysis of 198 , 199\narray implementation 132\n\ufb01xed-capacity 132\u2013133\ngeneric 134\niteration 138\u2013140\nlinked-list 147\u2013149\nresizing array 136\nStandard deviation 30\nStandard drawing 36, 42\u201345\nStandard input 36, 39\nStandard libraries 30\n INDEX950\nDraw 82\u201383\nIn 41 , 82\u201383\nOut 41 , 82\u201383\nStdDraw 43\nStdIn 39\nStdOut 37\nStdRandom 30\nStdStats 30\nStopwatch 174\u2013175\nStandard ", "start": 961, "end": 962}, "1450": {"text": "41 , 82\u201383\nOut 41 , 82\u201383\nStdDraw 43\nStdIn 39\nStdOut 37\nStdRandom 30\nStdStats 30\nStopwatch 174\u2013175\nStandard output 36, 37\u201338\nStatic method 22\u201325\nargument 22\nde\ufb01ning a 22\ninvoking a 22\noverloaded 24\npass by value 24\nrecursive 25\nreturn statement 24\nreturn value 22\nside effect 22 , 24\nsignature 22\nStatic variable 113\nStatistics\nchi-square 483\nmedian 345\nminimum and maximum 30\norder 345\nsample mean 30 , 125\nsample standard deviation 30\nsample variance 30, 125\nStdDraw library 43\nStdIn library 39\nStdOut library 37\nStdRandom library 30\nStdStats library 30\nSteque data type 167, 212\nStirling's approximation 185\nStopwatch data type 174\u2013175\nString data type 34 , 80\u201381\nAPI 80\ncharacters 696\ncharAt() method 696\nconcatenation 34, 697\nconversion 102\nimmutability 696\nindexing 696\nindexOf() method 779\nlength 696\nlength() method 696\nliteral 34\nmemory usage of 202\n+ operator 80 , 697\nsubstring extraction 696\nsubstring() method 696\nString processing 80\u201381, \n694\u2013851\ndata compression 810\u2013851\nregular expression 788\nsorting 702\u2013729\nsubstring search 758\u2013785\nsuf\ufb01x array 875\u2013885\ntries 730\u2013757\nString search. See Substring \nsearch; See also Trie\nString sorting 702\u2013729\n3-way quicksort 719\u2013723\nkey-indexed ", "start": 962, "end": 962}, "1451": {"text": "758\u2013785\nsuf\ufb01x array 875\u2013885\ntries 730\u2013757\nString search. See Substring \nsearch; See also Trie\nString sorting 702\u2013729\n3-way quicksort 719\u2013723\nkey-indexed counting 703\nLSD string sort 706\u2013709\nMSD string sort 710\u2013718\nStrong component 584\nStrong connectivity 584\u2013591\nStrongly connected component. \nSee Strong component\nStrongly connected relation 584\nStrongly typed language 14\nSubclass 101\nSubgraph 519\nSublinear running time 716, 779\nSubstring extraction\nmemory usage of 202\u2013204\nsubstring() method 696\nSubstring search 758\u2013785\nBoyer-Moore 770\u2013773\nbrute-force 760\u2013761\n \n \nindexOf() method 779\nKnuth-Morris-Pratt 762\u2013769\nRabin-Karp 774\u2013778\nSubtyping 100\nSuf\ufb01x array 875\u2013885\nSuf\ufb01x array data type 879\nSuf\ufb01x-free code 847\nSuperclass 101\nSymbol digraph 581\nSymbol graph 548\u2013555\nSymbol table 360\u2013513\n2-3 search tree 424\u2013431\nAPI 363 , 366\nassociative array 363\nbalanced search tree 424\u2013457\nbinary search 378\u2013384\nbinary search tree 396\u2013423\nB-tree 866\u2013874\ncost model 369\nde\ufb01ned 362\nduplicate key policy 363\n\ufb02oor and ceiling 367\nhash table 458\u2013485\ninsertion 362\nkey equality 365\nlazy deletion 364\nlinear-probing 469\u2013474\nminimum and maximum 367\nnull value 364\nordered 366\u2013369\nordered array 378\nrange query 368\nrank ", "start": 962, "end": 962}, "1452": {"text": "362\nkey equality 365\nlazy deletion 364\nlinear-probing 469\u2013474\nminimum and maximum 367\nnull value 364\nordered 366\u2013369\nordered array 378\nrange query 368\nrank and selection 367\nred-black BST 432\u2013447\nR-way trie 732\u2013745\nsearch 362\nseparate-chaining 464\u2013468\nsequential search 374\nstring keys 730\u2013757\nternary search trie 746\u2013751\ntrie 730\u2013757\nunordered linked list 374\nSymmetric order 396\n INDEX 951\nSymmetric relation 102, 216, \n584\nSzpankowski, W. 882\nT\nTail ver tex 566\nTale of  Two Cities 371\nTandem repeat 784\nTarjan, R. E. 590, 628\nTe r m i n a l  w i n d ow  1 0, 36\nTe r n a r y  s e a rc h  t r i e  7 4 6 \u2013 7 5 1\nalphabet 750\nanalysis of 749\ncollecting keys 750\ndeletion 750\ninsertion 746\none-way branching 751, 755\npre\ufb01x match 750\nsearch 746\nwildcard match 750\nTheseus 530\nthis reference 87\nThreading 420\nTilde notation 178, 206\nTime-driven simulation 856\nTiming a program 174\u2013175\nTo p - d ow n  2 - 3 - 4  t re e  4 4 1\nTo p - d ow n  m e r g e s o r t  2 7 2\nTo p o l o g i c a l  s o r t  5 7 4 ", "start": 962, "end": 963}, "1453": {"text": "1\nTo p - d ow n  m e r g e s o r t  2 7 2\nTo p o l o g i c a l  s o r t  5 7 4 \u2013 5 8 3\ndepth-\ufb01rst search 578\nqueue-based algorithm 599\ntoString() method 66 , 102\nTo t a l  o rd e r  2 4 7\nTransaction data t y pe 78\u201379\ncompare() 340\ncompareTo() 266 , 337\nhashCode() 462\nTr ansitive closure 592\nTr ansitive relation 102, 216, \n247, 584\nTr anspose a matr ix 56\nTree. \n2-3 search tree. See 2-3 search \ntree\nbinary. See Binary tree\nbinary search tree. See Binary \nsearch tree\nbalanced search tree. See Bal-\nanced search tree\nbinomial 237\ndepth of a node 226\nheight of 226\ninorder traversal 412\nmin spanning tree. See Mini-\nmum spanning tree\nparent-link 535, 539\npreorder traversal 834\nrooted 640\nsize 226\nspanning tree. See Spanning \ntree\nundirected graph 520\nunion-\ufb01nd 224\u2013226\nTremaux explor ation 530\nTr iangular sum 185\nTr ie 730\u2013757. See also R-way \ntrie; See also T ernary \nsearch trie\ncollecting keys 731\nLempel-Ziv-Welch 840\nlongest pre\ufb01x match 731, 842\none-way branching 744\u2013745, \n751, 755\npre\ufb01x-free code 827\npreorder traversal 834\nreading and writing 834\u2013835\nwildcard match 731\nTufte ", "start": 963, "end": 963}, "1454": {"text": "731, 842\none-way branching 744\u2013745, \n751, 755\npre\ufb01x-free code 827\npreorder traversal 834\nreading and writing 834\u2013835\nwildcard match 731\nTufte plot 456\nTukey ninther 306\nTuring , A. 910\nTuring machine 910\nChurch-Turing thesis 910\ncomputability 910\nnondeterministic 914\nuniversality 910\nType conversion 13\nType erasure 158\nType parameter 122, 134\nU\nUndecidability 97, 817\nUndirected graph\nacyclic 520\nadjacency-lists 524\nadjacency-matrix 524\nadjacency-sets 527\nadjacent vertex 519\narticulation point 562\nbiconnected 562\nbipartite 521 , 546\u2013547, 562\nbreadth-\ufb01rst search 538\u2013542\nbridge 562\ncenter 559\nconnected 519\nconnected component 519\nconnected to relation 519\nconnectivity 534 , 543\u2013546\ncycle 519\ncycle detection 546\u2013547\nde\ufb01ned 518\ndegree 519\ndense 520\ndepth-\ufb01rst search 530\u2013533\ndiameter 559\nedge 518\nedge-connected 562\nedge-weighted.\nSee Edge-weighted graph\nEuler tour 562\nforest 520\ngirth 559\nHamilton tour 562\ninterval graph 564\nisomorphism 561\nmultigraph 518\n INDEX952\nodd cycle detection 562\nparallel edge 518\npath 519\nradius 559\nself-loop 518\nsimple 518\nsimple cycle 519, 567\nsimple path 519\nsingle-source connectivity 556\nsingle-source paths 534\nsingle-source shortest \npaths ", "start": 963, "end": 964}, "1455": {"text": "518\npath 519\nradius 559\nself-loop 518\nsimple 518\nsimple cycle 519, 567\nsimple path 519\nsingle-source connectivity 556\nsingle-source paths 534\nsingle-source shortest \npaths 538\nspanning forest 520\nspanning tree 520\nsparse 520\nsubgraph 519\ntree 520\ntwo-colorability 546\u2013547, 562\nvertex 518\nweighted.\nSee Edge-weighted graph\nUnicode 696\nUniform hashing 463\nUnion-\ufb01nd 216\u2013241\nand depth-\ufb01rst search 546\nbinomial tree 237\nBoruvka\u2019s algorithm 636\ndynamic connectivity 216\nforest-of-trees 225\nKruskal's algorithm 625\nparent-link 225\npath compression 231, 237\nquick-\ufb01nd 222\u2013223\nquick-union 224\u2013227\nweighted quick-\ufb01nd 236\nweighted quick-union 227\u2013\n231\nweighted quick-union by \nheight 237\nweighted quick-union with \npath compression 237\nUniquely decodable code 826\nUnit testing 26\nUniversal data compression 816\nUniversality 910\nUpper bound 206, 207, 281\nV\nValue type parameter\nsymbol table 361\ntrie 730\nVar iable 10\nVar iable-length code 826\nVar iance 30\nVe c to r  d a t a  t y p e  1 0 6\nVer tex\nadjacent 519\nconnected to relation 519\ndegree of 519\neccentricity 559\nhead and tail 566\nindegree and outdegree 566\nreachable 567\nsource 528\nVer tex  cover  p ro b l e m  9 2 0\nVer tex  re l ", "start": 964, "end": 964}, "1456": {"text": "559\nhead and tail 566\nindegree and outdegree 566\nreachable 567\nsource 528\nVer tex  cover  p ro b l e m  9 2 0\nVer tex  re l a x a t i o n  6 4 8\nVirtual terminal 10\nVyssotsky\u2019s algorithm 633\nW\nWeb search 496\nWeig hted dig raph. See Edge-\nweighted digraph\nWeig hted edge 604, 638\nWeig hted external path  \nlength 832\nWeig hted g raph. See Edge-\nweighted graph\nWeig hted quick-union 227\u2013231\nWeig hted quick-union w ith path  \ncompression 237\nWeiner, P. 884\nWelch, T. 839\nwhile loop 15\nWhitelist \ufb01lter 8 , 48\u201349, 99, 491\nWide interface 160, 557\nWildcard character 791\nWildcard match 750\nWorst-case guarantee 197\nWrapper type 102, 122\nZ\nZev, J. 839\nZero-based indexing 53\nZipf\u2019s law 393\n This page intentionally left blank \n 954\nGraphs\n4.1 Depth-\ufb01rst search\n4.2 Breadth-\ufb01rst search\n4.3 Connected components\n4.4 Reachability\n4.5 Topolog ical sor t\n4.6 Strong componenets (Kosaraju)\n4.7 Minimum spanning tree (Prim)\n4.8 Minimum spanning tree (Kruskal)\n4.9 Shortest paths (Dijkstra)\n4.10 Shortest paths in DAGs\n4.11 Shortest paths (Bellman-Ford)\nStrings\n5.1 LSD string sort\n5.2 MSD string sort\n5.3 ", "start": 964, "end": 966}, "1457": {"text": "paths (Dijkstra)\n4.10 Shortest paths in DAGs\n4.11 Shortest paths (Bellman-Ford)\nStrings\n5.1 LSD string sort\n5.2 MSD string sort\n5.3 Three-way string quicksort\n5.4 Tr ie sy mbol table\n5.5 TST symbol table\n5.6 Substring search (Knuth-Morris-Pratt)\n5.7 Substring search (Boyer-Moore)\n5.8 Substring search (Rabin-Karp)\n5.9 Regular expression pattern matching\n5.10 Huffman compression/expansion\n5.11 LZW compression/expansion\nFundamentals\n1.1 Pushdown stack (resizing array)\n1.2 Pushdown stack (linked-list)\n1.3 FIFO queue\n1.4 Bag\n1.5 Union-\ufb01nd\nSorting\n2.1 Selection sort\n2.2 Insertion sort\n2.3 Shellsort\n2.4 Top-down mergesort\nBottom-up mergesort\n2.5 Quicksort\nQuicksort with 3-way partitioning\n2.6 Heap priority queue\n2.7 Heapsort\nSymbol Tables\n3.1 Sequential search\n3.2 Binary search\n3.3 Binary tree search\n3.4 Red-black BST search\n3.5 Hashing with separate chaining\n3.6 Hashing with linear probing\nALGORITHMS\n 955\nStrings\nRegular expression pattern matching\nHuffman compression\nLempel-Ziv-Welch compression\nContext\n \nColliding particle simulation\nB-tree set\nSuf\ufb01x array (elementary)\nLongest repeated substring\nKeyword in context\nMax\ufb02ow (Ford-Fulkerson)\nFundamentals\nWhitelisting\nExpression evaluation\nConnectivity\nSorting\nComparing two algorithms\nTop M\nMultiway merge\nSymbol Tables\nDedup\nFrequency count\nDictionary ", "start": 966, "end": 967}}